{"results": [{"createdAt": null, "postedAt": "2010-02-14T08:31:47.668Z", "modifiedAt": null, "url": null, "title": "Two Challenges", "slug": "two-challenges", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:32.122Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Daniel_Burfoot", "createdAt": "2009-03-12T02:28:50.970Z", "isAdmin": false, "displayName": "Daniel_Burfoot"}, "userId": "XhcXE3Qk5adX6v2Cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p55jWjjNXaiao9hXA/two-challenges", "pageUrlRelative": "/posts/p55jWjjNXaiao9hXA/two-challenges", "linkUrl": "https://www.lesswrong.com/posts/p55jWjjNXaiao9hXA/two-challenges", "postedAtFormatted": "Sunday, February 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20Challenges&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20Challenges%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp55jWjjNXaiao9hXA%2Ftwo-challenges%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20Challenges%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp55jWjjNXaiao9hXA%2Ftwo-challenges", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp55jWjjNXaiao9hXA%2Ftwo-challenges", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1500, "htmlBody": "<p>Followup To: <a href=\"/lw/1ov/play_for_a_cause/\">Play for a Cause</a>, <a href=\"/lw/1l5/singularity_institute_100k_challenge_grant_2009/\">Singularity Institute $100k Challenge Grant</a><br /><br />In the spirit of informal intellectual inquiry and friendly wagering, and with an eye toward raising a bit of money for SIAI, I offer the following two challenges to the LW community. <br /><br /><strong>Challenge #1 - Bayes' Nets Skeptics' Challenge </strong><br /><br />Many LWers seem to be strong believers in the family of modeling methods variously called Bayes' Nets, belief networks, or graphical models. These methods are the topic of two <a href=\"http://intelligence.org/reading/foundations\">SIAI-recommended books</a> by Judea Pearl: \"Probabilistic Reasoning in Intelligent Systems\" and \"Causality: Models, Reasoning and Inference\".<br /><br />The belief network paradigm has several attractive conceptual features. One feature is the ability of the networks to encode conditional independence relationships, which are intuitively natural and therefore attractive to humans. Often a na&iuml;ve investigation of the statistical relationship between variables will produce nonsensical conclusions, and the idea of conditional independence can sometimes be used to unravel the mystery. A good example would be a data set relating to traffic accidents, which shows that red cars are more likely to be involved in accidents. But it's nearly absurd to believe that red cars are intrinsically more dangerous. Rather, red cars are preferred by young men, who tend to be reckless drivers. So the color of a car is not independent of the likelihood of a collision, but it is <em>conditionally</em> independent given the age and sex of the person driving the car. This relationship could be expressed by the following belief network:</p>\n<p><a id=\"more\"></a></p>\n<p style=\"padding-left: 300px;\"><img src=\"http://www.lingoquiz.net/img/youngmanredcar.png\" alt=\"\" width=\"180\" height=\"150\" /></p>\n<p><br /><br />Where \"YM\" is true if the driver is a young man, \"R\" denotes whether the car is red, and \"A\" indicates an accident. The fact that there is no edge betwee \"R\" and \"A\" indicates that they are conditionally independent given the other nodes in the network, in this case \"YM\". <br /><br />A key property of the belief network scheme of constructing probability distributions is that they can be used to achieve a good balance between expressive power and model complexity. Consider the family of probability distributions over N variables that can take on K different values. Then the most na&iuml;ve model is just an N-dimensional array of numbers P(X1, X2, ... XN), requiring K^N parameters to specify. The number of parameters required for a belief network can be drastically lower, if many of the nodes are conditionally independent of one another. For example, if all but one node in the graph has exactly one parent, then the number of parameters required is basically NK^2 (N conditional distributions of the form P(Xchild|Xparent)). This is clearly a drastic improvement for reasonable values of K and N. Even though the model is much less complex, every variable is still related to every other variable - knowing the value of any Xi will change the probability of any Xj, unless some other intervening node is also known. <br /><br />Another attractive feature of the belief network paradigm is the existence of a fast inference algorithm for updating the probability distributions of some unobserved variables in response to evidence. For example, given that a patient has the feature \"smoker=T\" and \"chest pain=T\", the inference algorithm can rapidly calculate the probability distribution of the unobserved variable \"heart disease\". Unfortunately, there is a big catch - this inference algorithm works only for belief networks that can be expressed as acyclic graphs. If the graph is not acyclic, the computational cost of the inference algorithm is much larger (IIRC, it is exponential in the size of the largest clique in the graph). <br /><br />In spite of these benefits, belief networks have some serious drawbacks as well. One major flaw is the difficulty of <em>learning </em>networks from data. Here the goal is to obtain a belief network specifying a distribution P(x) that assigns a very high likelihood to an empirical data set X={X1,X2...XM}, where now each X is an N-dimensional vector and there are M samples. The difficulty of learning belief networks stems from the fact that the number of graphs representing relationships between N variables is exponential in N.<br /><br />There is a second, more subtle flaw in the belief network paradigm. A belief network is defined based on a graph which has one node for each data variable. This is fine, if you know what the correct variables are. But very often the correct variables are unknown; indeed, finding the correct variables is probably the key problem in learning. Once you know that the critical variables are mass, force, and acceleration, it is almost trivial to determine the relation between them. As an example, consider a system composed of three variables A,B,C and X. The unknown relationship between these variables is X~F(A+B+C), where F is some complex distribution that depends on a single parameter. Now, a na&iuml;ve representation will yield a belief network that looks like the following:</p>\n<p style=\"padding-left: 300px;\"><img src=\"http://www.lingoquiz.net/img/grapha.png\" alt=\"\" width=\"200\" height=\"150\" /></p>\n<p>If we reencode the variables as A'=A, B'=A'+B, C'=B'+C, then we get the following graph:</p>\n<p>&nbsp;</p>\n<p style=\"padding-left: 240px;\"><img src=\"http://www.lingoquiz.net/img/graphb.png\" alt=\"\" width=\"300\" height=\"80\" /></p>\n<p>&nbsp;</p>\n<p>If you count the number of links this might not seem like much of an improvement. But the number of links is not the important factor; the key factor is the required number of entries in the conditional probability tables. In the first graph, the CPT for X requires O(K^4) entries, where K is again the number of values A, B, and C can take on. But in the second graph the number of entries is O(K^2). So clearly the second graph should be preferred, but the basic belief network learning algorithms provide no way of obtaining it. <br /><br />On the basis of these remarks I submit the following qualified statement: while the belief network paradigm is mathematically elegant and intuitively appealing, it is NOT very useful for describing real data. <br /><br />The challenge is to prove the above claim wrong. This can be done as follows. First, find a real data set (see below for definition of the word \"real\"). Construct a belief network model of the data, in any way you choose. Then post a link to the data set, and I will then attempt to model it using alternative methods of my own choosing (probably <a href=\"http://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution\">Maximum Entropy</a> or a variant thereof). We will then compare the likelihoods achieved by the two methods; higher likelihood wins. If there is ambiguity concerning the validity of the result, then we will <em>compress</em> the data set using compression algorithms based on the models and compare compression rates.&nbsp; Constructing a compressor from a statistical model is essentially a technical exercise; I can provide a Java implementation of <a href=\"http://en.wikipedia.org/wiki/Arithmetic_coding\">arithmetic encoding</a>. The compression rates must take into account the size of the compressor itself.<br /><br />The challenge hinges on the meaning of the word \"real data\". Obviously it is trivial to construct a data set for which a belief network is the best possible model, simply by building a network and then sampling from it. So my requirement is that the data set be non-synthetic. Other than that, there are no limitations - it can be image data, text, speech, machine learning sets, NetFlix, social science databases, etc. <br /><br />To make things interesting, the loser of the challenge will donate $100 (more?) to SIAI. Hopefully we can agree on the challenge (but not necessarily resolve it) before the Feb. 28th deadline for matching donations. In principle I will accept challenges until I lose so many that my wallet starts to hurt. <br /><br /><strong>Challenge #2: Compress the GSS </strong><br /><br />The General Social Survey is a widely used data set in the social sciences. Most analyses based on the GSS tend to use standard statistical tools such as correlation, analysis of variance, and so on. These kinds of analysis run into the usual problems associated with statistics - how do you choose a prior, how do you avoid overfitting, and so on.<br /><br />I propose a new way of analyzing the data in the GSS - a compression-based challenge as outlined above. To participate in the challenge, you build a model of the data contained in the GSS using whatever methods appeal to you. Then you connect the model to an encoding method and compress the dataset. Whoever achieves the best compression rate, taking into account the size of the compressor itself, is the winner. <br /><br />The GSS contains data about a great variety of economic, cultural, psychological, and educational factors. If you are a social scientist with a theory of how these factors relate, you can prove your theory by transforming it into a statistical model and then into a compression program, and demonstrating better compression results than rival theories. <br /><br />If people are interested, I propose the following scheme. There is a $100 entry fee, of which half will go to SIAI. The other half goes to the winner. Again, hopefully we can agree on the challenge before the Feb. 28th deadline.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p55jWjjNXaiao9hXA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 5.612944456468204e-07, "legacy": true, "legacyId": "2295", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Gt2jBeo36288HdXqH", "4TsSb8N8BBwtNZ2v9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-15T07:58:48.585Z", "modifiedAt": null, "url": null, "title": "You're Entitled to Arguments, But Not (That Particular) Proof", "slug": "you-re-entitled-to-arguments-but-not-that-particular-proof", "viewCount": null, "lastCommentedAt": "2022-05-09T21:18:06.283Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vqbieD9PHG8RRJddu/you-re-entitled-to-arguments-but-not-that-particular-proof", "pageUrlRelative": "/posts/vqbieD9PHG8RRJddu/you-re-entitled-to-arguments-but-not-that-particular-proof", "linkUrl": "https://www.lesswrong.com/posts/vqbieD9PHG8RRJddu/you-re-entitled-to-arguments-but-not-that-particular-proof", "postedAtFormatted": "Monday, February 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You're%20Entitled%20to%20Arguments%2C%20But%20Not%20(That%20Particular)%20Proof&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou're%20Entitled%20to%20Arguments%2C%20But%20Not%20(That%20Particular)%20Proof%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvqbieD9PHG8RRJddu%2Fyou-re-entitled-to-arguments-but-not-that-particular-proof%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You're%20Entitled%20to%20Arguments%2C%20But%20Not%20(That%20Particular)%20Proof%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvqbieD9PHG8RRJddu%2Fyou-re-entitled-to-arguments-but-not-that-particular-proof", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvqbieD9PHG8RRJddu%2Fyou-re-entitled-to-arguments-but-not-that-particular-proof", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2527, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/1p1/logical_rudeness/\">Logical Rudeness</a></p>\n<blockquote>\n<p style=\"padding-left: 30px;\">\"Modern man is so committed to empirical knowledge, that he sets the standard for evidence higher than either side in his disputes can attain, thus suffering his disputes to be settled by philosophical arguments as to which party must be crushed under the burden of proof.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- <a href=\"http://www.overcomingbias.com/2008/09/beware-high-sta.html#comment-396465\">Alan Crowe</a></p>\n</blockquote>\n<p>There's a story - in accordance with <a href=\"http://rationalwiki.com/wiki/Poe%27s_Law\">Poe's Law</a>, I have no idea whether it's a joke or it actually happened - about a creationist who was trying to claim a \"gap\" in the fossil record, two species without an intermediate fossil having been discovered.&nbsp; When an intermediate species <em>was</em> discovered, the creationist responded, \"Aha!&nbsp; Now there are <em>two</em> gaps.\"</p>\n<p>Since I'm not a professional evolutionary biologist, I couldn't begin to rattle off all the ways that we know evolution is true; true facts tend to <a href=\"/lw/uw/entangled_truths_contagious_lies/\">leave traces</a> of themselves behind, and <a href=\"http://wiki.lesswrong.com/wiki/Evolution\">evolution</a> is the hugest fact in all of biology.&nbsp; My specialty is the cognitive sciences, so I can tell you <a href=\"/lw/la/truly_part_of_you/\">of my own knowledge</a> that the human brain looks just like we'd expect it to look if it had evolved, and not at all like you'd think it would look if it'd been intelligently designed.&nbsp; And I'm not really going to say much more on that subject.&nbsp; As I once said to someone who questioned whether humans were really related to apes:&nbsp; \"That question might have made sense when Darwin first came up with the hypothesis, but this is the twenty-first century.&nbsp; We can <em>read the genes.</em>&nbsp; Human beings and chimpanzees have <em>95% shared genetic material.</em>&nbsp; It's <em>over.</em>\"</p>\n<p>Well, it's over, unless you're crazy like a human (ironically, more evidence that the human brain was fashioned by <a href=\"/lw/kr/an_alien_god/\">a sloppy and alien god</a>).&nbsp; If you're crazy like a human, you will engage in <a href=\"http://wiki.lesswrong.com/wiki/Motivated_cognition\">motivated cognition</a>; and instead of focusing on the unthinkably huge heaps of evidence in favor of evolution, the innumerable signs by which the fact of evolution has left its heavy footprints on all of reality, the uncounted observations that discriminate between the world we'd expect to see if intelligent design ruled and the world we'd expect to see if evolution were true...</p>\n<p>...instead you search your mind, and you pick out one form of proof that you think evolutionary biologists <em>can't</em> provide; and you demand, you insist upon that one form of proof; and when it is not provided, you take that as a refutation.</p>\n<p>You say, \"Have you ever <em>seen </em>an ape species evolving into a human species?\"&nbsp; You insist on videotapes - on that <em>particular</em> proof.</p>\n<p>And that <em>particular proof</em> is one we couldn't possibly be expected to have on hand; it's a form of evidence we couldn't possibly be expected to be able to provide, <em>even given that evolution is true.</em></p>\n<p>Yet it follows illogically that if a video tape would provide definite proof, then, likewise, the absence of a videotape must constitute definite disproof.&nbsp; Or perhaps just render all other arguments void and turn the issue into a mere matter of personal opinion, with no one's opinion being better than anyone else's.<em><a id=\"more\"></a></em></p>\n<p>So far as I can tell, the position of human-caused global warming (anthropogenic global warming aka AGW) has the ball.&nbsp; I get the impression there's a lot of evidence piled up, a lot of people trying and failing to poke holes, and so I have no reason to play contrarian here.&nbsp; It's now heavily politicized science, which means that I take the assertions with a grain of skepticism and worry - well, to be honest I <em>don't </em>spend a whole lot of time worrying about it, because (a) there are worse global catastrophic risks and (b) lots of other people are worrying about AGW already, so there are much better places to invest the next marginal minute of worry.</p>\n<p>But if I pretend for a moment to live in the mainstream mental universe in which there is <a href=\"http://www.nickbostrom.com/existential/risks.html\">nothing scarier to worry about</a> than global warming, and a 6 <span style=\"white-space: nowrap;\">&deg;</span>C (11 <span style=\"white-space: nowrap;\">&deg;F) </span>rise in global temperatures by 2100 seems like a top issue for the care and feeding of humanity's future...</p>\n<p>Then I must shake a disapproving finger at anyone who claims the state of evidence on AGW is indefinite.</p>\n<p>Sure, if we waited until 2100 to see how much global temperatures increased and how high the seas rose, we would have definite proof.&nbsp; We would have definite proof in 2100, however, and that sounds just a little bit way the hell too late.&nbsp; If there are cost-effective things we can do to mitigate global warming - and by this I don't mean ethanol-from-corn or cap-and-trade, more along the lines of standardizing on a liquid fluoride thorium reactor design and building 10,000 of them - if there's something we can do about AGW, we need to do it <em>now,</em> not in a hundred years.</p>\n<p>When the hypothesis at hand makes <em>time valuable </em>- when the proposition at hand, conditional on its being true, means there are certain things we should be doing NOW - then you've got to do your best to figure things out with the evidence that we have.&nbsp; Sure, if we had annual data on global temperatures and CO2 going back to 100 million years ago, we would know more than we do right now.&nbsp; But we don't have that time-series data - not because global-warming advocates destroyed it, or because they were neglectful in gathering it, but because they couldn't possibly be expected to provide it in the first place.&nbsp; And so we've got to look among the observations we <em>can</em> perform, to find those that discriminate between \"the way the world could be expected to look if AGW is true / a big problem\", and \"the way the world would be expected to look if AGW is false / a small problem\".&nbsp; If, for example, we discover large deposits of <a href=\"http://en.wikipedia.org/wiki/Arctic_methane_release\">frozen methane clathrates</a> that are released with rising temperatures, this at least <em>seems</em> like \"the sort of observation\" we might be making if we live in the sort of world where AGW is a big problem.&nbsp; It's not a necessary connection, it's not sufficient on its own, it's something we <em>could </em>potentially also observe in a world where AGW is <em>not</em> a big problem - but unlike the perfect data we can never obtain, it's something we can actually find out, and in fact <em>have</em> found out.</p>\n<p>Yes, we've never actually experimented to observe the results over 50 years of artificially adding a large amount of carbon dioxide to the atmosphere.&nbsp; But we know from physics that it's a greenhouse gas.&nbsp; It's not a <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">privileged hypothesis</a> we're pulling out of nowhere.&nbsp; It's not like saying \"You can't <em>prove </em>there's no invisible pink unicorn in my garage!\"&nbsp; AGW is, <em>ceteris paribus,</em> what we should expect to happen if the other things we believe are true.&nbsp; We don't have any experimental results on what will happen 50 years from now, and so you can't grant the proposition the special, super-strong status of something that has been <a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">scientifically confirmed</a> by a replicable experiment.&nbsp; But as I point out in \"<a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">Scientific Evidence, Legal Evidence, Rational Evidence</a>\", if science couldn't say anything about that which has not already been observed, we couldn't ever make scientific <em>predictions </em>by which the theories could be <em>confirmed.</em>&nbsp; Extrapolating from the science we <em>do </em>know, global warming <em>should</em> be occurring; you would need specific experimental evidence to <em>contradict</em> that.</p>\n<p>We are, I think, dealing with that old problem of motivated cognition.&nbsp; As Gilovich says:&nbsp; \"Conclusions a person does not want to believe are held to a higher standard than conclusions a person wants to believe.&nbsp; In the former case, the person asks if the evidence <em>compels </em>one to accept the conclusion, whereas in the latter case, the person asks instead if the evidence <em>allows </em>one to accept the conclusion.\"&nbsp; People <a href=\"/lw/mn/absolute_authority/\">map the domain of belief onto the social domain of authority</a>, with a qualitative difference between absolute and nonabsolute demands:&nbsp; If a teacher tells you certain things, and you have to believe them, and you have to recite them back on the test.&nbsp; But when a student makes a suggestion in class, you don't have to go along with it - you're free to agree or disagree (it seems) and no one will punish you.</p>\n<p>And so the implicit emotional theory is that if something is not <em>proven</em> - better yet, proven using a <em>particular </em>piece of evidence that isn't available and that you're pretty sure is never going to become available - then you are allowed to disbelieve; it's like something a student says, not like something a teacher says.</p>\n<p>You demand particular proof P; and if proof P is not available, then you're <em>allowed to disbelieve.</em></p>\n<p>And this is flatly wrong as probability theory.</p>\n<p>If the hypothesis at hand is H, and we have access to pieces of evidence E1, E2, and E3, but we do <em>not</em> have access to proof X one way or the other, then the rational probability estimate is the result of the <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27s_Theorem\">Bayesian</a> <a href=\"http://wiki.lesswrong.com/wiki/Updating\">update</a> P(H|E1,E2,E3).&nbsp; You do not get to say, \"Well, we don't know whether X or ~X, so I'm going to throw E1, E2, and E3 out the window until you tell me about X.\"&nbsp; I cannot begin to describe how much that is not the way the laws of probability theory work.&nbsp; You do not get to <a href=\"/lw/lx/argument_screens_off_authority/\">screen off</a> E1, E2, and E3 based on your <em>ignorance</em> of X!</p>\n<p>Nor do you get to ignore the arguments that influence the prior probability of H - the standard science by which, <em>ceteris paribus </em>and without anything unknown at work, carbon dioxide is a greenhouse gas and <em>ought</em> to make the Earth hotter.</p>\n<p>Nor can you hold up the nonobservation of your particular proof X as a triumphant refutation.&nbsp; If we <em>had</em> time cameras and could look into the past, then indeed, the fact that no one had ever \"seen with their own eyes\" primates evolving into humans would refute the hypothesis.&nbsp; But, <em>given</em> that time cameras don't exist, then <em>assuming evolution to be true</em> we don't expect anyone to have witnessed humans evolving from apes with our own eyes, for the laws of natural selection <em>require</em> that this have happened far in the distant past.&nbsp; And so, once you have updated on the fact that time cameras don't exist - computed P(<em>Evolution</em>|<em>~Camera</em>) - and the fact that time cameras don't exist hardly seems to refute the theory of evolution - then you obtain no further evidence by observing <em>~Video,</em> i.e., P(<em>Evolution</em>|<em>~Video</em>,<em>~Camera</em>) = P(<em>Evolution</em>|<em>~Camera</em>).&nbsp; In slogan-form, \"The absence of unobtainable proof is not even weak evidence of absence.\"&nbsp; See appendix for details.</p>\n<p>(And while we're on the subject, yes, the laws of probability theory are laws, rather than suggestions.&nbsp; It is like something the teacher tells you, okay?&nbsp; If you're going to ignore the Bayesian update you logically have to perform when you see a new piece of evidence, you might as well ignore outright mathematical proofs.&nbsp; I see no reason why it's any less epistemically sinful to ignore probabilities than to ignore certainties.)</p>\n<p>Throwing E1, E2 and E3 out the window, and ignoring the prior probability of H, because you haven't seen unobtainable proof x; or holding up the nonobservation of X as a triumphant refutation, when you couldn't reasonably expect to see X even given that the underlying theory is true; all this is more than just a formal probability-theoretic mistake.&nbsp; It is <em><a href=\"/lw/1p1/logical_rudeness/\">logically rude</a>.</em></p>\n<p>After all - in the absence of your unobtainable particular proof, there may be plenty of other arguments by which you can hope to figure out whether you live in a world where the hypothesis of interest is true, or alternatively false.&nbsp; <em>It takes work to provide you with those arguments.</em>&nbsp; It takes work to provide you with extrapolations of existing knowledge to prior probabilities, and items of evidence with which to update those prior probabilities, to form a prediction about the unseen.&nbsp; <em>Someone who does the work to provide those arguments is doing the best they can by you; throwing the arguments out the window is not just irrational, but logically rude.</em></p>\n<p>And I emphasize this, because it seems to me that the underlying metaphor of demanding particular proof is to say as if, \"You are supposed to provide me with a video of apes evolving into humans, I am entitled to see it with my own eyes, and it is your responsibility to make that happen; and if you do not provide me with that particular proof, you are deficient in your duties of argument, and I have no obligation to believe you.\"&nbsp; And this is, in the first place, bad math as probability theory.&nbsp; And it is, in the second place, an attitude of trying to be <em>defensible</em> rather than <em>accurate,</em> the attitude of someone who wants to be allowed to retain the beliefs they have, and not the attitude of someone who is <a href=\"/lw/jz/the_meditation_on_curiosity/\">honestly curious</a> and trying to figure out which possible world they live in, by whatever signs <em>are </em>available.&nbsp; But if these considerations do not move you, then even in terms of the original and flawed metaphor, you are in the wrong: you are <em>entitled to arguments, but not that particular proof.</em></p>\n<p>Ignoring someone's hard work to <em>provide</em> you with the arguments you need - the extrapolations from existing knowledge to make predictions about events not yet observed, the items of evidence that are suggestive even if not definite and that fit some possible worlds better than others - and instead demanding proof they can't possibly give you, proof they couldn't be expected to provide <em>even if they were right</em> - <em>that </em>is logically rude.&nbsp; It is invalid as probability theory, foolish on the face of it, and logically rude.</p>\n<p>And of course if you go so far as to <em>act smug </em>about the absence of an unobtainable proof, or chide the other for their credulity, then you have crossed the line into outright ordinary rudeness as well.</p>\n<p>It is likewise a madness of decision theory to hold off pending positive proof until it's too late to do anything; the whole point of decision theory is to choose under conditions of uncertainty, and that is not how the expected value of information is likely to work out.&nbsp; Or in terms of plain common sense:&nbsp; There are signs and portents, smoke alarms and hot doorknobs, by which you can hope to determine whether your house is on fire <em>before</em> your face melts off your skull; and to delay leaving the house until <em>after</em> your face melts off, because only this is the positive and particular proof that you demand, is decision-theoretical insanity.&nbsp; It doesn't matter if you cloak your demand for that unobtainable proof under the heading of scientific procedure, saying, \"These are the proofs you could not obtain even if you were right, which I know you will not be able to obtain until the time for action has long passed, which surely any scientist would demand before confirming your proposition as a <a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">scientific truth</a>.\"&nbsp; It's still nuts.</p>\n<hr />\n<p>&nbsp;</p>\n<p><em>Since this post has already gotten long, I've moved some details of probability theory, the subtext on cryonics, the sub-subtext on molecular nanotechnology, and the sub-sub-subtext on Artificial Intelligence, into:</em></p>\n<p><strong><a href=\"/lw/1rv/demands_for_particular_proof_appendices/\">Demands for Particular Proof:&nbsp; Appendices</a></strong>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LDTSbmXtokYAsEq8e": 1, "nZCb9BSnmXZXSNA2u": 1, "NSMKfa8emSbGNXRKD": 1, "frcrRgCk9PDbEScua": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vqbieD9PHG8RRJddu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 73, "extendedScore": null, "score": 0.000124, "legacy": true, "legacyId": "2213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 73, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 229, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["srge9MCLHSiwzaX6r", "wyyfFfaRar2jEdeQK", "fg9fXrHpeaDD6pEPL", "pLRogvJLPPg6Mrvg4", "fhojYBGGiYAFcryHZ", "PmQkensvTGg7nGtJE", "5yFRd3cjLpm3Nd6Di", "3nZMgRTfFEfHp34Gb", "FwMhhzt8RSLAWNFAB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 17, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-15T07:58:57.574Z", "modifiedAt": null, "url": null, "title": "Demands for Particular Proof: Appendices", "slug": "demands-for-particular-proof-appendices", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:57.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FwMhhzt8RSLAWNFAB/demands-for-particular-proof-appendices", "pageUrlRelative": "/posts/FwMhhzt8RSLAWNFAB/demands-for-particular-proof-appendices", "linkUrl": "https://www.lesswrong.com/posts/FwMhhzt8RSLAWNFAB/demands-for-particular-proof-appendices", "postedAtFormatted": "Monday, February 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Demands%20for%20Particular%20Proof%3A%20Appendices&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADemands%20for%20Particular%20Proof%3A%20Appendices%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwMhhzt8RSLAWNFAB%2Fdemands-for-particular-proof-appendices%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Demands%20for%20Particular%20Proof%3A%20Appendices%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwMhhzt8RSLAWNFAB%2Fdemands-for-particular-proof-appendices", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwMhhzt8RSLAWNFAB%2Fdemands-for-particular-proof-appendices", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2100, "htmlBody": "<p><strong>Appendices to</strong>:&nbsp; <a href=\"/lw/1ph/youre_entitled_to_arguments_but_not_that/\">You're Entitled to Arguments, But Not (That Particular) Proof</a></p>\n<p>(The main article was getting long, so I decided to move the appendices to a separate article which wouldn't be promoted, thus minimizing the size of the article landing in a promoted-article-only-reader's feed.)</p>\n<p>A.&nbsp; <em>The absence of unobtainable proof is not even weak evidence of absence.</em></p>\n<p>The wise will already know that <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">absence of evidence actually <em>is</em> evidence of absence</a>; and they may ask, \"Since a time-lapse video record of apes evolving into humans would, in fact, be strong evidence in favor of the theory of evolution, is it not <em>mandated by the laws of probability theory</em> that the absence of this videotape constitute <em>some</em> degree of evidence against the theory of evolution?\"</p>\n<p>(Before you reject that proposition out of hand for containing the substring \"evidence against the theory of evolution\", bear in mind that grownups understand that <a href=\"/lw/ij/update_yourself_incrementally/\">evidence accumulates</a>.&nbsp; You don't get to pick out just one piece of evidence and ignore all the rest; true hypotheses can easily generate a <em>minority</em> of <em>weak</em> pieces of evidence against themselves; conceding one point of evidence does not mean conceding the debate; and people who try to act as if it does are nitwits.&nbsp; Also there are probably no creationists reading this blog.)</p>\n<p>The laws of probability theory do <a href=\"http://wiki.lesswrong.com/wiki/Conservation_of_expected_evidence\">mandate</a> that if P(H|E) &gt; P(H), then P(H|~E) &lt; P(H).&nbsp; So - even if absence of proof is by no means proof of absence, and even if we reject the philosophy that absence of a particular proof means you get to discard all the other arguments about evidence and priors - must we not at least concede that absence of proof is necessarily <em>evidence </em>of absence, even though it may be very weak evidence?<a id=\"more\"></a></p>\n<p>Actually, in cases like creationism, not even that much follows.&nbsp; Suppose we had a time camera - a device that lets us look into the distant past and see historical events with our own eyes.&nbsp; Let the proposition \"we have a time camera\" be labeled <em>Camera</em>.&nbsp; Then we would either be able to videotape the evolution of humans from apes; let the presence of this video record be labeled <em>Video</em>, and its absence <em>~Video</em>.&nbsp; Let <em>Evolution</em> stand for the hypothesis that evolution is true.&nbsp; And let <em>True</em> and <em>False</em> stand for the epistemic states \"Pretty much likely to be true\" and \"Pretty much likely to be false\", respectively.</p>\n<p>Then, given that evolution is true <em>and that we have a time camera,</em> we should expect to see <em>Video</em>:</p>\n<p>P(<em>Video</em>|<em>Evolution</em>,<em>Camera</em>) = <em>True</em>&nbsp;&nbsp; and&nbsp;&nbsp; P(<em>Video</em>|~<em>Evolution</em>,<em>Camera</em>) = <em>False</em></p>\n<p>So <em>if we had a time camera,</em> if we could look into the past, then \"no one has seen apes evolving into humans\" would be strong evidence against the theory of natural selection:</p>\n<p>P(<em>Evolution</em>|<em>~Video</em>,<em>Camera</em>) = <em>False</em></p>\n<p>But if we <em>don't</em> have a time camera, then <em>regardless </em>of whether evolution is true or false, we can't expect to have \"seen apes evolving into humans\":</p>\n<p>P(<em>Video</em>|<em>Evolution</em>,<em>~Camera</em>) = <em>False&nbsp;&nbsp; </em>and&nbsp;&nbsp; P(<em>Video</em>|<em>~Evolution</em>,<em>~Camera</em>) = <em>False</em></p>\n<p>From which it follows that <em>once you know ~Camera,</em> observing <em>~Video</em> tells you nothing further about <em>Evolution:</em></p>\n<p>P(<em>Evolution</em>|<em>~Video</em>,<em>~Camera</em>) = P(<em>Evolution</em>|<em>~Camera</em>)</p>\n<p>If you didn't <em>know</em> whether or not we had time cameras, and I told you <em>only</em> that no one had ever seen apes evolving into humans, then you would have to evaluate P(<em>Evolution</em>|<em>~Video</em>) which includes some contributions from both P(<em>Evolution</em>|<em>~Video</em>,<em>Camera</em>) and P(<em>Evolution</em>|<em>~Video</em>,<em>~Camera</em>).&nbsp; You don't know whether the video is missing because evolution is false, or the video is missing because we don't have a time camera.&nbsp; And so, as the laws of probability require, P(<em>Evolution</em>|<em>~Video</em>) &lt; P(<em>Evolution</em>) just as P(<em>Evolution</em>|<em>Video</em>) &gt; P(<em>Evolution</em>).&nbsp; But <em>once you know you don't have a time camera,</em> you can start by evaluating P(<em>Evolution</em>|<em>~Camera</em>) - and it's hard to see why lack of time cameras should, in and of itself, be evidence against evolution.&nbsp; The process of natural selection hardly requires a time camera.&nbsp; So P(<em>Evolution</em>|<em>~Camera</em>) = P(<em>Evolution</em>), and then&nbsp;<em>~Video</em> isn't evidence one way or another.&nbsp; The observation&nbsp;<em>~Camera</em> screens off any evidence from observing <em>~Video</em>.</p>\n<p>And this is only what should be expected: once you know you don't have a time camera, and once you've updated your views on evolution in light of the fact that time cameras don't exist to begin with (which doesn't seem to have much of an impact on the matter of evolution), it makes no further difference when you learn that no one has ever witnessed apes evolving into humans.</p>\n<hr />\n<p>&nbsp;</p>\n<p>B.&nbsp; <em>Subtext on <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a>:&nbsp; Demanding that cryonicists produce a successful revival before you'll credit the possibility of cryonics, is logically rude; specifically, it is a demand for particular proof.</em></p>\n<p>A successful cryonics revival performed with <em>modern-day technology</em> is not a piece of evidence you could possibly expect modern cryonicists to provide, <em>even given that the proposition of interest is true.</em>&nbsp; The whole point of cryonics is as an ambulance ride to the future; to take advantage of the <em>asymmetry</em> between the technology needed to successfully preserve a patient (cryoprotectants, liquid nitrogen storage) and the technology needed to revive a patient (probably molecular nanotechnology).</p>\n<p>In particular, the screening-off condition (playing the role of <em>~Camera </em>in the example above) is the observation that we presently lack molecular nanotechnology.&nbsp; Given that you don't currently have molecular nanotechnology, you can't reasonably expect to revive a cryonics patient today <em>even given</em> that they <em>could in fact </em>be revived using future molecular nanotechnology.</p>\n<p>You are entitled to arguments, though not that particular proof, and cryonicists have done their best to provide you with whatever evidence <em>can</em> be obtained.&nbsp; <a href=\"http://www.cryonics.org/reports/Scientific_Justification.pdf\">For example</a>:</p>\n<blockquote>A study on rat hippocampal slices showed that it is possible for vitrified slices cooled to a solid state at -130&ordm;C to have viability upon re-warming comparable to that of control slices that had not been vitrified or cryopreserved. Ultrastructure of the CA1 region (the region of the brain most vulnerable to ischemic damage) of the re-warmed slices is seen to be quite well preserved compared to the ultrastructure of control CA1 tissue (24). Cryonics organizations perfuse brains with vitrification solution until saturation is achieved...</blockquote>\n<blockquote>A rabbit kidney has been vitrified, cooled to -135&ordm;C, re-warmed and transplanted into a rabbit. The formerly vitrified transplant functioned well enough as the sole kidney to keep the rabbit alive indefinitely (25)... The vitrification mixture used in preserving the rabbit kidney is known as M22. M22 is used by the cryonics organization Alcor for vitrifying cryonics subjects. Perfusion of rabbits with M22 has been shown to preserve brain ultrastructure without ice formation (26).<br /></blockquote>\n<p>This is the sort of evidence we <em>can</em> reasonably expect to obtain today, and it took <em>work</em> to provide you with that evidence.&nbsp; Ignoring it in favor of demanding proof that you couldn't expect to see even if cryonicists were right, is (a) invalid as probability theory, (b) a sign of trying to defend an allowed belief rather than being honestly curious about which possible world we live in, and (c) logically rude.</p>\n<p>Formally:</p>\n<ol>\n<li><em>Even given </em>that the proposition put forth by cryonicists is true - that people suspended with modern-day technology will be revivable by future technology - you cannot expect them to revive a cryonics patient using modern-day technology.</li>\n<li>Cryonicists have put forth considerable effort, requiring years of work by many people, to provide you with such evidence as <em>can</em> be obtained today.&nbsp; The lack of that particular proof is not owing to any defect of diligence on the part of cryonicists, or disinterest in <em>their</em> part on doing the research.</li>\n<li>The prediction that a properly cryoprotected patient does not suffer information-theoretical death is not a <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">privileged hypothesis</a> pulled out of nowhere; it is the <em>default extrapolation</em> from modern neuroscience.&nbsp; If we learn that a patient cryoprotected using current technologies has undergone erasure of critical brain information, we have learned something that is not in current neuroscience textbooks - and actually rather surprising, all things considered.&nbsp; The <em>straight-line</em> extrapolation from the science we <em>do</em> know is that if you can see the neurons nicely preserved under a microscope, the information sure ought to be there.&nbsp; (The idea that critical brain information is stored dynamically in spiking patterns has already been contraindicated by the evidence; dogs taken to very low (above-freezing) temperatures, sufficient to suppress brain activity, do not seem to suffer any memory loss or personality change.)</li>\n<li><em>Given that the proposition of interest is true,</em> there is something drastically urgent we ought to be doing <em>RIGHT NOW</em>, namely cryopreserving as many as possible of the 150,000 humans per day who undergo mind-state annihilation.&nbsp; (Economies of scale would very likely drive down costs by an order of magnitude or more; this is an entirely feasible goal economically and technologically, the only question is the political will.)</li>\n</ol>\n<p>Given these points: to discard the straight-line extrapolation from modern science and all the hard work that cryonicists have done to provide further distinguishing evidence, in favor of a demand for particular proof that you know cannot possibly be obtained and which you couldn't expect to see even given that the underlying proposition is true, when there are things we ought to be doing <em>NOW</em> given the truth of the proposition and much value will be lost by waiting; all this is indefensible as decision theory in a formal sense, and is, in an informal sense, madness.</p>\n<p>Which all goes to say only what Xiaoguang \"Mike\" Li observed to me some time ago:&nbsp; That saying you'll only sign up for cryonics when someone demonstrates a successful revival of a cryonics patient is sort of like saying that you won't get on the airplane until after it arrives at the destination.&nbsp; Only a very small amount of common sense is necessary to see this, and the objection really does demonstrate the degree to which, when most people feel an innate flinch away from an idea, they hardly feel obligated to come up with objections that make the slightest bit of sense.</p>\n<p>This beautiful <a href=\"http://wimp.com/seatbelt/\">public service announcement</a>, with only a slight change of metaphor, could serve as a PSA for cryonics.&nbsp; Stop making a big deal out of the decision.&nbsp; It's not that complicated.</p>\n<hr />\n<p>&nbsp;</p>\n<p>C.&nbsp; <em>Demanding the demonstration of a working nanomachine before you'll credit the possibility of molecular nanotechnology is logically rude, specifically, a demand for particular proof.</em></p>\n<p>Given humanity's current level of technology, you can't reasonably expect a demonstration of molecular nanotechnology <em>right now,</em> <em>even given </em>that the proposition of interest is true: that molecular nanotechnology is physically possible to operate, physically possible to manufacture, and likely to be developed within some number of decades.&nbsp; Even if we live in that world, you can't necessarily expect to see nanotechnology <em>now</em>.&nbsp; And yet nonetheless the advocates of nanotechnology have gone to whatever extent possible to provide the arguments by which you could, <em>today,</em> figure out whether or not you live in a world where molecular nanotechnology is possible.&nbsp; Eric Drexler put forth around six years of hard work to produce <em>Nanosystems,</em> doing as much of the basic physics as one man working more or less alone could be expected to do; and since then Robert Freitas, in particular, has been designing and simulating molecular devices, and even trying to work out simple synthesis paths, which is about as much as one person could do, and the funding hasn't really been provided for more than that.&nbsp; To ignore all this hard work that has been put into providing you with such observations and arguments as <em>can</em> be reasonably obtained, and throw them out the window because of a demand for particular proof that you think they can't obtain and that they wouldn't be able to obtain <em>even if the proposition at hand is true</em> - this is not just invalid as probability theory, not just defensiveness rather than curiosity, it is <em>logically rude.</em></p>\n<p>Although actually, of course, you <em>can</em> see tiny molecularly-precise machines on parade, including freely rotating gears and general assemblers - just look inside a biological cell, particularly at ATP synthase and the ribosome.&nbsp; But by that power of cognitive chaos which generates <em>the demand for unobtainable proof</em> in the first place, there can be little doubt that, as soon as this overlooked demonstration is pointed out, the one will immediately find some clause by which to exclude it.&nbsp; To actually <em>provide</em> the unobtainable proof would hardly be fair, after all.</p>\n<p><em> \n<hr />\n<br /></em></p>\n<p>D.&nbsp; <em>It is invalid as probability theory, suboptimal as decision theory, and generally insane, to insist that you want to see someone develop a full-blown Artificial General Intelligence before you'll credit that it's worth anyone's time to work on problems of Friendly AI.<br /></em></p>\n<p>Not precisely analogous to the above cases, but it <em>is</em> a demand for particular proof.&nbsp; Delineating the specifics is left as an exercise to the reader.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1, "LDTSbmXtokYAsEq8e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FwMhhzt8RSLAWNFAB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 34, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "2299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vqbieD9PHG8RRJddu", "mnS2WYLCGJP2kQkRn", "627DZcvme7nLDrbZu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-15T14:18:11.036Z", "modifiedAt": null, "url": null, "title": "Two probabilities", "slug": "two-probabilities", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:27.794Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rwallace", "createdAt": "2009-03-01T16:13:25.493Z", "isAdmin": false, "displayName": "rwallace"}, "userId": "cPhXNeZvnK7LgPMnv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7pcgQWjndDT3xhFus/two-probabilities", "pageUrlRelative": "/posts/7pcgQWjndDT3xhFus/two-probabilities", "linkUrl": "https://www.lesswrong.com/posts/7pcgQWjndDT3xhFus/two-probabilities", "postedAtFormatted": "Monday, February 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pcgQWjndDT3xhFus%2Ftwo-probabilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pcgQWjndDT3xhFus%2Ftwo-probabilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pcgQWjndDT3xhFus%2Ftwo-probabilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 409, "htmlBody": "<p>Consider the following statements:<br /><br />1. The result of this coin flip is heads.<br /><br />2. There is life on Mars.<br /><br />3. The millionth digit of pi is odd.<br /><br />What is the probability of each statement?<br /><br />A frequentist might say, \"P1 = 0.5. P2 is either epsilon or 1-epsilon, we don't know which. P3 is either 0 or 1, we don't know which.\"<br /><br />A Bayesian might reply, \"P1 = P2 = P3 = 0.5. By the way, there's no such thing as a probability of exactly 0 or 1.\"<br /><br />Which is right? As with many such long-unresolved debates, the problem is that two different concepts are being labeled with the word 'probability'. Let's separate them and replace P with:<br /><br />F = the fraction of possible worlds in which a statement is true. F can be exactly 0 or 1.<br /><br />B = the Bayesian probability that a statement is true. B cannot be exactly 0 or 1.<br /><br />Clearly there must be a relationship between the two concepts, or the confusion wouldn't have arisen in the first place, and there is: apart from both obeying&nbsp; various laws of probability, in the case where we know F but don't know which world we are in, B = F. That's what's going on in case 1. In the other cases, we know F != 0.5, but our ignorance of its actual value makes it reasonable to assign B = 0.5.<br /><br />When does the difference matter?<br /><br />Suppose I offer to bet my $200 the millionth digit of pi is odd, versus your $100 that it's even. With B3 = 0.5, that looks like a good bet from your viewpoint. But you also know F3 = either 0 or 1. You can also infer that I wouldn't have offered that bet unless I knew F3 = 1, from which inference you are likely to update your B3 to more than 2/3, and decline.<br /><br />On a larger scale, suppose we search Mars thoroughly enough to be confident there is no life there. Now we know F2 = epsilon. Our Bayesian estimate of the probability of life on Europa will also decline toward 0.<br /><br />Once we understand F and B are different functions, there is no contradiction.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7pcgQWjndDT3xhFus", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 1, "extendedScore": null, "score": 5.61632413513313e-07, "legacy": true, "legacyId": "2301", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-15T18:29:56.968Z", "modifiedAt": null, "url": null, "title": "Boo lights: groupthink edition", "slug": "boo-lights-groupthink-edition", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:28.592Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eQx2aHxMgfdNKNomZ/boo-lights-groupthink-edition", "pageUrlRelative": "/posts/eQx2aHxMgfdNKNomZ/boo-lights-groupthink-edition", "linkUrl": "https://www.lesswrong.com/posts/eQx2aHxMgfdNKNomZ/boo-lights-groupthink-edition", "postedAtFormatted": "Monday, February 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boo%20lights%3A%20groupthink%20edition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoo%20lights%3A%20groupthink%20edition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeQx2aHxMgfdNKNomZ%2Fboo-lights-groupthink-edition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boo%20lights%3A%20groupthink%20edition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeQx2aHxMgfdNKNomZ%2Fboo-lights-groupthink-edition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeQx2aHxMgfdNKNomZ%2Fboo-lights-groupthink-edition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 569, "htmlBody": "<p>In conversations on LessWrong you may be surprised (in fact, dismayed) to find an apparent majority of the community agreeing with each other, and disagreeing with some view you hold dear. You may be tempted to call \"groupthink\". Whenever that happens, please hold yourself to at least as high an epistemic standard as the people who are participating in the community, and substantiate your accusation of groupthink with actual evidence and analysis.</p>\n<p>\"Groupthink\" can be an instance of <a href=\"/lw/jb/applause_lights/\">applause lights</a>, terms or explanations used not so much for their semantic content as for the warm fuzzies they are intended to trigger in your audience. Or... since \"groupthink\" isn't so much intended to generate applause for you, but to generate disapproval of those who disagree with you, we might coin the phrase \"boo lights\".</p>\n<p>At any rate, you may be cheaply establishing (in your own eyes and the eyes of people \"on your side\") your status as a skeptic, without actually doing any critical thinking or even basic due diligence. Are you sure you that's what you want?</p>\n<p><a id=\"more\"></a></p>\n<p>(N.B. links in this post either point to examples, or to more complete definitions of the concepts referenced; they are intended as supplementary material and this post stands on its own, you can ignore the links on a first read-through.)</p>\n<p>Apparent consensus <a href=\"/lw/9n/the_uniquely_awful_example_of_theism/6mb?context=3#comments\">is not sufficient grounds</a> for suspecting groupthink, because the \"groupthink\" explanatory scheme leads to further predictions than the mere appearance of consensus. For instance, groupthink results in \"selection bias in collecting information\" (from the <a href=\"http://en.wikipedia.org/wiki/Groupthink\">Wikipedia entry</a>). If the community has shown <a href=\"/lw/1r0/a_survey_of_anticryonics_writing/\">diligence in seeking contrary information</a>, and yet has not rallied to your favored point of view, your <a href=\"/lw/1r0/a_survey_of_anticryonics_writing/1ld2\">accusations of groupthink are unjustified</a>.</p>\n<p>Disapproval of your contributions (in the form of downvoting) is not sufficient grounds for suspecting groupthink. Communities establish <a href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\">mechanisms of defence</a> against disruption, in a legitimate response to a <a href=\"http://en.wikipedia.org/wiki/Eternal_September\">context of discourse</a> where disruption is an ever present threat, the flip side of open participation. The voting/karma system is the current mechanism, probably flawed and probably better than nothing. Downvotes signal \"we would like to see fewer comments like this one\". The appropriate thing to do if you receive downvotes and you're neither a troll nor a crackpot is to simply seek feedback: <strong>ask what's wrong</strong>. Complaining only makes things worse. Complaining that the community is exhibiting censorship or groupthink makes things <em>much</em> worse.</p>\n<p>Disapproval of your accusations of groupthink is <strong>still not</strong> sufficient grounds for suspecting groupthink. This community is aware of <a href=\"/lw/z/information_cascades/\">information cascades</a> and other effects leading to groupthink, <a href=\"/lw/z3/less_wrong_progress_report/rgt\">discusses them openly</a>, and strives to adopt countervailing norms. (Note that this post generalizes to further concepts, such as censorship. Downvotes are not censorship; they are a collaborative filtering mechanism, whereby readers are encouraged to skip over some content; that content is nevertheless preserved, visible to anyone who chooses to read it; censorship, i.e. banning, <em>does</em> occur but much more seldom than downvoting.)</p>\n<p><a href=\"/lw/1mc/normal_cryonics/1huz\">Here is a good example</a> of someone<del></del> substantiating their accusations of groupthink by reference to the <em>actual</em> research on groupthink. Note how much more work this is.</p>\n<p>If you're still thinking of calling \"groupthink\" without doing that work... or, perhaps, if you have already done so...</p>\n<p>Please reconsider: your behaviour devalues the <a href=\"http://www.abacon.com/commstudies/groups/groupthink.html\">technical meaning</a> of \"groupthink\", which this community does have a use for (as do other communities of sincere inquiry). We want the term groupthink to still be useful when we <em>really</em> need it - when we actually succumb to groupthink.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ALwRRZqvhaop8gxkT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eQx2aHxMgfdNKNomZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 27, "extendedScore": null, "score": 5.616800740978785e-07, "legacy": true, "legacyId": "2302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dLbkrPu5STNCBLRjr", "ZXaRHHLsxaTTQQsZb", "tscc3e5eujrsEeFN4", "DNQw596nPCX4x7xT9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-15T23:50:36.541Z", "modifiedAt": null, "url": null, "title": "Hayekian Prediction Markets?", "slug": "hayekian-prediction-markets", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:31.974Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c9cRiR2bsXdxAyX8E/hayekian-prediction-markets", "pageUrlRelative": "/posts/c9cRiR2bsXdxAyX8E/hayekian-prediction-markets", "linkUrl": "https://www.lesswrong.com/posts/c9cRiR2bsXdxAyX8E/hayekian-prediction-markets", "postedAtFormatted": "Monday, February 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hayekian%20Prediction%20Markets%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHayekian%20Prediction%20Markets%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9cRiR2bsXdxAyX8E%2Fhayekian-prediction-markets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hayekian%20Prediction%20Markets%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9cRiR2bsXdxAyX8E%2Fhayekian-prediction-markets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9cRiR2bsXdxAyX8E%2Fhayekian-prediction-markets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1132, "htmlBody": "<p>I think I basically get the idea behind prediction markets. People take their money seriously, so the opinions of people who are confident enough to bet real money on those opinions deserve to be taken seriously as well. That kid on the schoolyard who was always saying \"wanna bet?\" might have been annoying but he also had a point: your willingness or unwillingness to bet does say something about how seriously your opinions ought to be taken. Furthermore, there are serious problems with the main alternative prediction method, which consists of asking experts what they think is going to happen. Almost nobody ever keeps track of whose predictions turned out to be right and then listens to those people more. Some predictions involve events that are so rare or so far in the future that there's no way for an expert to accumulate a track record at all. Some issues give experts incentives to be impressively wrong rather than boringly right. And so on. These are all good points, and they make enough sense to me to convince me that prediction markets deserve to be taken seriously and tested empirically. If they reliably produce better predictions than the alternatives, then they deserve to win the day.*<br /><br />But there is <a href=\"http://lessig.org/blog/2005/07/prediction_markets.html\" target=\"_blank\">a</a> <a href=\"http://www.lewrockwell.com/rozeff/rozeff88.html\" target=\"_blank\">particular</a> <a href=\"https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/csi-studies/studies/vol50no4/using-prediction-markets-to-enhance-us-intelligence-capabilities.html\" target=\"_blank\">claim</a> that is made about prediction markets that I am skeptical of. It starts with the well-known idea, usually associated with <a href=\"http://en.wikipedia.org/wiki/Friedrich_von_Hayek\" target=\"_blank\">Friedrich on Hayek</a>, that a major virtue of free markets is that there is all kinds of useful information spread out in local chunks throughout the economy, which individuals can usefully exploit but a central planner never could,  which is reflected in market prices, and which in turn cause resources to be allocated efficiently. It then goes on to argue that prediction markets have a similar virtue. As an example, suppose there's a prediction market for a national election, and you happen to know that Candidate X is more popular in your little town than most people think. There's no way that some faraway expert could have known this or incorporated it into his or her prediction in any way, but it gives you an incentive to bet on Candidate X, which causes your local information to be reflected in the prediction market price. Lots and lots of people doing the same thing will cause lots and lots of such little local pieces of information, which couldn't have been obtained any other way, to also be reflected in the market price.<br /><a id=\"more\"></a><br />But it seems to me that this \"Hayekian\" mechanism should work a lot less well in the prediction market context than in the standard context. In the standard version, you benefit directly from a piece of local information that only you happen to have. If you know that a particular machine in your factory only works right if you kick it three times on the left side and then smack it twice on the top, then you can do that and directly reap the benefits, and the fact that you were able to do it (i.e., the fact that output in your particular industry is very slightly less scarce than someone who didn't know that trick would have thought) will be reflected in the market price. In contrast if you're the only one who knows that Candidate X is surprisingly popular in your little town (say because you're the mail carrier and you count yard signs along your route), could you really benefit from trading on that information? There are a number of barriers to your doing so. First, there are transactions costs associated with trading. Second, there is garden-variety risk aversion: if you're risk-averse then you won't want to invest a large share of your total wealth in this highly risky and urnhedged asset, which means that you won't bet much and so the price won't move much to reflect your information. Third, in order to believe that your little piece of local information constitutes a reason to bet on Candidate X, you'd have to believe that the current price accurately reflects all the *other* pieces of information besides yours. In some sense you should believe this: if you thought the price was off and you thought you knew which direction it was off, that would be a good reason to bet against the mispricing. But even if you had no actionable beliefs regarding a mispricing, you might just not have a lot of confidence that all the other information has been aggregated correctly. This would translate into another form of risk, and so risk-aversion would kick in once again. Fourth, there may be uncertainly about whether you really are the only one who knows knows your piece of local information (maybe the paper boy also noticed the yard signs, but then again maybe not). If you're not sure, then you're not sure to what extent that piece of information is already reflected in the current price. Again, you might have beliefs about this, and those beliefs might be right on average (though they might not) but it is yet another layer of uncertainty that should have an effect similar to ordinary risk aversion.<br /><br />I asked Robin Hanson about this once at lunch a few years ago, and we had an interesting chat about it, along with some other George Mason folks. I won't try to summarize everyone's positions here (I'd feel obligated to ask their permission before I'd even try), but suffice it to say that I don't think he foreswore the Hayekian idea entirely as an argument in favor of prediction markets. And there is a quote by him <a href=\"https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/csi-studies/studies/vol50no4/using-prediction-markets-to-enhance-us-intelligence-capabilities.html\" target=\"_blank\">here</a> that seems to embrace it. In any case, I'd be interested to know what he thinks about it. And of course it matters whether or not this Hayekian claim is being made for prediction markets, and it matters whether the claim is correct, because whether or not prediction markets have this additional theoretical advantage should go into one's priors about their merits before evaluating whatever empirical evidence becomes available. <br /><br />*The question is not purely an empirical one though. There are issues related to how susceptible prediction markets are to manipulation, how well they'll work when the people doing the betting about what will happen also have some influence over what does happen, whether they'll work for rare or distant events, or for big picture questions where in some states of the world there's no one around to pay out the winnings, and so on. So even a strong empirical finding that prediction markets work in more straightforward settings is not the last word on the subject, which means that there will be a continuing role for theoretical arguments even as more evidence comes in.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c9cRiR2bsXdxAyX8E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 10, "extendedScore": null, "score": 5.617407870609622e-07, "legacy": true, "legacyId": "2305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-16T02:57:47.014Z", "modifiedAt": null, "url": null, "title": "Friendly AI at the NYC Future Salon", "slug": "friendly-ai-at-the-nyc-future-salon", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:25.403Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jasen", "createdAt": "2009-06-11T15:05:07.288Z", "isAdmin": false, "displayName": "Jasen"}, "userId": "hMDxPMjrPyw8vGzMa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t2aMuKYpBJHoFpGiA/friendly-ai-at-the-nyc-future-salon", "pageUrlRelative": "/posts/t2aMuKYpBJHoFpGiA/friendly-ai-at-the-nyc-future-salon", "linkUrl": "https://www.lesswrong.com/posts/t2aMuKYpBJHoFpGiA/friendly-ai-at-the-nyc-future-salon", "postedAtFormatted": "Tuesday, February 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendly%20AI%20at%20the%20NYC%20Future%20Salon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendly%20AI%20at%20the%20NYC%20Future%20Salon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft2aMuKYpBJHoFpGiA%2Ffriendly-ai-at-the-nyc-future-salon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendly%20AI%20at%20the%20NYC%20Future%20Salon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft2aMuKYpBJHoFpGiA%2Ffriendly-ai-at-the-nyc-future-salon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft2aMuKYpBJHoFpGiA%2Ffriendly-ai-at-the-nyc-future-salon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>For those in the New York City area: &nbsp;I will be giving a <a href=\"http://www.meetup.com/futuresalon/calendar/12478841/?a=ce1p_grp&amp;rv=ce1p\">talk on Friendly AI</a> at the NYC Future Salon this Saturday, the 20th of February, at 3:00pm. &nbsp;We'll be gathering in the back room of Stone Creek Bar, located at 140 E. 27th Street in Manhattan.</p>\n<p>At the moment, 29 people have RSVPed \"Yes\" and 10 \"maybe,\" with 21 spots left. &nbsp;If you're interested in comming, please RSVP through the Future Salon's&nbsp;<a href=\"http://www.meetup.com/futuresalon/\">meetup group</a>, as space is limited.</p>\n<p>Those of you familiar with the existing FAI literature probably won't hear anything new, but we have the venue for over 3 hours and my talk should only take 20 to 30 minutes, so there should be plenty of time for stimulating discussion. &nbsp;Be sure to bring your toughest questions! &nbsp;This would also be a good opportunity for those who haven't made any of the local LW/OB meetups (we have a <a href=\"http://www.meetup.com/Less-Wrong-Overcoming-Bias-NYC/\">meetup group</a> now!) to meet other aspiring rationalists in the NYC area.</p>\n<p>I look forward to seeing some new faces.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t2aMuKYpBJHoFpGiA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 5.617762316378959e-07, "legacy": true, "legacyId": "2306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-16T08:29:56.690Z", "modifiedAt": null, "url": null, "title": "Open Thread: February 2010, part 2", "slug": "open-thread-february-2010-part-2", "viewCount": null, "lastCommentedAt": "2012-05-12T21:11:16.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bg8bSdLdyRpzescQJ/open-thread-february-2010-part-2", "pageUrlRelative": "/posts/bg8bSdLdyRpzescQJ/open-thread-february-2010-part-2", "linkUrl": "https://www.lesswrong.com/posts/bg8bSdLdyRpzescQJ/open-thread-february-2010-part-2", "postedAtFormatted": "Tuesday, February 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20February%202010%2C%20part%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20February%202010%2C%20part%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbg8bSdLdyRpzescQJ%2Fopen-thread-february-2010-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20February%202010%2C%20part%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbg8bSdLdyRpzescQJ%2Fopen-thread-february-2010-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbg8bSdLdyRpzescQJ%2Fopen-thread-february-2010-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>The Open Thread posted at the beginning of the month has gotten really, really big, so I've gone ahead and made another one. Post your new discussions here!</p>\n<p><em>This thread is for the discussion of Less Wrong topics that have not  appeared in recent posts. If a discussion gets unwieldy, celebrate by  turning it into a top-level post.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bg8bSdLdyRpzescQJ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 5.618391415738105e-07, "legacy": true, "legacyId": "2308", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 917, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2010-02-16T08:29:56.690Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-19T00:18:05.339Z", "modifiedAt": null, "url": null, "title": "Things You Can't Countersignal", "slug": "things-you-can-t-countersignal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:37.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ThhNvdBxcTYdzm69s/things-you-can-t-countersignal", "pageUrlRelative": "/posts/ThhNvdBxcTYdzm69s/things-you-can-t-countersignal", "linkUrl": "https://www.lesswrong.com/posts/ThhNvdBxcTYdzm69s/things-you-can-t-countersignal", "postedAtFormatted": "Friday, February 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Things%20You%20Can't%20Countersignal&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThings%20You%20Can't%20Countersignal%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThhNvdBxcTYdzm69s%2Fthings-you-can-t-countersignal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Things%20You%20Can't%20Countersignal%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThhNvdBxcTYdzm69s%2Fthings-you-can-t-countersignal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThhNvdBxcTYdzm69s%2Fthings-you-can-t-countersignal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1112, "htmlBody": "<p><em>Countersignaling can backfire if your audience doesn't have enough information about you to start with.&nbsp; For some traits, it's especially dangerous, because you're likely to do it for traits you don't have the credibility to countersignal at all based on a misunderstanding of your relation to the general population.<br /></em></p>\n<p>Countersignaling is \"showing off by not showing off\" - you understate, avoid drawing attention to, or otherwise downplay your communications of and about some valuable trait you have, because a) you are sure you won't be mistaken for someone with very poor characteristics in that area, and b) signaling could make you look like a merely medium-grade specimen.&nbsp; (Actual medium-grade specimens have to signal to distinguish themselves from low-quality ones.)&nbsp; For instance, if you are so obviously high-status that no one could possibly miss it, it may be both unnecessary and counterproductive to signal status, because this would let others conflate you with mid-status people.&nbsp; So you can show up in a t-shirt and jeans instead of formal wear.&nbsp; If you are so obviously brilliant that no one could possibly think you're some crackpot who wandered in off the street, you can afford to rave a little, while people who have to prove their smarts will find it expedient to keep calm and measured in their communication.</p>\n<p>In homogeneous communities, or in any situation where you are well-known, countersignaling is effective.&nbsp; Your traits exceeding some minimum threshold is assumed where everyone's traits so exceed, and so failing to signal is unlikely to give anyone the impression that you have somehow managed to be the only person in the room who is deficient.&nbsp; If you're personally acquainted with the people around whom you attempt countersignaling, your previous signals (or other evidence to the effect that you are awesome) will already have accumulated.&nbsp; It's not necessary to further prove yourself.&nbsp; In other words, if your audience's prior for you being medium-or-good is high enough, then your not signaling is evidence in favor of good over medium; if their prior for your being medium-or-low is too high, then your not signaling is instead evidence in favor of low over medium.</p>\n<p>But there are some things you can't effectively countersignal.</p>\n<p><a id=\"more\"></a>Or rather, there are some things that you can't effectively countersignal to some people.&nbsp; The most self-deprecating remarks about your positive qualities, spoken to your dear friends who know your most excellent traits like the backs of their own hands, will be interpreted \"correctly\", no matter what they're about.&nbsp; For instance, when I explained my change in life plans to people who are very familiar with me, I was able to use the phrasing \"I'm dropping out of school to join a doomsday cult\"<sup>1</sup> because I knew this sounded so unlike me that none of them would take it at face value.&nbsp; Alicorn wouldn't <em>really</em> join a doomsday cult; it must be something else!&nbsp; It elicited curiosity, but not contempt for my cult-joining behavior.&nbsp; To more distant acquaintances, I used the less loaded term \"nonprofit\".&nbsp; I couldn't countersignal my clever life choices to people who didn't have enough knowledge of my clever life choices; so I had to rely on the connotation of \"nonprofit\" rather than playing with the word \"cult\" for my amusement.</p>\n<p>Similar to close personal connection, people in a homogeneous environment can readily understand one another's countersignals.&nbsp; Someone who has joined the same cult as me isn't going to get the wrong idea if I call it that, even without much historical data about how sensible I generally am in choosing what comes next in my life.&nbsp; But in the wider world where people really do join real cults that really have severely negative features, there's no way to tell me apart from someone who's joined one of those and might start chanting or something any moment.&nbsp; I would not announce that I had joined a cult when explaining to a TSA agent why I was flying across the country.</p>\n<p>The trouble is that it's easy to think one's positive traits are so obvious that no one could miss them when really they aren't.&nbsp; You are <a href=\"/lw/kg/expecting_short_inferential_distances/\">not as well known</a> as you think you should be.&nbsp; Your countersignals are <a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">more opaque</a> than you think they are.&nbsp; If you tell a stranger you've joined a cult, they will probably think you actually joined a cult.</p>\n<p>Here's an example at work: in a homogeneous group of white liberals, talking casually about assorted minority races is commonplace if race is going to be discussed at all.&nbsp; Everybody present knows that the group is a homogeneous group of white liberals.&nbsp; Nobody has reason to suspect that anyone in the room has ever been disposed to practice overt racism of any kind, and odds are that no one in the group is well-informed enough about implicit biases to suspect covert racism (even though that's almost certainly present).&nbsp; So people in the group can countersignal their lack of racism to each other with the loose, casual talk, making generalizations when it's convenient.&nbsp; Nobody listening will take them for \"real\" racists.<em>&nbsp; </em>And being hyper-concerned with political correctness would make one seem concerned with being racist - it would look like one considered oneself to be in some kind of <em>danger</em>, which doesn't speak kindly of how well one is doing to begin with.</p>\n<p>But to an outside observer - especially one who is informed about implicit biases, or has personal experiences with how ineffectively people screen off casual attitudes and prevent them from causing bad behavior - feeling that one is in this kind of danger, and speaking carefully to reflect that, is the best-case scenario.&nbsp; To an outside observer, the homogeneous group of white liberals cannot credibly countersignal, because there are too many people who look just like them and talk just like them and <em>don't</em> have the lovely qualities they advertise by acting confidently.&nbsp; In the general population, loose race talk is more likely to accompany racism than non-racism, and non-racism is more likely to accompany political correctness than loose race talk.&nbsp; The outside observer can't separate the speaker from the general population and has to judge them against those priors, not local, fine-tuned priors.</p>\n<p>So to sum up, countersignaling is hazardous when your audience can't separate you from the general population via personal acquaintance or context.&nbsp; But often, you aren't as different from the general population as you think (even if your immediate audience, like you, thinks you are).&nbsp; Or, the general population is in poorer shape than you suspect (increasing the prior that you're in a low-quality tier for the quality you might countersignal).&nbsp; Therefore, you should prudentially exercise caution when deciding when to be uncautious about your signals.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>I am visiting the Singularity Institute.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q6P8jLn8hH7kbuXRr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ThhNvdBxcTYdzm69s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 72, "extendedScore": null, "score": 0.000119, "legacy": true, "legacyId": "2314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 126, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HLqWn5LASfhhArZ7w", "sSqoEw9eRP2kPKLCz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-19T01:30:44.399Z", "modifiedAt": null, "url": null, "title": "Explicit Optimization of Global Strategy (Fixing a Bug in UDT1)", "slug": "explicit-optimization-of-global-strategy-fixing-a-bug-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g8xh9R7RaNitKtkaa/explicit-optimization-of-global-strategy-fixing-a-bug-in", "pageUrlRelative": "/posts/g8xh9R7RaNitKtkaa/explicit-optimization-of-global-strategy-fixing-a-bug-in", "linkUrl": "https://www.lesswrong.com/posts/g8xh9R7RaNitKtkaa/explicit-optimization-of-global-strategy-fixing-a-bug-in", "postedAtFormatted": "Friday, February 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Explicit%20Optimization%20of%20Global%20Strategy%20(Fixing%20a%20Bug%20in%20UDT1)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExplicit%20Optimization%20of%20Global%20Strategy%20(Fixing%20a%20Bug%20in%20UDT1)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8xh9R7RaNitKtkaa%2Fexplicit-optimization-of-global-strategy-fixing-a-bug-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Explicit%20Optimization%20of%20Global%20Strategy%20(Fixing%20a%20Bug%20in%20UDT1)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8xh9R7RaNitKtkaa%2Fexplicit-optimization-of-global-strategy-fixing-a-bug-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8xh9R7RaNitKtkaa%2Fexplicit-optimization-of-global-strategy-fixing-a-bug-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 607, "htmlBody": "<p>When describing UDT1 solutions to various <a href=\"/lw/1qo/a_problem_with_timeless_decision_theory_tdt/1ks7\">sample</a> <a href=\"/lw/1qo/a_problem_with_timeless_decision_theory_tdt/1kzp\">problems</a>, I've often talked about UDT1 finding the function S* that would optimize its preferences over the world program P, and then return what S* would return, given its input. But in <a href=\"/lw/15m/towards_a_new_decision_theory/\">my original description of UDT1</a>, I never explicitly mentioned optimizing S as a whole, but instead specified UDT1 as, upon receiving input X, finding the optimal output Y* for that input, by considering the logical consequences of choosing various possible outputs. I have been implicitly assuming that the former (optimization of the global strategy) would somehow fall out of the latter (optimization of the local action) without having to be explicitly specified, due to how UDT1 takes into account logical correlations between different instances of itself. But recently I found an apparent counter-example to this assumption.</p>\n<p>(I think this \"bug\" also exists in TDT, but I don't understand it well enough to make a definite claim. Perhaps Eliezer or someone else can tell me if TDT correctly solves the sample problem given here.)</p>\n<p><a id=\"more\"></a>Here is the problem. Suppose Omega appears and tells you that you have just been copied, and each copy has been assigned a different number, either 1 or 2. Your number happens to be 1. You can choose between option A or option B. If the two copies choose different options without talking to each other, then each gets $10, otherwise they get $0.</p>\n<p>Consider what happens in the original formulation of UDT1. Upon receiving the input \"1\", it can choose \"A\" or \"B\" as output. What is the logical implication of S(1)=\"A\" on the computation S(2)? It's not clear whether S(1)=\"A\" implies S(2)=\"A\" or S(2)=\"B\", but actually neither can be the right answer.</p>\n<p>Suppose S(1)=\"A\" implies S(2)=\"A\". Then by symmetry S(1)=\"B\" implies S(2)=\"B\", so both copies choose the same option, and get $0, which is clearly not right.</p>\n<p>Now instead suppose S(1)=\"A\" implies S(2)=\"B\". Then by symmetry S(1)=\"B\" implies S(2)=\"A\", so UDT1 is indifferent between \"A\" and \"B\" as output, since both have the logical consequence that it gets $10. So it might as well choose \"A\". But the other copy, upon receiving input \"2\", would go though this same reasoning, and also output \"A\".</p>\n<p>The fix is straightforward in the case where every agent already has the same source code and preferences. UDT1.1, upon receiving input X, would put that input aside and first iterate through all possible input/output mappings that it could implement and determine the logical consequence of choosing each one upon the executions of the world programs that it cares about. After determining the optimal S* that best satisfies its preferences, it then outputs S*(X).</p>\n<p>Applying this to the above example, there are 4 input/output mappings to consider:</p>\n<ol>\n<li>S<sub>1</sub>(1)=\"A\", S<sub>1</sub>(2)=\"A\"</li>\n<li>S<sub>2</sub>(1)=\"B\", S<sub>2</sub>(2)=\"B\"</li>\n<li>S<sub>3</sub>(1)=\"A\", S<sub>3</sub>(2)=\"B\"</li>\n<li>S<sub>4</sub>(1)=\"B\", S<sub>4</sub>(2)=\"A\"</li>\n</ol>\n<p>Being indifferent between S<sub>3</sub> and S<sub>4</sub>, UDT1.1 picks S*=S<sub>3</sub> and returns S<sub>3</sub>(1)=\"A\". The other copy goes through the same reasoning, also picks S*=S<sub>3</sub> and returns S<sub>3</sub>(2)=\"B\". So everything works out.</p>\n<p>What about when there are agents with difference source codes and different preferences? The result here suggests that one of our big unsolved problems, that of generally <a href=\"/lw/13o/fairness_and_geometry/yrx\">deriving</a> a \"good and fair\" global outcome from agents optimizing their own preferences while taking logical correlations into consideration, may be unsolvable, since consideration of logical correlations does not seem powerful enough to always obtain a \"good and fair\" global outcome even in the single-player case. Perhaps we need to take an approach more like <a href=\"/lw/13o/fairness_and_geometry/\">cousin_it's</a>, and try to solve the cooperation problem from the top down. That is, by explicitly specifying a fair way to merge preferences, and simultaneously figuring out how to get agents to join into such a cooperation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "5f5c37ee1b5cdee568cfb1dc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g8xh9R7RaNitKtkaa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 39, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "2309", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["de3xjFaACCAk6imzv", "oZ33pz2FWzFbWrgHT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-19T08:11:21.500Z", "modifiedAt": null, "url": null, "title": "Med Patient Social Networks Are Better Scientific Institutions", "slug": "med-patient-social-networks-are-better-scientific", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.160Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Liron", "createdAt": "2009-02-27T04:43:11.294Z", "isAdmin": false, "displayName": "Liron"}, "userId": "AyzRrs8hNm54QptLi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JSkctLfsWZLhsgGyK/med-patient-social-networks-are-better-scientific", "pageUrlRelative": "/posts/JSkctLfsWZLhsgGyK/med-patient-social-networks-are-better-scientific", "linkUrl": "https://www.lesswrong.com/posts/JSkctLfsWZLhsgGyK/med-patient-social-networks-are-better-scientific", "postedAtFormatted": "Friday, February 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Med%20Patient%20Social%20Networks%20Are%20Better%20Scientific%20Institutions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMed%20Patient%20Social%20Networks%20Are%20Better%20Scientific%20Institutions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJSkctLfsWZLhsgGyK%2Fmed-patient-social-networks-are-better-scientific%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Med%20Patient%20Social%20Networks%20Are%20Better%20Scientific%20Institutions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJSkctLfsWZLhsgGyK%2Fmed-patient-social-networks-are-better-scientific", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJSkctLfsWZLhsgGyK%2Fmed-patient-social-networks-are-better-scientific", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 527, "htmlBody": "<p>When you're suffering from a life-changing illness, where do you find information about its likely progression? How do you decide among treatment options?</p>\n<p>You don't want to rely on studies in medical journals because their conclusion-drawing methodologies are haphazard. You'll be better off getting your prognosis and treatment decisions from a social networking site: <a href=\"http://patientslikeme.com\">PatientsLikeMe.com</a>.</p>\n<p>PatientsLikeMe.com lets patients with similar illnesses compare symptoms, treatments and outcomes. As <a href=\"http://www.youtube.com/watch?v=LMSGP2ONfBc\">Jamie Heywood at TEDMED 2009</a> explains, this represents an enormous leap forward in the scope and methodology of clinical trials. I highly recommend his excellent talk, and I will paraphrase part of it below.<a id=\"more\"></a></p>\n<blockquote>\n<p>Here is a report in the Proceedings of the US National Academy of Sciences (PNAS) about Lithium, which is a drug used to treat Bipolar disorder that a group in Italy found slowed ALS down in 16 patients. When PNAS published this, 10% of the patients in our system started taking Lithium, based on 16 patients' data in a bad publication.<br /><br />This one patient, Humberto, said, \"Can you help us answer these kinds of treatment questions? I don't want to wait for the next trial; I want to know now!\"<br /><br />So we launched some tools to help patients track their medical data like blood levels, symptoms, side effects... and share it.<br /><br />People said, \"You can't run a clinical trial like this. You don't have blinding, you don't have data, it doesn't follow the scientific method -- you can't do it.\"<br /><br />So we said, OK, we can't do a clinical trial? Let's do something even harder. Let's use all this data to say whether Lithium is going to work on Humberto.<br /><br />We took all the patients like Humberto and brought their data together, bringing their histories into it, lining up their timelines along meaningful points, and integrating everything we know about the patient -- full information about the entire course of their disease. And we saw that this orange line, that's what's going to happen to Humberto.</p>\n<p><img src=\"http://www.quixey.com/image/111971\" alt=\"\" /><br /><br />And in fact he took Lithium, and he went down the line. This works almost all the time -- it's scary.</p>\n<p><img src=\"http://www.quixey.com/image/111972\" alt=\"\" /><br /><br />So we couldn't run a clinical trial, but we could see whether Lithium was going to work for Humberto.<br /><br />Here's the mean decline curve for the most dedicated Lithium patients we had, the ones who stuck with it for at least a year because they believed it was working. And even for this hard core sample, we still have N = 4x the number in the journal study.</p>\n<p><img src=\"http://www.quixey.com/image/111975\" alt=\"\" /></p>\n<p>When we line up these patients' timelines, it's clear that the ones who took Lithium didn't do any better. And we had the power to detect an effect only 1/4 the strength of the one reported in the journal. And we did this one year before the time when the first clinical trial, funded with millions of dollars by the NIH, announced negative results last week.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JSkctLfsWZLhsgGyK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 46, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "2315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-19T20:27:47.777Z", "modifiedAt": null, "url": null, "title": "Study: Making decisions makes you tired", "slug": "study-making-decisions-makes-you-tired", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:26.900Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bgrah449", "createdAt": "2009-06-09T14:45:39.500Z", "isAdmin": false, "displayName": "bgrah449"}, "userId": "tm3ckPCxYm6gvSvXF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MhmNh9GrRh2efJBA2/study-making-decisions-makes-you-tired", "pageUrlRelative": "/posts/MhmNh9GrRh2efJBA2/study-making-decisions-makes-you-tired", "linkUrl": "https://www.lesswrong.com/posts/MhmNh9GrRh2efJBA2/study-making-decisions-makes-you-tired", "postedAtFormatted": "Friday, February 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Study%3A%20Making%20decisions%20makes%20you%20tired&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudy%3A%20Making%20decisions%20makes%20you%20tired%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhmNh9GrRh2efJBA2%2Fstudy-making-decisions-makes-you-tired%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Study%3A%20Making%20decisions%20makes%20you%20tired%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhmNh9GrRh2efJBA2%2Fstudy-making-decisions-makes-you-tired", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhmNh9GrRh2efJBA2%2Fstudy-making-decisions-makes-you-tired", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/fh/willpower_hax_487_execute_by_default/\" target=\"_blank\">Willpower Hax #487: Execute by Default</a>, <a href=\"/lw/fh/willpower_hax_487_execute_by_default/\" target=\"_blank\"><a href=\"/lw/10x/the_physiology_of_willpower/\" target=\"_blank\">The Physiology of Willpower</a><br /></a></p>\n<p>Making your own decisions makes you procrastinate more and quit sooner than when you're following orders:</p>\n<p><a href=\"http://www.bakadesuyo.com/can-making-decisions-leave-you-tired-and-burn\">Key quote</a>:</p>\n<blockquote>\n<p>Making choices led to reduced self-control (i.e., less physical stamina, <strong>reduced persistence in the face of failure, more procrastination</strong>, and less quality and quantity of arithmetic calculations). A field study then found that reduced self-control was predicted by shoppers' self-reported degree of previous active decision making. Further studies suggested that <strong>choosing is more depleting than merely deliberating</strong> and forming preferences about options and more depleting than implementing choices made by someone else and that anticipating the choice task as enjoyable can reduce the depleting effect for the first choices but not for many choices.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YrLoz567b553YouZ2": 1, "dqx5k65wjFfaiJ9sQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MhmNh9GrRh2efJBA2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 31, "extendedScore": null, "score": 5.627948196485061e-07, "legacy": true, "legacyId": "2317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FHukyfMagq4HrBYNt", "ob6FdjoXnirRkodNs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-20T15:00:34.487Z", "modifiedAt": null, "url": null, "title": "Conversation Halters", "slug": "conversation-halters", "viewCount": null, "lastCommentedAt": "2020-06-28T12:20:33.048Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters", "pageUrlRelative": "/posts/wqmmv6NraYv4Xoeyj/conversation-halters", "linkUrl": "https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters", "postedAtFormatted": "Saturday, February 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Conversation%20Halters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConversation%20Halters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwqmmv6NraYv4Xoeyj%2Fconversation-halters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Conversation%20Halters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwqmmv6NraYv4Xoeyj%2Fconversation-halters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwqmmv6NraYv4Xoeyj%2Fconversation-halters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1085, "htmlBody": "<p><strong>Related to</strong>:&nbsp; <a href=\"/lw/1p1/logical_rudeness/\">Logical Rudeness</a>, <a href=\"/lw/it/semantic_stopsigns/\">Semantic Stopsigns</a></p>\n<p>While working on my book, I found in passing that I'd developed a list of what I started out calling \"stonewalls\", but have since decided to refer to as \"conversation halters\".&nbsp; These tactics of argument are distinguished by their being attempts to <em>cut off the flow of debate</em> - which is rarely the wisest way to think, and should certainly rate an alarm bell.</p>\n<p>Here's my assembled list, on which I shall expand shortly:</p>\n<ul>\n<li>Appeal to permanent unknowability;</li>\n<li>Appeal to humility;</li>\n<li>Appeal to egalitarianism;</li>\n<li>Appeal to common guilt;</li>\n<li>Appeal to inner privacy;</li>\n<li>Appeal to personal freedom;</li>\n<li>Appeal to arbitrariness;</li>\n<li>Appeal to inescapable assumptions.</li>\n<li>Appeal to unquestionable authority;</li>\n<li>Appeal to absolute certainty.</li>\n</ul>\n<p>Now all of these might seem like dodgy moves, some dodgier than others.&nbsp; But they become dodgier still when you take a step back, feel the flow of debate, observe the cognitive traffic signals, and view these as attempts to <em>cut off the flow of further debate.<a id=\"more\"></a></em></p>\n<p>Hopefully, most of these are obvious, but to define terms:</p>\n<p><em>Appeal to permanent unknowability</em> - something along the lines of \"Why did God allow smallpox?&nbsp; Well, no one can know the mind of God.\"&nbsp; Or, \"There's no way to distinguish among interpretations of quantum mechanics, so we'll never know.\"&nbsp; Arguments like these can be refuted easily enough by anyone who knows the rules for reasoning under uncertainty and how they imply a correct probability estimate given a state of knowledge... but of course you'll probably have to explain the rules to the other, and the reason they appealed to unknowability is probably to <em>cut off further discussion</em>.</p>\n<p><em>Appeal to humility</em> - much the same as above, but said with a different emphasis:&nbsp; \"How can <em>we</em> know?\", where of course the speaker doesn't much want to know, and so the real meaning is \"How can <em>you</em> know?\"&nbsp; Of course one may gather entangled evidence in most such cases, and <a href=\"http://wiki.lesswrong.com/wiki/Occam%27s_razor\">Occam's Razor</a> or extrapolation from already-known facts takes care of the other cases.&nbsp; But you're not likely to get a chance to explain it, because by continuing to speak, you are committing the sin of pride.</p>\n<p><em>Appeal to egalitarianism</em> - something along the lines of \"No one's opinion is better than anyone else's.\"&nbsp; Now if you keep talking you're committing <a href=\"/lw/h9/tsuyoku_vs_the_egalitarian_instinct/\">an offense against tribal equality</a>.</p>\n<p><em>Appeal to common guilt</em> - \"<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">everyone is irrational</a> now and then\", so if you keep talking, you're claiming to be better than them.&nbsp; An implicit subspecies of appeal to egalitarianism.</p>\n<p><em>Appeal to inner privacy</em> - \"you can't possibly know how I feel!\"&nbsp;&nbsp;It's true that modern technology still encounters some slight difficulties in reading thoughts out of the brain, though work is underway as we speak.&nbsp; But it is rare that <em>the exact details of how you feel </em>are the <em>key subject matter </em>being disputed.&nbsp; Here the bony borders of the skull are being redeployed as a hard barrier to keep out further arguments.</p>\n<p><em>Appeal to personal freedom</em> - \"<a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">I can define a word any way I want!</a>\"&nbsp; Now if you keep talking you're infringing on their civil rights.</p>\n<p><em>Appeal to arbitrariness</em> - again, the notion that <a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">word definitions are arbitrary</a> serves as a good example (in fact I was harvesting some of these appeals from that sequence).&nbsp; It's not just that this is wrong, but that it serves to cut off further discourse.&nbsp; Generally, anything that people are motivated to argue about is not <em>arbitrary.</em>&nbsp; It is being controlled by invisible criteria of evaluation, it has connotations with consequences, and if <em>that</em> isn't true either, the topic of discourse is probably not \"arbitrary\" but just \"meaningless\".&nbsp; No map that corresponds to an external territory can be arbitrary.</p>\n<p><em>Appeal to inescapable assumptions</em> - closely related, the idea that you need some assumptions and therefore everyone is free to choose whatever assumptions they want.&nbsp; This again is almost never true.&nbsp; In the realm of physical reality, reality is one way or another and you don't get to make it that way by choosing an opinion, and so some \"assumptions\" are right and others wrong.&nbsp; In the realm of math, once you choose enough axioms to specify the subject matter, the remaining theorems are matters of logical implication.&nbsp; What I want you to notice is not just that \"appeal to inescapable assumptions\" is a bad idea, but that it is supposed to halt further conversation.</p>\n<p><em>Appeal to unquestionable authority </em>- for example, defending a definition by <a href=\"/lw/nr/the_argument_from_common_usage/\">appealing to the dictionary</a>, which is supposed to be a final settlement of the argument.&nbsp; Of course it is very rare that whatever is <em>really</em> at stake is something that ought to turn out differently if a Merriam-Webster editor writes a different definition.&nbsp; Only in matters of the solidest, most replicable science, do we have information so authoritative that there is no longer much point in considering other sources of evidence.&nbsp; And even then we shouldn't expect to see strong winds of evidence blowing in an opposing direction - under the Bayesian definition of evidence, strong evidence is just that sort of evidence which you only ever expect to find on at most one side of a factual question.&nbsp; More usually, this argument runs something along the lines of \"How dare you argue with the dictionary?\" or \"How dare you argue with Professor Picklepumper of Harvard University?\"</p>\n<p><em>Appeal to absolute certainty</em> - if you <em>did</em> have some source of absolute certainty, it would do no harm to cut off debate at that point.&nbsp; Needless to say, <a href=\"/lw/nf/the_parable_of_hemlock/\">this usually doesn't happen</a>.</p>\n<p>And again:&nbsp; These appeals are all flawed in their separate ways, but what I want you to notice is the thing they have in common, the stonewall-effect, the conversation-halting cognitive traffic signal.</p>\n<p>The only time it would <em>actually</em> be appropriate to use such a traffic signal is when you have information so strong, or coverage so complete, that there really is no point in further debate.&nbsp; This condition is rarely if ever met.&nbsp; A truly definite series of replicated experiments might settle an issue pending really surprising new experimental results, a la Newton's laws of gravity versus Einstein's GR.&nbsp; Or a gross prior improbability, combined with failure of the advocates to provide confirming evidence in the face of repeated opportunities to do so.&nbsp; Or you might simply run out of time.</p>\n<p>But then you should state the stoppage condition outright and plainly, not package it up in one of these appeals.&nbsp; By and large, these traffic signals are simply bad traffic signals.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wqmmv6NraYv4Xoeyj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 54, "extendedScore": null, "score": 9.5e-05, "legacy": true, "legacyId": "2198", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["srge9MCLHSiwzaX6r", "FWMfQKG3RpZx6irjm", "gWGA8Da539EQmAR9F", "DoLQN5ryZ9XkZjq5h", "FaJaCgqBKphrDzDSj", "9ZooAqfh2TC9SBDvq", "bcM5ft8jvsffsZZ4Y"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-21T04:25:49.130Z", "modifiedAt": null, "url": null, "title": "Akrasia Tactics Review", "slug": "akrasia-tactics-review", "viewCount": null, "lastCommentedAt": "2022-05-21T16:16:27.003Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rRmisKb45dN7DK4BW/akrasia-tactics-review", "pageUrlRelative": "/posts/rRmisKb45dN7DK4BW/akrasia-tactics-review", "linkUrl": "https://www.lesswrong.com/posts/rRmisKb45dN7DK4BW/akrasia-tactics-review", "postedAtFormatted": "Sunday, February 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Akrasia%20Tactics%20Review&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAkrasia%20Tactics%20Review%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrRmisKb45dN7DK4BW%2Fakrasia-tactics-review%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Akrasia%20Tactics%20Review%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrRmisKb45dN7DK4BW%2Fakrasia-tactics-review", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrRmisKb45dN7DK4BW%2Fakrasia-tactics-review", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 716, "htmlBody": "<p>I <a href=\"/lw/1rv/demands_for_particular_proof_appendices/1mqb\">recently had occasion</a> to review some of the akrasia tricks I've found on Less Wrong, and it occurred to me that there's probably quite a lot of others who've tried them as well.&nbsp; Perhaps it's a good idea to organize the experiences of a couple dozen procrastinating rationalists?</p>\n<p>Therefore, I'll aggregate any such data you provide in the comments, according to the following scheme:</p>\n<ol>\n<li><strong>Note which trick you've tried.</strong>&nbsp; If it's something that's not yet on the list below, please provide a link and I'll add it; if there's not a link for it anywhere, you can describe it in your comment and I'll link that.</li>\n<li><strong>Give your experience with it a score from -10 to +10</strong> (0 if it didn't change the status quo, 10 if it ended your akrasia problems forever with no side effects, negative scores if it actually made your life worse, -10 if it nearly killed you); if you don't do so, I'll suggest a score for you based on what else you say.</li>\n<li><strong>Describe your experience with it</strong>, including any significant side effects.</li>\n</ol>\n<p>Every so often, I'll combine all the data back into the main post, listing average scores, sample size and common effects for each technique.&nbsp; Ready?</p>\n<p><a id=\"more\"></a>Here's the list of specific akrasia tactics I've found around LW (and also in outside links from here); again, if I'm missing one, let me know and I'll add it.&nbsp; Special thanks to Vladimir Golovin for the <a href=\"/lw/fu/share_your_antiakrasia_tricks\">Share Your Anti-Akrasia Tricks</a> post.</p>\n<p>Without further ado, here are the results so far as I've recorded them, with average score, number of reviews, standard deviation and recurring comments.</p>\n<p>&nbsp;</p>\n<p><strong>3 or More Reviews:</strong></p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nts\">Collaboration with Others</a>: Average +7.7 (3 reviews) (SD 0.6)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1o2w\">No Multitasking</a>: Average +6.0 (3 reviews) (SD 2.0); note <a href=\"/lw/1sm/akrasia_tactics_review/1os8\">variants</a></p>\n<p>P.J. Eby's <a href=\"/lw/1sm/akrasia_tactics_review/1oh3\">Motivation Trilogy</a>: Average +5.8 (6 reviews) (SD 3.3)</p>\n<p style=\"padding-left: 30px;\"><a href=\"/lw/fh/willpower_hax_487_execute_by_default/c2k\">Monoidealism</a>: Average +8.0 (3 reviews) (SD 2.0)</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://dirtsimple.org/2006/07/hidden-meaning-of-just-do-it.html\">\"Just Do It\"</a>: Average +4 (2 reviews) (SD 4.2)</p>\n<p style=\"padding-left: 30px;\"><a href=\"/lw/1sm/akrasia_tactics_review/1o1d\">Irresistible Instant Motivation</a>: +3 (1 review)</p>\n<p><a href=\"http://en.wikipedia.org/wiki/GTD\">Getting Things Done</a>: Average +4.9 (7 reviews) (SD 2.6)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nu3\">Regular Exercise</a>: Average +4.4 (5 reviews) (SD 2.3)</p>\n<p>Cripple your Internet: Average +4.2 (11 reviews) (SD 3.0)</p>\n<p style=\"padding-left: 30px;\"><a href=\"https://addons.mozilla.org/en-US/firefox/addon/4476\">LeechBlock</a>: Average +5.4 (5 reviews) (SD 2.9); basically everyone who's tried has found it helpful.</p>\n<p style=\"padding-left: 30px;\"><a href=\"https://addons.mozilla.org/addon/pageaddict/\">PageAddict</a>: +3 (1 review)</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://macfreedom.com/\">Freedom</a> (Mac)</p>\n<p><a href=\"/lw/1lt/case_study_melatonin/\">Melatonin</a>: Average +4.0 (5 reviews) (SD 5.4); works well for some, others feel groggy the next day; might help to <a href=\"/lw/1sm/akrasia_tactics_review/1o5d\">vary the dosage</a></p>\n<p><a href=\"/lw/fh/willpower_hax_487_execute_by_default\">Execute by Default</a>: Average +3.7 (7 reviews) (SD 2.4); all sorts of variants; universally helpful, not typically a life-changer.</p>\n<p><a href=\"http://www.pomodorotechnique.com/\">Pomodoro Technique</a>: Average +3.3 (3 reviews) (SD 4.2); mathemajician <a href=\"/lw/1sm/akrasia_tactics_review/1nrv\">suggests a 45-minute variant</a></p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/ck0\">Being Watched</a>: Average +3.2 (6 reviews) (SD 4.1); variations like co-working seem more effective; see \"collaboration\" below</p>\n<p><a href=\"/lw/178/notes_on_utility_function_experiment\">Utility Function Experiment</a>: Average +2.8 (4 reviews) (SD 2.8)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx3\">Meditation</a>: Average +2.8 (5 reviews) (SD 2.8)</p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/cl7\">Modafinil and Equivalents</a>: Average -0.8 (5 reviews) (SD 8.5); fantastic for some, terrible for others.&nbsp; Seriously, look at that standard deviation!</p>\n<p><a href=\"/lw/1fe/antiakrasia_technique_structured_procrastination\">Structured Procrastination</a>: Average -1.0 (3 reviews) (SD 4.4); polarized opinion</p>\n<p><a href=\"/lw/ep/applied_picoeconomics\">Resolutions (Applied Picoeconomics)</a>: Average -3.2 (5 reviews) (SD 3.3); easy to fail &amp; get even more demotivated</p>\n<p>&nbsp;</p>\n<p><strong>1 or 2 Reviews:</strong></p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">Dual n-back</a>: Average +6.5 (2 reviews) (SD 2.1)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">Think It, Do It</a>: Average +6 (2 reviews) (SD 1.4)</p>\n<p><a href=\"/lw/eg/what_i_tell_you_three_times_is_true/\">Self-Affirmation</a>: Average +4 (2 reviews) (SD 2.8)</p>\n<p>Create <a href=\"/lw/f1/beware_trivial_inconveniences/\">Trivial Inconveniences</a> to Procrastination</p>\n<p style=\"padding-left: 30px;\"><a href=\"/lw/fu/share_your_antiakrasia_tricks/ckx\">Close the Dang Browser</a>: Average +3.5 (2 reviews) (SD 3.5)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx8\">Get More Sleep</a>: Average +3 (2 reviews) (SD 1.4)</p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/cja\">Every Other Day Off</a>: Average +0.5 (2 reviews) (SD 0.7)</p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/ck2\">Strict Scheduling</a>: Average -9 (2 reviews) (SD 1.4)</p>\n<p>&nbsp;</p>\n<p><a href=\"http://en.wikipedia.org/wiki/The_4-Hour_Workweek\">Elimination (80/20 Rule)</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx3\">Methylphenidate</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1o2w\">Begin Now</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1ojp\">Learning to Say No</a>: +8 (1 review)</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Nap#The_caffeine_nap\">Caffeine Nap</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1oz8\">Write While Doing</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1ol4\">Leave Some Tasty Bits</a>: +7 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">Preserve the Mental State</a>: +6 (1 review)</p>\n<p><a href=\"http://www.amazon.com/Acedia-Me-Marriage-Monks-Writers/dp/1594489963\">Acedia and Me</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">Third Person Perspective</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx8\">Watching Others</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nrl\">Multiple Selves Theory</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1o2b\">Getting Back to the Music</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1ojp\">Remove Trivial Inconveniences</a>: +4 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx8\">Accountability</a>: +2 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1o2w\">Scheduling Aggressively...</a>: +2 (1 review)</p>\n<p><a href=\"http://www.markforster.net/blog/2009/9/5/preliminary-instructions-for-autofocus-v-4.html\">Autofocus</a>: 0 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1njs\">Take Every Other 20 to 40 Minutes Off</a>: -4 (1 review)</p>\n<p>&nbsp;</p>\n<p><strong>Not Yet Reviewed:</strong></p>\n<p><a href=\"http://www.joelonsoftware.com/articles/fog0000000339.html\">Fire and Motion</a></p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/cjx\">Stare at the Wall</a></p>\n<p><a href=\"/lw/am/how_a_pathological_procrastinor_can_lose_weight/\">Kibotzer</a></p>\n<p>&nbsp;</p>\n<p>Thanks for your data!</p>\n<p><strong>EDIT:</strong> People seem to enjoy throwing <em>really</em> low scores out there for things that just didn't work, had some negative side effects and annoyed them.&nbsp; I added \"-10 if it nearly killed you\" to give a sense of perspective on this bounded scale... although, looking at the comments, it looks like the -10 and -8 were pretty much justified after all.&nbsp; Anyway, here's your anchor for the negative side!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rRmisKb45dN7DK4BW", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 80, "extendedScore": null, "score": 0.000133, "legacy": true, "legacyId": "2326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 80, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I <a href=\"/lw/1rv/demands_for_particular_proof_appendices/1mqb\">recently had occasion</a> to review some of the akrasia tricks I've found on Less Wrong, and it occurred to me that there's probably quite a lot of others who've tried them as well.&nbsp; Perhaps it's a good idea to organize the experiences of a couple dozen procrastinating rationalists?</p>\n<p>Therefore, I'll aggregate any such data you provide in the comments, according to the following scheme:</p>\n<ol>\n<li><strong>Note which trick you've tried.</strong>&nbsp; If it's something that's not yet on the list below, please provide a link and I'll add it; if there's not a link for it anywhere, you can describe it in your comment and I'll link that.</li>\n<li><strong>Give your experience with it a score from -10 to +10</strong> (0 if it didn't change the status quo, 10 if it ended your akrasia problems forever with no side effects, negative scores if it actually made your life worse, -10 if it nearly killed you); if you don't do so, I'll suggest a score for you based on what else you say.</li>\n<li><strong>Describe your experience with it</strong>, including any significant side effects.</li>\n</ol>\n<p>Every so often, I'll combine all the data back into the main post, listing average scores, sample size and common effects for each technique.&nbsp; Ready?</p>\n<p><a id=\"more\"></a>Here's the list of specific akrasia tactics I've found around LW (and also in outside links from here); again, if I'm missing one, let me know and I'll add it.&nbsp; Special thanks to Vladimir Golovin for the <a href=\"/lw/fu/share_your_antiakrasia_tricks\">Share Your Anti-Akrasia Tricks</a> post.</p>\n<p>Without further ado, here are the results so far as I've recorded them, with average score, number of reviews, standard deviation and recurring comments.</p>\n<p>&nbsp;</p>\n<p><strong id=\"3_or_More_Reviews_\">3 or More Reviews:</strong></p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nts\">Collaboration with Others</a>: Average +7.7 (3 reviews) (SD 0.6)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1o2w\">No Multitasking</a>: Average +6.0 (3 reviews) (SD 2.0); note <a href=\"/lw/1sm/akrasia_tactics_review/1os8\">variants</a></p>\n<p>P.J. Eby's <a href=\"/lw/1sm/akrasia_tactics_review/1oh3\">Motivation Trilogy</a>: Average +5.8 (6 reviews) (SD 3.3)</p>\n<p style=\"padding-left: 30px;\"><a href=\"/lw/fh/willpower_hax_487_execute_by_default/c2k\">Monoidealism</a>: Average +8.0 (3 reviews) (SD 2.0)</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://dirtsimple.org/2006/07/hidden-meaning-of-just-do-it.html\">\"Just Do It\"</a>: Average +4 (2 reviews) (SD 4.2)</p>\n<p style=\"padding-left: 30px;\"><a href=\"/lw/1sm/akrasia_tactics_review/1o1d\">Irresistible Instant Motivation</a>: +3 (1 review)</p>\n<p><a href=\"http://en.wikipedia.org/wiki/GTD\">Getting Things Done</a>: Average +4.9 (7 reviews) (SD 2.6)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nu3\">Regular Exercise</a>: Average +4.4 (5 reviews) (SD 2.3)</p>\n<p>Cripple your Internet: Average +4.2 (11 reviews) (SD 3.0)</p>\n<p style=\"padding-left: 30px;\"><a href=\"https://addons.mozilla.org/en-US/firefox/addon/4476\">LeechBlock</a>: Average +5.4 (5 reviews) (SD 2.9); basically everyone who's tried has found it helpful.</p>\n<p style=\"padding-left: 30px;\"><a href=\"https://addons.mozilla.org/addon/pageaddict/\">PageAddict</a>: +3 (1 review)</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://macfreedom.com/\">Freedom</a> (Mac)</p>\n<p><a href=\"/lw/1lt/case_study_melatonin/\">Melatonin</a>: Average +4.0 (5 reviews) (SD 5.4); works well for some, others feel groggy the next day; might help to <a href=\"/lw/1sm/akrasia_tactics_review/1o5d\">vary the dosage</a></p>\n<p><a href=\"/lw/fh/willpower_hax_487_execute_by_default\">Execute by Default</a>: Average +3.7 (7 reviews) (SD 2.4); all sorts of variants; universally helpful, not typically a life-changer.</p>\n<p><a href=\"http://www.pomodorotechnique.com/\">Pomodoro Technique</a>: Average +3.3 (3 reviews) (SD 4.2); mathemajician <a href=\"/lw/1sm/akrasia_tactics_review/1nrv\">suggests a 45-minute variant</a></p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/ck0\">Being Watched</a>: Average +3.2 (6 reviews) (SD 4.1); variations like co-working seem more effective; see \"collaboration\" below</p>\n<p><a href=\"/lw/178/notes_on_utility_function_experiment\">Utility Function Experiment</a>: Average +2.8 (4 reviews) (SD 2.8)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx3\">Meditation</a>: Average +2.8 (5 reviews) (SD 2.8)</p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/cl7\">Modafinil and Equivalents</a>: Average -0.8 (5 reviews) (SD 8.5); fantastic for some, terrible for others.&nbsp; Seriously, look at that standard deviation!</p>\n<p><a href=\"/lw/1fe/antiakrasia_technique_structured_procrastination\">Structured Procrastination</a>: Average -1.0 (3 reviews) (SD 4.4); polarized opinion</p>\n<p><a href=\"/lw/ep/applied_picoeconomics\">Resolutions (Applied Picoeconomics)</a>: Average -3.2 (5 reviews) (SD 3.3); easy to fail &amp; get even more demotivated</p>\n<p>&nbsp;</p>\n<p><strong id=\"1_or_2_Reviews_\">1 or 2 Reviews:</strong></p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">Dual n-back</a>: Average +6.5 (2 reviews) (SD 2.1)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">Think It, Do It</a>: Average +6 (2 reviews) (SD 1.4)</p>\n<p><a href=\"/lw/eg/what_i_tell_you_three_times_is_true/\">Self-Affirmation</a>: Average +4 (2 reviews) (SD 2.8)</p>\n<p>Create <a href=\"/lw/f1/beware_trivial_inconveniences/\">Trivial Inconveniences</a> to Procrastination</p>\n<p style=\"padding-left: 30px;\"><a href=\"/lw/fu/share_your_antiakrasia_tricks/ckx\">Close the Dang Browser</a>: Average +3.5 (2 reviews) (SD 3.5)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx8\">Get More Sleep</a>: Average +3 (2 reviews) (SD 1.4)</p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/cja\">Every Other Day Off</a>: Average +0.5 (2 reviews) (SD 0.7)</p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/ck2\">Strict Scheduling</a>: Average -9 (2 reviews) (SD 1.4)</p>\n<p>&nbsp;</p>\n<p><a href=\"http://en.wikipedia.org/wiki/The_4-Hour_Workweek\">Elimination (80/20 Rule)</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx3\">Methylphenidate</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1o2w\">Begin Now</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1ojp\">Learning to Say No</a>: +8 (1 review)</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Nap#The_caffeine_nap\">Caffeine Nap</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1oz8\">Write While Doing</a>: +8 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1ol4\">Leave Some Tasty Bits</a>: +7 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">Preserve the Mental State</a>: +6 (1 review)</p>\n<p><a href=\"http://www.amazon.com/Acedia-Me-Marriage-Monks-Writers/dp/1594489963\">Acedia and Me</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nl4\">Third Person Perspective</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx8\">Watching Others</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nrl\">Multiple Selves Theory</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1o2b\">Getting Back to the Music</a>: +5 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1ojp\">Remove Trivial Inconveniences</a>: +4 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1nx8\">Accountability</a>: +2 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1o2w\">Scheduling Aggressively...</a>: +2 (1 review)</p>\n<p><a href=\"http://www.markforster.net/blog/2009/9/5/preliminary-instructions-for-autofocus-v-4.html\">Autofocus</a>: 0 (1 review)</p>\n<p><a href=\"/lw/1sm/akrasia_tactics_review/1njs\">Take Every Other 20 to 40 Minutes Off</a>: -4 (1 review)</p>\n<p>&nbsp;</p>\n<p><strong id=\"Not_Yet_Reviewed_\">Not Yet Reviewed:</strong></p>\n<p><a href=\"http://www.joelonsoftware.com/articles/fog0000000339.html\">Fire and Motion</a></p>\n<p><a href=\"/lw/fu/share_your_antiakrasia_tricks/cjx\">Stare at the Wall</a></p>\n<p><a href=\"/lw/am/how_a_pathological_procrastinor_can_lose_weight/\">Kibotzer</a></p>\n<p>&nbsp;</p>\n<p>Thanks for your data!</p>\n<p><strong>EDIT:</strong> People seem to enjoy throwing <em>really</em> low scores out there for things that just didn't work, had some negative side effects and annoyed them.&nbsp; I added \"-10 if it nearly killed you\" to give a sense of perspective on this bounded scale... although, looking at the comments, it looks like the -10 and -8 were pretty much justified after all.&nbsp; Anyway, here's your anchor for the negative side!</p>", "sections": [{"title": "3 or More Reviews:", "anchor": "3_or_More_Reviews_", "level": 1}, {"title": "1 or 2 Reviews:", "anchor": "1_or_2_Reviews_", "level": 1}, {"title": "Not Yet Reviewed:", "anchor": "Not_Yet_Reviewed_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "151 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 152, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["p9gtfDNup7sNjsMB8", "XYA9nBud8joDjTy86", "FHukyfMagq4HrBYNt", "A2XNfwqqqp8A5DZJn", "n5Yfhygz42QNK2vFe", "NjzBrtvDS4jXi5Krp", "B4AyJXYPpGbBmxQzd", "reitXJgJXFzKpdKyd", "Z6ESPufeiC4P8c8en"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-02-21T04:25:49.130Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-21T06:35:24.216Z", "modifiedAt": null, "url": null, "title": "Case study: abuse of frequentist statistics", "slug": "case-study-abuse-of-frequentist-statistics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:31.141Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XrxsFR2WLXWzgLEoy/case-study-abuse-of-frequentist-statistics", "pageUrlRelative": "/posts/XrxsFR2WLXWzgLEoy/case-study-abuse-of-frequentist-statistics", "linkUrl": "https://www.lesswrong.com/posts/XrxsFR2WLXWzgLEoy/case-study-abuse-of-frequentist-statistics", "postedAtFormatted": "Sunday, February 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Case%20study%3A%20abuse%20of%20frequentist%20statistics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACase%20study%3A%20abuse%20of%20frequentist%20statistics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrxsFR2WLXWzgLEoy%2Fcase-study-abuse-of-frequentist-statistics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Case%20study%3A%20abuse%20of%20frequentist%20statistics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrxsFR2WLXWzgLEoy%2Fcase-study-abuse-of-frequentist-statistics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrxsFR2WLXWzgLEoy%2Fcase-study-abuse-of-frequentist-statistics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 898, "htmlBody": "<p>Recently,&nbsp;a&nbsp;colleague&nbsp;was reviewing an article whose key justification rested on some statistics that seemed dodgy to him, so he came to me for advice. (I guess my boss, the resident statistician, was out of his office.)&nbsp;Now,&nbsp;I'm no expert in frequentist statistics. My formal schooling in frequentist statistics comes from my undergraduate chemical engineering curriculum -- I wouldn't rely on it for consulting. But I've been working for someone who is essentially a frequentist for a year and a half, so I've had some hands-on experience. My boss hired me on the strength of my experience with Bayesian statistics, which I taught myself in grad school, and one thing reading the Bayesian literature voraciously will equip you for is critiquing frequentist statistics.&nbsp;So I felt competent enough to take a look.<sup>1</sup></p>\r\n<p><a id=\"more\"></a></p>\r\n<p>The article compared an old, trusted experimental method with the authors' new method; the authors sought to show that the new method gave the same results on average as the trusted method. They performed three replicates using the trusted method and three&nbsp;replicates using&nbsp;the new method; each replicate generated a real-valued data point.&nbsp;They did this in nine different conditions, and for each condition, they did a <a href=\"http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#The_testing_process\">statistical hypothesis test</a>. (I'm going to lean heavily on Wikipedia&nbsp;for explanations of the jargon terms I'm&nbsp;using, so this post is actually a lot longer than it appears on the page. If you don't feel like following&nbsp;along, the punch line is three&nbsp;paragraphs down, last sentence.)&nbsp;</p>\r\n<p>The authors used what's called a <a href=\"http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U\">Mann-Whitney U test</a>, which, in simplified terms, aims to determine&nbsp;if two sets of data come from different distributions.&nbsp;The essential thing to know about this test is that it doesn't depend on the actual data except insofar as those data determine the ranks of the data points when the two data sets are combined. That is, it throws away most of the data, in the sense that data sets that generate the same ranking are equivalent under the test. The rationale for doing this&nbsp;is that it makes the test \"non-parametric\" -- you don't need to assume a&nbsp;particular form for the probability density when all you look at are the ranks.</p>\r\n<p>The output of a statistical hypothesis test is a <a href=\"http://en.wikipedia.org/wiki/P-value\">p-value</a>; one pre-establishes a threshold for statistical significance, and if the the p-value is lower than the threshold, one draws a certain conclusion called \"rejecting the <a href=\"http://en.wikipedia.org/wiki/Null_hypothesis\">null hypothesis</a>\".&nbsp;In the present case, the null hypothesis is that the old method and the new method produce&nbsp;data from the same distribution; the authors would like to see data that do not lead to rejection of the null hypothesis. They established the conventional threshold of 0.05, and for each of the nine conditions, they reported either \"p &gt; 0.05\" or \"p = 0.05\"<sup>2</sup>. Thus they did not reject the null hypothesis, and argued that the analysis supported their thesis.</p>\r\n<p>Now even from a frequentist perspective, this is wacky. Hypothesis testing can reject a null hypothesis, but cannot confirm it, as discussed in the first paragraph of the Wikipedia article on null hypotheses.&nbsp;But this is not the real WTF, <a href=\"http://thedailywtf.com/\">as they say</a>. There are&nbsp;twenty ways to <a href=\"http://en.wikipedia.org/wiki/Binomial_coefficient\">choose</a> three&nbsp;objects out of six, so there are only twenty possible p-values, and these can be computed even when the original data are not available, since they only depend on ranks. I&nbsp;put these facts together within a day of being presented with the analysis&nbsp;and quickly computed all twenty p-values. Here I only need discuss the most extreme case, where all three of the data points for the new method&nbsp;are to one side (either&nbsp;higher or lower)&nbsp;of the three&nbsp;data points for the trusted method. This case provides the most evidence against the notion that the&nbsp;two methods produce data from the same distribution, resulting in the smallest possible p-value<sup>3</sup>: p = 0.05. In other words, <em><strong>even before the data were collected it could have been known that this analysis would give the result the authors wanted</strong></em>.<sup>4</sup></p>\r\n<p>When I canvassed the Open Thread for interest in this article, Douglas Knight <a href=\"/lw/1s4/open_thread_february_2010_part_2/1nhn\">wrote</a>: \"If it's really frequentism that caused the problem, please spell this out.\" Frequentism <em>per se</em> is not the proximate cause of this problem, that being that the authors&nbsp;either never noticed that their analysis could not falsify their hypothesis, or they tried to pull a fast one. But it is a distal cause, in&nbsp;the sense that it forbids the Bayesian approach, and thus requires practitioners to become familiar with a grab-bag of unrelated methods for statistical inference<sup>5</sup>, leaving plenty of room for confusion and malfeasance.&nbsp;Technologos's reply to Douglas Knight&nbsp;got it exactly right; I almost jokingly requested a spoiler warning.</p>\r\n<p>&nbsp;</p>\r\n<p><sup>1</sup>&nbsp;I don't mind that it wouldn't be too hard to figure out who I am based on this paragraph. I just use a pseudonym to keep Google from indexing all my blog comments to my actual name.</p>\r\n<p><sup>2</sup> It's rather odd to report a p-value that is exactly equal to the significance threshold, one of many suspicious things about this analysis (the rest of which I've left out as they are not directly germane).</p>\r\n<p>3 For those anxious to check my math, I've omitted some blah blah blah about one- and two-sides tests and alternative hypotheses.</p>\r\n<p><sup>4</sup> I quickly emailed the reviewer; it didn't make much difference, because when we initially talked about the analysis we had noticed enough other flaws that he had decided to recommend rejection. This was just the icing on the coffin.</p>\r\n<p><sup>5</sup> ... none of which <em>actually address <strong>the question</strong></em><strong> <em>OF DIRECT INTEREST</em></strong>! ... phew. Sorry.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XrxsFR2WLXWzgLEoy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 38, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "2324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-21T08:19:58.587Z", "modifiedAt": null, "url": null, "title": "Woo!", "slug": "woo", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:28.321Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BenAlbahari", "createdAt": "2010-01-04T10:39:05.080Z", "isAdmin": false, "displayName": "BenAlbahari"}, "userId": "yHHBZZDtuWZw92SAf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mqHTmcPdwEacW2msC/woo", "pageUrlRelative": "/posts/mqHTmcPdwEacW2msC/woo", "linkUrl": "https://www.lesswrong.com/posts/mqHTmcPdwEacW2msC/woo", "postedAtFormatted": "Sunday, February 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Woo!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWoo!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqHTmcPdwEacW2msC%2Fwoo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Woo!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqHTmcPdwEacW2msC%2Fwoo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqHTmcPdwEacW2msC%2Fwoo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 395, "htmlBody": "<p><span style=\"color:red\">[MAJOR UPDATE: I have changed \"Woo\" to \"Pitch\" everywhere on the website and on this post due to extensive feedback from everyone. Thanks!]</span></p>\n<p>I'm adding rhetorical-device/common-argument/argument-fallacy tags to the expert quotes on TakeOnIt and calling them \"pitches\".</p>\n<p>The list of pitches so far is&nbsp;<a href=\"http://www.takeonit.com/pitches.aspx\">here</a>.</p>\n<p>Arguments have common patterns. The most notorious of these are rhetorical devices and argument fallacies. While these techniques are obviously not new and are published on several sites on the internet, they are woefully&nbsp;under appreciated&nbsp;by most people. I contend that this is partly because:</p>\n<ol>\n<li>Argument fallacies and rhetorical devices can be too general. Most of their real-world usage occurs in a larger number of specialized forms. These specialized forms are often unlabeled yet are intuitively recognized and prey on our cognitive biases. It takes a lot of cognitive energy to consciously connect the general form(s) to the specialized form.</li>\n<li>The sites about argument fallacies and rhetorical devices are not integrated with debate sites. A google for argument fallacies will give you pages with stagnant lists of fallacies where each one has perhaps a couple of historical or hypothetical applications of the fallacy. Why can't I see every debate where some expert or influential person used that fallacy, and why can't I see every fallacy used in a debate?</li>\n</ol>\n<p>To solve these problems, I'm introducing the concept of a \"pitch\". Any quote from an expert or influential person on TakeOnIt can now be tagged with a pitch. A pitch is a label for a commonly used argument or strategy to persuade. You can think of pitches as the \"<a href=\"http://tvtropes.org/\">tv tropes</a> of argumentation\". Here's some examples:</p>\n<p><a href=\"http://www.takeonit.com/pitch/the_consensus_pitch.aspx\">\"The Consensus Pitch\"</a>&nbsp;<br /><a href=\"http://www.takeonit.com/pitch/the_patriot_pitch.aspx\">\"The Patriot Pitch\"</a>&nbsp;<br /><a href=\"http://www.takeonit.com/pitch/the_convert_pitch.aspx\">\"The Convert Pitch\"</a>&nbsp;&nbsp;&nbsp;</p>\n<p>Pitches encompass both argument fallacies and rhetorical devices. However, they allow for greater specialization. For example, there is the \"The Evil Corporation Pitch\". On a more minor note, I personally think the names should be simple and ideally guessable from the name alone&nbsp;(e.g. maybe it's just me, but \"Post hoc ergo propter hoc\" feels like it has some Web 2.0 marketing issues).</p>\n<p>Eliezer's \"<a href=\"/lw/1p2/conversation_halters/\">Conversation Halters</a>\" and Robin Hanson's \"<a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">Contrarian Excuses</a>\" are good candidates for pitches. (My impression is the \"halters\" and \"excuses\" listed are perhaps too specialized for pitches, but in any case at minimum provide fertile material for pitches.)</p>\n<p>I only implemented this feature over the last few days and before developing the concept further I'd like to get some feedback.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mqHTmcPdwEacW2msC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 5.632037765444365e-07, "legacy": true, "legacyId": "2329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqmmv6NraYv4Xoeyj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-22T01:53:07.125Z", "modifiedAt": null, "url": null, "title": "Babies and Bunnies: A Caution About Evo-Psych", "slug": "babies-and-bunnies-a-caution-about-evo-psych", "viewCount": null, "lastCommentedAt": "2021-02-06T14:21:41.569Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u9pfbkZeG8mFTPNi2/babies-and-bunnies-a-caution-about-evo-psych", "pageUrlRelative": "/posts/u9pfbkZeG8mFTPNi2/babies-and-bunnies-a-caution-about-evo-psych", "linkUrl": "https://www.lesswrong.com/posts/u9pfbkZeG8mFTPNi2/babies-and-bunnies-a-caution-about-evo-psych", "postedAtFormatted": "Monday, February 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Babies%20and%20Bunnies%3A%20A%20Caution%20About%20Evo-Psych&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABabies%20and%20Bunnies%3A%20A%20Caution%20About%20Evo-Psych%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu9pfbkZeG8mFTPNi2%2Fbabies-and-bunnies-a-caution-about-evo-psych%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Babies%20and%20Bunnies%3A%20A%20Caution%20About%20Evo-Psych%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu9pfbkZeG8mFTPNi2%2Fbabies-and-bunnies-a-caution-about-evo-psych", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu9pfbkZeG8mFTPNi2%2Fbabies-and-bunnies-a-caution-about-evo-psych", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 678, "htmlBody": "<p>Daniel Dennett has <a href=\"http://www.ted.com/talks/dan_dennett_cute_sexy_sweet_funny.html\">advanced the opinion</a> that the evolutionary purpose of the cuteness response in humans is to make us respond positively to babies.&nbsp; This does seem plausible.&nbsp; Babies are pretty cute, after all.&nbsp; It's a tempting explanation.</p>\n<p><a href=\"http://fellowearthling.files.wordpress.com/2007/10/cute-baby.jpg\">Here</a> is one of the cutest baby pictures I found on a Google search.</p>\n<p>And <a href=\"http://elcenia.com/itemz/cutestbunny.jpg\">this</a> is a bunny.</p>\n<p>Correct me if I'm wrong, but the bunny is about 75,119 times cuter than the baby.</p>\n<p>Now, bunnies are not evolutionarily important for humans to like and want to nurture.&nbsp; In fact, bunnies are edible.&nbsp; By rights, my evolutionary response to the bunny should be \"mmm, needs a sprig of rosemary and thirty minutes on a spit\".&nbsp; But instead, that bunny - and not the baby or any other baby I've seen - strikes the epicenter of my cuteness response, and being more baby-like along any dimension would not improve the bunny.&nbsp; It would not look better bald.&nbsp; It would not be improved with little round humanlike ears.&nbsp; It would not be more precious with thumbs, easier to love if it had no tail, more adorable if it were enlarged to weigh about seven pounds.</p>\n<p>If \"awwww\" is a response designed to make me love human babies and everything else that makes me go \"awwww\" is a mere side effect of that engineered reaction, it is <em>drastically</em> misaimed.&nbsp; Other responses for which we have similar evolutionary psychology explanations don't seem badly targeted in this way.&nbsp; If they miss their supposed objects at all, at least it's not in most people.&nbsp; (Furries, for instance, exist, but they're not a <em>common</em> variation on human sexual interest - the most generally applicable superstimuli for sexiness look like at-least-superficially healthy, mature humans with prominent human sexual characteristics.)&nbsp; We've invested enough energy into transforming our food landscape that we can happily eat virtual poison, but that's a departure from the ancestral environment - bunnies?&nbsp; All natural, every whisker.<sup>1</sup></p>\n<p><a id=\"more\"></a></p>\n<p>It is <em>embarrassingly</em> easy to come up with evolutionary psychology stories to explain little segments of data and have it sound good to a surface understanding of how evolution works.&nbsp; Why are babies cute?&nbsp; They have to be, so we'll take care of them.&nbsp; And then someone with a slightly better cause and effect understanding turns it right-side-up, as Dennett has, and then it sounds <em>really</em> clever.&nbsp; You can have this entire conversation without mentioning bunnies (or kittens or jerboas or any other adorable thing).&nbsp; But by excluding those items from a discussion that is, ostensibly, about cuteness, you do not have a hypothesis that actually fits all of the data - only the data that seems relevant to the answer that presents itself immediately.</p>\n<p>Evo-psych explanations are tempting even when they're cheaply wrong, because the knowledge you need to construct ones that sound good to the educated is itself not cheap at all. You have to know lots of stuff about what \"motivates\" evolutionary changes, reject group selection, understand that the brain is just an organ, dispel the illusion of little XML tags attached to objects in the world calling them \"cute\" or \"pretty\" or anything else - but you also have to account for a decent proportion of the facts to not be steering completely left of reality.</p>\n<p>Humans are frickin' complicated beasties.&nbsp; It's a hard, hard job to model us in a way that says anything useful without contradicting information we have about ourselves.&nbsp; But that's no excuse for abandoning the task.&nbsp; What causes the cuteness response?&nbsp; Why is that bunny so outrageously adorable?&nbsp; Why are babies, well, pretty cute?&nbsp; I don't know - but I'm pretty sure it's not the cheap reason, because evolution doesn't want me to nurture bunnies.&nbsp; Inasmuch as it wants me to react to bunnies, it wants me to eat them, or at least be motivated to keep them away from my salad fixings.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>It is possible that the bunny depicted is a domestic specimen, but it doesn't look like it to me.&nbsp; In any event, I chose it for being a really great example; there are many decidedly wild animals that are also cuter than cute human babies.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"exZi6Bing5AiM4ZQB": 3, "Dw5Z6wtTgk4Fikz9f": 1, "R6uagTfhhBeejGrrf": 1, "kEX5CzbfiAzGn4q8B": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u9pfbkZeG8mFTPNi2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 103, "baseScore": 81, "extendedScore": null, "score": 0.000142, "legacy": true, "legacyId": "2332", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 81, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 844, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-24T05:53:34.133Z", "modifiedAt": null, "url": null, "title": "\"Outside View!\" as Conversation-Halter", "slug": "outside-view-as-conversation-halter", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:15.683Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter", "pageUrlRelative": "/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter", "linkUrl": "https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter", "postedAtFormatted": "Wednesday, February 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Outside%20View!%22%20as%20Conversation-Halter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Outside%20View!%22%20as%20Conversation-Halter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFsfnDfADftGDYeG4c%2Foutside-view-as-conversation-halter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Outside%20View!%22%20as%20Conversation-Halter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFsfnDfADftGDYeG4c%2Foutside-view-as-conversation-halter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFsfnDfADftGDYeG4c%2Foutside-view-as-conversation-halter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2170, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/ri/the_outside_views_domain/\">The Outside View's Domain</a>, <a href=\"/lw/1p2/conversation_halters/\">Conversation Halters</a><br /><strong>Reply to</strong>:&nbsp; <a href=\"/lw/1lx/reference_class_of_the_unclassreferenceable/\">Reference class of the unclassreferenceable</a></p>\n<p>In \"<a href=\"/lw/1p2/conversation_halters/\">conversation halters</a>\", I pointed out a number of arguments which are particularly pernicious, not just because of their inherent flaws, but because they attempt to chop off further debate - an \"argument stops here!\" traffic sign, with some implicit penalty (at least in the mind of the speaker) for trying to continue further.</p>\n<p>This is not the <em>right</em> traffic signal to send, unless the state of knowledge is such as to make an actual halt a good idea.&nbsp; Maybe if you've got a replicable, replicated series of experiments that squarely target the issue and settle it with strong significance and large effect sizes (or great power and null effects), you could say, \"Now we know.\"&nbsp; Or if the other is blatantly <a href=\"/lw/19m/privileging_the_hypothesis/\">privileging the hypothesis</a> - starting with something improbable, and offering no positive evidence to believe it - then it may be time to throw up hands and walk away.&nbsp; (Privileging the hypothesis is the state people tend to be driven to, when they start with a bad idea and then witness the defeat of all the positive arguments they thought they had.)&nbsp; Or you could simply run out of time, but then you just say, \"I'm out of time\", not \"here the gathering of arguments should end.\"</p>\n<p>But there's also another <em>justification for ending argument-gathering </em>that has recently seen some advocacy on Less Wrong.</p>\n<p>An experimental group of subjects were asked to describe highly specific plans for their Christmas shopping:&nbsp; Where, when, and how.&nbsp; On average, this group expected to finish shopping more than a week before Christmas.&nbsp; Another group was simply asked when they expected to finish their Christmas shopping, with an average response of 4 days.&nbsp; Both groups finished an average of 3 days before Christmas.&nbsp; Similarly, Japanese students who expected to finish their essays 10 days before deadline, actually finished 1 day before deadline; and when asked when they had previously completed similar tasks, replied, \"1 day before deadline.\"&nbsp; (See <a href=\"/lw/jg/planning_fallacy/\">this post</a>.)</p>\n<p>Those and similar experiments seem to show us a class of cases where you can do better by asking a certain specific question <em>and then halting:</em>&nbsp; Namely, the students could have produced better estimates by asking themselves \"When did I finish last time?\" <em>and then ceasing to consider further arguments,</em> without trying to take into account the specifics of where, when, and how they expected to do better than last time.</p>\n<p>From this we learn, <em>allegedly</em>, that \"the 'outside view' is better than the 'inside view'\"; from which it follows that when you're faced with a difficult problem, you should find a reference class of similar cases, use that as your estimate, and deliberately not take into account any arguments about specifics.&nbsp; But this <em>generalization</em>, I fear, is somewhat more questionable...<a id=\"more\"></a></p>\n<p>For example, <a href=\"/lw/1lx/reference_class_of_the_unclassreferenceable/\">taw alleged upon this very blog</a> that belief in the 'Singularity' (a term I usually take to refer to the <a href=\"http://yudkowsky.net/singularity/schools\">intelligence explosion</a>) ought to be dismissed out of hand, because it is part of the reference class \"beliefs in coming of a new world, be it good or evil\", with a historical success rate of (allegedly) 0%.</p>\n<p>Of course <a href=\"http://www.overcomingbias.com/2008/06/singularity-out.html\">Robin Hanson has a different idea of what constitutes the reference class and so makes a rather different prediction</a> - a problem I refer to as \"reference class tennis\":</p>\n<blockquote>\n<p>Taking a long historical long view, <a href=\"http://www.overcomingbias.com/2008/06/economics-of-si.html\">we see</a> steady total growth rates punctuated by rare transitions when new faster growth modes appeared with little warning.&nbsp; We know of perhaps four such \"singularities\": animal brains (~600MYA), humans (~2MYA), farming (~1OKYA), and industry (~0.2KYA)...</p>\n<p>Excess inside viewing usually continues even after folks are warned that outside viewing works better; after all, inside viewing better show offs inside knowledge and abilities.&nbsp; People usually justify this via reasons why the current case is exceptional.&nbsp; (Remember how all the old rules didn&rsquo;t apply to the new dotcom economy?)&nbsp; So expect to hear excuses why the next singularity is also an exception where outside view estimates are misleading.&nbsp; Let&rsquo;s keep an open mind, but a wary open mind.</p>\n</blockquote>\n<p>If I were to play the game of reference class tennis, I'd put <a href=\"/lw/we/recursive_selfimprovement/\">recursively self-improving AI</a> in the reference class \"huge mother#$%@ing changes in the nature of the optimization game\" whose other two instances are the <a href=\"/lw/w0/the_first_world_takeover/\">divide between life and nonlife</a> and the <a href=\"/lw/w4/surprised_by_brains/\">divide between human design and evolutionary design</a>; and I'd draw the lesson \"If you try to predict that things will just go on sorta the way they did before, you are going to end up looking <a href=\"/lw/w4/surprised_by_brains/\">pathetically overconservative</a>\".</p>\n<p>And if we do have a local hard takeoff, as I predict, then there will be nothing to say afterward except \"This was similar to the origin of life and dissimilar to the invention of agriculture\".&nbsp; And if there is a nonlocal economic acceleration, as Robin Hanson predicts, we just say \"This was similar to the invention of agriculture and dissimilar to the origin of life\".&nbsp; And if nothing happens, as taw seems to predict, then we must say \"The whole foofaraw was similar to the apocalypse of Daniel, and dissimilar to the origin of life or the invention of agriculture\".&nbsp; This is why I don't like reference class tennis.</p>\n<p>But mostly I would simply <a href=\"/lw/ri/the_outside_views_domain/\">decline to reason by analogy</a>, preferring to drop back into causal reasoning in order to make <a href=\"/lw/vz/the_weak_inside_view/\">weak,<em> </em>vague predictions</a>.&nbsp; In the end, the dawn of recursive self-improvement is <em>not</em> the dawn of life and it is <em>not</em> the dawn of human intelligence, it is the dawn of recursive self-improvement.&nbsp; And it's not the invention of agriculture either, and I am not the prophet Daniel.&nbsp; Point out a \"similarity\" with this many differences, and reality is liable to respond \"So what?\"</p>\n<p>I sometimes say that the fundamental question of rationality is \"Why do you believe what you believe?\" or \"What do you think you know and how do you think you know it?\"</p>\n<p>And when you're asking a question like that, one of the most useful tools is zooming in on the map by <a href=\"/lw/nv/replace_the_symbol_with_the_substance/\">replacing</a> summary-phrases with the concepts and chains of inferences that they stand for.</p>\n<p>Consider what inference we're actually carrying out, when we cry \"Outside view!\" on a case of a student turning in homework.&nbsp; How do we think we know what we believe?</p>\n<p>Our information looks something like this:</p>\n<ul>\n<li>In January 2009, student X<sub>1</sub> predicted they would finish their homework 10 days before deadline, and actually finished 1 day before deadline.</li>\n<li>In February 2009, student X<sub>1</sub> predicted they would finish their homework 9 days before deadline, and actually finished 2 days before deadline.</li>\n<li>In March 2009, student X<sub>1</sub> predicted they would finish their homework 9 days before deadline, and actually finished 1 day before deadline.</li>\n<li>In January 2009, student X<sub>2</sub> predicted they would finish their homework 8 days before deadline, and actually finished 2 days before deadline.</li>\n<li>And so on through 157 other cases.</li>\n<li>Furthermore, in another 121 cases, asking students to visualize specifics actually made them <em>more</em> optimistic.</li>\n</ul>\n<p><em>Therefore,</em> when new student X<sub>279</sub> comes along, even though we've never actually tested <em>them </em>before, we ask:</p>\n<p>\"How long before deadline did you plan to complete your last three assignments?\"</p>\n<p>They say:&nbsp; \"10 days, 9 days, and 10 days.\"</p>\n<p>We ask:&nbsp; \"How long before did you actually complete them?\"</p>\n<p>They reply:&nbsp; \"1 day, 1 day, and 2 days\".</p>\n<p>We ask:&nbsp; \"How long before deadline do you plan to complete <em>this</em> assignment?\"</p>\n<p>They say:&nbsp; \"8 days.\"</p>\n<p>Having <em>gathered this information,</em> we now <em>think we know enough </em>to make this prediction:</p>\n<p>\"You'll probably finish 1 day before deadline.\"</p>\n<p>They say:&nbsp; \"No, this time will be different because -\"</p>\n<p>We say:&nbsp; \"Would you care to make a side bet on that?\"</p>\n<p>We now believe that previous cases have given us <em>strong, veridical information </em>about how this student functions - how long before deadline they tend to complete assignments - <em>and </em>about the <em>unreliability </em>of the student's planning attempts, as well.&nbsp; The chain of \"What do you think you know and how do you think you know it?\" is clear and strong, both with respect to the prediction, and with respect to ceasing to gather information.&nbsp; We have historical cases aplenty, and they are all as similar to each other as they are similar to this new case.&nbsp; We might not know all the details of how the inner forces work, but we suspect that it's pretty much the <em>same </em>inner forces inside the black box each time, or the same rough <em>group </em>of inner forces, varying no more in this new case than has been observed on the previous cases that are as similar to each other as they are to this new case, selected by no different a criterion than we used to select this new case.&nbsp; And so we think it'll be the same outcome all over again.</p>\n<p>You're just drawing another ball, at random, from the same barrel that produced a lot of similar balls in previous random draws, and those previous balls told you a lot about the barrel.&nbsp; Even if your estimate is a probability distribution rather than a point mass, it's a solid, stable probability distribution based on plenty of samples from a process that is, if not independent and identically distributed, still pretty much blind draws from the same big barrel.</p>\n<p>You've got <em>strong</em> information, and it's not that strange to think of stopping and making a prediction.</p>\n<p>But now consider the analogous chain of inferences, the <em>what do you think you know and how do you think you know it,</em> of trying to take an outside view on self-improving AI.</p>\n<p>What is our data?&nbsp; Well, <a href=\"http://hanson.gmu.edu/longgrow.html\">according</a> to Robin Hanson:</p>\n<ul>\n<li>Animal brains showed up in 550M BC and doubled in size every 34M years</li>\n<li>Human hunters showed up in 2M BC, doubled in population every 230Ky</li>\n<li>Farmers, showing up in 4700BC, doubled every 860 years</li>\n<li>Starting in 1730 or so, the economy started doubling faster, from 58 years in the beginning to a 15-year approximate doubling time now.</li>\n</ul>\n<p>From this, Robin extrapolates, the next big growth mode will have a doubling time of 1-2 weeks.</p>\n<p>So far we have an interesting argument, though I wouldn't really buy it myself, because the distances of difference are too large... but in any case, Robin then <em>goes on </em>to say:&nbsp; We should accept this estimate <em>flat,</em> we have probably just gathered all the evidence we should use.&nbsp; Taking into account other arguments... well, there's something to be said for considering them, keeping an open mind and all that; but if, foolishly, we <a href=\"/lw/ib/the_proper_use_of_doubt/\">actually accept</a> those arguments, our estimates will probably get worse.&nbsp; We might be tempted to try and adjust the estimate Robin has given us, but we should resist that temptation, since it comes from a desire to show off insider knowledge and abilities.</p>\n<p>And how do we know <em>that?</em>&nbsp; How do we know this much more interesting proposition that it is <em>now </em>time to stop and make an estimate - that Robin's facts were the relevant arguments, and that other arguments, especially attempts to think about the interior of an AI undergoing recursive self-improvement, are not relevant?</p>\n<p>Well... because...</p>\n<ul>\n<li>In January 2009, student X<sub>1</sub> predicted they would finish their homework 10 days before deadline, and actually finished 1 day before deadline.</li>\n<li>In February 2009, student X<sub>1</sub> predicted they would finish their homework 9 days before deadline, and actually finished 2 days before deadline.</li>\n<li>In March 2009, student X<sub>1</sub> predicted they would finish their homework 9 days before deadline, and actually finished 1 day before deadline.</li>\n<li>In January 2009, student X<sub>2</sub> predicted they would finish their homework 8 days before deadline, and actually finished 2 days before deadline...</li>\n</ul>\n<p>It seems to me that once you subtract out the scary labels \"inside view\" and \"outside view\" and look at what is actually being inferred from what - ask \"What do you think you know and how do you think you know it?\" - that it doesn't really follow very well.&nbsp; The Outside View that experiment has shown us works better than the Inside View, is pretty far removed from the \"Outside View!\" that taw cites in support of predicting against any epoch.&nbsp; My own similarity metric puts the latter closer to the <a href=\"/lw/ri/the_outside_views_domain/\">analogies of Greek philosophers</a>, actually.&nbsp; And I'd also say that trying to use causal reasoning to produce weak, vague, qualitative predictions like \"Eventually, some AI will go FOOM, locally self-improvingly rather than global-economically\" is a bit different from \"I will complete this homework assignment 10 days before deadline\".&nbsp; (The <a href=\"/lw/vz/the_weak_inside_view/\">Weak Inside View</a>.)</p>\n<p>I don't think that \"Outside View!&nbsp; Stop here!\" is a good cognitive traffic signal to use so far beyond the realm of homework - or other cases of many draws from the same barrel, no more dissimilar to the next case than to each other, and with similarly structured forces at work in each case.</p>\n<p>After all, the wider reference class of cases of telling people to <em>stop gathering arguments</em>, is one of which we should all be wary...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 2, "rWzGNdjuep56W5u2d": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FsfnDfADftGDYeG4c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 88, "extendedScore": null, "score": 0.000146, "legacy": true, "legacyId": "2201", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 88, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pqoxE3AGMbse68dvb", "wqmmv6NraYv4Xoeyj", "yKRTxAohwmqLcbbJi", "X2AD2LgtKgkRNPj2a", "CPm5LTwHrvBJCa9h5", "JBadX7rwdcRFzGuju", "spKYZgoh3RmhxMqyu", "XQirei3crsLxsCQoi", "w9KWNWFTXivjJ7rjF", "GKfPL6LQFgB49FEnv", "43PTNr4ZMaezyAJ5o"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 16, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-26T03:46:27.287Z", "modifiedAt": null, "url": null, "title": "The Last Days of the Singularity Challenge", "slug": "the-last-days-of-the-singularity-challenge", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:30.735Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xZoegLcqmsMP6awXY/the-last-days-of-the-singularity-challenge", "pageUrlRelative": "/posts/xZoegLcqmsMP6awXY/the-last-days-of-the-singularity-challenge", "linkUrl": "https://www.lesswrong.com/posts/xZoegLcqmsMP6awXY/the-last-days-of-the-singularity-challenge", "postedAtFormatted": "Friday, February 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Last%20Days%20of%20the%20Singularity%20Challenge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Last%20Days%20of%20the%20Singularity%20Challenge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxZoegLcqmsMP6awXY%2Fthe-last-days-of-the-singularity-challenge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Last%20Days%20of%20the%20Singularity%20Challenge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxZoegLcqmsMP6awXY%2Fthe-last-days-of-the-singularity-challenge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxZoegLcqmsMP6awXY%2Fthe-last-days-of-the-singularity-challenge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p><em>From Michael Anissimov on the Singularity Institute blog:</em></p>\n<p>Thanks to generous contributions by our donors, we are only $11,840 away  from fulfilling our $100,000 goal for the <a href=\"http://intelligence.org/challenge\">2010 Singularity Research  Challenge</a>.  For every dollar you contribute to SIAI, another dollar  is contributed by our matching donors, who have pledged to match all  contributions made before February 28th up to $100,000.  That means that  this <strong>Sunday</strong> is your final chance to donate for maximum  impact.</p>\n<p>Funds from the challenge campaign will be used to support all SIAI  activities: our <a href=\"http://intelligence.org/aboutus/team\">core  staff</a>, the <a href=\"http://www.vimeo.com/siai/videos/sort:oldest\">Singularity  Summit</a>, the <a href=\"http://intelligence.org/aboutus/visitingfellows\">Visiting Fellows  program</a>, and <a href=\"http://intelligence.org/accomplishments2009\">more</a>.   Donors can earmark their funds for specific <a href=\"http://intelligence.org/grants/challenge#grantproposals\">grant  proposals</a>, many of which are targeted towards academic  paper-writing, or just contribute to our general fund</p>\n<p><em>[<a href=\"http://intelligence.org/blog/2010/02/25/last-chance-to-contribute-to-2010-singularity-research-challenge/\">Continue reading at the Singularity Institute blog.</a>]</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z6DgiCrMtpSNxwuYW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xZoegLcqmsMP6awXY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 24, "extendedScore": null, "score": 5.645243161108682e-07, "legacy": true, "legacyId": "2363", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-26T07:43:53.375Z", "modifiedAt": null, "url": null, "title": "What is Bayesianism?", "slug": "what-is-bayesianism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:27.881Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AN2cBr6xKWCB8dRQG/what-is-bayesianism", "pageUrlRelative": "/posts/AN2cBr6xKWCB8dRQG/what-is-bayesianism", "linkUrl": "https://www.lesswrong.com/posts/AN2cBr6xKWCB8dRQG/what-is-bayesianism", "postedAtFormatted": "Friday, February 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20Bayesianism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20Bayesianism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAN2cBr6xKWCB8dRQG%2Fwhat-is-bayesianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20Bayesianism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAN2cBr6xKWCB8dRQG%2Fwhat-is-bayesianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAN2cBr6xKWCB8dRQG%2Fwhat-is-bayesianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1250, "htmlBody": "<p><em>This article is an attempt to <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">summarize basic material</a>, and thus probably won't have anything new for the hard core posting crowd. It'd be interesting to know whether you think there's anything essential I missed, though.</em></p>\n<p>You've probably seen the word 'Bayesian' used a lot on this site, but may be a bit uncertain of what exactly we mean by that. You may have read the <a href=\"http://www.yudkowsky.net/rational/bayes/\">intuitive explanation</a>, but that only seems to explain a certain math formula. There's a <a href=\"http://wiki.lesswrong.com/wiki/Bayesian\">wiki entry about \"Bayesian\"</a>, but that doesn't help much. And the LW usage seems different from just the \"Bayesian and frequentist statistics\" thing, too. As far as I can tell, there's no article explicitly defining what's meant by Bayesianism. The core ideas are sprinkled across a large amount of posts, 'Bayesian' <a href=\"/tag/bayesian/\">has its own tag</a>, but there's not a single post that explicitly comes out to make the connections and say \"<strong>this</strong> is Bayesianism\". So let me try to offer my definition, which boils Bayesianism down to three core tenets.<br /><br />We'll start with a brief example, illustrating Bayes' theorem. Suppose you are a doctor, and a patient comes to you, complaining about a headache. Further suppose that there are two reasons for why people get headaches: they might have a brain tumor, or they might have a cold. A brain tumor always causes a headache, but exceedingly few people have a brain tumor. In contrast, a headache is rarely a symptom for cold, but most people manage to catch a cold every single year. Given no other information, do you think it more likely that the headache is caused by a tumor, or by a cold?<br /><br />If you thought a cold was more likely, well, that was the answer I was after. Even if a brain tumor caused a headache every time, and a cold caused a headache only one per cent of the time (say), having a cold is so much more common that it's going to cause a lot more headaches than brain tumors do. Bayes' theorem, basically, says that if cause A might be the reason for symptom X, then we have to take into account <em>both</em> the probability that A caused X (found, roughly, by multiplying the frequency of A with the chance that A causes X) <em>and</em> the probability that <em>anything else</em> caused X. (For a thorough mathematical treatment of Bayes' theorem, see Eliezer's <a href=\"http://www.yudkowsky.net/rational/bayes/\">Intuitive Explanation</a>.)<br /><a id=\"more\"></a><br />There should be nothing surprising about that, of course. Suppose you're outside, and you see a person running. They might be running for the sake of exercise, or they might be running because they're in a hurry somewhere, or they might even be running because it's cold and they want to stay warm. To figure out which one is the case, you'll try to consider which of the explanations is true most often, and fits the circumstances best.<br /><br /><strong>Core tenet 1:</strong> Any given observation has many different possible causes.<br /><br />Acknowledging this, however, leads to a somewhat less intuitive realization. For any given observation, how you should interpret it <em>always</em> depends on previous information. Simply seeing that the person was running wasn't enough to tell you that they were in a hurry, or that they were getting some exercise. Or suppose you had to choose between two competing scientific theories about the motion of planets. A theory about the laws of physics governing the motion of planets, devised by Sir Isaac Newton, or a theory simply stating that the Flying Spaghetti Monster pushes the planets forwards with His Noodly Appendage. If these both theories made the same predictions, you'd have to depend on your prior knowledge - your <a href=\"/lw/hk/priors_as_mathematical_objects/\">prior</a>, for short - to judge which one was more likely. And even if they didn't make the same predictions, you'd need some prior knowledge that told you which of the predictions were better, or that the predictions matter in the first place (as opposed to, say, theoretical elegance).</p>\n<p>Or take the debate we had on 9/11 conspiracy theories. Some people thought that unexplained and otherwise suspicious things in the official account had to mean that it was a government conspiracy. Others considered their prior for \"the government is ready to conduct massively risky operations that kill thousands of its own citizens as a publicity stunt\", judged that to be overwhelmingly unlikely, and thought it far more probable that something else caused the suspicious things.</p>\n<p>Again, this might seem obvious. But there are many well-known instances in which people forget to apply this information. Take <a href=\"/lw/gv/outside_the_laboratory/\">supernatural phenomena</a>: yes, if there were spirits or gods influencing our world, some of the things people experience would certainly be the kinds of things that supernatural beings cause. But then there are also countless of mundane explanations, from coincidences to mental disorders to an overactive imagination, that could cause them to perceived. Most of the time, postulating a supernatural explanation <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">shouldn't even occur to you</a>, because the mundane causes already have lots of evidence in their favor and supernatural causes have none.</p>\n<p><strong>Core tenet 2:</strong> How we interpret <em>any</em> event, and the new information we get from anything, depends on information we <em>already</em> had.<br /><br /><em>Sub-tenet 1:</em> If you experience something that you think could only be caused by cause A, ask yourself \"if this cause didn't exist, would I regardless expect to experience this with equal probability?\" If the answer is \"yes\", then it probably wasn't cause A.<br /><br />This realization, in turn, leads us to<br /><br /><strong>Core tenet 3:</strong> We can use the concept of probability to measure our <a href=\"http://wiki.lesswrong.com/wiki/Bayesian_probability\">subjective belief in something</a>. Furthermore, we can apply the mathematical laws regarding probability to choosing between different beliefs. If we want our beliefs to be correct, we <a href=\"/lw/k1/no_one_can_exempt_you_from_rationalitys_laws/\"><em>must</em></a> do so.<br /><br />The fact that anything can be caused by an infinite amount of things explains why Bayesians are so strict about the theories they'll endorse. It isn't enough that a theory explains a phenomenon; if it can explain <em>too many</em> things, <a href=\"/lw/if/your_strength_as_a_rationalist/\">it isn't a good theory</a>. Remember that if you'd expect to experience something even when your supposed cause was untrue, then that's no evidence for your cause. Likewise, if a theory can explain anything you see - if the theory allowed any possible event - then nothing you see can be <a href=\"/lw/jl/what_is_evidence/\">evidence</a> for the theory.<br /><br />At its heart, Bayesianism isn't anything more complex than this: a mindset that takes three core tenets fully into account. Add a sprinkle of idealism: a perfect Bayesian is someone who processes all information perfectly, and always arrives at the best conclusions that can be drawn from the data. When we talk about Bayesianism, that's the ideal we aim for.</p>\n<p>Fully internalized, that mindset does tend to color your thought in its own, peculiar way. Once you realize that all the beliefs you have today are based - in a mechanistic, lawful fashion - on the beliefs you had yesterday, which were based on the beliefs you had last year, which were based on the beliefs you had as a child, which were based on the assumptions about the world that were embedded in your brain while you were growing in your mother's womb... it does make you question your beliefs more. Wonder about whether all of those previous beliefs really corresponded maximally to reality.</p>\n<p>And that's basically what this site is for: to help us become good Bayesians.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2, "EewHHv3ewvQ3mqbyb": 2, "bh7uxTTqmsQ8jZJdB": 2, "Ng8Gice9KNkncxqcj": 2, "4fxcbJ8xSv4SAYkkx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AN2cBr6xKWCB8dRQG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 101, "baseScore": 111, "extendedScore": null, "score": 0.000186, "legacy": true, "legacyId": "2364", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 112, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 217, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2TPph4EGZ6trEbtku", "jzf4Rcienrm6btRyt", "N2pENnTPB75sfc9kb", "eY45uCCX7DdwJ4Jha", "5JDkW4MYXit2CquLs", "6s3xABaXKPdFwA3FS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-26T11:48:36.092Z", "modifiedAt": null, "url": null, "title": "Creating a Less Wrong prediction market", "slug": "creating-a-less-wrong-prediction-market", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:39.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J9WR5YDQp4zkWBS3t/creating-a-less-wrong-prediction-market", "pageUrlRelative": "/posts/J9WR5YDQp4zkWBS3t/creating-a-less-wrong-prediction-market", "linkUrl": "https://www.lesswrong.com/posts/J9WR5YDQp4zkWBS3t/creating-a-less-wrong-prediction-market", "postedAtFormatted": "Friday, February 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Creating%20a%20Less%20Wrong%20prediction%20market&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACreating%20a%20Less%20Wrong%20prediction%20market%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9WR5YDQp4zkWBS3t%2Fcreating-a-less-wrong-prediction-market%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Creating%20a%20Less%20Wrong%20prediction%20market%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9WR5YDQp4zkWBS3t%2Fcreating-a-less-wrong-prediction-market", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9WR5YDQp4zkWBS3t%2Fcreating-a-less-wrong-prediction-market", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p><a href=\"/lw/1ss/babies_and_bunnies_a_caution_about_evopsych/1o7c\">I will&nbsp;bet 500 karma that a funny picture thread will appear on Less Wrong within one year.</a>&nbsp;If anyone is interested in the bet, we can better define terms.</p>\n<p>Right now the LW software doesn't support karma transfers. Until it does and we can develop a more robust prediction market, let's just record the karma transfers on <a href=\"http://wiki.lesswrong.com/wiki/Bets_registry\">the wiki page that already exists for this purpose</a>.</p>\n<p>I will also give 100 karma to anyone that donates $10 to the SIAI before the <a href=\"/lw/1tn/the_last_days_of_the_singularity_challenge/\">current fundraising campaign</a> is over.</p>\n<p>10,000 karma for the first person with a karma transfer source code patch?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J9WR5YDQp4zkWBS3t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 9, "extendedScore": null, "score": 5.646164285507687e-07, "legacy": true, "legacyId": "2367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xZoegLcqmsMP6awXY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-26T20:45:19.942Z", "modifiedAt": null, "url": null, "title": "Improving The Akrasia Hypothesis", "slug": "improving-the-akrasia-hypothesis", "viewCount": null, "lastCommentedAt": "2015-02-15T20:22:28.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pjeby", "createdAt": "2009-02-27T23:51:22.854Z", "isAdmin": false, "displayName": "pjeby"}, "userId": "Zzxr5JZpkitaNxL4Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uKoqrgnRoWjhneDvM/improving-the-akrasia-hypothesis", "pageUrlRelative": "/posts/uKoqrgnRoWjhneDvM/improving-the-akrasia-hypothesis", "linkUrl": "https://www.lesswrong.com/posts/uKoqrgnRoWjhneDvM/improving-the-akrasia-hypothesis", "postedAtFormatted": "Friday, February 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Improving%20The%20Akrasia%20Hypothesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImproving%20The%20Akrasia%20Hypothesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuKoqrgnRoWjhneDvM%2Fimproving-the-akrasia-hypothesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Improving%20The%20Akrasia%20Hypothesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuKoqrgnRoWjhneDvM%2Fimproving-the-akrasia-hypothesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuKoqrgnRoWjhneDvM%2Fimproving-the-akrasia-hypothesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2555, "htmlBody": "<p><em>Abstract: This article proposes a hypothesis that effective anti-akrasia methods operate by reducing or eliminating the activation of conflicting voluntary motor programs at the time the user's desired action is to be carried out, or by reducing or eliminating the negative effects of managing the conflict.&nbsp; This hypothesis is consistent with the notion of \"ego depletion\" (willpower burnout) being driven by the need to consciously manage conflicting motor programs.&nbsp; It also supports a straightforward explanation of why different individuals will fare better with some anti-akrasia methods than others, and provides a framework for both classifying existing methods, and generating new ones.&nbsp; Finally, it demonstrates why no single technique can be a panacea, and shows how the common problems of certain methods shape the form of both the self-help industry, and most people's experiences with it.</em></p>\n<h4>The Hypothesis</h4>\n<p>Recently, <a href=\"/user/orthonormal\">orthonormal</a> posted an <a href=\"/lw/1sm/akrasia_tactics_review\">Akrasia Tactics Review</a>, collecting data from LessWrong members on their results using different anti-akrasia techniques.&nbsp; And although I couldn't quite put my finger on it at first, something about the review (and the discussion around it) was <em>bothering</em> me.</p>\n<p>See, I've never been fond of the idea that \"different things work for different people\".&nbsp; As a predictive hypothesis, after all, this is only <em>slightly</em> more useful than saying \"a wizard did it\".&nbsp; It says nothing about how (or why) different things work, and therefore gives you no basis to select <em>which</em> different things might work for which different people.</p>\n<p>For that reason, it kind of bugs me whenever I see discussion and advocacy of \"different things\", independent of any framework for classifying those things in a way that would help \"different people\" select or design the \"different things\" that would \"work for\" them.&nbsp; (In fact, this is a pretty big factor in why I'm a self-help writer/speaker in the first place!)</p>\n<p>So in this post, I want to share two slightly better working hypotheses for akrasia technique classification than \"different things work for different people\":<a id=\"more\"></a></p>\n<ol>\n<li>\n<p>Akrasia happens when there are conflicting active voluntary motor programs, and the opposite of akrasia is either the <em>absence<strong> </strong></em>of such conflict, or a <em>manageable quantity</em> of it.</p>\n</li>\n<li>The nature and source of the specific motor program conflicts are individual, so the effectiveness of a given anti-akrasia technique will be determined by an individual's <em>specific sources of conflict</em> </li>\n</ol>\n<p>Accepting these as working hypotheses allows us to quickly develop a classification scheme for anti-akrasia techniques, based on what part of the problem they work on.</p>\n<h4>A Classification Scheme</h4>\n<p>For example, <strong>hygenic/systemic methods</strong> such as exercise, nutrition, drugs, and meditation, all act to improve one's ability to manage conflict, attempting to reduce or minimize ego depletion: either by improving one's capacity, or, in the case of meditation, by developing more efficient conflict-management capability (so that ego depletion occurs more slowly or not at all).&nbsp; The effectiveness of these methods will then depend on the amount and type of conflict to be managed, and will most likely be effective in cases where the sources of conflict are many or frequent, but the <em>intensity</em> of any given conflict is low.</p>\n<p><strong>Focusing methods</strong>, such as Getting Things Done and the Pomodoro Technique, operate on a principle of helping people to let go of thinking about other tasks, even though they may vary widely in <em>how</em> they go about this.&nbsp; Restricting internet access, taking vows, or simply \"deciding not to do anything else\" are also within this class.&nbsp; The motor-program conflict hypothesis predicts that the effectiveness of each of these techniques will vary widely between individuals, depending on whether it addresses the actual source of their conflicts.</p>\n<p>For example, blocking one's internet access is unlikely to provide lasting help someone who's procrastinating on finishing their thesis due to a fear of failure - they will likely find another way to procrastinate.&nbsp; It also won't help someone who <em>really</em> wants to get on the internet, vs. someone who's just being distracted by its availability! (i.e., has relevant motor programs primed by its availability)</p>\n<p>Meanwhile, Getting Things Done is unlikely to help someone whose conflict <em>isn't</em> because they have too many things to keep track of, and conversely the Pomodoro technique won't help someone who's having trouble deciding what to do in the first place.</p>\n<p><strong>Motivational methods</strong>, on the other hand, (such as the ones of mine that users mentioned, or Vladimir Golovin's version of \"self-affirmation\"), operate by attempting to prime or fill one's motor program buffers with exactly one program: the action to be taken.</p>\n<p>And, to the extent that people actually fill their mind sufficiently to block out other programs from activating, these methods will definitely <em>work</em>.&nbsp; However, in many cases, these methods ironically drop off in effectiveness over time, as their users get <em>better</em> at doing them.</p>\n<p>The more often the technique is used, the easier it becomes to do it with only a part of their mental resources...&nbsp; and so, unless care is taken to do the technique in precisely the same way each time (i.e., with full conscious attention/intention), it may not produce the same effect.</p>\n<p>Another flaw in motivational methods is that they don't address any <em>actual</em> sources of conflict.&nbsp; They are much more effective at overcoming simple inertia, and most useful when incorporated into something that you are trying to make into a habit anyway.</p>\n<p>Which brings us to our final category (and my personal favorite/specialty), <strong>conflict-resolution methods</strong>.&nbsp; These are techniques which seek to eliminate conflicts directly, either through manipulating the outside world (e.g. removing obstacles) or the inside world (getting rid of fears and doubts, clarifying priorities, etc.).</p>\n<p>And the <em>goal</em> of these techniques is to bypass a potentially \"fatal flaw\" that exists with the other three classes of technique:</p>\n<h4>Most Akrasia Techniques Are Subject To \"Meta\"-Akrasia</h4>\n<p>If you procrastinate taking your pills or doing your exercises, your hygenic method is unstable: the more you delay, the more likely you are to delay some more.&nbsp; The same is true for maintaining your \"trusted system\" in Getting Things Done, breaking your tasks into Pomodoros, or whatever other focusing method you use.&nbsp; And of course, if you put off doing your motivation technique, it's not going to motivate you.</p>\n<p>So the idea behind conflict-resolution methods is to <em>permanently</em> alter the situation so that a particular source of conflict can no longer arise.&nbsp; For example, if you can never find a pair of scissors, buying a pair for each place where you might need them could forever eliminate the priming of the procrastination motor program titled, \"but I don't know where the scissors are.\"&nbsp; (This would be an example of resolving an <em>external</em> conflict, as opposed to an internal one.)</p>\n<p>In my own work, however, I specialize in helping people get rid of chronic <em>internal</em> conflicts like not believing in themselves, fear of criticism, and all that sort of thing.&nbsp; These kinds of conflicts tend to not be helped at all by techniques in the other three categories, since the feelings (motor programs) involved tend to be intense (i.e. less likely to respond to hygenic methods), and also tend to interfere with the mental prerequisites for performing focusing or motivational methods.</p>\n<p>For example, a person who is afraid of doing things wrong can easily be just as afraid of doing GTD or Pomodoro wrong, as they are of doing whatever it is they're supposed to be doing!&nbsp; And in the case of motivational methods, a person with a chronic fear will usually just transfer the fear to the motivational technique itself, since it's immediately followed by them doing whatever it is they're already afraid of.</p>\n<p>Thus, as a general rule, the more chronic your akrasia, the less likely you will be helped by any kind of method that is not aimed at a \"one time pays for all\" elimination of your conflict source(s).</p>\n<p>That being said, however, I have in recent months begun making more use of hygenic and focusing methods for myself personally.&nbsp; But that's <em>only</em> because:</p>\n<ol>\n<li>\n<p>Having gotten rid of most of the chronic conflicts that plagued me before, I now am more able to actually <em>use</em> those other methods, and</p>\n</li>\n<li>Just getting rid of the chronic conflicts, doesn't get rid of&nbsp; health problems and routine distractions </li>\n</ol>\n<p>Which brings me to an important final point regarding these classifications:</p>\n<h4>No Anti-Akrasia Technique Can Be A Cure-All</h4>\n<p>...because no <em>one</em> technique can eliminate <em>all</em> conflicts.&nbsp; And removing one source of conflict may expose another: if you haven't actually started work on your thesis, you might not yet know what conflicts will arise <em>during</em> the work!</p>\n<p>Also, <em>learning</em> anti-akrasia techniques is itself often a source of conflict.&nbsp; Not just in that a person who believes themselves stupid or \"not good at this\" may not apply themselves well, but also in that one's beliefs about a technique may interfere with the choice to use it in the first place.</p>\n<p>For example, if you believe that the \"law of attraction\" is mumbo-jumbo (and it <strong>is</strong>), then you may choose not to learn the very effective motivational methods that are taught by \"attraction\" gurus.&nbsp; (Many of the motivational methods LessWrongers reported success with are essentially <em>identical</em> to methods taught in various \"law of attraction\" programs.)</p>\n<p>And, beyond such obstacles to learning, there is the further problem of <em>teaching</em>.&nbsp; I have seen numerous self-help books describe essentially the same motivational method in utterly different ways, most of which were incomprehensible if you didn't already \"get\" (or hadn't already \"clicked\" on) what it was that you were supposed to do.</p>\n<p>This is partly a problem of imprecise language for internal mental states and activities, and partly a problem of the authors focusing on whatever aspect of a method that they themselves most recently \"clicked\" on, ala <a href=\"/lw/1jf/manwithahammer_syndrome/\">Man With Hammer Syndrome</a>.&nbsp; (I have to fight this tendency constantly, myself.)&nbsp; This is only useful to a reader if they are missing the <em>same</em> piece of the puzzle as the author was.</p>\n<p>So, the net result is that problems with teaching and learning are the most common reason that \"internal\" anti-akrasia methods (within the focus, motivation, and conflict-resolution categories) vary so widely in <em>apparent</em>&nbsp;usefulness by individual.&nbsp; If a method's explanation lacks a way for its user to determine whether they have done it correctly, there is a very high probability that the user will simply give up on it as \"not working for me\", without once having actually performed the actual technique!&nbsp; (This was extremely common for me; I can now go back and read dozens of self-help books describing techniques that once appeared useless to me, when in fact I was never really <em>doing</em> them.)</p>\n<p>Thus, a method can appear useless, even if it is not, and the same inner technique can appear to be dozens of <em>different</em> techniques, simply as a result of using different words or metaphors to describe it.&nbsp; (For example, at least two \"different\" techniques of mine currently listed in the LessWrong survey are <em>exactly the same thing</em>, just described differently!)</p>\n<h4>Selection Pressures On The Self-Help Industry</h4>\n<p>This also explains why two rather frustrating phenomena occur in the self-help world: authors continually inventing \"new\" techniques, and writing books which do more listing or selling of techniques than actually teaching them.</p>\n<p>If the author focuses on teaching to the exclusion of selling, it's statistically likely they will lose not only that customer (due to their lack of success), but also <em>other</em> customers, due to word of mouth.&nbsp; And, if they are promoting the same technique as other authors, it is somewhat more likely that a customer who's already \"tried\" that technique will not buy the book in the first place.&nbsp; (Unless, as with many \"Secret\" books, the author positions themselves as supplying a needed \"missing piece\".)</p>\n<p>On the other hand, if the author invents a <em>new</em> name for a technique (or a new way to describe it) and focuses within their book on giving insight, entertainment, and persuading the customer that the technique is a good idea that they should learn, then they can get a happy-but-hungry customer: a customer who may go on to a seminar or coaching program, wherein the author can actually <em>teach </em>them something<em>, </em>in a situation where feedback is possible.</p>\n<p>And before I tried to write a book of my own, I underestimated just how difficult it is to <em>avoid</em> these pressures.&nbsp; Indeed, I thought many gurus were charlatans for writing their books in this way, while not grasping the unfortunate fact that this is <em>also</em> what you have to do, if your ultimate goal is to <strong>help people</strong>.</p>\n<p>In practice, the industry can probably be divided into those whose intentions are good (but don't grasp that there's a problem), those who <em>do</em> grasp the problem, but can't really change it, and those who simply exploit the existence of the problem to hide their own lack of comprehension, competence, or compassion.&nbsp; (Applying this classification is left as an exercise for the reader, although doing so accurately may well require more than just reading a given guru's book(s).)</p>\n<h4>Conclusions and Feedback Request<br /></h4>\n<p>In summary:</p>\n<ul>\n<li>\n<p>Anti-akrasia methods may be classified by how they address the problem of conflicting motor programs; specifically, by whether they attempt to fix ego depletion, remove sources of priming, invoke monoidealism by priming, or attempt to remove conflicts permanently at their source (e.g. by changing procedures, tools, environment, or emotional responses).</p>\n</li>\n<li>\n<p>The likely effectiveness of a given anti-akrasia method for a given person can be predicted both in a general way (class of technique vs. class of problems) and in a more specific way (specific technique vs. specific problems)</p>\n</li>\n<li>\n<p>There is no \"silver bullet\" for akrasia, even for a single individual: no single technique can cure all akrasia all the time for all people; nor even for one person, unless they have only one conflict source...&nbsp; which is ridiculously unlikely, given how many potential sources there are.</p>\n</li>\n<li>\n<p>A virtually unlimited number of \"new\" anti-akrasia methods can be developed within all method classifications, given an understanding of the specific conflict sources one wishes to address, but creating entirely new categories of method is hard (which is why on some level, all self-help books must essentially teach the same things).</p>\n</li>\n<li>\n<p>Teaching and learning are significant sources of problems with anti-akrasia methods that involve internal mental steps, and this exerts a powerful selection pressure on the way the self-help industry operates.</p>\n</li>\n</ul>\n<p>So...&nbsp; that about does it for now.&nbsp; I'd love to hear your feedback, whether it's in the form of supportive/confirming comments, or counterexamples and critical comments that might help improve the framework presented here. In particular, despite the length of this article, I still feel as if I've left out far more information than I've given.</p>\n<p>For example, I've not shown the details of many of the cause-effect chains at work in the processes and effects described, nor have I included examples of internal conflict-resolution methods, in order to avoid completely exploding the length of the piece.&nbsp; So, if you feel that something is left out, or that something included would be <em>better</em> left out, let me know!</p>\n<p>Also, one other topic on which I'd like feedback: I've attempted here to modify here my usual writing style, to better fit readers of LessWrong; if it's an improvement -- or failed to be one -- I'd appreciate comments on that as well.</p>\n<p>Thanks in advance for your input!</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uKoqrgnRoWjhneDvM", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 87, "baseScore": 98, "extendedScore": null, "score": 0.000162, "legacy": true, "legacyId": "2370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 99, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Abstract: This article proposes a hypothesis that effective anti-akrasia methods operate by reducing or eliminating the activation of conflicting voluntary motor programs at the time the user's desired action is to be carried out, or by reducing or eliminating the negative effects of managing the conflict.&nbsp; This hypothesis is consistent with the notion of \"ego depletion\" (willpower burnout) being driven by the need to consciously manage conflicting motor programs.&nbsp; It also supports a straightforward explanation of why different individuals will fare better with some anti-akrasia methods than others, and provides a framework for both classifying existing methods, and generating new ones.&nbsp; Finally, it demonstrates why no single technique can be a panacea, and shows how the common problems of certain methods shape the form of both the self-help industry, and most people's experiences with it.</em></p>\n<h4 id=\"The_Hypothesis\">The Hypothesis</h4>\n<p>Recently, <a href=\"/user/orthonormal\">orthonormal</a> posted an <a href=\"/lw/1sm/akrasia_tactics_review\">Akrasia Tactics Review</a>, collecting data from LessWrong members on their results using different anti-akrasia techniques.&nbsp; And although I couldn't quite put my finger on it at first, something about the review (and the discussion around it) was <em>bothering</em> me.</p>\n<p>See, I've never been fond of the idea that \"different things work for different people\".&nbsp; As a predictive hypothesis, after all, this is only <em>slightly</em> more useful than saying \"a wizard did it\".&nbsp; It says nothing about how (or why) different things work, and therefore gives you no basis to select <em>which</em> different things might work for which different people.</p>\n<p>For that reason, it kind of bugs me whenever I see discussion and advocacy of \"different things\", independent of any framework for classifying those things in a way that would help \"different people\" select or design the \"different things\" that would \"work for\" them.&nbsp; (In fact, this is a pretty big factor in why I'm a self-help writer/speaker in the first place!)</p>\n<p>So in this post, I want to share two slightly better working hypotheses for akrasia technique classification than \"different things work for different people\":<a id=\"more\"></a></p>\n<ol>\n<li>\n<p>Akrasia happens when there are conflicting active voluntary motor programs, and the opposite of akrasia is either the <em>absence<strong> </strong></em>of such conflict, or a <em>manageable quantity</em> of it.</p>\n</li>\n<li>The nature and source of the specific motor program conflicts are individual, so the effectiveness of a given anti-akrasia technique will be determined by an individual's <em>specific sources of conflict</em> </li>\n</ol>\n<p>Accepting these as working hypotheses allows us to quickly develop a classification scheme for anti-akrasia techniques, based on what part of the problem they work on.</p>\n<h4 id=\"A_Classification_Scheme\">A Classification Scheme</h4>\n<p>For example, <strong>hygenic/systemic methods</strong> such as exercise, nutrition, drugs, and meditation, all act to improve one's ability to manage conflict, attempting to reduce or minimize ego depletion: either by improving one's capacity, or, in the case of meditation, by developing more efficient conflict-management capability (so that ego depletion occurs more slowly or not at all).&nbsp; The effectiveness of these methods will then depend on the amount and type of conflict to be managed, and will most likely be effective in cases where the sources of conflict are many or frequent, but the <em>intensity</em> of any given conflict is low.</p>\n<p><strong>Focusing methods</strong>, such as Getting Things Done and the Pomodoro Technique, operate on a principle of helping people to let go of thinking about other tasks, even though they may vary widely in <em>how</em> they go about this.&nbsp; Restricting internet access, taking vows, or simply \"deciding not to do anything else\" are also within this class.&nbsp; The motor-program conflict hypothesis predicts that the effectiveness of each of these techniques will vary widely between individuals, depending on whether it addresses the actual source of their conflicts.</p>\n<p>For example, blocking one's internet access is unlikely to provide lasting help someone who's procrastinating on finishing their thesis due to a fear of failure - they will likely find another way to procrastinate.&nbsp; It also won't help someone who <em>really</em> wants to get on the internet, vs. someone who's just being distracted by its availability! (i.e., has relevant motor programs primed by its availability)</p>\n<p>Meanwhile, Getting Things Done is unlikely to help someone whose conflict <em>isn't</em> because they have too many things to keep track of, and conversely the Pomodoro technique won't help someone who's having trouble deciding what to do in the first place.</p>\n<p><strong>Motivational methods</strong>, on the other hand, (such as the ones of mine that users mentioned, or Vladimir Golovin's version of \"self-affirmation\"), operate by attempting to prime or fill one's motor program buffers with exactly one program: the action to be taken.</p>\n<p>And, to the extent that people actually fill their mind sufficiently to block out other programs from activating, these methods will definitely <em>work</em>.&nbsp; However, in many cases, these methods ironically drop off in effectiveness over time, as their users get <em>better</em> at doing them.</p>\n<p>The more often the technique is used, the easier it becomes to do it with only a part of their mental resources...&nbsp; and so, unless care is taken to do the technique in precisely the same way each time (i.e., with full conscious attention/intention), it may not produce the same effect.</p>\n<p>Another flaw in motivational methods is that they don't address any <em>actual</em> sources of conflict.&nbsp; They are much more effective at overcoming simple inertia, and most useful when incorporated into something that you are trying to make into a habit anyway.</p>\n<p>Which brings us to our final category (and my personal favorite/specialty), <strong>conflict-resolution methods</strong>.&nbsp; These are techniques which seek to eliminate conflicts directly, either through manipulating the outside world (e.g. removing obstacles) or the inside world (getting rid of fears and doubts, clarifying priorities, etc.).</p>\n<p>And the <em>goal</em> of these techniques is to bypass a potentially \"fatal flaw\" that exists with the other three classes of technique:</p>\n<h4 id=\"Most_Akrasia_Techniques_Are_Subject_To__Meta__Akrasia\">Most Akrasia Techniques Are Subject To \"Meta\"-Akrasia</h4>\n<p>If you procrastinate taking your pills or doing your exercises, your hygenic method is unstable: the more you delay, the more likely you are to delay some more.&nbsp; The same is true for maintaining your \"trusted system\" in Getting Things Done, breaking your tasks into Pomodoros, or whatever other focusing method you use.&nbsp; And of course, if you put off doing your motivation technique, it's not going to motivate you.</p>\n<p>So the idea behind conflict-resolution methods is to <em>permanently</em> alter the situation so that a particular source of conflict can no longer arise.&nbsp; For example, if you can never find a pair of scissors, buying a pair for each place where you might need them could forever eliminate the priming of the procrastination motor program titled, \"but I don't know where the scissors are.\"&nbsp; (This would be an example of resolving an <em>external</em> conflict, as opposed to an internal one.)</p>\n<p>In my own work, however, I specialize in helping people get rid of chronic <em>internal</em> conflicts like not believing in themselves, fear of criticism, and all that sort of thing.&nbsp; These kinds of conflicts tend to not be helped at all by techniques in the other three categories, since the feelings (motor programs) involved tend to be intense (i.e. less likely to respond to hygenic methods), and also tend to interfere with the mental prerequisites for performing focusing or motivational methods.</p>\n<p>For example, a person who is afraid of doing things wrong can easily be just as afraid of doing GTD or Pomodoro wrong, as they are of doing whatever it is they're supposed to be doing!&nbsp; And in the case of motivational methods, a person with a chronic fear will usually just transfer the fear to the motivational technique itself, since it's immediately followed by them doing whatever it is they're already afraid of.</p>\n<p>Thus, as a general rule, the more chronic your akrasia, the less likely you will be helped by any kind of method that is not aimed at a \"one time pays for all\" elimination of your conflict source(s).</p>\n<p>That being said, however, I have in recent months begun making more use of hygenic and focusing methods for myself personally.&nbsp; But that's <em>only</em> because:</p>\n<ol>\n<li>\n<p>Having gotten rid of most of the chronic conflicts that plagued me before, I now am more able to actually <em>use</em> those other methods, and</p>\n</li>\n<li>Just getting rid of the chronic conflicts, doesn't get rid of&nbsp; health problems and routine distractions </li>\n</ol>\n<p>Which brings me to an important final point regarding these classifications:</p>\n<h4 id=\"No_Anti_Akrasia_Technique_Can_Be_A_Cure_All\">No Anti-Akrasia Technique Can Be A Cure-All</h4>\n<p>...because no <em>one</em> technique can eliminate <em>all</em> conflicts.&nbsp; And removing one source of conflict may expose another: if you haven't actually started work on your thesis, you might not yet know what conflicts will arise <em>during</em> the work!</p>\n<p>Also, <em>learning</em> anti-akrasia techniques is itself often a source of conflict.&nbsp; Not just in that a person who believes themselves stupid or \"not good at this\" may not apply themselves well, but also in that one's beliefs about a technique may interfere with the choice to use it in the first place.</p>\n<p>For example, if you believe that the \"law of attraction\" is mumbo-jumbo (and it <strong>is</strong>), then you may choose not to learn the very effective motivational methods that are taught by \"attraction\" gurus.&nbsp; (Many of the motivational methods LessWrongers reported success with are essentially <em>identical</em> to methods taught in various \"law of attraction\" programs.)</p>\n<p>And, beyond such obstacles to learning, there is the further problem of <em>teaching</em>.&nbsp; I have seen numerous self-help books describe essentially the same motivational method in utterly different ways, most of which were incomprehensible if you didn't already \"get\" (or hadn't already \"clicked\" on) what it was that you were supposed to do.</p>\n<p>This is partly a problem of imprecise language for internal mental states and activities, and partly a problem of the authors focusing on whatever aspect of a method that they themselves most recently \"clicked\" on, ala <a href=\"/lw/1jf/manwithahammer_syndrome/\">Man With Hammer Syndrome</a>.&nbsp; (I have to fight this tendency constantly, myself.)&nbsp; This is only useful to a reader if they are missing the <em>same</em> piece of the puzzle as the author was.</p>\n<p>So, the net result is that problems with teaching and learning are the most common reason that \"internal\" anti-akrasia methods (within the focus, motivation, and conflict-resolution categories) vary so widely in <em>apparent</em>&nbsp;usefulness by individual.&nbsp; If a method's explanation lacks a way for its user to determine whether they have done it correctly, there is a very high probability that the user will simply give up on it as \"not working for me\", without once having actually performed the actual technique!&nbsp; (This was extremely common for me; I can now go back and read dozens of self-help books describing techniques that once appeared useless to me, when in fact I was never really <em>doing</em> them.)</p>\n<p>Thus, a method can appear useless, even if it is not, and the same inner technique can appear to be dozens of <em>different</em> techniques, simply as a result of using different words or metaphors to describe it.&nbsp; (For example, at least two \"different\" techniques of mine currently listed in the LessWrong survey are <em>exactly the same thing</em>, just described differently!)</p>\n<h4 id=\"Selection_Pressures_On_The_Self_Help_Industry\">Selection Pressures On The Self-Help Industry</h4>\n<p>This also explains why two rather frustrating phenomena occur in the self-help world: authors continually inventing \"new\" techniques, and writing books which do more listing or selling of techniques than actually teaching them.</p>\n<p>If the author focuses on teaching to the exclusion of selling, it's statistically likely they will lose not only that customer (due to their lack of success), but also <em>other</em> customers, due to word of mouth.&nbsp; And, if they are promoting the same technique as other authors, it is somewhat more likely that a customer who's already \"tried\" that technique will not buy the book in the first place.&nbsp; (Unless, as with many \"Secret\" books, the author positions themselves as supplying a needed \"missing piece\".)</p>\n<p>On the other hand, if the author invents a <em>new</em> name for a technique (or a new way to describe it) and focuses within their book on giving insight, entertainment, and persuading the customer that the technique is a good idea that they should learn, then they can get a happy-but-hungry customer: a customer who may go on to a seminar or coaching program, wherein the author can actually <em>teach </em>them something<em>, </em>in a situation where feedback is possible.</p>\n<p>And before I tried to write a book of my own, I underestimated just how difficult it is to <em>avoid</em> these pressures.&nbsp; Indeed, I thought many gurus were charlatans for writing their books in this way, while not grasping the unfortunate fact that this is <em>also</em> what you have to do, if your ultimate goal is to <strong>help people</strong>.</p>\n<p>In practice, the industry can probably be divided into those whose intentions are good (but don't grasp that there's a problem), those who <em>do</em> grasp the problem, but can't really change it, and those who simply exploit the existence of the problem to hide their own lack of comprehension, competence, or compassion.&nbsp; (Applying this classification is left as an exercise for the reader, although doing so accurately may well require more than just reading a given guru's book(s).)</p>\n<h4 id=\"Conclusions_and_Feedback_Request\">Conclusions and Feedback Request<br></h4>\n<p>In summary:</p>\n<ul>\n<li>\n<p>Anti-akrasia methods may be classified by how they address the problem of conflicting motor programs; specifically, by whether they attempt to fix ego depletion, remove sources of priming, invoke monoidealism by priming, or attempt to remove conflicts permanently at their source (e.g. by changing procedures, tools, environment, or emotional responses).</p>\n</li>\n<li>\n<p>The likely effectiveness of a given anti-akrasia method for a given person can be predicted both in a general way (class of technique vs. class of problems) and in a more specific way (specific technique vs. specific problems)</p>\n</li>\n<li>\n<p>There is no \"silver bullet\" for akrasia, even for a single individual: no single technique can cure all akrasia all the time for all people; nor even for one person, unless they have only one conflict source...&nbsp; which is ridiculously unlikely, given how many potential sources there are.</p>\n</li>\n<li>\n<p>A virtually unlimited number of \"new\" anti-akrasia methods can be developed within all method classifications, given an understanding of the specific conflict sources one wishes to address, but creating entirely new categories of method is hard (which is why on some level, all self-help books must essentially teach the same things).</p>\n</li>\n<li>\n<p>Teaching and learning are significant sources of problems with anti-akrasia methods that involve internal mental steps, and this exerts a powerful selection pressure on the way the self-help industry operates.</p>\n</li>\n</ul>\n<p>So...&nbsp; that about does it for now.&nbsp; I'd love to hear your feedback, whether it's in the form of supportive/confirming comments, or counterexamples and critical comments that might help improve the framework presented here. In particular, despite the length of this article, I still feel as if I've left out far more information than I've given.</p>\n<p>For example, I've not shown the details of many of the cause-effect chains at work in the processes and effects described, nor have I included examples of internal conflict-resolution methods, in order to avoid completely exploding the length of the piece.&nbsp; So, if you feel that something is left out, or that something included would be <em>better</em> left out, let me know!</p>\n<p>Also, one other topic on which I'd like feedback: I've attempted here to modify here my usual writing style, to better fit readers of LessWrong; if it's an improvement -- or failed to be one -- I'd appreciate comments on that as well.</p>\n<p>Thanks in advance for your input!</p>\n<p>&nbsp;</p>", "sections": [{"title": "The Hypothesis", "anchor": "The_Hypothesis", "level": 1}, {"title": "A Classification Scheme", "anchor": "A_Classification_Scheme", "level": 1}, {"title": "Most Akrasia Techniques Are Subject To \"Meta\"-Akrasia", "anchor": "Most_Akrasia_Techniques_Are_Subject_To__Meta__Akrasia", "level": 1}, {"title": "No Anti-Akrasia Technique Can Be A Cure-All", "anchor": "No_Anti_Akrasia_Technique_Can_Be_A_Cure_All", "level": 1}, {"title": "Selection Pressures On The Self-Help Industry", "anchor": "Selection_Pressures_On_The_Self_Help_Industry", "level": 1}, {"title": "Conclusions and Feedback Request", "anchor": "Conclusions_and_Feedback_Request", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "70 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rRmisKb45dN7DK4BW", "WHP3tKPXppBF2S8e8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-02-26T20:45:19.942Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-26T23:59:15.227Z", "modifiedAt": null, "url": null, "title": "Superstimuli, setpoints, and obesity", "slug": "superstimuli-setpoints-and-obesity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:31.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KtFhvSsLYPRHcL8Pe/superstimuli-setpoints-and-obesity", "pageUrlRelative": "/posts/KtFhvSsLYPRHcL8Pe/superstimuli-setpoints-and-obesity", "linkUrl": "https://www.lesswrong.com/posts/KtFhvSsLYPRHcL8Pe/superstimuli-setpoints-and-obesity", "postedAtFormatted": "Friday, February 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superstimuli%2C%20setpoints%2C%20and%20obesity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperstimuli%2C%20setpoints%2C%20and%20obesity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtFhvSsLYPRHcL8Pe%2Fsuperstimuli-setpoints-and-obesity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superstimuli%2C%20setpoints%2C%20and%20obesity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtFhvSsLYPRHcL8Pe%2Fsuperstimuli-setpoints-and-obesity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtFhvSsLYPRHcL8Pe%2Fsuperstimuli-setpoints-and-obesity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 332, "htmlBody": "<p>Related to: <a href=\"/lw/1ss/babies_and_bunnies_a_caution_about_evopsych/\">Babies and Bunnies: A Caution About Evo-Psych</a>, <a href=\"/lw/h3/superstimuli_and_the_collapse_of_western/\">Superstimuli and the Collapse of Western Civilization</a>.</p>\n<p>The main proximate cause of increase in human weight over the last few decades is over-eating - other factors like decreased energy need due to less active lifestyle seem at best secondary if relevant at all. The big question is what misregulates homeostatic system controlling food intake towards higher calorie consumption?</p>\n<p>The most common accepted answer is some sort of superstimulus theory - modern food is so tasty people find it irresistible. This seems backwards to me in its basic assumption - almost any \"traditional\" food seems to taste better than almost any \"modern\" food.</p>\n<p>It is as easy to construct the opposite theory of tastiness set point - tastiness is some estimate of nutritional value of food - more nutritious food should taste better than less nutritious food. So according to the theory - if you eat very tasty food, your appetite thinks it's highly nutritious, and demands less of it; and if you eat bland tasteless food - your appetite underestimates its nutritious content and demands too much of it.<a id=\"more\"></a></p>\n<p>It's not even obvious that your appetite is \"wrong\" - if you need certain amount of nutritionally balanced food, and all you can gets is nutritionally balanced food with a lot of added sugar - the best thing is eating more and getting all the micro-nutrients needed in spite of excess calories. Maybe it is not confused at all, just doing its best in a bad situation, and prioritizes evolutionarily common threat of too little micronutrients over evolutionarily less common threat of excess calories.</p>\n<p>As some extra evidence - it's a fact that poor people with narrower choice of food are more obese than rich people with wider choice of food. If everyone buys the tastier food they can afford, superstimulus theory says rich should be more obese, setpoint theory says poor should be more obese.</p>\n<p>Is there any way in which setpoint theory is more wrong than superstimulus theory?</p>\n<p><img src=\"http://upload.wikimedia.org/wikipedia/commons/2/2e/WFC2009.png\" alt=\"\" width=\"817\" height=\"467\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KtFhvSsLYPRHcL8Pe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -3, "extendedScore": null, "score": 5.647560679138741e-07, "legacy": true, "legacyId": "2371", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["u9pfbkZeG8mFTPNi2", "Jq73GozjsuhdwMLEG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-27T01:04:54.333Z", "modifiedAt": null, "url": null, "title": "Mental Crystallography", "slug": "mental-crystallography", "viewCount": null, "lastCommentedAt": "2022-02-28T18:41:36.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SRaRHemkbsHWzzbPN/mental-crystallography", "pageUrlRelative": "/posts/SRaRHemkbsHWzzbPN/mental-crystallography", "linkUrl": "https://www.lesswrong.com/posts/SRaRHemkbsHWzzbPN/mental-crystallography", "postedAtFormatted": "Saturday, February 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mental%20Crystallography&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMental%20Crystallography%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRaRHemkbsHWzzbPN%2Fmental-crystallography%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mental%20Crystallography%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRaRHemkbsHWzzbPN%2Fmental-crystallography", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRaRHemkbsHWzzbPN%2Fmental-crystallography", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 710, "htmlBody": "<p><em>Brains organize things into familiar patterns, which are different for different people.&nbsp; This can make communication tricky, so it's useful to conceptualize these patterns and use them to help translation efforts.</em></p>\n<p>Crystals are nifty things!&nbsp; The same sort of crystal will reliably organize in the same pattern, and always <a href=\"http://en.wikipedia.org/wiki/Cleavage_plane\">break the same way</a> under stress.</p>\n<p>Brains are also nifty things!&nbsp; The same person's brain will typically view everything through a favorite lens (or two), and will need to work hard to translate input that comes in through another channel or in different terms.&nbsp; When a brain acquires new concepts - even really vital ones - the new idea will result in recognizeably-shaped brain-bits.&nbsp; Different brains, therefore, handle concepts differently, and this can make it hard for us to talk to each other.</p>\n<p>This works on a number of levels, although perhaps the most obvious is the divide between styles of thought on the order of \"visual thinker\", \"verbal thinker\", etc.&nbsp; People who differ here have to constantly reinterpret everything they say to one another, moving from non-native mode to native mode and back with every bit of data exchanged.&nbsp; People also store and retrieve memories differently, form first-approximation hypotheses and models differently, prioritize sensory input differently, have different levels of introspective luminosity<sup>1</sup>, and experience different affect around concepts and propositions.&nbsp; Over time, we accumulate different skills, knowledge, cognitive habits, shortcuts, and mental filing debris.&nbsp; Intuitions differ - appeals to intuition will only convert people who share the premises natively.&nbsp; We have <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">lots in common</a>, but high enough variance that it's impressive how much we do manage to communicate over not only <a href=\"/lw/kg/expecting_short_inferential_distances/\">inferential distances</a>, but also fundamentally diverse brain plans.&nbsp; Basically, you can hit two crystals the same way with the same hammer, but they can still break along different cleavage planes.<a id=\"more\"></a></p>\n<p>This phenomenon is a little like <a href=\"/lw/1jf/manwithahammer_syndrome/\">man-with-a-hammer syndrome</a>, which is why I chose that extension of my crystal metaphor.&nbsp; But a person's dependence on their mental crystallography, unlike their wanton use of their hammer, rarely seems to diminish with time.&nbsp; (In fact, youth probably confers some <em>increased</em> flexibility - it seems that you can probably train children to have different crystalline structures to some degree, but much less so with adults).&nbsp; MWaH is actually partially explained by the brain's crystallographic regularities.&nbsp; A hammer-idea will only be compelling to you if it aligns with the crystals in your head.</p>\n<p>Having \"useful\" mental crystallography - which lets you comprehend, synthesize, and apply ideas in their most accurate, valuable form - is a type of <a href=\"/lw/1r1/epistemic_luck/\">epistemic luck</a> about the things you can best understand.&nbsp; If you're intrinsically oriented towards mathematical explanations, for instance, and this lets you promptly apprehend the truth and falsity of strings of numbers that would leave my head swimming, you're epistemically lucky about math (while I'm rather likely to be led astray if someone takes the time to put together a plausible verbal explanation that may not match up to the numbers).&nbsp; Some brain structures can use more notions than others, although I'm skeptical that any human has a pure generalist crystal pattern that can make great use of every sort of concept interchangeably without <em>some</em> native mode to touch base with regularly.</p>\n<p>When you're trying to communicate facts, opinions, and concepts - most especially concepts - it is a useful investment of effort to try to categorize both your audience's crystallography <em>and your own</em>.&nbsp; With only one of these pieces of information, you can't optimize your message for its recipient, because you need to know what you're translating <em>from</em>, not just have a bead on what you are translating <em>to</em>.&nbsp; (If you want to translate the word \"blesser\" into, say, Tagalog, it might be useful to know if \"blesser\" is English or French.)&nbsp; And even with fairly good information on both origin and destination, you can wind up with <a href=\"/lw/1og/deontology_for_consequentialists/1jfa?context=1#comments\">a frustrating disconnect</a>; but given that head start on bridging the gap, you can find wherever the two crystals are most likely to touch with less trial and error.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>Introspective luminosity (or just \"luminosity\") is the subject of a sequence I have planned - this is a preparatory post of sorts.&nbsp; In a nutshell, I use it to mean the discernibility of mental states to their haver - if you're luminously happy, clap your hands.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SRaRHemkbsHWzzbPN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 29, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "2374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "generalizing-from-one-example", "canonicalPrevPostSlug": "sorting-out-sticky-brains", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cyj6wQLW6SeF6aGLy", "HLqWn5LASfhhArZ7w", "WHP3tKPXppBF2S8e8", "8bWbNwiSGbGi9jXPS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-28T08:26:27.931Z", "modifiedAt": null, "url": null, "title": "Splinters and Wooden Beams", "slug": "splinters-and-wooden-beams", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:30.495Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "knb", "createdAt": "2009-02-27T04:55:09.459Z", "isAdmin": false, "displayName": "knb"}, "userId": "YEwEAL5Mu6YxaX4rD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iYzZS4h8a7uGNhouE/splinters-and-wooden-beams", "pageUrlRelative": "/posts/iYzZS4h8a7uGNhouE/splinters-and-wooden-beams", "linkUrl": "https://www.lesswrong.com/posts/iYzZS4h8a7uGNhouE/splinters-and-wooden-beams", "postedAtFormatted": "Sunday, February 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Splinters%20and%20Wooden%20Beams&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASplinters%20and%20Wooden%20Beams%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiYzZS4h8a7uGNhouE%2Fsplinters-and-wooden-beams%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Splinters%20and%20Wooden%20Beams%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiYzZS4h8a7uGNhouE%2Fsplinters-and-wooden-beams", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiYzZS4h8a7uGNhouE%2Fsplinters-and-wooden-beams", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 517, "htmlBody": "<p>I recently told a friend that I was planning to write (and post online) a paper that rigorously refutes every argument<sup>1</sup> I&rsquo;ve ever heard that homosexuality is inherently immoral. The purpose of this effort was to provide a handy link for people who want to persuade family members or friends who are marginal believers of the homosexuality-is-immoral theory. As a key part of this effort, I intended to demonstrate that the predominant religious arguments against homosexuality cause contradictions within the religion. For example, the tortured reasoning of the Roman Catholic Church<sup>2</sup> goes like this:</p>\n<ol>\n<li>Sex without marriage is forbidden.</li>\n<li>Marriage is only for those who are &ldquo;open to natural reproduction&rdquo;.</li>\n<li>Gays can&rsquo;t reproduce (in an acceptably &ldquo;natural&rdquo; way) and therefore gay sex is not &ldquo;open to reproduction&rdquo;.</li>\n<li>Since gays cannot be open to reproduction, they cannot marry.</li>\n<li>Since they cannot marry, they can&rsquo;t have sex.</li>\n</ol>\n<p>This argument seems to be logically valid, if you accept the insane assumptions. Bizarrely, though, the Catholic Church also recommends a practice called \"Natural Family Planning\", in which married couples who want to prevent pregnancy have sex only when the woman is believed to be infertile! To be consistent, the Catholic Church would have to oppose such deliberate efforts to prevent natural reproduction.<a id=\"more\"></a></p>\n<p>My paper was going to be full of little examples like this, of how opposing homosexuality leads to contradictions within Christian Virtue Ethics, established interpretations of the Koran, or whatever. However, my friend told me that he thought my efforts were misguided. Why try curing these folks of the splinter of intolerance, when they still have the wooden beam<sup>3</sup> of theism in their eyes?</p>\n<p>After all, if someone you know is planning to quit her job and move to Alaska because her horoscope told her that Tauruses need more spontaneity, you shouldn't tell her to stay because <em>she's actually an Aries</em>. You tell her to stay because astrology is provably bogus.</p>\n<p>I'm uncertain. Most of those wooden beams are staying right where they are for the foreseeable future. But attitudes toward homosexuality are changing relatively quickly. On the other hand, there is something to be said for striking at the root of the problem. Overall, I'm leaning toward making these smaller arguments instead of trying to convert people to atheism.</p>\n<p>**A lot of people have said they don't think this approach will be very effective. I mentioned in the beginning of the article that the purpose was to<strong> </strong><em><strong>help</strong> <strong>others</strong></em> persuade <strong><em>marginal believers</em></strong> of the homosexuality-is-immoral theory.</p>\n<p>&nbsp;</p>\n<p>1. Most of these arguments are religion-based.</p>\n<p>2. Ironically, the Catholic Church is an easier target because they have the decency to actually lay their arguments out formally (though often in gratuitous Latin), since they believe that the Church's dogma can always be confirmed using pure Reason. Protestant churches tend to simply cite scripture--and they believe the scripture <em>because they have faith</em>. Yes this is a tautology. Actually, I wonder if the Protestants' refusal to justify their beliefs rationally protects them from <a href=\"/lw/r/no_really_ive_deceived_myself/\">Escher-brain</a> effect. Faith claims can be neatly compartmentalized, sequestered away, protecting the rest of the mind.</p>\n<p>3. A reference to <a href=\"http://bible.cc/matthew/7-3.htm\">Matthew 7:3</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iYzZS4h8a7uGNhouE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "2379", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rZX4WuufAPbN6wQTv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-28T09:33:58.318Z", "modifiedAt": null, "url": null, "title": "Great Product. Lousy Marketing.", "slug": "great-product-lousy-marketing", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:34.087Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BenAlbahari", "createdAt": "2010-01-04T10:39:05.080Z", "isAdmin": false, "displayName": "BenAlbahari"}, "userId": "yHHBZZDtuWZw92SAf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m9S5qQrLDhybvx4f5/great-product-lousy-marketing", "pageUrlRelative": "/posts/m9S5qQrLDhybvx4f5/great-product-lousy-marketing", "linkUrl": "https://www.lesswrong.com/posts/m9S5qQrLDhybvx4f5/great-product-lousy-marketing", "postedAtFormatted": "Sunday, February 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Great%20Product.%20Lousy%20Marketing.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreat%20Product.%20Lousy%20Marketing.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm9S5qQrLDhybvx4f5%2Fgreat-product-lousy-marketing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Great%20Product.%20Lousy%20Marketing.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm9S5qQrLDhybvx4f5%2Fgreat-product-lousy-marketing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm9S5qQrLDhybvx4f5%2Fgreat-product-lousy-marketing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 630, "htmlBody": "<p>The product of Less Wrong is truth. However, there seems to be a reluctance of the personality types here - myself included - to sell that product. Here's my evidence:</p>\n<blockquote>Yvain&nbsp;<a href=\"/lw/3k/how_to_not_lose_an_argument/\">said</a>:&nbsp;But the most important reason to argue with someone is to change his mind. ... I make the anecdotal observation that a lot of smart people are very good at winning arguments in the first sense [(logic)], and very bad at winning arguments in the second sense [(persuasion)]. Does that correspond to your experience?</blockquote>\n<blockquote><br /></blockquote>\n<blockquote>Eliezer&nbsp;<a href=\"/lw/yg/informers_and_persuaders/\">said</a>: I finally note, with regret, that in a world containing Persuaders, it may make sense for a second-order Informer to be deliberately eloquent if the issue has already been obscured by an eloquent Persuader - just exactly as elegant as the previous Persuader, no more, no less. &nbsp;It's a pity that this wonderful excuse exists, but in the real world, well...</blockquote>\n<blockquote><br /></blockquote>\n<blockquote>Robin Hanson&nbsp;<a href=\"http://www.overcomingbias.com/2010/01/real-rationality.html\">said</a>:&nbsp;So to promote rationality on interesting important topics, your overwhelming consideration simply must be: on what topics will the world&rsquo;s systems for deciding who to hear on what listen substantially to you? Your efforts to ponder and make progress will be largely wasted if you focus on topics where none of the world&rsquo;s &ldquo;who to hear on what&rdquo; systems rate you as someone worth hearing. You must not only find something worth saying, but also something that will be heard.</blockquote>\n<p>We actually label many highly effective persuasive strategies that can be used to market our true ideas as \"<a href=\"http://wiki.lesswrong.com/wiki/Dark_Arts\">dark arts</a>\". What's the justification for this negative branding? A necessary evil is not evil. Even if - and this is a huge if - our future utopia is free of dark arts, that's not the world we live in today. Choosing not to use them is&nbsp;analogous&nbsp;to a peacenik wanting to rid the world of violence by suggesting that police not use weapons.<a id=\"more\"></a></p>\n<p>We treat our dislike of dark arts as if it's a simple corollary of the axiom of the virtue of truth. Does this mean we assume the ends (more people believe the truth) doesn't justify the means (persuasion to the truth via exploiting cognitive biases)? Or are we just worried about being hypocrites? Whatever the reason, such an impactful assumption deserves an explanation. Speaking practically, the successful practice of dark arts requires the psychological skill of switching <a href=\"http://en.wikipedia.org/wiki/Six_Thinking_Hats\">hats</a>, to use Edward de Bono's terminology. While posting on Less Wrong, we can avoid and are in fact praised for avoiding dark arts, but we need to switch up in other environments, and that's difficult. Frankly, we're not great at it, and it's very tempting to externalize the problem and say \"the art is bad\" rather than \"we're bad at the art\".</p>\n<p>Our distaste for rhetorical tactics, both aesthetically and morally, profoundly affects the way we communicate. That distaste is tightly coupled with the mental habit of always interpreting the value of what is said purely for its informational content, logical consistency, and insight. I'm basing the following question on my own introspection, but I wonder if this almost religiously entrenched mental habit could make us blind to the value of the art of persuasion? Let's imagine for a moment, the most convincing paragraph ever written. It was truly a world-wonder of persuasion&nbsp;- it converted fundamentalist Christians into atheists, suicide bombers into diplomats, and Ann Coulter-4-President supporters into Less Wrong&nbsp;sycophants. What would <em>your</em> reaction to the paragraph be? Would you \"up-vote\" this work of genius? No way. We'd be competing to tell the fundamentalist&nbsp;Christian&nbsp;that there were at least three argument fallacies in the first sentence, we'd explain to the suicide bomber that the rhetoric could be used equally well to justify blowing us all up right now, and for completeness we'd give the Ann Coulter supporter a brief overview of Bayesianism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rnvHPB3X2TiD5NMwY": 1, "XYHzLjwYiqpeqaf4c": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m9S5qQrLDhybvx4f5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 18, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "2380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6yTShbTdtATxKonY5", "njb9cyyzqLTHewups"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-01T02:32:33.652Z", "modifiedAt": null, "url": null, "title": "Hedging our Bets: The Case for Pursuing Whole Brain Emulation to Safeguard Humanity's Future", "slug": "hedging-our-bets-the-case-for-pursuing-whole-brain-emulation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:13.893Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "inklesspen", "createdAt": "2009-02-27T05:22:19.956Z", "isAdmin": false, "displayName": "inklesspen"}, "userId": "5pWSeqwGW4K6TrXKf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v5AJZyEY7YFthkzax/hedging-our-bets-the-case-for-pursuing-whole-brain-emulation", "pageUrlRelative": "/posts/v5AJZyEY7YFthkzax/hedging-our-bets-the-case-for-pursuing-whole-brain-emulation", "linkUrl": "https://www.lesswrong.com/posts/v5AJZyEY7YFthkzax/hedging-our-bets-the-case-for-pursuing-whole-brain-emulation", "postedAtFormatted": "Monday, March 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hedging%20our%20Bets%3A%20The%20Case%20for%20Pursuing%20Whole%20Brain%20Emulation%20to%20Safeguard%20Humanity's%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHedging%20our%20Bets%3A%20The%20Case%20for%20Pursuing%20Whole%20Brain%20Emulation%20to%20Safeguard%20Humanity's%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv5AJZyEY7YFthkzax%2Fhedging-our-bets-the-case-for-pursuing-whole-brain-emulation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hedging%20our%20Bets%3A%20The%20Case%20for%20Pursuing%20Whole%20Brain%20Emulation%20to%20Safeguard%20Humanity's%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv5AJZyEY7YFthkzax%2Fhedging-our-bets-the-case-for-pursuing-whole-brain-emulation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv5AJZyEY7YFthkzax%2Fhedging-our-bets-the-case-for-pursuing-whole-brain-emulation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 896, "htmlBody": "<p>It is the fashion in some circles to promote funding for Friendly AI research as a guard against the existential threat of Unfriendly AI. While this is an admirable goal, the path to Whole Brain Emulation is in many respects more straightforward and presents fewer risks. Accordingly, by working towards WBE, we may be able to \"weight\" the outcome probability space of the singularity such that humanity is more likely to survive.</p>\n<p><a id=\"more\"></a></p>\n<p>One of the potential existential risks in a technological singularity is that the recursively self-improving agent might be inimical to our interests, either through actual malevolence or \"mere\" indifference towards the best interests of humanity. Eliezer has written extensively on how a poorly-designed AI could lead to this existential risk. This is commonly termed Unfriendly AI.</p>\n<p>Since the first superintelligence can be presumed to have an advantage over any subsequently-arising intelligences, Eliezer and others advocate funding research into creating Friendly AI. Such research must not only reverse-engineer consciousness, but also human notions of morality. Unfriendly AI could potentially require only sufficiently fast hardware to evolve an intelligence via artificial life, as depicted in Greg Egan's short story \"<a href=\"http://ttapress.com/553/crystal-nights-by-greg-egan/\">Crystal Nights</a>\", or it may be created inadvertently by researchers at the NSA or a similar organization. It may be that creating Friendly AI is significantly harder than creating Unfriendly (or Indifferent) AI, perhaps so much so that we are unlikely to achieve it in time to save human civilization.</p>\n<p>Fortunately, there's a short-cut we can take. We already have a great many relatively stable and sane intelligences. We merely need to increase their rate of self-improvement. As far as I can tell, developing mind uploading via WBE is a simpler task than creating Friendly AI. If WBE is fast enough to constitute an augmented intelligence, then our augmented scientists can trigger the singularity by developing more efficient computing devices. An augmented human intelligence may have a slower \"take-off\" than a purpose-built intelligence, but we can reasonably expect it to be much easier to ensure such a superintelligence is Friendly. In fact, this slower take-off will likely be to our advantage; it may increase our odds of being able to abort an Unfriendly singularity.</p>\n<p>WBE may also be able to provide us with useful insights into the nature of consciousness, which will aid Friendly AI research. Even if it doesn't, it gets us most of the practical benefits of Friendly AI (immortality, feasible galactic colonization, etc) and makes it possible to wait longer for the rest of the benefits.</p>\n<p>But what if I'm wrong? What if it's just as easy to create an AI we think is Friendly as it is to upload minds into WBE? Even in that case, I think it's best to work on WBE first. Consider the following two worlds: World A creates an AI its best scientists believes is Friendly and, after a best-effort psychiatric evaluation (for whatever good that might do) gives it Internet access. World B uploads 1000 of its best engineers, physicists, psychologists, philosophers, and businessmen (someone's gotta fund the research, right?). World B seems to me to have more survivable failure cases; if some of the uploaded individuals turn out to be sociopaths, the rest of them can stop the \"bad\" uploads from ruining civilization. It seems exceedingly unlikely that we would select a large enough group of sociopaths that the \"good\" uploads can't keep the \"bad\" uploads in check.</p>\n<p>Furthermore, the danger of uploading sociopaths (or people who become sociopathic when presented with that power) is also a danger that the average person can easily comprehend, compared to the difficulty of ensuring Friendliness of an AI. I believe that the average person is also more likely to recognize where attempts at safeguarding an upload-triggered singularity may go wrong.</p>\n<p>The only downside of this approach I can see is that an upload-triggered Unfriendly singularity may cause more suffering than an Unfriendly AI singularity; sociopaths may be presumed to have more interest in torture of people than a paperclip-optimizing AI would have.</p>\n<p>Suppose, however, that everything goes right, the singularity occurs, and life becomes paradise by our standards. Can we predict anything of this future? It's a popular topic in science fiction, so many people certainly enjoy the effort. Depending on how we define a \"Friendly singularity\", there could be room for a wide range of outcomes.</p>\n<p>Perhaps the AI rules wisely and well, and can give us anything we want, <a href=\"http://dresdencodak.com/2008/06/07/eloi/\">\"save relevance\"</a>. Perhaps human culture adapts well to the utopian society, as it seems to have done in the universe of <a href=\"http://en.wikipedia.org/wiki/The_Culture\">The Culture</a>. Perhaps our uploaded descendants <a href=\"http://en.wikipedia.org/wiki/Diaspora_%28novel%29\">set off to discover the secrets of the universe</a>. I think the best way to ensure a human-centric future is to <strong>be</strong> the self-improving intelligences, instead of merely <a href=\"http://www.isfdb.org/cgi-bin/title.cgi?100345\">catching crumbs from the table</a> of our successors.</p>\n<p>In my view, the worst kind of \"Friendly\" singularity would be one where we discover we've made a weakly godlike entity who believes in benevolent dictatorship; if we must have gods, I want them to be made in our own image, beings who can be reasoned with and who can reason with one another. Best of all, though, is that singularity where we are the motivating forces, where we need not worry if we are being manipulated \"in our best interest\".</p>\n<p>Ultimately, I want the future to have room for our mistakes. For these reasons, we ought to concentrate on achieving WBE and mind uploading first.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2ac": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v5AJZyEY7YFthkzax", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 14, "extendedScore": null, "score": 5.653364354238346e-07, "legacy": true, "legacyId": "2307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 248, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-01T09:25:07.423Z", "modifiedAt": null, "url": null, "title": "Open Thread: March 2010", "slug": "open-thread-march-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:52.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AdeleneDawner", "createdAt": "2009-04-28T14:40:00.131Z", "isAdmin": false, "displayName": "AdeleneDawner"}, "userId": "MeSREm4SMRGxeQ8X3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3znTroyarywmiA9f3/open-thread-march-2010", "pageUrlRelative": "/posts/3znTroyarywmiA9f3/open-thread-march-2010", "linkUrl": "https://www.lesswrong.com/posts/3znTroyarywmiA9f3/open-thread-march-2010", "postedAtFormatted": "Monday, March 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20March%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20March%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3znTroyarywmiA9f3%2Fopen-thread-march-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20March%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3znTroyarywmiA9f3%2Fopen-thread-march-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3znTroyarywmiA9f3%2Fopen-thread-march-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p>We've had these for <a href=\"/tag/open_thread/\">a year</a>, I'm sure we all know what to do by now.</p>\n<p><em>This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3znTroyarywmiA9f3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 5.654154532463221e-07, "legacy": true, "legacyId": "2388", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 680, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-01T10:26:30.434Z", "modifiedAt": null, "url": null, "title": "Rationality quotes: March 2010", "slug": "rationality-quotes-march-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:07.662Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nJf3q38sQG2TT2WTL/rationality-quotes-march-2010", "pageUrlRelative": "/posts/nJf3q38sQG2TT2WTL/rationality-quotes-march-2010", "linkUrl": "https://www.lesswrong.com/posts/nJf3q38sQG2TT2WTL/rationality-quotes-march-2010", "postedAtFormatted": "Monday, March 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20quotes%3A%20March%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20quotes%3A%20March%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJf3q38sQG2TT2WTL%2Frationality-quotes-march-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20quotes%3A%20March%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJf3q38sQG2TT2WTL%2Frationality-quotes-march-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJf3q38sQG2TT2WTL%2Frationality-quotes-march-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>This is our monthly thread for collecting these little gems and pearls of wisdom, rationality-related quotes you've seen recently, or had stored in your quotesfile for ages, and which might be handy to link to in one of our discussions.</p>\n<ul>\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nJf3q38sQG2TT2WTL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 5.654272116475379e-07, "legacy": true, "legacyId": "2389", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 262, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-02T08:11:51.034Z", "modifiedAt": null, "url": null, "title": "For progress to be by accumulation and not by random walk, read great books", "slug": "for-progress-to-be-by-accumulation-and-not-by-random-walk", "viewCount": null, "lastCommentedAt": "2019-10-13T20:49:01.631Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ufBYjpi9gK6uvtkh5/for-progress-to-be-by-accumulation-and-not-by-random-walk", "pageUrlRelative": "/posts/ufBYjpi9gK6uvtkh5/for-progress-to-be-by-accumulation-and-not-by-random-walk", "linkUrl": "https://www.lesswrong.com/posts/ufBYjpi9gK6uvtkh5/for-progress-to-be-by-accumulation-and-not-by-random-walk", "postedAtFormatted": "Tuesday, March 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20For%20progress%20to%20be%20by%20accumulation%20and%20not%20by%20random%20walk%2C%20read%20great%20books&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFor%20progress%20to%20be%20by%20accumulation%20and%20not%20by%20random%20walk%2C%20read%20great%20books%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FufBYjpi9gK6uvtkh5%2Ffor-progress-to-be-by-accumulation-and-not-by-random-walk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=For%20progress%20to%20be%20by%20accumulation%20and%20not%20by%20random%20walk%2C%20read%20great%20books%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FufBYjpi9gK6uvtkh5%2Ffor-progress-to-be-by-accumulation-and-not-by-random-walk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FufBYjpi9gK6uvtkh5%2Ffor-progress-to-be-by-accumulation-and-not-by-random-walk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 837, "htmlBody": "<p><a href=\"http://www.infiniteinjury.org/blog/2010/02/25/reading-originals/\">This</a> recent blog post strikes me as an interesting instance of a common phenomenon.&nbsp; The phenomenon looks like the following; an intellectual, working within the assumption that the world is not mad, (an assumption not generally found outside of the Anglo-American Enlightenment intellectual tradition) notices that some feature of the world would only make sense if the world was mad.&nbsp; This intellectual responds by denouncing as silly one of the few features of this vale of tears to be, while not intelligently designed, at least structured by generalized evolution rather than by entropy.&nbsp; The key line in the post is&nbsp;</p>\n<blockquote>\n<p><em>\"Conversely in all those disciplines where we have reliable quantatative measurements of progress (with the obvious exception of history) returning to the original works of past great thinkers is decidedly unhelpful.\"</em><strong></strong></p>\n</blockquote>\n<p>I agree with the above statement, and find that the post makes a compelling argument for it.&nbsp; My only caveat is that&nbsp; we essentially never have quantitative measures of progress.&nbsp; Even in physics, when one regards not the theory but the technique of actually <em>doing </em>physics, tools and modes of thought rise and fall for reasons of fashion, and once widespread <a href=\"http://www.think-differently.org/2007/04/different-box-of-tools.html\">techniques</a> that remain useful fall into disuse.&nbsp;<a id=\"more\"></a></p>\n<p>Other important techniques, like the ones used to invent calculus in the first place, are never adequately articulated by those who use them and thus never come into general use.&nbsp; One might argue that Newton didn't use any technique to invent calculus, just a very high IQ or some other unusual set of biological traits.&nbsp; This, however, doesn't explain why a couple of people invented calculus at about the same time and place, especially given the low population of that time and place compared to the population of China over the many centuries when China was much more civilized than Europe.&nbsp;</p>\n<p>It seems likely to me that in cases like the invention of calculus, looking at the use of such techniques can contribute to their development in at least crude form.&nbsp; By analogy, even the best descriptions of how to do martial arts are inadequate to provide expertise without practice, but experience watching experts fight is a valuable complement to training by the relatively inept.&nbsp; If one wants to know the Standard Model, sure, study it directly, but if you want to actually understand how to do the sorts of things that Newton did, you would be advised to read him, Feynman and yes, Plato too, as Plato also did things which contributed greatly to the development of thought.&nbsp;</p>\n<p>Anyone who has ever had a serious intellectual following is worth some attention.&nbsp; Repeating errors is the default, so its valuable to look at ideas that were once taken seriously but are now recognized as errors.&nbsp; This is basically the converse of studying past thinkers to understand their techniques. &nbsp;&nbsp;</p>\n<p>Outside of physics, the evidence for progress is far weaker.&nbsp; Many current economists think that today we need to turn back to Keynes to find the tools that he developed but which were later abandoned or simply never caught on.&nbsp; A careful reading of Adam Smith and of Ben Franklin reveals them to use tools which did catch on centuries after he published, such as economic models of population growth which would have predicted the \"demographic transition\" which surprised almost all demographers just recently.&nbsp; Likewise, much in Darwin is part of contemporary evolutionary theory but was virtually unknown by evolutionary biologists half a century ago.&nbsp;&nbsp;</p>\n<p>As a practical matter a psychologist who knew the work of William James as well as that of B.F. Skinner or an economist who knows Hayek and Smith as well as Samuelson or Keynes is always more impressive than one who knows only the 'modern' field as 'modern' was understood by the previous generation.&nbsp; Naive induction strongly suggests that like all previous generations of social scientists, today's social scientists who specialize in contemporary theories will be judged by the next generation, who will have an even more modern theory, to be inferior to their more eclectic peers.&nbsp; Ultimately one has to look at the empirical question of the relative per-capita intellectual impressiveness of people who study only condensations and people who study original works.&nbsp; To me, the latter looks much much greater in most fields, OK, in every field that I can quickly think of except for astronomy.&nbsp;</p>\n<p>To the eclectic scholar of scholarly madness, progress is real.&nbsp; This decade's sludge contains a few gems that weren't present in the sludge of any previous decade.&nbsp; To the person who assumes that fields like economics or psychology effectively condense the findings of previous generations as background assumptions to today's work, however, progress means replacing one pile of sludge with another fashionable sludge-pile of similar quality.&nbsp; And to those few whom the stars bless with the coworkers of those who study stars?&nbsp; Well I have only looked at astronomy as through a telescope.&nbsp; I haven't seen the details on the ground.&nbsp; That said, for them maybe, just maybe, I can endorse the initial link.&nbsp; But then again, who reads old books of astronomy?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1, "bY5MaF2EATwDkomvu": 1, "sPpZRaxpNNJjw55eu": 1, "aHjTRDkGypPqbXWpN": 3, "iTe27Ced8s8bGuvMK": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ufBYjpi9gK6uvtkh5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 73, "baseScore": 67, "extendedScore": null, "score": 0.00011, "legacy": true, "legacyId": "2397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 67, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 115, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-02T21:18:28.332Z", "modifiedAt": null, "url": null, "title": "Meetup: Bay Area: Sunday, March 7th, 7pm", "slug": "meetup-bay-area-sunday-march-7th-7pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:33.078Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JustinShovelain", "createdAt": "2009-06-10T00:56:47.112Z", "isAdmin": false, "displayName": "JustinShovelain"}, "userId": "LEeresErqn3BpWrwG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/osNyB9LdEiaZLkEwv/meetup-bay-area-sunday-march-7th-7pm", "pageUrlRelative": "/posts/osNyB9LdEiaZLkEwv/meetup-bay-area-sunday-march-7th-7pm", "linkUrl": "https://www.lesswrong.com/posts/osNyB9LdEiaZLkEwv/meetup-bay-area-sunday-march-7th-7pm", "postedAtFormatted": "Tuesday, March 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%3A%20Bay%20Area%3A%20Sunday%2C%20March%207th%2C%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%3A%20Bay%20Area%3A%20Sunday%2C%20March%207th%2C%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FosNyB9LdEiaZLkEwv%2Fmeetup-bay-area-sunday-march-7th-7pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%3A%20Bay%20Area%3A%20Sunday%2C%20March%207th%2C%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FosNyB9LdEiaZLkEwv%2Fmeetup-bay-area-sunday-march-7th-7pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FosNyB9LdEiaZLkEwv%2Fmeetup-bay-area-sunday-march-7th-7pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p><a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/12747659/\">Overcoming Bias / Less Wrong meetup in the San Francisco Bay Area at SIAI House on March 7th, 2010, starting at 7PM.</a></p>\n<p>Eliezer Yudkowsky, Alicorn, and&nbsp; Michael Vassar will be present.</p>\n<p>Some other extra guests - Wei Dai, Stuart Armstrong, and Nick Tarleton - will be also be there, following our short Decision Theory mini-workshop.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "osNyB9LdEiaZLkEwv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 5.65828010755389e-07, "legacy": true, "legacyId": "2399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-02T21:46:11.930Z", "modifiedAt": null, "url": null, "title": "Individual vs. Group Epistemic Rationality", "slug": "individual-vs-group-epistemic-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:35.429Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kqdeufnJq9HdWmpaY/individual-vs-group-epistemic-rationality", "pageUrlRelative": "/posts/kqdeufnJq9HdWmpaY/individual-vs-group-epistemic-rationality", "linkUrl": "https://www.lesswrong.com/posts/kqdeufnJq9HdWmpaY/individual-vs-group-epistemic-rationality", "postedAtFormatted": "Tuesday, March 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Individual%20vs.%20Group%20Epistemic%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIndividual%20vs.%20Group%20Epistemic%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkqdeufnJq9HdWmpaY%2Findividual-vs-group-epistemic-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Individual%20vs.%20Group%20Epistemic%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkqdeufnJq9HdWmpaY%2Findividual-vs-group-epistemic-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkqdeufnJq9HdWmpaY%2Findividual-vs-group-epistemic-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 590, "htmlBody": "<p>It's common practice in this community to differentiate forms of rationality along the axes of <a href=\"http://wiki.lesswrong.com/wiki/Rationality\">epistemic vs. instrumental</a>, and <a href=\"/lw/146/the_second_best/\">individual vs. group</a>, giving rise to four possible combinations. I think our shared goal, as indicated by the motto \"rationalists win\", is ultimately to improve group instrumental rationality. Generally, improving each of these forms of rationality also tends to improve the others, but sometimes conflicts arise between them. In this post I point out one such conflict between individual epistemic rationality and group epistemic rationality.</p>\n<p>We place a lot of emphases here on calibrating individual levels of confidence (i.e., subjective probabilities), and on the idea that rational individuals will tend to converge toward agreement about the proper level of confidence in any particular idea as they update upon available evidence. But I argue that from a group perspective, it's sometimes better to have a spread of individual levels of confidence about the individually rational level. Perhaps paradoxically, disagreements among individuals can be good for the group.</p>\n<p><a id=\"more\"></a>A background fact that I start with is that almost every scientific ideas that humanity has ever come up with has been wrong. Some are obviously crazy and quickly discarded (e.g., every perpetual motion proposal), while others improve upon existing knowledge but are still subtly flawed (e.g., Newton's theory of gravity). If we accept that taking multiple approaches simultaneously is useful for solving hard problems, then upon the introduction of any new idea that is not obviously crazy, effort should be divided between extending the usefulness of the idea by working out its applications, and finding/fixing flaws in the underlying math, logic, and evidence.</p>\n<p>Having a spread of confidence levels in the new idea helps to increase individual motivation to perform these tasks. If you're overconfident in an idea, then you would tend to be more interested in working out its applications. Conversely, if you're underconfident in it (i.e., are excessively skeptical), you would tend to work harder to try to find its flaws. Since scientific knowledge is a <a href=\"http://en.wikipedia.org/wiki/Public_good\">public good</a>, individually rational levels of motivation to produce it are almost certainly too low from a social perspective, and so these individually irrational increases in motivation would tend to increase group rationality.</p>\n<p>Even amongst altruists (at least human ones), excessive skepticism can be a virtue, due to the phenomenon of <a href=\"http://en.wikipedia.org/wiki/Belief_bias\">belief bias</a>, in which \"someone's evaluation of the logical strength of an argument is biased by their belief in the truth or falsity of the conclusion\". In other words, given equal levels of motivation, you're still more likely to spot a flaw in the arguments supporting an idea if you don't believe in it. Consider a hypothetical idea, which a rational individual, after taking into account all available evidence and arguments, would assign a probability of .999 of being true. If it's a particularly important idea, then on a group level it might still be worth devoting the time and effort of a number of individuals to try to detect any hidden flaws that may remain. But if all those individuals believe that the idea is almost certainly true, then their performance in this task would likely suffer compared to those who are (irrationally) more skeptical.</p>\n<p>Note that I'm not arguing that our current \"natural\" spread of confidence levels is optimal in any sense. It may well be that the current spread is too wide even on a group level, and that we should work to reduce it, but I think it can't be right for us to aim right away for an endpoint where everyone literally agrees on everything.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zv7v2ziqexSn5iS9v": 1, "xgpBASEThXPuKRhbS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kqdeufnJq9HdWmpaY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 32, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "2398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DiBoWCBKTtzhQMwpu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-03T00:59:04.214Z", "modifiedAt": null, "url": null, "title": "Interesting Peter Norvig interview", "slug": "interesting-peter-norvig-interview", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:31.991Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/grLHzwB3pMXe7zdbQ/interesting-peter-norvig-interview", "pageUrlRelative": "/posts/grLHzwB3pMXe7zdbQ/interesting-peter-norvig-interview", "linkUrl": "https://www.lesswrong.com/posts/grLHzwB3pMXe7zdbQ/interesting-peter-norvig-interview", "postedAtFormatted": "Wednesday, March 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interesting%20Peter%20Norvig%20interview&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInteresting%20Peter%20Norvig%20interview%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrLHzwB3pMXe7zdbQ%2Finteresting-peter-norvig-interview%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interesting%20Peter%20Norvig%20interview%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrLHzwB3pMXe7zdbQ%2Finteresting-peter-norvig-interview", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrLHzwB3pMXe7zdbQ%2Finteresting-peter-norvig-interview", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<p>(Sorry this is mostly a link instead of a post, but I think it will interesting to the FAI folks here)</p>\n<p>I helped arrange this interview with Peter Norvig:</p>\n<p>http://www.reddit.com/r/blog/comments/b8aln/peter_norvig_answers_your_questions_ask_me/</p>\n<p>I think the answer to the AGI question 4 is telling, but judge for yourself. (BTW, the 'components' Peter referred to are probabilistic relational learning and hierarchical modeling. He singled these two in his singularity summit talk)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "grLHzwB3pMXe7zdbQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 6, "extendedScore": null, "score": 5.658703243114902e-07, "legacy": true, "legacyId": "2401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-03T08:27:53.554Z", "modifiedAt": null, "url": null, "title": "Priors and Surprise", "slug": "priors-and-surprise", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:33.502Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FwkdQTxZm8LSkhdeG/priors-and-surprise", "pageUrlRelative": "/posts/FwkdQTxZm8LSkhdeG/priors-and-surprise", "linkUrl": "https://www.lesswrong.com/posts/FwkdQTxZm8LSkhdeG/priors-and-surprise", "postedAtFormatted": "Wednesday, March 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Priors%20and%20Surprise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APriors%20and%20Surprise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwkdQTxZm8LSkhdeG%2Fpriors-and-surprise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Priors%20and%20Surprise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwkdQTxZm8LSkhdeG%2Fpriors-and-surprise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwkdQTxZm8LSkhdeG%2Fpriors-and-surprise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 534, "htmlBody": "<!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:DoNotOptimizeForBrowser /> </w:WordDocument> </xml><![endif]-->\n<p style=\"text-indent: 0.5in;\">I don&rsquo;t want to be too dogmatic about this claim, but Godzilla is unrealistic.<span>&nbsp; </span>I don&rsquo;t want to be too non-dogmatic about this claim either.<span>&nbsp; </span>OK then, just how dogmatic should I be?<span>&nbsp; </span>I have all sorts of reasons for thinking that skyscraper sized lizards or dinosaurs don&rsquo;t actually exist.<span>&nbsp; </span>Honestly, the most important of these is probably that none of the people who I imagine would know if they did exist seem to believe in them.<span>&nbsp; </span>I never hear any mention of them in the news, in history books, etc, and I don&rsquo;t see their effects in the national death statistics.<span>&nbsp; </span>No industries seem to exist to deal with their rampages, and no oil or shipping companies lose stock value from lizard attacks.<span>&nbsp; </span>Casually, at least, Godzilla attacks don&rsquo;t seem like the sort of basic fact about the world that people could just overlook.<span>&nbsp; </span>How confident should I be that Godzilla type creatures don't exist?&nbsp;</p>\n<p style=\"text-indent: 0.5in;\">I can also fairly easily recognize good biological reasons not to expect there to be giant rampaging lizards.<span>&nbsp; </span>The square/cube law, in its many manifestations, is the most basic of these, but by itself is not completely decisive.<span>&nbsp; </span>I can imagine physical workarounds that would allow <em>sequoia giganticus </em>sized reptiles, but not without novel bio-machinery that would take a long time to evolve and would surely be found in many other organisms.<span>&nbsp; </span>I can even vaguely imagine ways in which biology might prove resistant to conventional military weaponry and ecological niches and lifestyles that might support both such biology and such size, though much of my knowledge of Earth&rsquo;s ecosystems would have to be re-written.<span>&nbsp;&nbsp;&nbsp; </span>For all that, if I lived in a world where essentially all authorities did refer to the activities of <em>godzilla giganticus<span>&nbsp; </span></em>I would probably accept that they were probably correct regarding its existence.<span>&nbsp; </span>What should a hypothetical person who lived in a world where the existence of Godzilla type creatures was common knowledge and was regarded as an ordinary non-numinous fact about the world believe?</p>\n<p style=\"text-indent: 0.5in;\">Godzilla would be considerably more perplexing than <a href=\"http://www.unmuseum.org/rocksky.htm\">thunderstones</a>, and would have to be considerably better documented to be credible.<span>&nbsp; </span>Even with the strongest documentation I would have substantial unresolved questions, inferring that Godzilla&rsquo;s native ecosystem must be quite different from any known (possibly inferring that the details are classified), and even wondering whether Godzilla was a biological creature at all as opposed to, for instance, a giant robot left behind by an advanced and forgotten civilization, a line of inquiry that would greatly increase my credence in secret history of all kinds.<span>&nbsp; </span>For the most part though, I would probably go about life as normal.<span>&nbsp; </span>Even Natural Selection, the most damaged part of my world-view, would endure as a great intellectual triumph explaining the origins of <strong>almost</strong> all of Earth&rsquo;s life forms.<span>&nbsp; </span>Only peripheral facts, such as distant history and the nature of some exotic ecosystems would be deeply called into question, and such facts are not tightly integrated with the broader edifice of science.<span>&nbsp; </span>In a conversation with a hypothetical Michael Vassar who believed in Godzilla, the issue would typically not come up.<span>&nbsp; </span>Science in general would not be called into question in my mind, but <a href=\"http://www.overcomingbias.com/2007/08/your-strength-a.html\">should it be</a>?<span>&nbsp; <br /></span></p>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:DoNotOptimizeForBrowser /> </w:WordDocument> </xml><![endif]--><!-- /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-parent:\"\"; margin:0in; margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-fareast-font-family:\"Times New Roman\";} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.25in 1.0in 1.25in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} --></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb108": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FwkdQTxZm8LSkhdeG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 23, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "2403", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-04T22:13:59.800Z", "modifiedAt": null, "url": null, "title": "The Graviton as Aether", "slug": "the-graviton-as-aether", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:36.239Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mXPuYeQz29ystznrZ/the-graviton-as-aether", "pageUrlRelative": "/posts/mXPuYeQz29ystznrZ/the-graviton-as-aether", "linkUrl": "https://www.lesswrong.com/posts/mXPuYeQz29ystznrZ/the-graviton-as-aether", "postedAtFormatted": "Thursday, March 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Graviton%20as%20Aether&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Graviton%20as%20Aether%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXPuYeQz29ystznrZ%2Fthe-graviton-as-aether%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Graviton%20as%20Aether%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXPuYeQz29ystznrZ%2Fthe-graviton-as-aether", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmXPuYeQz29ystznrZ%2Fthe-graviton-as-aether", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1341, "htmlBody": "<blockquote>\n<p>Well, first:&nbsp; Does any collapse theory have any experimental support?&nbsp; No.</p>\n<p>With that out of the way...</p>\n<p>If collapse actually worked the way its adherents say it does, it would be:</p>\n<ol>\n<li>The only non-<a href=\"/lw/pq/the_socalled_heisenberg_uncertainty_principle\">linear</a> evolution in all of quantum mechanics.</li>\n<li>The only non-<a href=\"/lw/py/the_born_probabilities\">unitary</a> evolution in all of quantum mechanics.</li>\n<li>The only non-<a href=\"/lw/pw/decoherence_is_pointless\">differentiable</a> (in fact, discontinuous) phenomenon in all of quantum mechanics.</li>\n<li>The only phenomenon in all of quantum mechanics that is non-<a href=\"/lw/pr/which_basis_is_more_fundamental\">local</a> in the configuration space.</li>\n<li>The only phenomenon in all of physics that violates <a href=\"/lw/pp/decoherence\">CPT symmetry</a>.</li>\n<li>The only phenomenon in all of physics that violates <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of\">Liouville's Theorem</a> (has a many-to-one mapping from initial conditions to outcomes).</li>\n<li>The only phenomenon in all of physics that is acausal / non-deterministic / <a href=\"/lw/oj/probability_is_in_the_mind\">inherently random</a>.</li>\n<li>The only phenomenon in all of physics that is non-local in spacetime and <a href=\"/lw/q2/spooky_action_at_a_distance_the_nocommunication\">propagates an influence faster than light</a>.</li>\n</ol>\n<p>WHAT DOES THE GOD-DAMNED COLLAPSE POSTULATE HAVE TO <em>DO</em> FOR PHYSICISTS TO REJECT IT?&nbsp; KILL A GOD-DAMNED PUPPY?</p>\n</blockquote>\n<p>- Eliezer Yudkowsky, <a href=\"http://www.google.com/url?sa=t&amp;source=web&amp;ct=res&amp;cd=1&amp;ved=0CAYQFjAA&amp;url=http%3A%2F%2Flesswrong.com%2Flw%2Fq6%2Fcollapse_postulates%2F&amp;ei=dCCQS8zpI9CVtgej27WQCw&amp;usg=AFQjCNERowu4LnSq2tbCHXPSqp_hpVI-_Q\">Collapse Postulates</a></p>\n<p>In the olden days of physics, circa 1900, many prominent physicists believed in a substance known as aether. The principle was simple: Maxwell's equations of electromagnetism had shown that light was a wave, and light followed many of the same equations as sound waves and water waves. However, every other kind of wave- sound waves, water waves, waves in springs- needs some sort of medium for its transmission. A \"wave\" is not really a physical object; it is just a disturbance of some other substance. For instance, if you throw a rock into a pond, you cannot pluck the waves out of the pond and take them home with you in your backpack, because the \"waves\" are just peaks and troughs in the puddle of water (the medium). Hence, there should be some sort of medium for light waves, and the physicists named this medium \"aether\".<a id=\"more\"></a></p>\n<p>However, difficulties soon developed. If you have a jar, you can pump the air out of the jar, and then the jar will no longer transmit sound, demonstrating that the wave medium (the air) has been removed. But, there was no way to remove the aether from a jar; no matter what the experimentalists did, you could still shine light through it. There was, in fact, no way of detecting, altering, or experimenting with aether at all. Physicists knew that aether must be unlike all other matter, because it could apparently pass through closed containers made of any substance. And finally, the Michelson-Morely experiment showed that the \"aether\" was always stationary relative to Earth, even though the Earth changed direction every six months as it moved about in its orbit! Shortly thereafter, the inconsistencies were resolved with Albert Einstein's Theory of Special Relativity, and everyone realized that aether was imaginary.</p>\n<p>Shortly thereafter, during the 20th century, physicists discovered two new forces of nature: the strong nuclear force and the weak nuclear force. These two forces, as well as electromagnetism, could be described very well on the quantum level: they were created by the influence of mediator particles called (respectively) gluons, W and Z bosons, and photons, and these particles obeyed the laws of quantum mechanics just like electrons and mesons did. The description of these three forces, as well as the particles they act upon, has been neatly unified in a theory of physics known as the Standard Model, which has been our best known description of the universe for thirty years now.</p>\n<p>However, gravity is not a part of this model. Making an analogy to the other forces, physicists have proposed a mediator particle known as the \"graviton\". The graviton is thought to be similar to the photon, the gluon, and the W and Z bosons, except that it is massless and has spin 2. I posit that the \"graviton\" is essentially the same theory as the \"aether\": a misguided attempt to explain something by reference to similar-seeming things that were explained in the same way. Consider the following facts:</p>\n<ul>\n<li>Some of the brightest minds in the world have been working non-stop on how to to extend the Standard Model/quantum mechanics theory of forces to gravity, for more than forty years, and we have yet to find a satisfying way of doing it. Theories of physics have a known tendency towards elegance and simplicity, and nothing we have come up with in the past forty years is either simple or elegant, let alone both.</li>\n<li>Of the explanations that have been proposed, string theory is currently the most popular. The idea that a theory of physics can have \"popularity\" should, in and of itself, be a warning sign. There can only ever be one reality, and so there can only ever be one correct theory of reality. The physics community, of course, cannot all just switch to a new theory as soon as its correctness has been proven; there are lots of inevitable time lags. However, we have had ongoing disputes between people in the string theory camp, people in the loop quantum gravity camp, and people in other camps for more than <em>thirty years</em> now, with zero sign of a winner or of the matter resolving itself anytime soon. This, to me anyway, is a sign that no one really knows what is going on, and lots of people are just following their monkey intuitions and forming <a href=\"/lw/gt/a_fable_of_science_and_politics/\">Blue and Green teams</a>.</li>\n<li>String theory, of course, has zero testable predictions. Loop quantum gravity, another graviton-based theory, also has zero testable predictions. In fact, to my knowledge, <em>all </em>of the use-gravitons-to-explain-gravity-on-a-quantum-scale theories have zero testable predictions.</li>\n<li>String theory has additional problems: it's not a <em>satisfying </em>explanation in the same way that relativity, or Newtonian gravity, or Keplerian astronomy, or Maxwell's laws are, because there are so many different versions of it. You can't just go read a textbook on string theory, and say \"Aha! So that's how the world really works\", because there are a ton of different kinds of string theory. There are (as of this writing) bosonic string theory, type I string theory, type IIA string theory, type IIB string theory, and heterotic string theory. (This list does not include the different ways of describing the universe <em>within</em> each of these theories, which are practically infinite.) It also requires numerous additional dimensions, which are unobservable and untestable, but which are required to make the math work right.</li>\n</ul>\n<p>And, with reference to the graviton itself:</p>\n<ul>\n<li>Gravitons have, of course, never been detected, and (so far as I know) there aren't even any serious proposals on the table for how to detect them. In fact, it is widely thought that gravitons have such low interaction cross-sections that detecting them with <em>any physically possible detector</em> is a hopeless proposition.</li>\n<li>The classical theory of gravity that we <em>know</em> works well, General Relativity, makes no mention whatsoever of gravitons, and describes gravity in terms of the curvature of space-time. None of the other three forces (weak, strong, electromagnetic) have ever been described in this way, but gravity <em>has</em>, with a precision of more than twelve decimal places, in addition to many other unique predictions (such as the geodetic effect, which has recently been confirmed experimentally). </li>\n<li>Gravity is much, much, much weaker than all of the other forces. In fact, it is more than <em>thirty-five orders of magnitude</em> weaker than both the electromagnetic force and the strong nuclear force.&nbsp;</li>\n<li>Gravity is nonrenormalizable, which means that, if you try to calculate the strength of the gravitational interactions using a quantum-mechanical, quantized-graviton model, there's no simple way to make infinities go away, like there is for electromagnetism, the strong force and the weak force.</li>\n</ul>\n<p>So, what's <em>really</em> going on here? I don't know. I'm not Albert Einstein. But I suspect it will take someone like him- someone brilliant, very good at physics, yet largely outside the academic system- to resolve this mess, and tell us what's really happening.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mXPuYeQz29ystznrZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 15, "extendedScore": null, "score": 5.663917303429161e-07, "legacy": true, "legacyId": "2415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 136, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eWuuznxeebcjWpdnH", "3ZKvf9u2XEWddGZmS", "aWFwfk3MBEyR4Ne8C", "XDkeuJTFjM9Y2x6v6", "JrhoMTgMrMRJJiS48", "QkX2bAkwG2EpGvNug", "f6ZLxEWaankRZ2Crv", "DY9h6zxq6EMHrkkxE", "6hfGNLf4Hg5DXqJCF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-04T22:59:01.986Z", "modifiedAt": null, "url": null, "title": "The fallacy of work-life compartmentalization", "slug": "the-fallacy-of-work-life-compartmentalization", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:45.234Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZouugGbM4SqTEQBZW/the-fallacy-of-work-life-compartmentalization", "pageUrlRelative": "/posts/ZouugGbM4SqTEQBZW/the-fallacy-of-work-life-compartmentalization", "linkUrl": "https://www.lesswrong.com/posts/ZouugGbM4SqTEQBZW/the-fallacy-of-work-life-compartmentalization", "postedAtFormatted": "Thursday, March 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20fallacy%20of%20work-life%20compartmentalization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20fallacy%20of%20work-life%20compartmentalization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZouugGbM4SqTEQBZW%2Fthe-fallacy-of-work-life-compartmentalization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20fallacy%20of%20work-life%20compartmentalization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZouugGbM4SqTEQBZW%2Fthe-fallacy-of-work-life-compartmentalization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZouugGbM4SqTEQBZW%2Fthe-fallacy-of-work-life-compartmentalization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 853, "htmlBody": "<p><strong>Related to</strong>: <a href=\"/lw/gv/outside_the_laboratory/\">Outside the Laboratory</a>, <a href=\"/lw/rf/ghosts_in_the_machine/\">Ghosts in the Machine</a></p>\n<p>We've all observed how people can be very smart in some contexts, and stupid in others. People compartmentalize, which has been <a href=\"/lw/1mh/what_is_in_that_click/\">previously hypothesized</a> as the reason for some epic failures to understand things that should be obvious.</p>\n<p>It's also important to remember that we are not immune. To that end, I want to start off by considering some comfortable examples, where someone else is the butt of the joke, and then consider examples which might make you more uneasy.</p>\n<blockquote>\n<p>\"The mere presence of a computer can short circuit normally intelligent people's brains.\" -- Computer Stupidities</p>\n</blockquote>\n<p>The reassuring cases concern smart people who become stupid when confronted with our area of expertise. If you're a software developer, that tends to be people who can't figure out something basic about Windows. \"I've tried closing the app and restarting, and I've tried rebooting, and it doesn't work, I still can't find my file.\" You take a deep breath, refrain from rolling your eyes and asking what the <em>heck</em> their mental model is, what they think closing-and-restarting has to do with a misplaced file, and you go looking for some obvious places, like the Desktop, where they keep all their files but somehow neglected to look this time. If it's not there, chances are it will be in My Documents.</p>\n<p>It's sometimes draining to be called on for this kind of thing, but it can be reassuring. My dad is a high calibre mathematician, dealing in abstractions at a level that seems stratospheric compared to my rusty-college-math. But we sometimes get into conversations like the above, and I get a slightly guilty self-esteem boost from them.</p>\n<p>Now, the harder question: how do <em>we</em> compartmentalize?</p>\n<p><a id=\"more\"></a></p>\n<p>I propose work-life compartmentalization as a case study. \"Work-life balance\" is how we rationalize that separation. It's OK, we think, to put up with some unpleasantness from 9 to 5, as long as we can look forward to getting home, kicking our shoes off and relaxing, alone or among family or friends. And perhaps that's reasonable enough.</p>\n<p>But this logic leads many people to tolerate: stress, taking orders, doing work that we think is meaningless, filling out paperwork that will never actually be read, pouring our energy into projects we're certain are failure-bound but never speaking up about that to avoid being branded \"not a team player\", being bored in endless meetings which are thinly disguised status games, feeling unproductive and stupid but grinding on anyway because it's \"office hours\" and that's when we are supposed to work, and so on.</p>\n<p>And those are only the milder symptoms. <a href=\"http://www.workplacebullying.org/research/WBI-Zogby2007Survey.html\">Workplace bullying</a>, <a href=\"http://www.infoq.com/news/2008/01/crunch-mode\">crunch mode</a>, <a href=\"http://www.anonymousemployee.com/csssite/sidelinks/lying.php\">dodgy workplace ethics</a> are worryingly prevalent. (There are large variations in this type of workplace toxicity; some of us are lucky enough to never catch but a whiff of it, some of us unfortunately are exposed to a high degree. That these are real and widespread phenomena is evidenced by the success of TV shows showing office life as its darkest; humor is a defense mechanism.)</p>\n<p>Things snapped into focus for me one day when a manager asked me to lie to a client about my education record in order to get a contract. I refused, expecting to be fired; that didn't happen. Had I really been at risk? The incident anyway fueled a resolve to try and apply at work the same standards that I do in life - when I think rationally.</p>\n<p>In everyday life, rationality suggests we try to avoid boredom, tells us it's unwise to make promises we can't keep, to avoid getting <a href=\"/lw/uw/entangled_truths_contagious_lies/\">entangled in our own lies</a>, and so on. What might happen if we tried to apply the same standards in the workplace?</p>\n<p>Instead of tolerating boredom in meetings, you may find it more effective to apply a set of criteria to any meeting - does it have an agenda, a list of participants, a set ending time and a known objective - and not show up if it doesn't meet them.</p>\n<p>You might refuse to give long-term schedule estimate for tasks that you didn't really believe in, and instead try breaking the task down, working in short <a href=\"http://en.wikipedia.org/wiki/Timeboxing\">timeboxed iterations</a> and updating your estimates based on observed reality, committing only to as much work as is compatible with maintaining peak productivity.</p>\n<p>You might stop tolerating the most <a href=\"http://en.wikipedia.org/wiki/The_No_Asshole_Rule\">egregious status games</a> that go on in the workplace, and strive instead for effective collective action: teamwork.</p>\n<p>Those would be merely <em>sane</em> behaviours. It is, perhaps, optional to extend this thinking to actually challenging the usual workplace norms, and starting to do things differently <em>just</em> because they would be <a href=\"http://en.wikipedia.org/wiki/Maverick_%28book%29\">better that way</a>. The world is crazy, and that includes the world of work. People who insist on not checking their brain at the door of the workplace are still few and far between, and to really change a workplace it takes a critical mass of them.</p>\n<p>But I've seen it happen, and helped it happen, and the results make me want to find out more about other areas where I still compartmentalize. The prospect is a little scary - I still find it unpleasant to find out I've been stupid - but also exciting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZouugGbM4SqTEQBZW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 13, "extendedScore": null, "score": 5.66400286383087e-07, "legacy": true, "legacyId": "2416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["N2pENnTPB75sfc9kb", "cnYHFNBF3kZEyx24v", "R3ATEWWmBhMhbY2AL", "wyyfFfaRar2jEdeQK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-05T21:09:25.939Z", "modifiedAt": null, "url": null, "title": "Signaling Strategies and Morality", "slug": "signaling-strategies-and-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:38.690Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BviRaP4przARdmb8b/signaling-strategies-and-morality", "pageUrlRelative": "/posts/BviRaP4przARdmb8b/signaling-strategies-and-morality", "linkUrl": "https://www.lesswrong.com/posts/BviRaP4przARdmb8b/signaling-strategies-and-morality", "postedAtFormatted": "Friday, March 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Signaling%20Strategies%20and%20Morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASignaling%20Strategies%20and%20Morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBviRaP4przARdmb8b%2Fsignaling-strategies-and-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Signaling%20Strategies%20and%20Morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBviRaP4przARdmb8b%2Fsignaling-strategies-and-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBviRaP4przARdmb8b%2Fsignaling-strategies-and-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 909, "htmlBody": "<p>I am far from convinced that people in general wish to be seen as caring more about morality than they actually do.&nbsp; If this was the case, why would the persistent claim that people are -- and, logically, must be -- egoists have so long survived strong counter-arguments?&nbsp; The argument appears to me to be a way of signaling a lack of excessive and low status moral scruples.&nbsp;</p>\r\n<p>It seems to me that the desire to signal&nbsp;as much morality&nbsp;as possible is held by a minority of women and by a small minority of men.&nbsp; Those people are also the main people who talk about morality.&nbsp; This is commonly a problem in the development of thought.&nbsp; People with an interest in verbally discussing a subject may have systematically atypical attitudes towards that subject.&nbsp; Of course, this issue is further complicated by the fact that people don't agree on what broad type of thing morality is.&nbsp;</p>\r\n<p>The conflict within philosophy between Utilitarians and Kantians is among the most famous examples of this disagreement. &lt;a href=\" <a href=\"http://people.virginia.edu/~jdh6n/moraljudgment.html\">http://people.virginia.edu/~jdh6n/moraljudgment.html</a>&rdquo;&gt; Haidt&rsquo;s views on conservative vs. liberal morality &lt;/a&gt; is another.&nbsp; Major, usually&nbsp;implicit disagreements&nbsp;regard whether morality is supposed to serve as a decision system, a set of constraints on a decision system, or a set of reasons that should influence a person along with prudential, honor, spontaneity, authenticity, and other such types of reasons.</p>\r\n<p>It seems to me that people usually want to signal whatever gives others the most reason to respect their interests.&nbsp;&nbsp;Roughly, this amounts to wanting to signal what Haidt calls conservative morality.&nbsp; Basically, people would like to signal \"I am slightly more committed to the group&rsquo;s welfare, particularly to that of its weakest members (caring), than most of its members are.&nbsp; If you suffer a serious loss of status/well-being&nbsp;I will still help you in order to display affiliation to the group even though you will no longer be in a position to help me.&nbsp; I am substantially more kind and helpful to the people I like (loyal) and&nbsp;substantially more vindictive and aggressive towards those I dislike (honorable, ignored by Haidt).&nbsp; I am generally stable in who I like (loyalty and identity, implying&nbsp;low cognitive cost for allies, low variance long term investment).&nbsp; I am much more capable and popular than most members of the group, demand appropriate consideration, and grant appropriate consideration to those more capable than myself (status/hierarchy).&nbsp; I adhere to simple taboos (not disgusting) so that my reputation and health are secure and so that&nbsp;I am unlikely to contaminate the reputations or health of my friends.&nbsp; I currently like you and dislike your enemies but I am somewhat inclined towards ambivalence on regarding whether I like you right now so the pay-off would be very great for you if you were to expend resources pleasing me and get me into the stable 'liking you' region of my possible attitudinal space.&nbsp; Once there, I am likely to make a strong commitment to a friendly attitude towards you rather than wasting cognitive resources checking a predictable parameter among my set of derivative preferences.\"&nbsp;&nbsp;<br />&nbsp;<br />An interesting point here is that this suggests the existence of a trade-off in the level of intelligence that a person wishes to signal.&nbsp; In this model, intelligence is necessary to distinguish between costly/genuine and cheap/fake signals of affiliation and to be effective as a friend or as an enemy.&nbsp; For these reasons, people want to be seen as somewhat more intelligent than the average member of the group.&nbsp; People also want to appear slightly less intelligent than whoever they are addressing, in order to avoid appearing unpredictable.&nbsp;&nbsp;</p>\r\n<p>This is plausibly a multiple equilibrium model.&nbsp; You can appear slightly more&nbsp;or less intelligent&nbsp;with effort, confidence and affectations.&nbsp; Trying to appear much less intelligent than you are is difficult as you must essentially simulate one system with another system, which implies an overhead cost.&nbsp; If you can't appear to be little more intelligent than the higher status members of the group, who typically have modestly above average intelligence, you can't easily be a trusted ally of the people you most need to ally with.&nbsp; If you can't effectively show yourself to be a predictable ally for individuals you may want to show yourself to be a predictable ally of the group by predictably following rules (justice) and by predictably serving its collective interests (caring).&nbsp; That allows less intelligent individuals in the group to outsource the task of scrutinizing your loyalty.&nbsp; People can more easily communicate indicators of group disloyalty&nbsp;by asserting that you have broken a rule, so people who can't be&nbsp;conservatively moral will attend more closely to rules.&nbsp; On this model, Haidt's liberalism (which I believe includes libertarianism) is a consequence of difficulty credibly signaling personal loyalties and thus having to overemphasize caring and what he calls justice, by which he means following rules.&nbsp;</p>\r\n<p>In America, the explicit rules that people are given are descended from a frontier setting where independence was very practically important and where morality with very strong acts/omissions distinctions was sufficient to satisfy collective needs with low administrative costs and with easy cheater detection.&nbsp; Leaving others alone (and implicitly, tolerance)&nbsp;rather than enforcing purity works well when large distances make good neighbors.&nbsp; As a result, the explicit rules that people are taught de-emphasize status/hierarchy, disgust, to a lesser degree loyalty and identity, and to a still lesser extent caring.&nbsp; When the influence of justice, e.g. rules, is emphasized by difficulty in behaving predictably, liberal morality, or ultimately libertarian morality, are the result.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2, "Q6P8jLn8hH7kbuXRr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BviRaP4przARdmb8b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 20, "extendedScore": null, "score": 5.666561134868879e-07, "legacy": true, "legacyId": "2412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-06T01:45:39.377Z", "modifiedAt": null, "url": null, "title": "TED Talks: Daniel Kahneman", "slug": "ted-talks-daniel-kahneman", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:33.636Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NHwfLYYw5MfxGP4Zu/ted-talks-daniel-kahneman", "pageUrlRelative": "/posts/NHwfLYYw5MfxGP4Zu/ted-talks-daniel-kahneman", "linkUrl": "https://www.lesswrong.com/posts/NHwfLYYw5MfxGP4Zu/ted-talks-daniel-kahneman", "postedAtFormatted": "Saturday, March 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20TED%20Talks%3A%20Daniel%20Kahneman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATED%20Talks%3A%20Daniel%20Kahneman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHwfLYYw5MfxGP4Zu%2Fted-talks-daniel-kahneman%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=TED%20Talks%3A%20Daniel%20Kahneman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHwfLYYw5MfxGP4Zu%2Fted-talks-daniel-kahneman", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNHwfLYYw5MfxGP4Zu%2Fted-talks-daniel-kahneman", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p>People who have had a painful experience remember it as less painful if the pain tapers off, rather than cutting off sharply at the height of intensity, <em>even if they experience more pain overall</em>. I'd heard of this finding before (from Dan Ariely), but Kahneman uses the finding to throw the idea of \"experiencing self\" vs. \"remembering self\" into sharp relief. He then discusses the far-reaching implications of this dichotomy and our blindness to it.</p>\n<p>The talk is entitled \"<a href=\"http://www.youtube.com/watch?v=XgRlrBl-7Yg&amp;feature=sub\">The riddle of experience vs. memory</a>\".</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NHwfLYYw5MfxGP4Zu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 24, "extendedScore": null, "score": 5.667092349663029e-07, "legacy": true, "legacyId": "2427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-06T08:03:42.180Z", "modifiedAt": null, "url": null, "title": "The strongest status signals", "slug": "the-strongest-status-signals", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pwno", "createdAt": "2009-02-27T06:17:31.584Z", "isAdmin": false, "displayName": "pwno"}, "userId": "SCgoHNxqc2agmDWEg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FvBWB7rsdMDPgyzxE/the-strongest-status-signals", "pageUrlRelative": "/posts/FvBWB7rsdMDPgyzxE/the-strongest-status-signals", "linkUrl": "https://www.lesswrong.com/posts/FvBWB7rsdMDPgyzxE/the-strongest-status-signals", "postedAtFormatted": "Saturday, March 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20strongest%20status%20signals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20strongest%20status%20signals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvBWB7rsdMDPgyzxE%2Fthe-strongest-status-signals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20strongest%20status%20signals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvBWB7rsdMDPgyzxE%2Fthe-strongest-status-signals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvBWB7rsdMDPgyzxE%2Fthe-strongest-status-signals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 730, "htmlBody": "<p class=\"MsoNormalCxSpMiddle\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\" \" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!-- /* Font Definitions */ @font-face {font-family:\"Cambria Math\"; panose-1:0 0 0 0 0 0 0 0 0 0; mso-font-charset:1; mso-generic-font-family:roman; mso-font-format:other; mso-font-pitch:variable; mso-font-signature:0 0 0 0 0 0;} @font-face {font-family:Calibri; panose-1:2 15 5 2 2 2 4 3 2 4; mso-font-charset:0; mso-generic-font-family:swiss; mso-font-pitch:variable; mso-font-signature:-1610611985 1073750139 0 0 159 0;} /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-unhide:no; mso-style-qformat:yes; mso-style-parent:\"\"; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:Calibri; mso-fareast-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} .MsoChpDefault {mso-style-type:export-only; mso-default-props:yes; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:Calibri; mso-fareast-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} .MsoPapDefault {mso-style-type:export-only; margin-bottom:10.0pt; line-height:115%;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.0in 1.0in 1.0in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} --><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">The community&rsquo;s awareness and strong understanding of status-motivated behavior in humans is clearly evident. However, I still believe the community focuses too much on a small subset of observable status transactions; namely, status transactions that occur between people of approximately the same status level. My goal is to bring attention to the rest of the status game.</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">---</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">Because your attention is a limited resource and carries an opportunity cost, your mind is evolved to constantly be on the look-out for stimuli that may affect your survival and reproductive success and ignore stimuli that doesn&rsquo;t. Of course, the stimulus doesn&rsquo;t really have to affect your fitness, it just needs some experienceable property that correlates with an experience in the ancestral environment that did. But when our reaction to stimuli proves to be non-threatening, through repeated exposure, we eventually become desensitized and stop reacting. Much like how first time drivers are more reactive to stimuli than experienced drivers: the majority of past mental processes are demoted from executive functions and become automated. So it&rsquo;s safe to posit a sort of adaptive mechanism that filters sensory input to keep your attention-resources spent efficiently. This attention-conserving mechanism is the crux of status transactions.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">When someone is constantly surrounded by people who don&rsquo;t have power i.e. status over them, their attention-conserving mechanism goes to work. In this case, the stimulus they&rsquo;re filtering out is &ldquo;people who share experienceable characteristics with low status people they&rsquo;re constantly surrounded by.&rdquo; The stimulus, over time, proved it&rsquo;s not worthy of being paid attention to. And just like an experienced driver, the person devotes substantially less attention-resources towards the uninteresting stimuli. </span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">The important thing to note is the <em>behavior<strong> </strong></em>that&rsquo;s a function of how much attention-resources are used. These behaviors can be interpreted as evidence of the relative status levels in an interaction. And because it&rsquo;s evolutionarily advantageous to recognize your own status level, we&rsquo;ve evolved a mechanism that detects these behaviors in order to assist us in figuring out our status level. [Notice how this isn&rsquo;t a chicken and egg problem]. </span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">This behavior manifests itself in all sorts of ways in humans. Instead of enumerating all the behaviors, think of such behaviors like this: </span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">Assume an individual optimizes for their comfort in a given experienceable environment. If an additional stimulus (In terms of status, the relevant stimulus is other people) enters their environment and causes them to change their previous behavior, that stimulus has non-zero expected power over the individual. Why else would they change their most comfortable state if the stimulus presented nothing of value or no threat? Of course every stimulus will cause <em>some</em> change in behavior (at least initially) so the interesting question is <em>how</em> <em>much</em> behavior changed. The greater the reactivity from the stimulus, the more expected power the stimulus has over the individual. </span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">The strongest status signal is observable reactivity; not only because we naturally react to interesting stimuli, but also because we&rsquo;re evolved to interpret reactivity as evidence for status. </span></p>\n<pre class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></pre>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">Most status signaling discussed on Lesswrong is about certain stuff people wear, say, associate with, argue about, etc. What Lesswrongers are not realizing is how bothering to change your behavior <em>at all</em> towards other people is inherently status lowering. For instance, if you just engage in an argument with someone you&rsquo;re telling them they&rsquo;re important enough to use so much of your attention and effort&mdash;even if you &ldquo;act&rdquo; high status the whole time. If a rock star simply gazes at their biggest fan, the fan will feel higher status. That&rsquo;s because just getting the rock star&rsquo;s attention is an accomplishment. </span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">By engaging in an activity with others, like having a conversation, participants assume a plausible upper and lower bound status level for each other. The fact they both care enough to engage in an activity together is evidence they&rsquo;re approximately the same status level. Because of this, they can&rsquo;t do any signals that reliably indicate they&rsquo;re much higher status the other. So most status signaling they&rsquo;ll be doing to each other won&rsquo;t influence their status much. </span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">The behavior induced by indifference and reactivity to stimuli is where the strong evidence resides. Everything else merely budges what&rsquo;s already been proven by indifference and reactivity. In short, the sort of status signaling Lesswrong has been concerned with is only the tip of the iceberg.</span></p>\n<p class=\"MsoNormalCxSpMiddle\" style=\"margin-bottom: 0.0001pt; line-height: normal;\"><span style=\"font-size: 12pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;;\">&nbsp;</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FvBWB7rsdMDPgyzxE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "2431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-06T08:13:40.962Z", "modifiedAt": null, "url": null, "title": "The strongest status signals", "slug": "the-strongest-status-signals-4", "viewCount": null, "lastCommentedAt": "2019-08-15T14:08:11.026Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pwno", "createdAt": "2009-02-27T06:17:31.584Z", "isAdmin": false, "displayName": "pwno"}, "userId": "SCgoHNxqc2agmDWEg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qeswkGtzfGg7GSP97/the-strongest-status-signals-4", "pageUrlRelative": "/posts/qeswkGtzfGg7GSP97/the-strongest-status-signals-4", "linkUrl": "https://www.lesswrong.com/posts/qeswkGtzfGg7GSP97/the-strongest-status-signals-4", "postedAtFormatted": "Saturday, March 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20strongest%20status%20signals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20strongest%20status%20signals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqeswkGtzfGg7GSP97%2Fthe-strongest-status-signals-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20strongest%20status%20signals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqeswkGtzfGg7GSP97%2Fthe-strongest-status-signals-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqeswkGtzfGg7GSP97%2Fthe-strongest-status-signals-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 748, "htmlBody": "<p>The community&rsquo;s awareness and strong understanding of status-motivated behavior in humans is clearly evident. However, I still believe the community focuses too much on a small subset of observable status transactions; namely, status transactions that occur between people of approximately the same status level. My goal is to bring attention to the rest of the status game.<br /><br />Because your attention is a limited resource and carries an opportunity cost, your mind is evolved to constantly be on the look-out for stimuli that may affect your survival and reproductive success and ignore stimuli that doesn&rsquo;t. Of course, the stimulus doesn&rsquo;t really have to affect your fitness, it just needs some experienceable property that correlates with an experience in the ancestral environment that did. But when our reaction to stimuli proves to be non-threatening, through repeated exposure, we eventually become desensitized and stop reacting. Much like how first time drivers are more reactive to stimuli than experienced drivers: the majority of past mental processes are demoted from executive functions and become automated. So it&rsquo;s safe to posit a sort of adaptive mechanism that filters sensory input to keep your attention-resources spent efficiently. This attention-conserving mechanism is the crux of status transactions.<br /><br />When someone is constantly surrounded by people who don&rsquo;t have power i.e. status over them, their attention-conserving mechanism goes to work. In this case, the stimulus they&rsquo;re filtering out is &ldquo;people who share experienceable characteristics with low status people they&rsquo;re constantly surrounded by.&rdquo; The stimulus, over time, proved it&rsquo;s not worthy of being paid attention to. And just like an experienced driver, the person devotes substantially less attention-resources towards the uninteresting stimuli.<br /><br />The important thing to note is the behavior that&rsquo;s a function of how much attention-resources are used. These behaviors can be interpreted as evidence of the relative status levels in an interaction. And because it&rsquo;s evolutionarily advantageous to recognize your own status level, we&rsquo;ve evolved a mechanism that detects these behaviors in order to assist us in figuring out our status level. [Notice how this isn&rsquo;t a chicken or the egg problem].<br /><br />This behavior manifests itself in all sorts of ways in humans. Instead of enumerating all the behaviors, think of such behaviors like this:<br /><br />Assume an individual optimizes for their comfort in a given experienceable environment. If an additional stimulus (In terms of status, the relevant stimulus is other people) enters their environment and causes them to change their previous behavior, that stimulus has non-zero expected power over the individual. Why else would they change their most comfortable state if the stimulus presented nothing of value or no threat? Of course every stimulus will cause some change in behavior (at least initially) so the interesting question is how much behavior changed. The greater the reactivity from the stimulus, the more expected power the stimulus has over the individual.<br /><br />The strongest status signal is observable reactivity; not only because we naturally react to interesting stimuli, but also because we&rsquo;re evolved to interpret reactivity as evidence for status.<br /><br />Most status signaling discussed on Lesswrong is about certain stuff people wear, say, associate with, argue about, etc. What Lesswrongers may not realize is how bothering to change your behavior at all towards other people is inherently status lowering. For instance, if you just engage in an argument with someone you&rsquo;re telling them they&rsquo;re important enough to use so much of your attention and effort&mdash;even if you &ldquo;act&rdquo; high status the whole time. If a rock star simply gazes at their biggest fan, the fan will feel higher status. That&rsquo;s because just getting the rock star&rsquo;s attention is an accomplishment.<br /><br />By engaging in a high-involvement activity with others, like having a conversation, participants assume a plausible upper and lower bound status level for each other. The fact they both care enough to engage in an activity together is evidence they&rsquo;re approximately the same status level. Because of this, they can&rsquo;t do any signals that reliably indicate they&rsquo;re much higher status the other. So most status signaling they&rsquo;ll be doing to each other won&rsquo;t influence their status much.<br /><br />The behavior induced by indifference and reactivity to stimuli is where the strong evidence resides. Everything else merely budges what&rsquo;s already been proven by indifference and reactivity. In short, the sort of status signaling Lesswrong has been concerned with is only the tip of the iceberg.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2, "Q6P8jLn8hH7kbuXRr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qeswkGtzfGg7GSP97", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": -5, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "2434", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-07T03:38:30.190Z", "modifiedAt": null, "url": null, "title": "Selfishness Signals Status", "slug": "selfishness-signals-status", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:34.622Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Liron", "createdAt": "2009-02-27T04:43:11.294Z", "isAdmin": false, "displayName": "Liron"}, "userId": "AyzRrs8hNm54QptLi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MntNx6fFZ8KMHu5PM/selfishness-signals-status", "pageUrlRelative": "/posts/MntNx6fFZ8KMHu5PM/selfishness-signals-status", "linkUrl": "https://www.lesswrong.com/posts/MntNx6fFZ8KMHu5PM/selfishness-signals-status", "postedAtFormatted": "Sunday, March 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Selfishness%20Signals%20Status&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelfishness%20Signals%20Status%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMntNx6fFZ8KMHu5PM%2Fselfishness-signals-status%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Selfishness%20Signals%20Status%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMntNx6fFZ8KMHu5PM%2Fselfishness-signals-status", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMntNx6fFZ8KMHu5PM%2Fselfishness-signals-status", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 555, "htmlBody": "<p>The \"status\" hypothesis simply claims that we associate one another with a one-dimensional quantity: the perceived degree to which others' behavior can affect our well-being. And each of us behaves toward our peers according to our internally represented status mapping.</p>\n<p>Imagine that, within your group, you're in a position where everyone wants to please you and no one can afford to challenge you. What does this mean for your behavior? It means you get to act selfish -- focusing on what makes you most pleased, and becoming less sensitive to lower-grade pleasure stimuli.</p>\n<p>Now let's say you meet an outsider. They want to estimate your status, because it's a useful and efficient value to remember. And when they see you acting selfishly in front of others in your group, they will infer the lopsided balance of power.</p>\n<p>In your own life, when you interact with someone who could affect your well-being, you do your best to act in a way that is valuable to them, hoping they will be motivated to reciprocate. The thing is, if an observer witnesses your unselfish behavior, it's a telltale sign of your lower status. And this scenario is so general, and so common, that most people learn to be very observant of others' deviations from selfishness.<a id=\"more\"></a></p>\n<p>On Less Wrong, we already understand the phenomenon of status signaling -- the causal link from status to behavior, and the inferential link from behavior to status. If we also recognize the role of selfishness as a reliable status signal, we can gain a lot of predictive power about which specific behavioral mannerisms are high- and low-status.</p>\n<p>&nbsp;</p>\n<p>Are each of the following high- or low-status?</p>\n<p style=\"padding-left: 30px;\">1. Standing up straight</p>\n<p style=\"padding-left: 30px;\">2. Saying what's on your mind, without thinking it through</p>\n<p style=\"padding-left: 30px;\">3. Making an effort to have a pleasant conversation</p>\n<p style=\"padding-left: 30px;\">4. Wearing the most comfortable possible clothes</p>\n<p style=\"padding-left: 30px;\">5. Apologizing to someone you've wronged</p>\n<p style=\"padding-left: 30px;\">6. Blowing your nose in front of people</p>\n<p style=\"padding-left: 30px;\">7. Asking for permission</p>\n<p style=\"padding-left: 30px;\">8. Showing off</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<ol> </ol>\n<p>Answers:</p>\n<p style=\"padding-left: 30px;\">1. Standing up straight is low-status, because you're obviously doing it to make an impression on others -- there's no first-order benefit to yourself.</p>\n<p style=\"padding-left: 30px;\">2. Saying what's on your mind is high-status, because you're doing something pleasurable. This signal is most reliable when what you say doesn't have any intellectual merit.</p>\n<p style=\"padding-left: 30px;\">3. Making an effort to have a pleasant conversation is low-status. It's high-status to talk about what you care about.</p>\n<p style=\"padding-left: 30px;\">4. Wearing the most comfortable possible clothes is high-status, because you're clearly benefiting yourself. (Dressing in fashionable clothes is also high-status, through a different inferential pathway.)</p>\n<p style=\"padding-left: 30px;\">5. Apologizing is low-status because you're obviously not doing it for yourself.</p>\n<p style=\"padding-left: 30px;\">6. Blowing your nose is high-status because it's pleasurable and shows that you aren't affected enough by others to stop.</p>\n<p style=\"padding-left: 30px;\">7. Asking for permission is low-status. Compare: recognizing that proceeding would be pleasurable, and believing that you are immune to any negative consequences.</p>\n<p style=\"padding-left: 30px;\">8. Showing off is low-status, because it reveals that the prospect of impressing your peers drives you to do things which aren't first-order selfish. (Of course, the thing you are showing off might legitimately signal status.)</p>\n<ol> </ol>\n<p>&nbsp;</p>\n<p><a href=\"/lw/1vm/the_strongest_status_signals/\">Pwno's post</a> makes a good related point: The most reliable high-status signal is indifference. If you're indifferent to a person, it means their behavior doesn't even factor into your expectation of well-being. It means your computational resources are too limited to allocate them their own variable, since its value matters so little. How could you act indifferent if you weren't high-status?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MntNx6fFZ8KMHu5PM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 0, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "2440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qeswkGtzfGg7GSP97"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-10T06:14:56.186Z", "modifiedAt": null, "url": null, "title": "Coffee: When it helps, when it hurts", "slug": "coffee-when-it-helps-when-it-hurts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:36.005Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JustinShovelain", "createdAt": "2009-06-10T00:56:47.112Z", "isAdmin": false, "displayName": "JustinShovelain"}, "userId": "LEeresErqn3BpWrwG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dW9DTLZcScAPj98eR/coffee-when-it-helps-when-it-hurts", "pageUrlRelative": "/posts/dW9DTLZcScAPj98eR/coffee-when-it-helps-when-it-hurts", "linkUrl": "https://www.lesswrong.com/posts/dW9DTLZcScAPj98eR/coffee-when-it-helps-when-it-hurts", "postedAtFormatted": "Wednesday, March 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Coffee%3A%20When%20it%20helps%2C%20when%20it%20hurts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACoffee%3A%20When%20it%20helps%2C%20when%20it%20hurts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdW9DTLZcScAPj98eR%2Fcoffee-when-it-helps-when-it-hurts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Coffee%3A%20When%20it%20helps%2C%20when%20it%20hurts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdW9DTLZcScAPj98eR%2Fcoffee-when-it-helps-when-it-hurts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdW9DTLZcScAPj98eR%2Fcoffee-when-it-helps-when-it-hurts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<div id=\"magicdomid1900\" class=\"ace-line\"><span class=\"author-g-ec4lafin2wc5nxu1\">Many people take caffeine </span><span class=\"author-g-qo5xmv5ipruy04ig\">always, or never</span><span class=\"author-g-ec4lafin2wc5nxu1\">. </span><span class=\"author-g-qo5xmv5ipruy04ig\">But the evidence is clear: for some tasks, drink coffee -- for others, don't.</span></div>\n<div id=\"magicdomid1903\" class=\"ace-line\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam\">Caffeine:</span></div>\n<div id=\"magicdomid1904\" class=\"ace-line\">\n<ul class=\"list-bullet1\">\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17400186\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam b\"><strong>Impairs&nbsp;</strong></span></a><span class=\"author-g-xfz122ziz122z2mc0ycjviam\"><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17400186\"> hippocampal neurogenesis and long term memory</a></span></li>\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15174922\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam b\"><strong>Narrows focus&nbsp;</strong></span></a><span class=\"author-g-xfz122ziz122z2mc0ycjviam\"><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15174922\"> -- aiding short-term memory when the information is related to the current focus of thought, and making short-term recall more difficult when the information isn't related</a></span></li>\n<li><a href=\"http://cat.inist.fr/?aModele=afficheN&amp;cpsidt=21827779\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam b\"><strong>Increases short term recall</strong></span></a><span class=\"author-g-xfz122ziz122z2mc0ycjviam\"><a href=\"http://cat.inist.fr/?aModele=afficheN&amp;cpsidt=21827779\">&nbsp; of both true and false memories</a></span></li>\n<li><a href=\"http://news.softpedia.com/news/Caffeine-Boosts-Short-Time-Memory-13828.shtml\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam b\"><strong>Increases short term memory and attentional control</strong></span></a><span class=\"author-g-xfz122ziz122z2mc0ycjviam url\"><a href=\"http://news.softpedia.com/news/Caffeine-Boosts-Short-Time-Memory-13828.shtml%29\"></a></span></li>\n<li><a href=\"http://www.scielo.br/pdf/bjmbr/v35n10/4559.pdf\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam b\"><strong>Increases memory retention and retrieval</strong></span></a><span class=\"author-g-xfz122ziz122z2mc0ycjviam url\"><a href=\"http://www.scielo.br/pdf/bjmbr/v35n10/4559.pdf%29\"></a></span></li>\n</ul>\n</div>\n<div id=\"magicdomid1911\" class=\"ace-line\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam\">So:</span></div>\n<div id=\"magicdomid1912\" class=\"ace-line\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam i\"><em>Use</em></span><span class=\"author-g-xfz122ziz122z2mc0ycjviam\">&nbsp; caffeine for short-term performance on a focused task (such as an exam).</span></div>\n<div id=\"magicdomid1913\" class=\"ace-line\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam i\"><em>Avoid</em></span><span class=\"author-g-xfz122ziz122z2mc0ycjviam\">&nbsp; caffeine for tasks that require broad creativity and long-term learning.</span></div>\n<div id=\"magicdomid1915\" class=\"ace-line\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam\">(Disclaimer: The greater altertness, larger short-term memory capacity, and eased recall might make the memories you do make of higher quality.)</span></div>\n<div id=\"magicdomid1917\" class=\"ace-line\"><span class=\"author-g-xfz122ziz122z2mc0ycjviam\">At least, this is my take. But the issue is convoluted enough that I'm unsure. What do you think?</span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dW9DTLZcScAPj98eR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 52, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "2449", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-10T14:46:24.821Z", "modifiedAt": null, "url": null, "title": "The Blackmail Equation", "slug": "the-blackmail-equation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:39.386Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ljy3CSwTFPEpnGLLJ/the-blackmail-equation", "pageUrlRelative": "/posts/Ljy3CSwTFPEpnGLLJ/the-blackmail-equation", "linkUrl": "https://www.lesswrong.com/posts/Ljy3CSwTFPEpnGLLJ/the-blackmail-equation", "postedAtFormatted": "Wednesday, March 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Blackmail%20Equation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Blackmail%20Equation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjy3CSwTFPEpnGLLJ%2Fthe-blackmail-equation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Blackmail%20Equation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjy3CSwTFPEpnGLLJ%2Fthe-blackmail-equation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLjy3CSwTFPEpnGLLJ%2Fthe-blackmail-equation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1562, "htmlBody": "<p><em>This is Eliezer's model of blackmail in decision theory at the recent workshop at SIAI, filtered through my own understanding. Eliezer help and advice were much appreciated; any errors here-in are my own.</em></p>\n<p>The mysterious stranger blackmailing the Countess of Rectitude over her extra-marital affair with Baron Chastity doesn't have to run a complicated algorithm. He simply has to credibly commit to the course of action:</p>\n<p style=\"padding-left: 30px;\">\"If you don't give me money, I will reveal your affair.\"</p>\n<p>And then, generally, the Countess forks over the cash. Which means the blackmailer never does reveal the details of the affair, so that threat remains entirely counterfactual/hypothetical. Even if the blackmailer <em>is</em> Baron Chastity, and the revelation would be devastating for him as well, this makes no difference at all, as long as he can credibly commit to Z. In the world of perfect decision makers, there is no risk to doing so, because the Countess will hand over the money, so the Baron will not take the hit from the revelation.</p>\n<p>Indeed, the baron could replace \"I will reveal our affair\" with Z=\"I will reveal our affair, then sell my children into slavery, kill my dogs, burn my palace, and donate my organs to medical science while boiling myself in burning tar\" or even \"I will reveal our affair, then turn on an unfriendly AI\", and it would only matter if this changed his pre-commitment to Z. If the Baron can commit to counterfactually doing Z, then he never has to do Z (as the countess will pay him the hush money), so it doesn't matter how horrible the consequences of Z are to himself.</p>\n<p>To get some numbers in this model, assume the countess can either pay up or not do so, and the baron can reveal the affair or keep silent. The payoff matrix could look something like this:</p>\n<p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">(Baron, Countess)<br /></th><th style=\"cursor: text;\">Pay<br /></th><th style=\"cursor: text;\">Not pay</th>\n</tr>\n<tr>\n<th style=\"cursor: text;\">Reveal<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">&nbsp;(-90,-110)</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(-100,-100)</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">Silent<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(10,-10)</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(0,0)<br /></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p><a id=\"more\"></a>Both the countess and the baron get -100 utility if the affair is revealed, while the countess transfers 10 of her utilitons to the baron if she pays up. Staying silent and not paying have no effect on the utility of either.</p>\n<p>Let's see how we could implement the blackmailing if the baron and the countess were running simple decision algorithms. The baron has a variety of tactics he could implement. What is a tactic, for the baron? A tactic is a list of responses he could implement, depending on what the countess does. His four tactics are:</p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial;\">\n<li>(Pay, NPay)&rarr;(Reveal, Silent)&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; \"anti-blackmail\" : if she pays, tell all, if she doesn't, keep quiet</li>\n<li>(Pay, NPay)&rarr;(Reveal, Reveal)&nbsp; &nbsp;&nbsp;&nbsp; \"blabbermouth\" : whatever she does, tell all</li>\n<li>(Pay, NPay)&rarr;(Silent, Silent) &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; \"not-a-word\" : whatever she does, keep quiet</li>\n<li>(Pay ,NPay)&rarr;(Silent, Reveal)&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; \"blackmail\" : if she pays, keep quiet, if she doesn't, tell all</li>\n</ol>\n<p>The countess, in contract, has only two tactics: pay or don't pay. Each will try and estimate what the other will do, so the baron must model the countess, who must model the baron in turn. This seems as if it leads to infinite regress, but the baron has a short-cut: when reasoning counterfactually as to which tactic to implement, he will substitute that tactic in his model of how the countess models him.</p>\n<p>In simple terms, it means that when he is musing '<em>what were to happen if I were to anti-blackmail, hypothetically</em>', he assume that the countess would model him as an anti-blackmailer. In that case, the countess' decision is easy: her utility maximising decision is not to pay, leaving them with a payoff of (0,0).</p>\n<p>Similarly, if he counterfactually considers the blabbermouth tactic, then if the countess models him as such, her utility-maximising tactic is also not to pay up, giving a payoff of (-100,-100). Not-a-word results in a payoff of (0,0), and only if the baron implements the blackmail tactic will the countess pay up, giving a payoff of (10,-10). Since this maximises his utility, he will implement the blackmail tactic. And the countess will pay him, to minimise her utility loss.</p>\n<p>Notice that in order for this to work, the baron needs four things:</p>\n<ol>\n<li>The baron needs to make his decision after the countess does, so she cannot react to his action.</li>\n<li>The baron needs to make his decision after the countess does, so he can react to her action.</li>\n<li>The baron needs to be able to precommit to a specific tactic (in this case, blackmail).</li>\n<li>The baron needs the countess to find his precommitment plausible.</li>\n</ol>\n<p>If we were to model the two players as timeless AI's implementing specific decision theories, what would these conditions become? They can be cast as:</p>\n<ol>\n<li>The baron and the countess must exchange their source code.</li>\n<li>The baron and the countess must both be rational.</li>\n<li>The countess' available tactics are simply to pay or not to pay.</li>\n<li>The baron's available tactics are conditional tactics, dependent on what the countess' decision is.</li>\n<li>The baron must model the countess as seeing <em>his </em>decision as a fixed fact over which she has no influence.</li>\n<li>The countess must indeed see the baron's decision as a fixed fact over which she has no influence.</li>\n</ol>\n<p>The baron occupies what Eliezer termed a <span style=\"font-style: normal;\">superior epistemic vantage.</span></p>\n<p>Could two agents be in superior epistemic vantage, as laid out above, one over the other? This is precluded by the set-up above*, as two agents cannot be correct in assuming that the other treats their own decision as a fixed fact, while both running counterfactuals conditioning their response on the varrying tactics of the other.</p>\n<p style=\"padding-left: 30px;\">\"I'll tell, if you don't send me the money, or try and stop me from blackmailing you!\" versus \"I'll never send you the money, if you blackmail me or tell anyone about us!\"</p>\n<p>Can the countess' brother, the Archduke of Respectability, blackmail the baron on her behalf? If the archduke is in a superior epistemic vantage to the baron, then there is no problem. He could choose a tactic that is dependent on the baron's choice of tactics, without starting an infinite loop, as the baron cannot do the same to him. The most plausible version would go:</p>\n<p style=\"padding-left: 30px;\">\"If you blackmail my sister, I will shoot you. If you blabbermouth, I will shoot you. Anti-blackmail and not-a-word are fine by me, though.\"</p>\n<p>Note that Omega, in the <a href=\"http://en.wikipedia.org/wiki/Newcomb's_paradox\"><span style=\"font-style: normal;\">Newcomb's problem</span></a><span style=\"font-style: normal;\">, is occupying the superior epistemic vantage. His final tactic is the conditional Z=\"if you two-box, I put nothing in box A; if you one-box, I put in a million pounds,\" whereas you do not have access to tactics along the lines of \"if Omega implements Z, I will two-box; if he doesn't, I will one-box\". Instead, like the countess, you have to assume that Omega will indeed implement Z, accept this as fact, and then choose simply to one-box or two-box.</span></p>\n<p><br />*The argument, as presented here, is a lie, but spelling out the the true version would be tedious and tricky. The countess, for instance, is perfectly free to indulge in counterfactual speculations that the baron may decide something else, as long as she and the baron are both aware that these speculations will never influence her decision. Similarly, the baron is free to model her doing so, as long this similarly leads to no difference. The countess may have a dozen other options, not just the two presented here, as long as they both know she cannot make use of them. There is a whole issue of extracting information from an algorithm and a source code here, where you run into entertaining paradoxes such as if the baron knows the countess will do something, then he will be accurate, and can check whether his knowledge is correct; but if he didn't know this fact, then it would be incorrect. These are beyond the scope of this post.</p>\n<p>&nbsp;</p>\n<p><strong>[EDIT]</strong> The impossibility of the countess and the baron being each in epistemic vantage over the other has been clarified, and replaces the original point - about infinite loops - which only implied that result for certain naive algorithms.</p>\n<p><strong>[EDIT]</strong> Godelian reasons make it impossible to bandy about \"he is rational and believes X, hence X is true\" with such wild abandon. I've removed the offending lines.</p>\n<p><strong>[EDIT]</strong> To clarify issues, here is a formal model of how the baron and countess could run their decision theories. Let X be a fact about the world, and let S_B be the baron's source code.</p>\n<p>Baron(S_C):</p>\n<p style=\"padding-left: 30px;\">Utility of pay = 10, utility of reveal = -100</p>\n<p style=\"padding-left: 30px;\">Based on S_C, if the countess would accept the baron's behaviour as a fixed fact, run:</p>\n<p style=\"padding-left: 60px;\">Let T={anti-blackmail, blabbermouth, not-a-word, blackmail}</p>\n<p style=\"padding-left: 60px;\">For t_b in T, compute utility of the outcome implied by Countess(t_b,S_B). Choose the t_b that maximises it.</p>\n<p>&nbsp;</p>\n<p>Countess(X, S_B)</p>\n<p style=\"padding-left: 30px;\">If X implies the baron's tactic t_b, then accept t_b as fixed fact.</p>\n<p style=\"padding-left: 30px;\">If not, run Baron(S_C) to compute the baron's tactic t_b. Stop as soon as the tactic is found. Accept as fixed fact.</p>\n<p style=\"padding-left: 30px;\">Utility of pay = -10, utility of reveal = -100.</p>\n<p style=\"padding-left: 30px;\">Let T={pay, not pay}</p>\n<p style=\"padding-left: 30px;\">For t_c in T, under the assumption of t_b, compute utility of outcome. Choose t_c that maximises it.</p>\n<p>&nbsp;</p>\n<p>Both these agents are rational with each other, in that they correctly compute each other's ultimate decisions in this situation. They are not perfectly rational (or rather, their programs are incomplete) in that they do not perform well against general agents, and may fall into infinite loops as written.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2q2cK4FdnSeohTEaJ": 3, "dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ljy3CSwTFPEpnGLLJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 27, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "2443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-10T14:58:38.238Z", "modifiedAt": "2019-12-28T06:35:47.303Z", "url": null, "title": "Blackmail, Nukes and the Prisoner's Dilemma", "slug": "blackmail-nukes-and-the-prisoner-s-dilemma", "viewCount": null, "lastCommentedAt": "2010-09-22T21:59:26.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hDNg77Xx53tsxq6Ko/blackmail-nukes-and-the-prisoner-s-dilemma", "pageUrlRelative": "/posts/hDNg77Xx53tsxq6Ko/blackmail-nukes-and-the-prisoner-s-dilemma", "linkUrl": "https://www.lesswrong.com/posts/hDNg77Xx53tsxq6Ko/blackmail-nukes-and-the-prisoner-s-dilemma", "postedAtFormatted": "Wednesday, March 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Blackmail%2C%20Nukes%20and%20the%20Prisoner's%20Dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABlackmail%2C%20Nukes%20and%20the%20Prisoner's%20Dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDNg77Xx53tsxq6Ko%2Fblackmail-nukes-and-the-prisoner-s-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Blackmail%2C%20Nukes%20and%20the%20Prisoner's%20Dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDNg77Xx53tsxq6Ko%2Fblackmail-nukes-and-the-prisoner-s-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDNg77Xx53tsxq6Ko%2Fblackmail-nukes-and-the-prisoner-s-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 596, "htmlBody": "<p><em>This example (and the <a href=\"/lw/1vv/the_blackmail_equation/\">whole method</a> for modelling blackmail) are due to Eliezer. I have just recast them in my own words.</em></p>\n<p>We join our friends, the <em><em><span style=\"font-style: normal;\">Countess of Rectitude and</span></em></em> Baron Chastity, in bed together. Having surmounted their <a href=\"/lw/1vv/the_blackmail_equation/\">recent difficulties</a> (she paid him, by the way), they decide to relax with a good old game of prisoner's dilemma. The payoff matrix is as usual:</p>\n<p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">(Baron, Countess)<br /></th><th style=\"cursor: text;\">Cooperate<br /></th><th style=\"cursor: text;\">Defect<br /></th>\n</tr>\n<tr>\n<th style=\"cursor: text;\">Cooperate<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(3,3)</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(0,5)</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">Defect<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(5,0)</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(1,1)<br /></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>Were they both standard game theorists, they would both defect, and the payoff would be (1,1). But recall that the baron occupies an epistemic vantage over the countess. While the countess only gets to choose her own action, he can choose from among four more general tactics:</p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial;\">\n<li><span style=\"font-style: normal;\">(Countess C, Countess D)&rarr;(Baron D, Baron C)&nbsp;&nbsp; \"contrarian\" : do the opposite of what she does<br /></span></li>\n<li><span style=\"font-style: normal;\">(Countess C, Countess D)&rarr;(Baron C, Baron C)&nbsp;&nbsp; \"trusting soul\" : always cooperate<br /></span></li>\n<li><span style=\"font-style: normal;\">(Countess C, Countess D)&rarr;(Baron D, Baron D)&nbsp;&nbsp; \"bastard\" : always defect<br /></span></li>\n<li><span style=\"font-style: normal;\">(Countess C, Countess D)&rarr;(Baron C, Baron D)&nbsp;&nbsp; \"copycat\" : do whatever she does</span></li>\n</ol>\n<p>Recall that he counterfactually considers what the countess would do in each case, while assuming that the countess considers his decision a fixed fact about the universe. Were he to adopt the contrarian tactic, she would maximise her utility by defecting, giving a payoff of (0,5). Similarly, she would defect in both trusting soul and bastard, giving payoffs of (0,5) and (1,1) respectively. If he goes for copycat, on the other hand, she will cooperate, giving a payoff of (3,3).</p>\n<p>Thus when one player occupies a superior epistemic vantage over the other, they can do better than standard game theorists, and manage to both cooperate.</p>\n<p style=\"padding-left: 30px;\">\"Isn't it wonderful,\" gushed the Countess, pocketing her 3 utilitons and lighting a cigarette, \"how we can do such marvellously unexpected things when your position is over mine?\"</p>\n<p><a id=\"more\"></a>Next to the bed, the butler had absent-mindedly left a pair of nuclear bombs, along with the champagne flutes. The countess and the baron each picked up one of the nukes, and the wily baron proposed that they play another round of prisoner's dilemma. With the added option of setting off a nuke, the game payoff matrix looks like:</p>\n<p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">(Baron, Countess)<br /></th><th style=\"cursor: text;\">Cooperate<br /></th><th style=\"cursor: text;\">Defect<br /></th><th style=\"cursor: text;\"><span style=\"color: #ff0000;\"><em>Nuke!</em></span></th>\n</tr>\n<tr>\n<th style=\"cursor: text;\">Cooperate<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">&nbsp;(3,3)</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(0,5)</td>\n<td style=\"margin: 8px; font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 15px; cursor: text;\" align=\"center\"><strong><span style=\"color: #ff0000;\">-&infin;</span></strong></td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">Defect<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(5,0)</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">(1,1)<br /></td>\n<td style=\"margin: 8px; font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 15px; cursor: text;\" align=\"center\"><strong><span style=\"color: #ff0000;\">-&infin;</span></strong></td>\n</tr>\n<tr>\n<th><span style=\"color: #ff0000;\"><em>Nuke!</em></span>&nbsp;</th>\n<td style=\"margin: 8px; font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 15px; cursor: text;\" align=\"center\"><strong><span style=\"color: #ff0000;\">-&infin;</span></strong></td>\n<td style=\"margin: 8px; font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 15px; cursor: text;\" align=\"center\"><strong><span style=\"color: #ff0000;\">-&infin;</span></strong></td>\n<td style=\"margin: 8px; font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 15px; cursor: text;\" align=\"center\"><strong><span style=\"color: #ff0000;\">-&infin;</span></strong></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>In this new setup, the baron has the choice of nine tactics, corresponding to the ways the countess' three possible actions map to his three possible actions. The most interesting of his tactics is the following:</p>\n<ul>\n<li>(Countess C, Countess D, Countess N) -&gt; (Baron D, Baron N, Baron N)&nbsp; : \"Nuclear blackmail\"</li>\n</ul>\n<p>The baron, quite simply, is threatening to nuke the pair of them if the countess doesn't cooperate with him - but he's going to defect if she does cooperate. Under this assumption, the countess can choose to cooperate with payout (5,0), or defect or nuke, both with payoff -&infin;.</p>\n<p>To maximise her utility, she must therefore cooperate with the baron, even though she will get nothing out of this. Since the baron cannot make more utility than 5 in this game, Nuclear blackmail will be the tactic he will choose to implement, and thus the payoff will be (5,0).</p>\n<p>With the addition of nukes, the blackmailer has gone from being able to force cooperation, to being able to force his preferred option.</p>\n<p style=\"padding-left: 30px;\">\"Out, out!\" the countess said,&nbsp;propelling the baron away from her with a few well aimed kicks. \"On second thoughts, I don't want you over me any more!\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"chuP2QqQycjD8qakL": 2, "be2Mh2bddQ6ZaBcti": 2, "2q2cK4FdnSeohTEaJ": 1, "b8FHrKqyXuYGWc6vn": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hDNg77Xx53tsxq6Ko", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 25, "extendedScore": null, "score": 5.679719500148647e-07, "legacy": true, "legacyId": "2454", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ljy3CSwTFPEpnGLLJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-03-10T14:58:38.238Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-11T02:39:06.462Z", "modifiedAt": null, "url": null, "title": "Deception and Self-Doubt", "slug": "deception-and-self-doubt", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:51.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ArpXebYCLoyeXczMr/deception-and-self-doubt", "pageUrlRelative": "/posts/ArpXebYCLoyeXczMr/deception-and-self-doubt", "linkUrl": "https://www.lesswrong.com/posts/ArpXebYCLoyeXczMr/deception-and-self-doubt", "postedAtFormatted": "Thursday, March 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deception%20and%20Self-Doubt&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeception%20and%20Self-Doubt%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FArpXebYCLoyeXczMr%2Fdeception-and-self-doubt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deception%20and%20Self-Doubt%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FArpXebYCLoyeXczMr%2Fdeception-and-self-doubt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FArpXebYCLoyeXczMr%2Fdeception-and-self-doubt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p>A little while ago, I argued with a friend of mine over the efficiency of the Chinese government. I admitted he was clearly better informed on the subject than I. At one point, however, he claimed that the Chinese government executed fewer people than the US government. This statement is flat-out wrong; China executes ten times as many people as the US, if not far more. It's a blatant lie. I called him on it, and he copped to it. The outcome is besides the point. Why does it matter that he lied? In this case, it provides weak evidence that the basics of his claim were wrong, that he knew the point he was arguing was, at least on some level, incorrect.</p>\n<p>The fact that a person is willing to lie indefensibly in order to support their side of an argument shows that they have put \"winning\" the argument at the top of their priorities. Furthermore, they've decided, based on the evidence they have available, that lying was a more effective way to advance their argument than telling the truth. While exceptions obviously exist, if you believe that lying to a reasonably intelligent audience is the best way of advancing your claim, this suggests that you know your claim is ill-founded, even if you don't admit this fact to yourself.<a id=\"more\"></a></p>\n<p>Two major exceptions exist. First, the person may simply have no qualms about lying, and may just say anything they think will advance their point, regardless of its veracity. This indicates the speaker should never be trusted on basically any factual claim he makes, though it does not necessarily show self-doubt. Second, the speaker may have little respect for the intelligence of her audience, and believe that the audience is not sophisticated enough for the truth to persuade them. While this may be justified, depending on the audience,<sup>1</sup> unless there is good evidence to believe the audience legitimately would not process the truth accurately, this shows the speaker is likely wrong about his central point. However, \"the masses are ignorant and should be lead by their betters\" is a pretty effective cognitive dissonance resolver, so he may not experience the same self-doubt.</p>\n<p>This principle applies in direct proportion to the deception, and in direct proportion to the sophistication of the speaker. An informed person relying on outright lies indicates either an Escher-like mind or a belief in the wrongness of your position. Lesser deception indicates lesser reservations. An uninformed person may well lie to support proposition that a better informed person could support easily with the facts. But, if an apparently informed advocate is resorting lies and deception, it strongly suggests he has little else to work with.</p>\n<hr />\n<p>1- This is not necessarily unjustified. Consider a babysitter with a child who walk by a toy store. The babysitter needs to get the child home soon. The child gets excited and starts demanding they go in, and the babysitter says, \"We can't, they're closed!\" This may be patently false, but, in this case, the truth is very unlikely to convince his audience, even if better solutions may exist.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ArpXebYCLoyeXczMr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 5.681073286139907e-07, "legacy": true, "legacyId": "2279", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-11T09:39:42.922Z", "modifiedAt": null, "url": null, "title": "Nature editorial: Do scientists really need a PhD? ", "slug": "nature-editorial-do-scientists-really-need-a-phd", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:35.061Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QytY9CJyKtTueSh2z/nature-editorial-do-scientists-really-need-a-phd", "pageUrlRelative": "/posts/QytY9CJyKtTueSh2z/nature-editorial-do-scientists-really-need-a-phd", "linkUrl": "https://www.lesswrong.com/posts/QytY9CJyKtTueSh2z/nature-editorial-do-scientists-really-need-a-phd", "postedAtFormatted": "Thursday, March 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nature%20editorial%3A%20Do%20scientists%20really%20need%20a%20PhD%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANature%20editorial%3A%20Do%20scientists%20really%20need%20a%20PhD%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQytY9CJyKtTueSh2z%2Fnature-editorial-do-scientists-really-need-a-phd%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nature%20editorial%3A%20Do%20scientists%20really%20need%20a%20PhD%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQytY9CJyKtTueSh2z%2Fnature-editorial-do-scientists-really-need-a-phd", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQytY9CJyKtTueSh2z%2Fnature-editorial-do-scientists-really-need-a-phd", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p>This article is worth reading, updating based on the evidence if appropriate, and then discussing if you have something to say.</p>\n<p><a href=\"http://www.nature.com/nature/journal/v464/n7285/full/464007a.html\">Link</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QytY9CJyKtTueSh2z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -7, "extendedScore": null, "score": 5.681885873773156e-07, "legacy": true, "legacyId": "2459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-11T10:27:25.726Z", "modifiedAt": null, "url": null, "title": "Spring 2010 Meta Thread", "slug": "spring-2010-meta-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:03.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FAWS", "createdAt": "2010-01-09T18:58:38.832Z", "isAdmin": false, "displayName": "FAWS"}, "userId": "a7Neq3q2DbWrbo6B6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6tWKgRixfMfPPKwDr/spring-2010-meta-thread", "pageUrlRelative": "/posts/6tWKgRixfMfPPKwDr/spring-2010-meta-thread", "linkUrl": "https://www.lesswrong.com/posts/6tWKgRixfMfPPKwDr/spring-2010-meta-thread", "postedAtFormatted": "Thursday, March 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spring%202010%20Meta%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpring%202010%20Meta%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6tWKgRixfMfPPKwDr%2Fspring-2010-meta-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spring%202010%20Meta%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6tWKgRixfMfPPKwDr%2Fspring-2010-meta-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6tWKgRixfMfPPKwDr%2Fspring-2010-meta-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 14, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 20px;\">This post is a place to discuss meta-level issues regarding Less Wrong. <a href=\"/lw/1jx/december_2009_meta_thread/\">Previous thread.</a></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6tWKgRixfMfPPKwDr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 5.681978065981476e-07, "legacy": true, "legacyId": "2460", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 147, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hSavx8SBvbwYbrWig"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-11T17:25:40.695Z", "modifiedAt": "2021-11-21T05:20:20.152Z", "url": null, "title": "Open Thread: March 2010, part 2", "slug": "open-thread-march-2010-part-2", "viewCount": null, "lastCommentedAt": "2010-09-07T23:25:06.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ipoAMivCwrzajEucj/open-thread-march-2010-part-2", "pageUrlRelative": "/posts/ipoAMivCwrzajEucj/open-thread-march-2010-part-2", "linkUrl": "https://www.lesswrong.com/posts/ipoAMivCwrzajEucj/open-thread-march-2010-part-2", "postedAtFormatted": "Thursday, March 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20March%202010%2C%20part%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20March%202010%2C%20part%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipoAMivCwrzajEucj%2Fopen-thread-march-2010-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20March%202010%2C%20part%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipoAMivCwrzajEucj%2Fopen-thread-march-2010-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipoAMivCwrzajEucj%2Fopen-thread-march-2010-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>The Open Thread posted at the beginning of the month <a href=\"/lw/1wc/spring_2010_meta_thread/1qmk\">has exceeded 500 comments</a>&nbsp;&ndash; new Open Thread posts may be made here.</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px;\">\n<div id=\"entry_t3_1s4\" class=\"content clear\">\n<div class=\"md\" style=\"font-size: small;\">\n<div style=\"margin-bottom: 1em;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><em style=\"font-style: italic;\">This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>\n</div>\n</div>\n</div>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ipoAMivCwrzajEucj", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 5.682786324981295e-07, "legacy": true, "legacyId": "2461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 343, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-03-11T17:25:40.695Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-13T00:48:20.576Z", "modifiedAt": null, "url": null, "title": "Rational feelings: a crucial disambiguation", "slug": "rational-feelings-a-crucial-disambiguation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:38.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7xupXWqazyB6289Mv/rational-feelings-a-crucial-disambiguation", "pageUrlRelative": "/posts/7xupXWqazyB6289Mv/rational-feelings-a-crucial-disambiguation", "linkUrl": "https://www.lesswrong.com/posts/7xupXWqazyB6289Mv/rational-feelings-a-crucial-disambiguation", "postedAtFormatted": "Saturday, March 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20feelings%3A%20a%20crucial%20disambiguation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20feelings%3A%20a%20crucial%20disambiguation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xupXWqazyB6289Mv%2Frational-feelings-a-crucial-disambiguation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20feelings%3A%20a%20crucial%20disambiguation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xupXWqazyB6289Mv%2Frational-feelings-a-crucial-disambiguation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xupXWqazyB6289Mv%2Frational-feelings-a-crucial-disambiguation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 0, "htmlBody": "<p>Ever wonder something like, \"I know it's bad for me that I lost my job, but I actually feel happy about it... is that rational?\"</p> \n<p>What could a question like that mean? There is a divisive ambiguity here that really messes people up. A feeling <em>as an experienc</em>e is neither rational nor irrational. It's like asking how ethical a shade of purple is. The point is that a feeling must be framed as a <em>behavior</em> or a <em>statement</em> to ask whether it is rational, and which one matters heaps and loads to the answer.</p> \n<p><strong>If you think of the happiness as a </strong><em><strong>behavior</strong></em>, something that you're <em>doing</em>, then the question is secretly asking about \n<a href=\"http://wiki.lesswrong.com/wiki/Rationality#Instrumental_rationality\" title=\"instrumental rationality\">instrumental rationality</a>: whether you're applying your \n<a href=\"http://wiki.lesswrong.com/wiki/Belief\">beliefs</a> correctly to attain your values. In our opening example, the question becomes \"Does feeling happy serve my values?\", or simply \"Do I value feeling happy?\". If you're almost anyone, the answer is probably \"yes\".</p> \n<p><strong>If you think of the happiness as a </strong><em><strong>statement or instruction</strong></em> that says \"Your values are being served\", which can be true/false and justified/unjustified, then the question is really about \n<a href=\"http://wiki.lesswrong.com/wiki/Rationality#Epistemic_rationality\">epistemic rationality</a>, and asks: \"Am I justified to believe my values are being served?\". If \"it's bad for me\" means \"no\", then \"no\".</p> \n<p>Because of this ambiguity, although it can make sense to say \"I'm happy\" to indicate \"my values are being served\", I propose that in the interest of epistemic hygiene it's worth being more specific. Conflating feelings-as-behaviors with feelings-as-statements inflicts a great deal of pondering and confusion about \n<a href=\"/lw/hp/feeling_rational/\">whether feelings are rational</a> (also precipitated by \n<a href=\"http://wiki.lesswrong.com/wiki/Hollywood_rationality\">Hollywood</a>), and to make matters worse, each of these similes has only limited validity:</p> \n<p>\n<a></a></p> <strong>1) A feeling is a behavior only insofar as you have control over it</strong>. This is something perhaps to strive for, but which certainly varies in feasibility. If someone carefully injects you with dopamine at a funeral, you might feel happy. That doesn't mean you've made an instrumentally irrational choice. <strong>2) A feeling is a statement only insofar as it has a given interpretation</strong>. In my opening example, the happiness might rather signify \"There's nothing you can do about this so you can relax and move on.\" This might be clarified on reflection, and if the statement isn't a rational one, you might consider retraining your feeling to offer more rational suggestions. But then, as with any statement, <em>your</em> epistemic rationality hinges on whether you believe it, not on how you feel. On the other hand, it is conceivable that even after introspection reveals the mechanism of a feeling, it still does not present itself as a statement. So sometimes the question \"Is this feeling rational?\" just isn't applicable, but greater self control/awareness makes your feelings more often like behaviors/statements to be assessed as \"rational.\" For the ticklement of your visual cortex, the following table displays 15 scenario types (all of which can really happen), and what the scenario means for your instrumental/epistemic rationality (which is sometimes nothing: \"--\"). If you like, try thinking of an example scenario for the plausibility of each cell: \n<h2><strong> Your instrumental/epistemic </strong>rationality with respect to a feeling: \n<table> \n<colgroup>\n<col /> \n<col /> \n<col /> \n<col /> \n<col /> \n<col /> \n<col /> </colgroup>\n<tbody> \n<tr> \n<td>\n<p> </p></td> \n<td> \n<p>You haven't interpreted the feeling as a statement</p> </td> \n<td> \n<p>You've interpreted the feeling as a statement</p> </td> </tr> \n<tr> \n<td> \n<p>The statement is justified</p> </td> \n<td> \n<p>The statement is unjustified</p> </td> </tr> \n<tr> \n<td> \n<p>You believe it</p> </td> \n<td> \n<p>You don't believe it</p> </td> \n<td> \n<p>You believe it</p> </td> \n<td> \n<p>You don't believe it</p> </td> </tr> \n<tr> \n<td> \n<p>You can't (yet) control the feeling</p> </td> \n<td> \n<p><strong>--/yes</strong></p> </td> \n<td> \n<p><strong>--/no</strong></p> </td> \n<td> \n<p><strong>--/no</strong></p> </td> \n<td> \n<p><strong>--/yes</strong></p> </td> </tr> \n<tr> \n<td> \n<p>You can control the feeling</p> </td> \n<td> \n<p>You like (value) the feeling</p> </td> \n<td> \n<p><strong>yes/--</strong></p> </td> \n<td> \n<p><strong>yes/yes</strong></p> </td> \n<td> \n<p><strong>yes/no</strong></p> </td> \n<td> \n<p><strong>yes/no</strong></p> </td> \n<td> \n<p><strong>yes/yes</strong></p> </td> </tr> \n<tr> \n<td> \n<p>You dislike (devalue) the feeling</p> </td> \n<td> \n<p><strong>no/--</strong></p> </td> \n<td> \n<p><strong>no/yes</strong></p> </td> \n<td> \n<p><strong>no/no</strong></p> </td> \n<td> \n<p><strong>no/no</strong></p> </td> \n<td> \n<p><strong>no/yes</strong></p> </td> </tr> </tbody> </table> </h2> <strong> \n<br /> What's the point? Understanding your feelings means you can put them to better use. For one thing, you don't have to turn off a good feeling just because it makes a bad suggestion, as long as you can ignore the suggestion: if you lose your job, go ahead and be happy about it, just make sure you behave appropriately and keep looking for a new one. Likewise, you're not obliged to feel a \"smart\" feeling that you don't like, as long as you're smart enough to remember what good advice it might have given you: if you're worried about failing your exams, say \"Thanks Worry, good idea, I'll go study. Okay, I'm studying now, you've done your part, you can leave me alone for a while!\" Don't forget, in addition to communicating with the unconscious about epistemic issues, your feelings can be used for all sorts of other things, like energetic motivation, health benefits, and watching Avatar. And failing just one of these purposes doesn't entirely preclude the others, as long as you can keep them effectively separate... feeling like blue people are real can be highly advisable at times. </strong> ", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7xupXWqazyB6289Mv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 5.68642705752905e-07, "legacy": true, "legacyId": "2469", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Ever wonder something like, \"I know it's bad for me that I lost my job, but I actually feel happy about it... is that rational?\"</p> \n<p>What could a question like that mean? There is a divisive ambiguity here that really messes people up. A feeling <em>as an experienc</em>e is neither rational nor irrational. It's like asking how ethical a shade of purple is. The point is that a feeling must be framed as a <em>behavior</em> or a <em>statement</em> to ask whether it is rational, and which one matters heaps and loads to the answer.</p> \n<p><strong>If you think of the happiness as a </strong><em><strong>behavior</strong></em>, something that you're <em>doing</em>, then the question is secretly asking about \n<a href=\"http://wiki.lesswrong.com/wiki/Rationality#Instrumental_rationality\" title=\"instrumental rationality\">instrumental rationality</a>: whether you're applying your \n<a href=\"http://wiki.lesswrong.com/wiki/Belief\">beliefs</a> correctly to attain your values. In our opening example, the question becomes \"Does feeling happy serve my values?\", or simply \"Do I value feeling happy?\". If you're almost anyone, the answer is probably \"yes\".</p> \n<p><strong>If you think of the happiness as a </strong><em><strong>statement or instruction</strong></em> that says \"Your values are being served\", which can be true/false and justified/unjustified, then the question is really about \n<a href=\"http://wiki.lesswrong.com/wiki/Rationality#Epistemic_rationality\">epistemic rationality</a>, and asks: \"Am I justified to believe my values are being served?\". If \"it's bad for me\" means \"no\", then \"no\".</p> \n<p>Because of this ambiguity, although it can make sense to say \"I'm happy\" to indicate \"my values are being served\", I propose that in the interest of epistemic hygiene it's worth being more specific. Conflating feelings-as-behaviors with feelings-as-statements inflicts a great deal of pondering and confusion about \n<a href=\"/lw/hp/feeling_rational/\">whether feelings are rational</a> (also precipitated by \n<a href=\"http://wiki.lesswrong.com/wiki/Hollywood_rationality\">Hollywood</a>), and to make matters worse, each of these similes has only limited validity:</p> \n<p>\n<a></a></p> <strong>1) A feeling is a behavior only insofar as you have control over it</strong>. This is something perhaps to strive for, but which certainly varies in feasibility. If someone carefully injects you with dopamine at a funeral, you might feel happy. That doesn't mean you've made an instrumentally irrational choice. <strong>2) A feeling is a statement only insofar as it has a given interpretation</strong>. In my opening example, the happiness might rather signify \"There's nothing you can do about this so you can relax and move on.\" This might be clarified on reflection, and if the statement isn't a rational one, you might consider retraining your feeling to offer more rational suggestions. But then, as with any statement, <em>your</em> epistemic rationality hinges on whether you believe it, not on how you feel. On the other hand, it is conceivable that even after introspection reveals the mechanism of a feeling, it still does not present itself as a statement. So sometimes the question \"Is this feeling rational?\" just isn't applicable, but greater self control/awareness makes your feelings more often like behaviors/statements to be assessed as \"rational.\" For the ticklement of your visual cortex, the following table displays 15 scenario types (all of which can really happen), and what the scenario means for your instrumental/epistemic rationality (which is sometimes nothing: \"--\"). If you like, try thinking of an example scenario for the plausibility of each cell: \n<h2 id=\"_Your_instrumental_epistemic_rationality_with_respect_to_a_feeling______________________________You_haven_t_interpreted_the_feeling_as_a_statement_____You_ve_interpreted_the_feeling_as_a_statement________The_statement_is_justified_____The_statement_is_unjustified________You_believe_it_____You_don_t_believe_it_____You_believe_it_____You_don_t_believe_it________You_can_t__yet__control_the_feeling________yes________no________no________yes________You_can_control_the_feeling_____You_like__value__the_feeling_____yes________yes_yes_____yes_no_____yes_no_____yes_yes________You_dislike__devalue__the_feeling_____no________no_yes_____no_no_____no_no_____no_yes_____\"><strong> Your instrumental/epistemic </strong>rationality with respect to a feeling: \n<table> \n<colgroup>\n<col> \n<col> \n<col> \n<col> \n<col> \n<col> \n<col> </colgroup>\n<tbody> \n<tr> \n<td>\n<p> </p></td> \n<td> \n<p>You haven't interpreted the feeling as a statement</p> </td> \n<td> \n<p>You've interpreted the feeling as a statement</p> </td> </tr> \n<tr> \n<td> \n<p>The statement is justified</p> </td> \n<td> \n<p>The statement is unjustified</p> </td> </tr> \n<tr> \n<td> \n<p>You believe it</p> </td> \n<td> \n<p>You don't believe it</p> </td> \n<td> \n<p>You believe it</p> </td> \n<td> \n<p>You don't believe it</p> </td> </tr> \n<tr> \n<td> \n<p>You can't (yet) control the feeling</p> </td> \n<td> \n<p><strong id=\"___yes\">--/yes</strong></p> </td> \n<td> \n<p><strong id=\"___no\">--/no</strong></p> </td> \n<td> \n<p><strong id=\"___no1\">--/no</strong></p> </td> \n<td> \n<p><strong id=\"___yes1\">--/yes</strong></p> </td> </tr> \n<tr> \n<td> \n<p>You can control the feeling</p> </td> \n<td> \n<p>You like (value) the feeling</p> </td> \n<td> \n<p><strong id=\"yes___\">yes/--</strong></p> </td> \n<td> \n<p><strong id=\"yes_yes\">yes/yes</strong></p> </td> \n<td> \n<p><strong id=\"yes_no\">yes/no</strong></p> </td> \n<td> \n<p><strong id=\"yes_no1\">yes/no</strong></p> </td> \n<td> \n<p><strong id=\"yes_yes1\">yes/yes</strong></p> </td> </tr> \n<tr> \n<td> \n<p>You dislike (devalue) the feeling</p> </td> \n<td> \n<p><strong id=\"no___\">no/--</strong></p> </td> \n<td> \n<p><strong id=\"no_yes\">no/yes</strong></p> </td> \n<td> \n<p><strong id=\"no_no\">no/no</strong></p> </td> \n<td> \n<p><strong id=\"no_no1\">no/no</strong></p> </td> \n<td> \n<p><strong id=\"no_yes1\">no/yes</strong></p> </td> </tr> </tbody> </table> </h2> <strong> \n<br> What's the point? Understanding your feelings means you can put them to better use. For one thing, you don't have to turn off a good feeling just because it makes a bad suggestion, as long as you can ignore the suggestion: if you lose your job, go ahead and be happy about it, just make sure you behave appropriately and keep looking for a new one. Likewise, you're not obliged to feel a \"smart\" feeling that you don't like, as long as you're smart enough to remember what good advice it might have given you: if you're worried about failing your exams, say \"Thanks Worry, good idea, I'll go study. Okay, I'm studying now, you've done your part, you can leave me alone for a while!\" Don't forget, in addition to communicating with the unconscious about epistemic issues, your feelings can be used for all sorts of other things, like energetic motivation, health benefits, and watching Avatar. And failing just one of these purposes doesn't entirely preclude the others, as long as you can keep them effectively separate... feeling like blue people are real can be highly advisable at times. </strong> ", "sections": [{"title": " Your instrumental/epistemic rationality with respect to a feeling: \n \n\n \n \n \n \n \n \n \n \n \n\n  \n \nYou haven't interpreted the feeling as a statement  \n \nYou've interpreted the feeling as a statement   \n \n \nThe statement is justified  \n \nThe statement is unjustified   \n \n \nYou believe it  \n \nYou don't believe it  \n \nYou believe it  \n \nYou don't believe it   \n \n \nYou can't (yet) control the feeling  \n \n--/yes  \n \n--/no  \n \n--/no  \n \n--/yes   \n \n \nYou can control the feeling  \n \nYou like (value) the feeling  \n \nyes/--  \n \nyes/yes  \n \nyes/no  \n \nyes/no  \n \nyes/yes   \n \n \nYou dislike (devalue) the feeling  \n \nno/--  \n \nno/yes  \n \nno/no  \n \nno/no  \n \nno/yes     ", "anchor": "_Your_instrumental_epistemic_rationality_with_respect_to_a_feeling______________________________You_haven_t_interpreted_the_feeling_as_a_statement_____You_ve_interpreted_the_feeling_as_a_statement________The_statement_is_justified_____The_statement_is_unjustified________You_believe_it_____You_don_t_believe_it_____You_believe_it_____You_don_t_believe_it________You_can_t__yet__control_the_feeling________yes________no________no________yes________You_can_control_the_feeling_____You_like__value__the_feeling_____yes________yes_yes_____yes_no_____yes_no_____yes_yes________You_dislike__devalue__the_feeling_____no________no_yes_____no_no_____no_no_____no_yes_____", "level": 1}, {"title": "--/yes", "anchor": "___yes", "level": 2}, {"title": "--/no", "anchor": "___no", "level": 2}, {"title": "--/no", "anchor": "___no1", "level": 2}, {"title": "--/yes", "anchor": "___yes1", "level": 2}, {"title": "yes/--", "anchor": "yes___", "level": 2}, {"title": "yes/yes", "anchor": "yes_yes", "level": 2}, {"title": "yes/no", "anchor": "yes_no", "level": 2}, {"title": "yes/no", "anchor": "yes_no1", "level": 2}, {"title": "yes/yes", "anchor": "yes_yes1", "level": 2}, {"title": "no/--", "anchor": "no___", "level": 2}, {"title": "no/yes", "anchor": "no_yes", "level": 2}, {"title": "no/no", "anchor": "no_no", "level": 2}, {"title": "no/no", "anchor": "no_no1", "level": 2}, {"title": "no/yes", "anchor": "no_yes1", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 17}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SqF8cHjJv43mvJJzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-13T08:19:29.974Z", "modifiedAt": null, "url": null, "title": "The Importance of Goodhart's Law", "slug": "the-importance-of-goodhart-s-law", "viewCount": null, "lastCommentedAt": "2019-07-15T16:12:59.716Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blogospheroid", "createdAt": "2009-03-17T08:11:01.816Z", "isAdmin": false, "displayName": "blogospheroid"}, "userId": "dgscyYwrDh3u7dE7h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YtvZxRpZjcFNwJecS/the-importance-of-goodhart-s-law", "pageUrlRelative": "/posts/YtvZxRpZjcFNwJecS/the-importance-of-goodhart-s-law", "linkUrl": "https://www.lesswrong.com/posts/YtvZxRpZjcFNwJecS/the-importance-of-goodhart-s-law", "postedAtFormatted": "Saturday, March 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Importance%20of%20Goodhart's%20Law&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Importance%20of%20Goodhart's%20Law%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtvZxRpZjcFNwJecS%2Fthe-importance-of-goodhart-s-law%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Importance%20of%20Goodhart's%20Law%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtvZxRpZjcFNwJecS%2Fthe-importance-of-goodhart-s-law", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtvZxRpZjcFNwJecS%2Fthe-importance-of-goodhart-s-law", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 928, "htmlBody": "<p>This article introduces Goodhart's law, provides a few examples, tries to explain an origin for the law and lists out a few general mitigations.</p>\n<p>Goodhart's law states that once a social or economic measure is turned into a target for policy, it will lose any information content that had qualified it to play such a role in the first place. <a href=\"http://en.wikipedia.org/wiki/Goodhart's_law\">wikipedia</a>&nbsp;The law was named for its developer, Charles Goodhart, a chief economic advisor to the Bank of England.</p>\n<p>The much more famous <a href=\"http://en.wikipedia.org/wiki/Lucas_critique\">Lucas critique</a> is a relatively specific formulation of the same.&nbsp;</p>\n<p>The most famous examples of Goodhart's law should be the soviet factories which when given targets on the basis of numbers of nails produced many tiny useless nails and when given targets on basis of weight produced a few giant nails. Numbers and weight both correlated well in a pre-central plan scenario. After they are made targets (in different times and periods), they lose that value.<a id=\"more\"></a></p>\n<p>We laugh at such ridiculous stories, because our societies are generally much better run than Soviet Russia. But the key with Goodhart's law is that it is applicable at every level. The japanese countryside is apparently full of constructions that are going on because constructions once started in recession era are getting to be almost impossible to stop. Our society centres around money, which is supposed to be a relatively good measure of reified human effort. But many unscruplous institutions have got rich by pursuing money in many ways that people would find extremely difficult to place as value-adding.</p>\n<p>Recently <a href=\"http://www.econlib.org/library/Columns/y2010/HendersonGDP.html\">GDP Fetishism</a> by David henderson is another good article on how Goodhart's law is affecting societies.</p>\n<p>The way I look at Goodhart's law is <a href=\"/lw/iq/guessing_the_teachers_password/\">Guess the teacher's password</a> writ large.&nbsp;People and instituitions try to achieve their explicitly stated targets in the easiest way possible, often obeying the letter of the law.&nbsp;</p>\n<h2>A speculative origin of Goodhart's law</h2>\n<p>The way I see Goodhart's law work, or a target's utility break down, is the following.</p>\n<ul>\n<li>Superiors want an undefined goal G.</li>\n<li>They formulate G* which is not G, but until now in usual practice, G and G* have correlated.</li>\n<li>Subordinates are given the target G*.</li>\n<li>The well-intentioned subordinate may recognise G and suggest G** as a substitute, but such people are relatively few and far inbetween. Most people try to achieve G*.&nbsp;</li>\n<li>As time goes on, every means of achieving G* is sought.&nbsp;</li>\n<li>Remember that G* was formulated precisely because it is simple and more explicit than G. Hence, the persons, processes and organizations which aim at maximising G* achieve competitive advantage over those trying to juggle both G* and G.&nbsp;</li>\n<li>P(G|G*) reduces with time and after a point, the correlation completely breaks down.</li>\n</ul>\n<h2>The mitigations to Goodhart's law</h2>\n<p>If you consider the law to be true, solutions to Goodhart's law are an impossibility in a non-singleton scenario. So let's consider mitigations.</p>\n<ul>\n<li>Hansonian Cynicism</li>\n<li>Better Measures</li>\n<li>Solutions centred around Human Discretion</li>\n</ul>\n<h3>Hansonian Cynicism</h3>\n<p>Pointing out what most people would have in mind as G and showing that institutions all around are not following G, but their own convoluted G*s. Hansonian cynicism is definitely the second step to mitigation in many many cases (Knowing about Goodhart's law is the first). Most people expect universities to be about education and hospitals to be about health. Pointing out that they aren't doing what they are supposed to be doing creates a huge cognitive dissonance in the thinking person.</p>\n<h3>Better measures</h3>\n<h4>Balanced scorecards</h4>\n<p>Taking multiple factors into consideration, trying to make G* as strong and spoof-proof as possible.&nbsp;<span style=\"white-space: pre;\"> </span>The Scorecard approach is mathematically, the simplest solution that strikes a mind when confronted with Goodhart's law.</p>\n<h4>Optimization around the constraint</h4>\n<p>There are no generic solutions to bridging the gap between G and G*, but the body of&nbsp;knowledge of <a href=\"http://en.wikipedia.org/wiki/Thinking_Processes_(Theory_of_Constraints)\"><span style=\"color: #000000;\">theory of constraints</span></a> is a very good starting point for formulating better measures for corporates.</p>\n<h4>Extrapolated Volition</h4>\n<p><a href=\"http://en.wikipedia.org/wiki/Coherent_Extrapolated_Volition\">CEV</a> tries to mitigate Goodhart's law in a better way than mechanical measures by trying to create a complete map of human morality. If G is defined fully, there is no need for a G*. CEV tries to do it for all humanity, but as an example, individual extrapolated volition should be enough. The attempt is incomplete as of now, but it is promising.</p>\n<h3>Solutions centred around Human discretion</h3>\n<p>Human discretion is the one thing that can presently beat Goodhart's law because the constant checking and rechecking that G and G* match. Nobody will attempt to pull off anything as weird as the large nails in such a scenario. However, this is not scalable in a strict sense because of the added testing and quality control requirements.</p>\n<h4>Left Anarchist ideas</h4>\n<p>Left anarchist ideas about small firms and workgroups are based on the fact that hierarchy will inevitably introduce goodhart's law related problems and thus the best groups are small ones doing simple things.</p>\n<h4>Hierarchical rule</h4>\n<p>On the other end of the political spectrum, Molbuggian hierarchical rule completely eliminates the mechanical aspects of the law. There is no letter of the law, its all spirit. I am supposed to take total care of my slaves and have total obedience to my master. The scalability is ensured through hierarchy.</p>\n<p>&nbsp;</p>\n<p>Of all proposed solutions to the Goodhart's law problem confronted, I like CEV the most, but that is probably a reflection on me more than anything, wanting a relatively scalable and automated solution. I'm not sure whether the human discretion supporting people are really correct in this matter.</p>\n<p>Your comments are invited and other mitigations and solutions to Goodhart's law are also invited.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PvridmTCj2qsugQCH": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YtvZxRpZjcFNwJecS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 91, "baseScore": 114, "extendedScore": null, "score": 0.000191, "legacy": true, "legacyId": "2476", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 114, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This article introduces Goodhart's law, provides a few examples, tries to explain an origin for the law and lists out a few general mitigations.</p>\n<p>Goodhart's law states that once a social or economic measure is turned into a target for policy, it will lose any information content that had qualified it to play such a role in the first place. <a href=\"http://en.wikipedia.org/wiki/Goodhart's_law\">wikipedia</a>&nbsp;The law was named for its developer, Charles Goodhart, a chief economic advisor to the Bank of England.</p>\n<p>The much more famous <a href=\"http://en.wikipedia.org/wiki/Lucas_critique\">Lucas critique</a> is a relatively specific formulation of the same.&nbsp;</p>\n<p>The most famous examples of Goodhart's law should be the soviet factories which when given targets on the basis of numbers of nails produced many tiny useless nails and when given targets on basis of weight produced a few giant nails. Numbers and weight both correlated well in a pre-central plan scenario. After they are made targets (in different times and periods), they lose that value.<a id=\"more\"></a></p>\n<p>We laugh at such ridiculous stories, because our societies are generally much better run than Soviet Russia. But the key with Goodhart's law is that it is applicable at every level. The japanese countryside is apparently full of constructions that are going on because constructions once started in recession era are getting to be almost impossible to stop. Our society centres around money, which is supposed to be a relatively good measure of reified human effort. But many unscruplous institutions have got rich by pursuing money in many ways that people would find extremely difficult to place as value-adding.</p>\n<p>Recently <a href=\"http://www.econlib.org/library/Columns/y2010/HendersonGDP.html\">GDP Fetishism</a> by David henderson is another good article on how Goodhart's law is affecting societies.</p>\n<p>The way I look at Goodhart's law is <a href=\"/lw/iq/guessing_the_teachers_password/\">Guess the teacher's password</a> writ large.&nbsp;People and instituitions try to achieve their explicitly stated targets in the easiest way possible, often obeying the letter of the law.&nbsp;</p>\n<h2 id=\"A_speculative_origin_of_Goodhart_s_law\">A speculative origin of Goodhart's law</h2>\n<p>The way I see Goodhart's law work, or a target's utility break down, is the following.</p>\n<ul>\n<li>Superiors want an undefined goal G.</li>\n<li>They formulate G* which is not G, but until now in usual practice, G and G* have correlated.</li>\n<li>Subordinates are given the target G*.</li>\n<li>The well-intentioned subordinate may recognise G and suggest G** as a substitute, but such people are relatively few and far inbetween. Most people try to achieve G*.&nbsp;</li>\n<li>As time goes on, every means of achieving G* is sought.&nbsp;</li>\n<li>Remember that G* was formulated precisely because it is simple and more explicit than G. Hence, the persons, processes and organizations which aim at maximising G* achieve competitive advantage over those trying to juggle both G* and G.&nbsp;</li>\n<li>P(G|G*) reduces with time and after a point, the correlation completely breaks down.</li>\n</ul>\n<h2 id=\"The_mitigations_to_Goodhart_s_law\">The mitigations to Goodhart's law</h2>\n<p>If you consider the law to be true, solutions to Goodhart's law are an impossibility in a non-singleton scenario. So let's consider mitigations.</p>\n<ul>\n<li>Hansonian Cynicism</li>\n<li>Better Measures</li>\n<li>Solutions centred around Human Discretion</li>\n</ul>\n<h3 id=\"Hansonian_Cynicism\">Hansonian Cynicism</h3>\n<p>Pointing out what most people would have in mind as G and showing that institutions all around are not following G, but their own convoluted G*s. Hansonian cynicism is definitely the second step to mitigation in many many cases (Knowing about Goodhart's law is the first). Most people expect universities to be about education and hospitals to be about health. Pointing out that they aren't doing what they are supposed to be doing creates a huge cognitive dissonance in the thinking person.</p>\n<h3 id=\"Better_measures\">Better measures</h3>\n<h4 id=\"Balanced_scorecards\">Balanced scorecards</h4>\n<p>Taking multiple factors into consideration, trying to make G* as strong and spoof-proof as possible.&nbsp;<span style=\"white-space: pre;\"> </span>The Scorecard approach is mathematically, the simplest solution that strikes a mind when confronted with Goodhart's law.</p>\n<h4 id=\"Optimization_around_the_constraint\">Optimization around the constraint</h4>\n<p>There are no generic solutions to bridging the gap between G and G*, but the body of&nbsp;knowledge of <a href=\"http://en.wikipedia.org/wiki/Thinking_Processes_(Theory_of_Constraints)\"><span style=\"color: #000000;\">theory of constraints</span></a> is a very good starting point for formulating better measures for corporates.</p>\n<h4 id=\"Extrapolated_Volition\">Extrapolated Volition</h4>\n<p><a href=\"http://en.wikipedia.org/wiki/Coherent_Extrapolated_Volition\">CEV</a> tries to mitigate Goodhart's law in a better way than mechanical measures by trying to create a complete map of human morality. If G is defined fully, there is no need for a G*. CEV tries to do it for all humanity, but as an example, individual extrapolated volition should be enough. The attempt is incomplete as of now, but it is promising.</p>\n<h3 id=\"Solutions_centred_around_Human_discretion\">Solutions centred around Human discretion</h3>\n<p>Human discretion is the one thing that can presently beat Goodhart's law because the constant checking and rechecking that G and G* match. Nobody will attempt to pull off anything as weird as the large nails in such a scenario. However, this is not scalable in a strict sense because of the added testing and quality control requirements.</p>\n<h4 id=\"Left_Anarchist_ideas\">Left Anarchist ideas</h4>\n<p>Left anarchist ideas about small firms and workgroups are based on the fact that hierarchy will inevitably introduce goodhart's law related problems and thus the best groups are small ones doing simple things.</p>\n<h4 id=\"Hierarchical_rule\">Hierarchical rule</h4>\n<p>On the other end of the political spectrum, Molbuggian hierarchical rule completely eliminates the mechanical aspects of the law. There is no letter of the law, its all spirit. I am supposed to take total care of my slaves and have total obedience to my master. The scalability is ensured through hierarchy.</p>\n<p>&nbsp;</p>\n<p>Of all proposed solutions to the Goodhart's law problem confronted, I like CEV the most, but that is probably a reflection on me more than anything, wanting a relatively scalable and automated solution. I'm not sure whether the human discretion supporting people are really correct in this matter.</p>\n<p>Your comments are invited and other mitigations and solutions to Goodhart's law are also invited.</p>", "sections": [{"title": "A speculative origin of Goodhart's law", "anchor": "A_speculative_origin_of_Goodhart_s_law", "level": 1}, {"title": "The mitigations to Goodhart's law", "anchor": "The_mitigations_to_Goodhart_s_law", "level": 1}, {"title": "Hansonian Cynicism", "anchor": "Hansonian_Cynicism", "level": 2}, {"title": "Better measures", "anchor": "Better_measures", "level": 2}, {"title": "Balanced scorecards", "anchor": "Balanced_scorecards", "level": 3}, {"title": "Optimization around the constraint", "anchor": "Optimization_around_the_constraint", "level": 3}, {"title": "Extrapolated Volition", "anchor": "Extrapolated_Volition", "level": 3}, {"title": "Solutions centred around Human discretion", "anchor": "Solutions_centred_around_Human_discretion", "level": 2}, {"title": "Left Anarchist ideas", "anchor": "Left_Anarchist_ideas", "level": 3}, {"title": "Hierarchical rule", "anchor": "Hierarchical_rule", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "123 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NMoLJuDJEms7Ku9XS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-14T04:42:12.048Z", "modifiedAt": null, "url": null, "title": "Reasoning isn't about logic (it's about arguing)", "slug": "reasoning-isn-t-about-logic-it-s-about-arguing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:33.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zNawPJRktcJGWrtt9/reasoning-isn-t-about-logic-it-s-about-arguing", "pageUrlRelative": "/posts/zNawPJRktcJGWrtt9/reasoning-isn-t-about-logic-it-s-about-arguing", "linkUrl": "https://www.lesswrong.com/posts/zNawPJRktcJGWrtt9/reasoning-isn-t-about-logic-it-s-about-arguing", "postedAtFormatted": "Sunday, March 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reasoning%20isn't%20about%20logic%20(it's%20about%20arguing)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReasoning%20isn't%20about%20logic%20(it's%20about%20arguing)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNawPJRktcJGWrtt9%2Freasoning-isn-t-about-logic-it-s-about-arguing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reasoning%20isn't%20about%20logic%20(it's%20about%20arguing)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNawPJRktcJGWrtt9%2Freasoning-isn-t-about-logic-it-s-about-arguing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNawPJRktcJGWrtt9%2Freasoning-isn-t-about-logic-it-s-about-arguing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 909, "htmlBody": "<p>\"<a href=\"http://www.dan.sperber.fr/wp-content/uploads/2009/10/MercierSperberWhydohumansreason.pdf\">Why do humans reason</a>\" (PDF), a paper by Hugo Mercier and Dan Sperber, reviewing an impressive amount of research with a lot of overlap with themes previously explored on Less Wrong, suggests that our collective efforts in \"refining the art of human rationality\" may ultimately be more successful than most individual efforts to <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">become stronger</a>. The paper sort of turns the \"<a href=\"http://yudkowsky.net/rational/virtues\">fifth virtue</a>\" on its head; rather than argue in order to reason (as perhaps we should), in practice, we reason in order to argue, and that should change our views quite a bit.</p>\n<p>I summarize Mercier and Sperber's \"argumentative theory of reasoning\" below and point out what I believe its implications are to the mission of a site such as Less Wrong.</p>\n<p>Human reasoning is one mechanism of inference among others (for instance, the unconscious inference involved in <a href=\"/lw/1jy/reacting_to_inadequate_data/1cd7\">perception</a>). It is distinct in being a) conscious, b) cross-domain, c) used prominently in human communication. Mercier and Sperber make much of this last aspect, taking it as a huge hint to seek an adaptive explanation in the fashion of evolutionary psychology, which may provide better answers than previous attempts at explanations of the evolution of reasoning.<br /><a id=\"more\"></a></p>\n<p>The paper defends reasoning as serving argumentation, in line with evolutionary theories of communication and signaling. In rich human communication there is little opportunity for \"costly signaling\", that is, signals that are taken as honest because too expensive to fake. In other words, it's easy to lie.</p>\n<p>To defend ourselves against liars, we practice \"<a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/\">epistemic vigilance</a>\"; we check the communications we receive for attributes such as a trustworthy or authoritative source; we also evaluate the coherence of the content. If the message contains symbols that matches our existing beliefs, and packages its conclusions as an inference from these beliefs, we are more likely to accept it, and thus our interlocutors have an interest in constructing good arguments. Epistemic vigilance and argumentative reasoning are thus involved in an arms race, which we should expect to result in good argumentative skills.</p>\n<p>What of all the research suggesting that humans are in fact very poor at logical reasoning? Well, if in fact \"we reason in order to argue\", when the subjects are studied in non-argumentative situations this is precisely what we should expect.</p>\n<p>Mercier and Sperber argue that, when you look at research that studies people in the appropriate settings, we turn out to be in fact quite good at reasoning when we are in the process of arguing; specifically, we demonstrate skill at producing arguments and at evaluating others' arguments. M&amp;S also plead for the \"rehabilitation\" of confirmation bias as playing an adaptive, useful role in the production of arguments in favor of an intuitively preferred view.</p>\n<p>If reasoning is a skill evolved for social use, group settings should be particularly conducive to skilled arguing. Research findings in fact show that \"truth wins\": once a group participant has a correct solution they will convince others. A group in a debate setting can do <a href=\"http://www.sciencedaily.com/releases/2006/04/060423191907.htm\">better than its best member</a>.<br /><br />The argumentative theory, Mercier and Sperber argue, accounts nicely for <a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">motivated reasoning</a>, on the model that \"reasoning anticipates argument\". Such anticipation colors our evaluative attitudes, leading for instance to \"polarization\" whereby a counter-argument makes us even more strongly believe the original position, or \"bolstering\" whereby we defend a position more strongly after we have committed to it.</p>\n<p>These attitudes are favorable to argumentative goals but actually detrimental to epistemic goals. This is particularly evident in decision-making. Reasoning appears to help people little when deciding; it directs people to the decisions that will be easily justified, not to the best decisions!</p>\n<blockquote>\n<p>Reasoning falls quite short of reliably delivering&nbsp; rational beliefs and rational decisions, [and] may even be, in a variety of cases, detrimental to rationality.</p>\n</blockquote>\n<p>However, it isn't all bad news. The important asymmetry is between production of arguments, and their evaluation. In groups with an interest in finding correct answers, \"truth wins\".</p>\n<blockquote>\n<p>If we generalize to problems that do not&nbsp; have a provable solution, we should expect, if not necessarily truth, at least good arguments to win. [...] People are quite capable of reasoning in an unbiased manner at least when they are evaluating arguments rather than producing them and when they are after the truth rather than after winning a debate.</p>\n</blockquote>\n<p>Becoming individually stronger at sound reasoning is possible, Mercier and Sperber point out, but rare. The best achievements of reasoning, in science or morality, are collective.</p>\n<p>If this view of reasoning is correct, a site dedicated to \"refining the art of human rationality\" should recognize this asymmetry between producing arguments and evaluating arguments and strive to structure the \"work\" being done here accordingly.</p>\n<p>It should encourage individual participants to support their views, and perhaps take a less jaundiced view of \"confirmation bias\". But it should also encourage the breaking down of arguments into small, separable pieces, so that they can be evaluated and filtered individually; that lines up with the intent behind \"<a href=\"/lw/1qq/debate_tools_an_experience_report/\">debate tools</a>\", even if their execution currently leaves much to be desired.</p>\n<p>It should stress the importance of \"collectively seeking the truth\" and downplay attempts at \"winning the debate\". This, in particular, might lead us to take a more critical view of some common voting patterns, e.g. larger number of upvotes for snarky one-liner replies than for longer and well-thought out replies.</p>\n<p>There are probably further conclusions to be drawn from the paper, but I'll stop here and encourage you to read or skim it, then suggest your own in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 1, "5hpGj9nDLgokfghvR": 1, "exZi6Bing5AiM4ZQB": 1, "ksdiAMKfgSyEeKMo6": 1, "8SfkJYYMe75MwjHzN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zNawPJRktcJGWrtt9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 66, "extendedScore": null, "score": 0.000111, "legacy": true, "legacyId": "2478", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h", "ZP2om2oWHPhvWP2Q3", "L32LHWzy9FzSDazEg", "dJJYgmaYYFmHoQM4L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-14T22:53:59.998Z", "modifiedAt": null, "url": null, "title": "Crunchcourse - a tool for combating learning akrasia", "slug": "crunchcourse-a-tool-for-combating-learning-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:37.592Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gcpu8AoCy5R75Dt86/crunchcourse-a-tool-for-combating-learning-akrasia", "pageUrlRelative": "/posts/Gcpu8AoCy5R75Dt86/crunchcourse-a-tool-for-combating-learning-akrasia", "linkUrl": "https://www.lesswrong.com/posts/Gcpu8AoCy5R75Dt86/crunchcourse-a-tool-for-combating-learning-akrasia", "postedAtFormatted": "Sunday, March 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Crunchcourse%20-%20a%20tool%20for%20combating%20learning%20akrasia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACrunchcourse%20-%20a%20tool%20for%20combating%20learning%20akrasia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGcpu8AoCy5R75Dt86%2Fcrunchcourse-a-tool-for-combating-learning-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Crunchcourse%20-%20a%20tool%20for%20combating%20learning%20akrasia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGcpu8AoCy5R75Dt86%2Fcrunchcourse-a-tool-for-combating-learning-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGcpu8AoCy5R75Dt86%2Fcrunchcourse-a-tool-for-combating-learning-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p><a href=\"http://www.crunchcourse.com/\">Crunchcourse</a> is a free website that might be of use to people trying to learn things outside the normal classroom setting. It aims to get together groups of people interested in the same topic and use our social instincts to motivate us to do the work.</p>\n<p>It is in its early stages. If it proves useful, it might be useful to standardize on it as the place to learn the various prerequisites that lesswrong has.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gcpu8AoCy5R75Dt86", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 5.691782826225517e-07, "legacy": true, "legacyId": "2481", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-14T23:17:10.250Z", "modifiedAt": null, "url": null, "title": "Musings on probability", "slug": "musings-on-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:37.788Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bogdanb", "createdAt": "2009-03-01T17:05:24.777Z", "isAdmin": false, "displayName": "bogdanb"}, "userId": "XaMhiBmqFaeT5diLB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r8T9WJz24mC6ofkaT/musings-on-probability", "pageUrlRelative": "/posts/r8T9WJz24mC6ofkaT/musings-on-probability", "linkUrl": "https://www.lesswrong.com/posts/r8T9WJz24mC6ofkaT/musings-on-probability", "postedAtFormatted": "Sunday, March 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Musings%20on%20probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMusings%20on%20probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8T9WJz24mC6ofkaT%2Fmusings-on-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Musings%20on%20probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8T9WJz24mC6ofkaT%2Fmusings-on-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8T9WJz24mC6ofkaT%2Fmusings-on-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1886, "htmlBody": "<p>I read <a href=\"/lw/1uc/open_thread_march_2010/1p27\">this comment</a>, and after a bit of rambling I realized I was as confused as the poster. A bit more thinking later I ended up with the &ldquo;definition&rdquo; of probability under the next heading. It&rsquo;s not anything groundbreaking, just a distillation (specifically, mine) of things discussed here over the time. It&rsquo;s just what my brain thinks when I hear the word.<a id=\"more\"></a></p>\n<p>But I was surprised and intrigued when I actually put it in writing and read it back and thought about it. I don&rsquo;t remember seeing it stated like that (but I probably read some similar things).</p>\n<p>It probably won&rsquo;t teach anyone anything, but it might trigger a similar &ldquo;distillation&rdquo; of &ldquo;mind pictures&rdquo; in others, and I&rsquo;m curious to see that.</p>\n<h3>What &ldquo;probability&rdquo; is...<br /></h3>\n<p>Or, more exactly, what is the answer to &ldquo;what&rsquo;s the probability of X?&rdquo;</p>\n<p>Well, I don&rsquo;t actually know, and it probably depends on who asks. But here&rsquo;s the skeleton of the answer procedure:</p>\n<ol>\n<li>Take the set of all (logically) possible universes. Assign to each universe a finite, real value <em>m</em> (see below).</li>\n<li>Eliminate from the set those those that are inconsistent with your experiences. Call the remaining set E.</li>\n<li>Construct T, the subset of E where X (happens, or happened, or is true).</li>\n<li>Assign to each universe <em>u</em> in set E a value <em>p</em>, such that <em>p</em>(<em>u</em>) is inversely proportional to <em>m</em>(<em>u</em>), and the integral of <em>p</em> over set E is 1.</li>\n<li>Calculate the integral of <em>p</em> over the set T. The result is called &ldquo;the probability of X&rdquo;, and is the answer to the question.</li>\n</ol>\n<p>I&rsquo;m aware that this isn&rsquo;t quite a definition; in fact, it leaves more unsaid (undefined) than it explains. Nevertheless, to me it seems that the <em>structure</em> itself is <em>right</em>: people might have different interpretations for the details (and, like me, be uncertain about them), but those differences would <em>still</em> be mentally structured like above.</p>\n<p>In the next section I explain a bit where each piece comes from and what it means, and in the one after I&rsquo;m going to ramble a bit.</p>\n<h3>Clarifications</h3>\n<p>About (logically possible) <strong>universes</strong>: We don&rsquo;t actually know what our universe <em>is</em>; as such, other possible universes isn&rsquo;t quite a well-defined concept. For generality, the only constraint I put above is that they be logically possible, for the only reason that the description is (vaguely) mathematical and I don&rsquo;t have any idea what math without logic means. (I might be missing something, though.)</p>\n<p>Note that by &ldquo;universe&rdquo; I really mean an entire universe, not just &ldquo;until now&rdquo;. E.g., if it so happens your experiences allow for a single possible past (i.e., you <em>know</em> the entire history), but your universe is not deterministic, there are still many universes in E (one for each possible future); if it&rsquo;s deterministic, then E contains just one universe. (And your calculations are a lot easier...)</p>\n<p>Before you get too scared or excited by the concept of &ldquo;all possible universes&rdquo; remember that not all of them are actually used in the rest of the procedure. We actually need only those <strong>consistent with experience</strong>. That&rsquo;s still a lot when you think about it, but my mind seems to reel in panic more often I forget this point. (Lest this note makes you too comfortable, I must also mention that the possibility that experience is (even partly) simulated explodes the size of E.)</p>\n<p>About that <strong>real value <em>m</em></strong> I was talking about: &ldquo;m&rdquo; comes from &ldquo;measure&rdquo;, but that&rsquo;s a consequence of how I arrived at the schema above. Even now I&rsquo;m not quite sure it belongs there, because it depends on what you think &ldquo;possible universes&rdquo; means. If you just set it to 1 for all universes, everything works.</p>\n<p>But, for example, you might consider that the set U is countable, encoding them all as numbers using a well-defined rule, and use the Kolmogorov complexity of the bit-string encoding a universe for that universe&rsquo;s measure. (Given step [4] above, this would mean that you think simpler universes are more probable; except it doesn&rsquo;t quite mean that, because &ldquo;probable&rdquo; is defined only after you picked your &ldquo;m&rdquo;. It&rsquo;s probably closer to &ldquo;things that happen in simpler universes are more probable&rdquo;; more in the ramblings section.)</p>\n<p>A bit about the <strong>math</strong>: I used some math terms a bit loosely in the schema above. Depending exactly on how you mean by &ldquo;possible universes&rdquo;, the set of them might be finite, countably infinite, not countable, or might be <a href=\"http://en.wikipedia.org/wiki/Class_%28set_theory%29\">a proper class</a> rather than a set. Depending on that, &ldquo;integrating&rdquo; might become a different operation. If you can&rsquo;t (mathematically, not physically) do such an operation on your collection of possible universes (actually, on those in E) then you have to define your own concept of probability :-P</p>\n<p>With regards to <strong>computability</strong>, note that the series of steps above is <em>not</em> an algorithm, it&rsquo;s just the definition. It doesn&rsquo;t feel intuitive to me that there is any possible universe where you can actually follow the steps above, but math surprises me in that regard sometimes. But note that you don&rsquo;t really need <em>p</em>(X): you just need a good-enough approximation, and you&rsquo;re free to use any trick you want.</p>\n<h3>Musings</h3>\n<p>If the above didn&rsquo;t interest you, the rest probably won&rsquo;t, either. I&rsquo;ve put in this the most interesting consequences of the schema above. It&rsquo;s kind of rambling, and I apologize; as in the last section, I&rsquo;ll <strong>bold</strong> keywords, so you might just skim it for paragraphs that might interest you.</p>\n<p>I found it interesting (but not surprising) to note that <strong>Bayesian</strong> statistics correspond well to the schema above. As far as I can tell, the Bayesian <strong>prior</strong> for (any) X is the number assigned in step 5; Bayesian updating is just going back to step 2 whenever you have new experiences. The interesting part is that my description <em>smells</em> <strong>frequentist</strong>. I wasn&rsquo;t that surprised because the main difference (in my head) between the two is the use of priors; frequentist statistics ignore prior knowledge. If you just do frequentist statistics on <em>every possible event in every possible universe</em> (for some value of possible), then there is no &ldquo;prior knowledge&rdquo; left to ignore.</p>\n<p>The schema above describes only true/false&ndash;type problems. For <strong>non-binary problems</strong> you just split of E in step 3 into several subsets, one for each possible answer. If the problem is <strong>real-valued</strong> you need to split E into an uncountably infinite number of sets, but I&rsquo;ve abused set theory terms enough today that I&rsquo;m not very concerned. Anyway, in practice (in our universe) it&rsquo;s usually enough to just split the domain of the value in countably many intervals, according to precision you need, and split the universes in E according to which interval they fall in. That is, you don&rsquo;t actually need to know the probability that a value is, say, sqrt(2), just that it&rsquo;s closer to sqrt(2) than you can measure it.</p>\n<p>With regard to past discussions about a <strong>rationale for rationality</strong>, observe that it&rsquo;s possible to apply the procedure above to evaluate what is the &ldquo;rational way&rdquo;, supposing you define it by &ldquo;the rational guy <em>plays to win</em>&rdquo;: instead of step (3) generate the set of decision procedures that are applicable in all E, call it D; for each <em>d</em> in D, split E into universes where you win and those where you lose (don't win), and call these W(<em>d</em>) and L(<em>d</em>); instead of step 4, for each decision procedure <em>d</em>, calculate the &ldquo;winningness&rdquo; of <em>d</em> as the integral of <em>p</em> over W(<em>d</em>) divided by the integral over L(<em>d</em>) (with <em>p</em> defined like above); instead of step 5, pick a decision <em>d<sub>0</sub></em> such that it's &ldquo;winningness&rdquo; is maximal (no other has a larger value).<br /><br />Note that I&rsquo;ve no idea if doing this actually picks the decision procedure above, nor what exactly it would mean if it doesn&rsquo;t... Of course, if it does, it&rsquo;s still circular, like any &ldquo;reason for reasoning&rdquo;. The procedure might also give different results for people with different E. I found it interesting to contemplate that it might be &ldquo;possible&rdquo; for someone in another universe (one much friendlier to applied calculus than ours) to calculate <em>exactly</em> the solution of the procedure for <em>my</em> E, but at the same time for the best procedure for approximating it in <em>my</em> universe to give a different answer. They can&rsquo;t, of course, communicate this to me (since then they&rsquo;re <em>not</em> in a different universe in the sense used above).</p>\n<p>If your ontology implies a <strong>computable universe</strong> (thus you only need to consider those in E), you might want to use <strong>Kolmogorov complexity </strong>as a measure for the universes. I&rsquo;ve no idea which encoding you should use to calculate it; there are theorems that say the difference between two encodings is <em>bounded</em> by a constant, but I don't see why certain encodings can't be biased to have systematic effects on your probability calculations. (Other than &ldquo;it's kind of a long shot&rdquo;.) You might use the above procedure for deciding on decision procedures, of course :-P</p>\n<p>There&rsquo;s also a theorem that say you can&rsquo;t actually make a program to compute the KC for any arbitrary bit-string. There might be a universe&ndash;to&ndash;bit-string encoding that generates only bit-strings for which there is such a program, but that&rsquo;s also kind of a long shot.<br /><br />If your ontology implies <strong>quantum mechanics</strong> then I <em>think</em> the measure of the universes (<em>m</em>(<em>u</em>) in step 1) must involve wave functions somehow, but my understanding of QM doesn&rsquo;t allow me to think it through much.</p>\n<p>The schema above illuminated a bit something that puzzled me in <a href=\"/lw/1uc/open_thread_march_2010/1p27\">that comment</a> I was talking about at the beginning: say you are suddenly sent to the planet Progsta and a Sillpruk comes and asks you whether the game of Doldun will be won by the team Strigli; what&rsquo;s your prior for the answer? What puzzled me was that <em>the very fact that you were asked that question</em> communicates an enormous amount of information &mdash; see <a href=\"/lw/1uc/open_thread_march_2010/1qu8\">this comment</a> of mine for examples &mdash; and yet I couldn&rsquo;t actually see how that should affect my priors. Of course, the information content of the question restricts hugely the universes in my E. But there were so many there that it&rsquo;s still huge; more importantly, it <em>restricts the universes along boundaries that I&rsquo;ve not previously explored</em>, and I don&rsquo;t have ready heuristics to estimate that little <em>p</em> above:</p>\n<p>If I throw a (correct) dice, I can split the universes in six approximately equal parts on vague symmetry justifications, and just estimate the probability of each side as 1/6. If someone on the street asks me to bet him on <em>his</em> dice I can split the universes in those where I win and those where I lose and estimate (using a kind of Montecarlo-integration with various scenarios I can think of) that I&rsquo;ll probably lose. If I encounter an alien named Sillpruk I&rsquo;ve no idea how to split the universes to estimate the result of a Doldun match. But if I were to encounter lots of aliens with strange first-questions for a while, I might develop some such simple heuristics based on simple trial and error.</p>\n<h3>PS.</h3>\n<p>I&rsquo;m sorry if this was too long or just stupid. In the former case I welcome constructive criticism &mdash; don&rsquo;t hesitate to tell me what you think should have been cut. I hereby subject myself to <a href=\"http://www.sl4.org/crocker.html\">Crocker&rsquo;s Rules</a>. In the latter case... well, sorry :-)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r8T9WJz24mC6ofkaT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 6, "extendedScore": null, "score": 5.691827734872393e-07, "legacy": true, "legacyId": "2468", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I read <a href=\"/lw/1uc/open_thread_march_2010/1p27\">this comment</a>, and after a bit of rambling I realized I was as confused as the poster. A bit more thinking later I ended up with the \u201cdefinition\u201d of probability under the next heading. It\u2019s not anything groundbreaking, just a distillation (specifically, mine) of things discussed here over the time. It\u2019s just what my brain thinks when I hear the word.<a id=\"more\"></a></p>\n<p>But I was surprised and intrigued when I actually put it in writing and read it back and thought about it. I don\u2019t remember seeing it stated like that (but I probably read some similar things).</p>\n<p>It probably won\u2019t teach anyone anything, but it might trigger a similar \u201cdistillation\u201d of \u201cmind pictures\u201d in others, and I\u2019m curious to see that.</p>\n<h3 id=\"What__probability__is___\">What \u201cprobability\u201d is...<br></h3>\n<p>Or, more exactly, what is the answer to \u201cwhat\u2019s the probability of X?\u201d</p>\n<p>Well, I don\u2019t actually know, and it probably depends on who asks. But here\u2019s the skeleton of the answer procedure:</p>\n<ol>\n<li>Take the set of all (logically) possible universes. Assign to each universe a finite, real value <em>m</em> (see below).</li>\n<li>Eliminate from the set those those that are inconsistent with your experiences. Call the remaining set E.</li>\n<li>Construct T, the subset of E where X (happens, or happened, or is true).</li>\n<li>Assign to each universe <em>u</em> in set E a value <em>p</em>, such that <em>p</em>(<em>u</em>) is inversely proportional to <em>m</em>(<em>u</em>), and the integral of <em>p</em> over set E is 1.</li>\n<li>Calculate the integral of <em>p</em> over the set T. The result is called \u201cthe probability of X\u201d, and is the answer to the question.</li>\n</ol>\n<p>I\u2019m aware that this isn\u2019t quite a definition; in fact, it leaves more unsaid (undefined) than it explains. Nevertheless, to me it seems that the <em>structure</em> itself is <em>right</em>: people might have different interpretations for the details (and, like me, be uncertain about them), but those differences would <em>still</em> be mentally structured like above.</p>\n<p>In the next section I explain a bit where each piece comes from and what it means, and in the one after I\u2019m going to ramble a bit.</p>\n<h3 id=\"Clarifications\">Clarifications</h3>\n<p>About (logically possible) <strong>universes</strong>: We don\u2019t actually know what our universe <em>is</em>; as such, other possible universes isn\u2019t quite a well-defined concept. For generality, the only constraint I put above is that they be logically possible, for the only reason that the description is (vaguely) mathematical and I don\u2019t have any idea what math without logic means. (I might be missing something, though.)</p>\n<p>Note that by \u201cuniverse\u201d I really mean an entire universe, not just \u201cuntil now\u201d. E.g., if it so happens your experiences allow for a single possible past (i.e., you <em>know</em> the entire history), but your universe is not deterministic, there are still many universes in E (one for each possible future); if it\u2019s deterministic, then E contains just one universe. (And your calculations are a lot easier...)</p>\n<p>Before you get too scared or excited by the concept of \u201call possible universes\u201d remember that not all of them are actually used in the rest of the procedure. We actually need only those <strong>consistent with experience</strong>. That\u2019s still a lot when you think about it, but my mind seems to reel in panic more often I forget this point. (Lest this note makes you too comfortable, I must also mention that the possibility that experience is (even partly) simulated explodes the size of E.)</p>\n<p>About that <strong>real value <em>m</em></strong> I was talking about: \u201cm\u201d comes from \u201cmeasure\u201d, but that\u2019s a consequence of how I arrived at the schema above. Even now I\u2019m not quite sure it belongs there, because it depends on what you think \u201cpossible universes\u201d means. If you just set it to 1 for all universes, everything works.</p>\n<p>But, for example, you might consider that the set U is countable, encoding them all as numbers using a well-defined rule, and use the Kolmogorov complexity of the bit-string encoding a universe for that universe\u2019s measure. (Given step [4] above, this would mean that you think simpler universes are more probable; except it doesn\u2019t quite mean that, because \u201cprobable\u201d is defined only after you picked your \u201cm\u201d. It\u2019s probably closer to \u201cthings that happen in simpler universes are more probable\u201d; more in the ramblings section.)</p>\n<p>A bit about the <strong>math</strong>: I used some math terms a bit loosely in the schema above. Depending exactly on how you mean by \u201cpossible universes\u201d, the set of them might be finite, countably infinite, not countable, or might be <a href=\"http://en.wikipedia.org/wiki/Class_%28set_theory%29\">a proper class</a> rather than a set. Depending on that, \u201cintegrating\u201d might become a different operation. If you can\u2019t (mathematically, not physically) do such an operation on your collection of possible universes (actually, on those in E) then you have to define your own concept of probability :-P</p>\n<p>With regards to <strong>computability</strong>, note that the series of steps above is <em>not</em> an algorithm, it\u2019s just the definition. It doesn\u2019t feel intuitive to me that there is any possible universe where you can actually follow the steps above, but math surprises me in that regard sometimes. But note that you don\u2019t really need <em>p</em>(X): you just need a good-enough approximation, and you\u2019re free to use any trick you want.</p>\n<h3 id=\"Musings\">Musings</h3>\n<p>If the above didn\u2019t interest you, the rest probably won\u2019t, either. I\u2019ve put in this the most interesting consequences of the schema above. It\u2019s kind of rambling, and I apologize; as in the last section, I\u2019ll <strong>bold</strong> keywords, so you might just skim it for paragraphs that might interest you.</p>\n<p>I found it interesting (but not surprising) to note that <strong>Bayesian</strong> statistics correspond well to the schema above. As far as I can tell, the Bayesian <strong>prior</strong> for (any) X is the number assigned in step 5; Bayesian updating is just going back to step 2 whenever you have new experiences. The interesting part is that my description <em>smells</em> <strong>frequentist</strong>. I wasn\u2019t that surprised because the main difference (in my head) between the two is the use of priors; frequentist statistics ignore prior knowledge. If you just do frequentist statistics on <em>every possible event in every possible universe</em> (for some value of possible), then there is no \u201cprior knowledge\u201d left to ignore.</p>\n<p>The schema above describes only true/false\u2013type problems. For <strong>non-binary problems</strong> you just split of E in step 3 into several subsets, one for each possible answer. If the problem is <strong>real-valued</strong> you need to split E into an uncountably infinite number of sets, but I\u2019ve abused set theory terms enough today that I\u2019m not very concerned. Anyway, in practice (in our universe) it\u2019s usually enough to just split the domain of the value in countably many intervals, according to precision you need, and split the universes in E according to which interval they fall in. That is, you don\u2019t actually need to know the probability that a value is, say, sqrt(2), just that it\u2019s closer to sqrt(2) than you can measure it.</p>\n<p>With regard to past discussions about a <strong>rationale for rationality</strong>, observe that it\u2019s possible to apply the procedure above to evaluate what is the \u201crational way\u201d, supposing you define it by \u201cthe rational guy <em>plays to win</em>\u201d: instead of step (3) generate the set of decision procedures that are applicable in all E, call it D; for each <em>d</em> in D, split E into universes where you win and those where you lose (don't win), and call these W(<em>d</em>) and L(<em>d</em>); instead of step 4, for each decision procedure <em>d</em>, calculate the \u201cwinningness\u201d of <em>d</em> as the integral of <em>p</em> over W(<em>d</em>) divided by the integral over L(<em>d</em>) (with <em>p</em> defined like above); instead of step 5, pick a decision <em>d<sub>0</sub></em> such that it's \u201cwinningness\u201d is maximal (no other has a larger value).<br><br>Note that I\u2019ve no idea if doing this actually picks the decision procedure above, nor what exactly it would mean if it doesn\u2019t... Of course, if it does, it\u2019s still circular, like any \u201creason for reasoning\u201d. The procedure might also give different results for people with different E. I found it interesting to contemplate that it might be \u201cpossible\u201d for someone in another universe (one much friendlier to applied calculus than ours) to calculate <em>exactly</em> the solution of the procedure for <em>my</em> E, but at the same time for the best procedure for approximating it in <em>my</em> universe to give a different answer. They can\u2019t, of course, communicate this to me (since then they\u2019re <em>not</em> in a different universe in the sense used above).</p>\n<p>If your ontology implies a <strong>computable universe</strong> (thus you only need to consider those in E), you might want to use <strong>Kolmogorov complexity </strong>as a measure for the universes. I\u2019ve no idea which encoding you should use to calculate it; there are theorems that say the difference between two encodings is <em>bounded</em> by a constant, but I don't see why certain encodings can't be biased to have systematic effects on your probability calculations. (Other than \u201cit's kind of a long shot\u201d.) You might use the above procedure for deciding on decision procedures, of course :-P</p>\n<p>There\u2019s also a theorem that say you can\u2019t actually make a program to compute the KC for any arbitrary bit-string. There might be a universe\u2013to\u2013bit-string encoding that generates only bit-strings for which there is such a program, but that\u2019s also kind of a long shot.<br><br>If your ontology implies <strong>quantum mechanics</strong> then I <em>think</em> the measure of the universes (<em>m</em>(<em>u</em>) in step 1) must involve wave functions somehow, but my understanding of QM doesn\u2019t allow me to think it through much.</p>\n<p>The schema above illuminated a bit something that puzzled me in <a href=\"/lw/1uc/open_thread_march_2010/1p27\">that comment</a> I was talking about at the beginning: say you are suddenly sent to the planet Progsta and a Sillpruk comes and asks you whether the game of Doldun will be won by the team Strigli; what\u2019s your prior for the answer? What puzzled me was that <em>the very fact that you were asked that question</em> communicates an enormous amount of information \u2014 see <a href=\"/lw/1uc/open_thread_march_2010/1qu8\">this comment</a> of mine for examples \u2014 and yet I couldn\u2019t actually see how that should affect my priors. Of course, the information content of the question restricts hugely the universes in my E. But there were so many there that it\u2019s still huge; more importantly, it <em>restricts the universes along boundaries that I\u2019ve not previously explored</em>, and I don\u2019t have ready heuristics to estimate that little <em>p</em> above:</p>\n<p>If I throw a (correct) dice, I can split the universes in six approximately equal parts on vague symmetry justifications, and just estimate the probability of each side as 1/6. If someone on the street asks me to bet him on <em>his</em> dice I can split the universes in those where I win and those where I lose and estimate (using a kind of Montecarlo-integration with various scenarios I can think of) that I\u2019ll probably lose. If I encounter an alien named Sillpruk I\u2019ve no idea how to split the universes to estimate the result of a Doldun match. But if I were to encounter lots of aliens with strange first-questions for a while, I might develop some such simple heuristics based on simple trial and error.</p>\n<h3 id=\"PS_\">PS.</h3>\n<p>I\u2019m sorry if this was too long or just stupid. In the former case I welcome constructive criticism \u2014 don\u2019t hesitate to tell me what you think should have been cut. I hereby subject myself to <a href=\"http://www.sl4.org/crocker.html\">Crocker\u2019s Rules</a>. In the latter case... well, sorry :-)</p>\n<p>&nbsp;</p>", "sections": [{"title": "What \u201cprobability\u201d is...", "anchor": "What__probability__is___", "level": 1}, {"title": "Clarifications", "anchor": "Clarifications", "level": 1}, {"title": "Musings", "anchor": "Musings", "level": 1}, {"title": "PS.", "anchor": "PS_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-14T23:23:25.539Z", "modifiedAt": "2021-06-03T21:27:21.661Z", "url": null, "title": "Undiscriminating Skepticism", "slug": "undiscriminating-skepticism", "viewCount": null, "lastCommentedAt": "2022-05-06T20:10:06.802Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jko7pt7MwwTBrfG3A/undiscriminating-skepticism", "pageUrlRelative": "/posts/Jko7pt7MwwTBrfG3A/undiscriminating-skepticism", "linkUrl": "https://www.lesswrong.com/posts/Jko7pt7MwwTBrfG3A/undiscriminating-skepticism", "postedAtFormatted": "Sunday, March 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Undiscriminating%20Skepticism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUndiscriminating%20Skepticism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJko7pt7MwwTBrfG3A%2Fundiscriminating-skepticism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Undiscriminating%20Skepticism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJko7pt7MwwTBrfG3A%2Fundiscriminating-skepticism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJko7pt7MwwTBrfG3A%2Fundiscriminating-skepticism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2841, "htmlBody": "<p><em>Tl;dr:&nbsp; Since it can be cheap and easy to attack everything your tribe doesn't believe, you shouldn't trust the rationality of just anyone who slams astrology and creationism; these beliefs aren't just false, they're also non-tribal among educated audiences.&nbsp; Test what happens when a \"skeptic\" argues for a non-tribal belief, or argues against a tribal belief, before you decide they're good general rationalists.</em>&nbsp; This post is intended to be reasonably accessible to outside audiences.</p>\n<p>I don't believe in UFOs.&nbsp; I don't believe in astrology.&nbsp; I don't believe in homeopathy.&nbsp; I don't believe in creationism.&nbsp; I don't believe there were explosives planted in the World Trade Center.&nbsp; I don't believe in haunted houses.&nbsp; I don't believe in perpetual motion machines.&nbsp; I believe that all these beliefs are not only wrong but visibly insane.</p>\n<p>If you know nothing else about me but this, how much credit should you give me for general rationality?</p>\n<p>Certainly anyone who <em>was</em> skillful at adding up evidence, considering alternative explanations, and assessing prior probabilities, would end up disbelieving in all of these.</p>\n<p>But there would also be a simpler explanation for my views, a less rare factor that could explain it:&nbsp; I could just be anti-non-mainstream.&nbsp; I could be in the habit of hanging out in moderately educated circles, and know that astrology and homeopathy are not accepted beliefs of my tribe.&nbsp; Or just perceptually recognize them, on a wordless level, as \"sounding weird\".&nbsp; And I could mock anything that sounds weird and that my fellow tribesfolk don't believe, much as creationists who hang out with fellow creationists mock evolution for its ludicrous assertion that apes give birth to human beings.</p>\n<p>You can get <em>cheap</em> credit for rationality by mocking wrong beliefs that everyone in your social circle already believes to be wrong.&nbsp; It wouldn't mean that I have any ability at all to notice a wrong belief that the people around me believe to be right, or vice versa - to <em>further discriminate</em> truth from falsity, beyond the fact that my social circle doesn't already believe in something.</p>\n<p>Back in the good old days, there was a simple test for this syndrome that would get quite a lot of mileage:&nbsp; You could just ask me what I thought about God.<a id=\"more\"></a>&nbsp; If I treated the idea with deeper respect than I treated astrology, holding it worthy of serious debate even if I said I disbelieved in it, then you knew that I was taking my cues from my social surroundings - that if the people around me treated a belief as high-prestige, high-status, I wouldn't start mocking it no matter what the state of evidence.</p>\n<p>On the other hand suppose I said without hesitation that my epistemic state on God was similar to my epistemic state on psychic powers: no positive evidence, lots of failed tests, highly unfavorable prior, and if you believe it under those circumstances then something is wrong with your mind.&nbsp; Then you would have heard a bit of skepticism that might <em>cost me something socially</em>, and that not everyone around me would have endorsed, even in educated circles.&nbsp; You would know it wasn't just a cheap way of picking up cheap points.</p>\n<p>Today the God-test no longer works, because some people realized that the taking-it-seriously aura of religion is in fact the main thing left which prevents people from noticing the epistemic awfulness; there has been a concerted and, I think, well-advised effort to mock religion and strip it of its respectability.&nbsp; The upshot is that there are now quite wide social circles in which God is just another stupid belief that we all know we don't believe in, on the same list with astrology.&nbsp; You could be dealing with an adept rationalist, or you could just be dealing with someone who reads Reddit.</p>\n<p>And of course I could easily go on to name some beliefs that others think are wrong and that I think are right, or vice versa, but would inevitably lose some of my audience at each step along the way - just as, a couple of decades ago, I would have lost a lot of my audience by saying that religion was unworthy of serious debate.&nbsp; (Thankfully, today this outright dismissal is at least considered a respectable, mainstream position even if not everyone holds it.)</p>\n<p>I probably won't lose much by citing anti-Artificial-Intelligence views as an example of <em>undiscriminating skepticism</em>.&nbsp; I think a majority among educated circles are sympathetic to the argument that brains are not magic and so there is no obstacle in principle to building machines that think.&nbsp; But there are others, albeit in the minority, who recognize Artificial Intelligence as \"weird-sounding\" and \"sci-fi\", a belief in something that has never yet been demonstrated, hence unscientific - the same epistemic reference class as believing in aliens or homeopathy.</p>\n<p>(This is technically a <a href=\"/lw/1ph/youre_entitled_to_arguments_but_not_that/\">demand for unobtainable evidence</a>.&nbsp; The asymmetry with homeopathy can be summed up as follows:&nbsp; First:&nbsp; If we learn that Artificial Intelligence is definitely impossible, we must have learned some <em>new</em> fact unknown to modern science - everything we <em>currently</em> know about neurons and the evolution of intelligence suggests that no magic was involved.&nbsp; On the other hand, if we learn that homeopathy is <em>possible,</em> we must have learned some new fact unknown to modern science; if everything else we believe about physics is true, homeopathy shouldn't work.&nbsp; Second:&nbsp; If homeopathy works, we can expect double-blind medical studies to demonstrate its efficacy right now; the absence of this evidence is very strong evidence of absence.&nbsp; If Artificial Intelligence is possible in theory and in practice, we can't necessarily expect its creation to be demonstrated using current knowledge - this absence of evidence is only weak evidence of absence.)</p>\n<p>I'm using Artificial Intelligence as an example, because it's a case where you can see some \"skeptics\" directing their skepticism at a belief that <em>is</em> very popular in educated circles, that is, the nonmysteriousness and ultimate reverse-engineerability of mind.&nbsp; You can even see two skeptical principles brought into conflict - does a good skeptic disbelieve in Artificial Intelligence because it's a load of sci-fi which has never been demonstrated?&nbsp; Or does a good skeptic disbelieve in human exceptionalism, since it would require some mysterious, unanalyzable essence-of-mind unknown to modern science?</p>\n<p>It's on questions like these where we find the <em>frontiers</em> of knowledge, and everything now in the settled lands was once on the frontier.&nbsp; It might seem like a matter of little importance to debate weird non-mainstream beliefs; a matter for easy dismissals and open scorn.&nbsp; But if this policy is implemented in <em>full generality</em>, progress goes down the tubes.&nbsp; The mainstream is not <em>completely </em>right, and future science will not just consist of things that sound reasonable to everyone today - there will be at least some things in it that sound weird to us.&nbsp; (This is certainly the case if something along the lines of Artificial Intelligence is considered weird!)&nbsp; And yes, <em>eventually</em> such scientific truths will be established by experiment, but somewhere along the line - <em>before</em> they are definitely established and everyone already believes in them - the testers will need funding.</p>\n<p>Being skeptical about some non-mainstream beliefs is not a fringe project of little importance, not always a slam-dunk, not a bit of occasional pointless drudgery - though I can certainly understand why it feels that way to argue with creationists.&nbsp; Skepticism is just the converse of acceptance, and so to be skeptical of a non-mainstream belief is to <em>try</em> to contribute to the project of advancing the borders of the known - to stake an additional epistemic claim that the borders should <em>not</em> expand in this direction, and should advance in some other direction instead.</p>\n<p>This is high and difficult work - certainly much more difficult than the work of mocking everything that sounds weird and that the people in your social circle don't already seem to believe.</p>\n<p>To put it more formally, before I believe that someone is performing useful cognitive work, I want to know that their skepticism <em>discriminates truth from falsehood</em>, making a contribution <em>over and above</em> the contribution of this-sounds-weird-and-is-not-a-tribal-belief.&nbsp; In <a href=\"http://yudkowsky.net/rational/bayes\">Bayesian</a> terms, I want to know that p(mockery|belief false &amp; not a tribal belief) &gt; p(mockery|belief true &amp; not a tribal belief).</p>\n<p>If I recall correctly, the US Air Force's Project Blue Book, on UFOs, explained away as a sighting of the planet Venus what turned out to actually <em>be</em> an experimental aircraft.&nbsp; No, I don't believe in UFOs either; but if you're going to explain away experimental aircraft as Venus, then nothing else you say provides <em>further Bayesian evidence</em> against UFOs either.&nbsp; You are merely an <em>undiscriminating skeptic.</em>&nbsp; I don't believe in UFOs, but in order to credit Project Blue Book with <em>additional</em> help in establishing this, I would have to believe that if there <em>were</em> UFOs then Project Blue Book would have turned in a different report.</p>\n<p>And so if you're just as skeptical of a weird, non-tribal belief that turns out to have pretty good support, you just blew the whole deal - that is, if I pay any <em>extra</em> attention to your skepticism, it ought to be because I believe you <em>wouldn't</em> mock a weird non-tribal belief that was worthy of debate.</p>\n<p>Personally, I think that Michael Shermer blew it by mocking molecular nanotechnology, and Penn and Teller blew it by mocking cryonics (justification: more or less <a href=\"/lw/1rv/demands_for_particular_proof_appendices/\">exactly the same reasons</a> I gave for Artificial Intelligence).&nbsp; Conversely, Richard Dawkins scooped up a huge truckload of actual-discriminating-skeptic points, at least in my book, for <em>not</em> making fun of the many-worlds interpretation when he was asked about in an interview; indeed, Dawkins noted (correctly) that <a href=\"/lw/q7/if_manyworlds_had_come_first/\">the traditional collapse postulate pretty much has to be incorrect</a>.&nbsp; The many-worlds interpretation isn't just <a href=\"/lw/q3/decoherence_is_simple/\">the formally simplest explanation that fits the facts</a>, it also <em>sounds weird</em> and is not yet a tribal belief of the educated crowd; so whether someone makes fun of MWI is indeed a good test of whether they understand Occam's Razor or are just mocking everything that's not a tribal belief.</p>\n<p>Of course you may not trust me about any of that.&nbsp; And so my purpose today is not to propose a new litmus test to replace atheism.</p>\n<p>But I do propose that before you give anyone credit for being a smart, rational skeptic, that you ask them to defend some <em>non-mainstream</em> belief.&nbsp; And no, atheism doesn't count as non-mainstream anymore, no matter what the polls show.&nbsp; It has to be something that most of their social circle doesn't believe, or something that most of their social circle <em>does</em> believe which they think is wrong.&nbsp; Dawkins endorsing many-worlds still counts for now, although its usefulness as an indicator is fading fast... but the point is not to endorse many-worlds, but to see them take <em>some</em> sort of positive stance on where the frontiers of knowledge <em>should</em> change.</p>\n<p>Don't get me wrong, there's a whole crazy world out there, and when Richard Dawkins starts whaling on astrology in \"The Enemies of Reason\" documentary, he is doing good and necessary work. But it's dangerous to let people pick up too much credit <em>just</em> for slamming astrology and homeopathy and UFOs and God.&nbsp; What if they become famous skeptics by picking off the cheap targets, and then use that prestige and credibility to go after nanotechnology?&nbsp; Who will dare to consider cryonics now that it's been featured on an episode of Penn and Teller's \"Bullshit\"?&nbsp; On the current system you can gain high prestige in the educated circle just by targeting beliefs like astrology that are widely believed to be uneducated; but then the same guns can be turned on new ideas like the many-worlds interpretation, even though it's being actively debated by physicists.&nbsp; And that's why I suggest, not any particular litmus test, but just that you ought to have to stick your neck out and say <em>something</em> a little less usual - say where you are <em>not</em> skeptical (and most of your tribemates are) or where you <em>are</em> skeptical (and most of the people <em>in your tribe</em> are not).</p>\n<p>I am minded to pay attention to Robyn Dawes as a skillful rationalist, not because Dawes has slammed easy targets like astrology, but because he <em>also</em> took the lead in assembling and popularizing the total lack of experimental evidence for nearly all schools of psychotherapy and the persistence of multiple superstitions such as Rorschach ink-blot interpretation in the face of literally <em>hundreds</em> of experiments trying and failing to find any evidence for it.&nbsp; It's not that psychotherapy seemed like a difficult target after Dawes got through with it, but that, at the time he attacked it, people in educated circles still thought of it as something that educated people believed in.&nbsp; It's not quite as useful today, but back when Richard Feynman published \"Surely You're Joking, Mr. Feynman\" you could pick up evidence that he was <em>actually thinking </em>from the fact that he disrespected psychotherapists as well as psychics.</p>\n<p>I'll conclude with some simple and <em>non-trustworthy </em>indicators that the skeptic is just filling in a cheap and largely automatic mockery template:</p>\n<ul>\n<li>The \"skeptic\" opens by remarking about the crazy true believers and wishful thinkers who believe in X, where there seem to be a surprising number of physicists making up the population of those wacky cult victims who believe in X.&nbsp; (The physicist-test is not an infallible indicator of rightness or even non-stupidity, but it's a filter that rapidly picks up on, say, strong AI, molecular nanotechnology, cryonics, the many-worlds interpretation, and so on.)&nbsp; Bonus point losses if the \"skeptic\" remarks on how easily physicists are seduced by sci-fi ideas.&nbsp; The reason why this is a particularly negative indicator is that when someone is in a mode of automatically arguing against everything that seems weird and isn't a belief of their tribe - of rejecting weird beliefs as a matter of naked perceptual recognition of weirdness - then they tend to perceptually fill-in-the-blank by <em>assuming</em> that anything weird is believed by wacky cult victims (i.e., people Not Of Our Tribe).&nbsp; And they don't backtrack, or wonder otherwise, even if they find out that the \"cult\" seems to exhibit a surprising number of people who go around talking about rationality and/or members with PhDs in physics.&nbsp; Roughly, they have an automatic template for mocking weird beliefs, and if this requires them to just swap in physicists for astrologers as gullible morons, that's what they'll do.&nbsp; Of course physicists can be gullible morons too, but you should be establishing that as a surprising conclusion, not using it as an opening premise!</li>\n<li>The \"skeptic\" offers up items of \"evidence\" against X which are not much less expected in the case that X is true than in the case that X is false; in other words, they fail to grasp the elementary Bayesian notion of evidence.&nbsp; I don't believe that UFOs are alien visitors, but my skepticism has <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">nothing to do with all the crazy people who believe in UFOs</a> - the existence of wacky cults is not much less expected in the case that aliens <em>do </em>exist, than in the case that they do not.&nbsp; (I am skeptical of UFOs, not because I fear affiliating myself with the low-prestige people who believe in UFOs, but because I don't believe aliens would (a) travel across interstellar distances AND (b) hide all signs of their presence AND THEN (c) fly gigantic non-nanotechnological aircraft over our military bases with their exterior lights on.)</li>\n<li>The <a href=\"/lw/1ph/youre_entitled_to_evidence_but_not_that/\">demand for unobtainable evidence</a> is a special case of the above, and of course a very common mode of skepticism gone wrong.&nbsp; Artificial Intelligence and molecular nanotechnology both involve beliefs in the future feasibility of technologies that we can't build right now, but (arguendo) seem to be strongly permitted by current scientific belief, i.e., the non-ineffability of the brain, or the basic physical calculations which seem to show that simple nanotechnological machines should work.&nbsp; To discard all the arguments from cognitive science and rely on the knockdown argument \"no reliable reporter has ever seen an AI!\" is blindly filling in the template from haunted houses.</li>\n<li>The \"skeptic\" tries to <em>scare you away</em> from the belief <em>in their very first opening remarks</em>: for example, pointing out how UFO cults beat and starve their victims (when this can just as easily happen if aliens <em>are</em> visiting the Earth).&nbsp; The negative consequences of a false belief may be real, legitimate truths to be communicated; but <em>only after you establish by other means that the belief is factually false</em> - otherwise it's the logical fallacy of appeal to consequences.</li>\n<li>They mock first and counterargue later or not at all.&nbsp; I do believe there's a place for mockery in the war on dumb ideas, but <em>first</em> you write the crushing factual counterargument, then you <em>conclude</em> with the mockery.</li>\n</ul>\n<p>I'll conclude the conclusion by observing that poor skepticism can just as easily exist in a case where a belief is wrong as when a belief is right, so pointing out these flaws in someone's skepticism can hardly serve to establish a positive belief about where the frontiers of knowledge should move.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "LDTSbmXtokYAsEq8e": 1, "Q6P8jLn8hH7kbuXRr": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jko7pt7MwwTBrfG3A", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 118, "baseScore": 130, "extendedScore": null, "score": 0.000217, "legacy": true, "legacyId": "2480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 130, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1361, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vqbieD9PHG8RRJddu", "FwMhhzt8RSLAWNFAB", "WqGCaRhib42dhKWRL", "Atu4teGvob5vKvEAF", "qNZM3EGoE5ZeMdCRt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-03-14T23:23:25.539Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-15T14:17:00.528Z", "modifiedAt": null, "url": null, "title": "The problem of pseudofriendliness", "slug": "the-problem-of-pseudofriendliness", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:37.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "KDAxApfJFLPgNEzua", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P4dxapqPQAJP4i9fD/the-problem-of-pseudofriendliness", "pageUrlRelative": "/posts/P4dxapqPQAJP4i9fD/the-problem-of-pseudofriendliness", "linkUrl": "https://www.lesswrong.com/posts/P4dxapqPQAJP4i9fD/the-problem-of-pseudofriendliness", "postedAtFormatted": "Monday, March 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20problem%20of%20pseudofriendliness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20problem%20of%20pseudofriendliness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP4dxapqPQAJP4i9fD%2Fthe-problem-of-pseudofriendliness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20problem%20of%20pseudofriendliness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP4dxapqPQAJP4i9fD%2Fthe-problem-of-pseudofriendliness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP4dxapqPQAJP4i9fD%2Fthe-problem-of-pseudofriendliness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 798, "htmlBody": "<p>The Friendly AI problem is complicated enough that it can be divided into a large number of subproblems. Two such subproblems could be:</p>\n<ol>\n<li>The problem of goal interpretation &ndash; This means that the human expectation of the results of an AI implementing a goal differ from the results that the AI actually works toward.</li>\n<li>The problem of innate drives (see Steve Omohundro&rsquo;s <a title=\"Basic AI Drives\" href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\" target=\"_blank\">&lsquo;Basic Drives&rsquo;</a> paper for more detail) &ndash; This is when either specific goals, or goal based reasoning in general, creates subgoals that humans do not anticipate.</li>\n</ol>\n<p>Let's call an AI which does not suffer from these problem a pseudofriendly AI. Would this be a useful type of AI to produce? Well, maybe or maybe not. But even if it fails to be useful in and of itself, solving the pseudofriendly AI problem may be a helpful step toward developing the mode of thinking needed to solve the Friendly AI problem.</p>\n<p>It's also possible that pseudofriendliness might be able to interact usefully with Eliezer's Coherent Extrapoltated Volition (CEV - see <a title=\"Coherent Extrapolted Volition\" href=\"http://intelligence.org/upload/CEV.html\" target=\"_blank\">here for more details)</a>. Eliezer has expressed CEV as follows:</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>In poetic terms, our <em>coherent extrapolated volition</em> is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.</p>\n</blockquote>\n<p>However, an FAI is not to be given the CEV as it's goal but rather a superintelligence is to use our CEV to determine what goals an FAI should be given. What this means though is that there will be a point where a superintelligence exists that is not friendly. Could a pseudofriendly AI fill a gap here? Probably not - pseudofriendliness is not friendliness, nor should it be confused with it. However, it might be part of a solution that help the CEV approach to be safely implemented.</p>\n<p>Why all this hassle though? We seem to have exchanged one very important problem for two less important ones. Well, part of the benefit of pseudofriendliness is that it seems like it should be easier to formalise. First, let us introduce the concept of an interpretation system.</p>\n<p><em>An interpretation system takes a partially specified world state</em> <em>(called a goal) and outputs a triple (W<sub>x</sub>, S<sub>x</sub>, C<sub>x</sub>) where W<sub>x</sub> is a partially specified world state, S<sub>x</sub> is a set containing sets of subgoals and C<sub>x</sub> is a chosen subgoal.</em></p>\n<p>What does all of this mean? Well, the input could be thought of as a goal (stop the humans on that island from being drowned by rising sea waters) which is expressed as a partial world state (ie. the world state where the humans on the island remain undrowned). The interpretation system then outputs a partially specified world state which may be the same or different. In humans, various aspects of our cognitive system would make us interpret this goal as a different world state. For example, we may implicitly not consider tying all of the humans to giant stakes so they were above the level of the water but were unable to move or act.So we would output one world state while an AI may well output another. This is enough to specific the problem of goal interpretation as follows:</p>\n<p><em>The problem of goal interpretation is as follows. An interpretation system I<sub>x</sub> given a goal G outputs W<sub>x</sub>. A second interpretation system I<sub>y</sub> outputs W<sub>y</sub> on receiving the goal. Systems I<sub>x</sub> and I<sub>y</sub> suffer from the goal interpretation problem if W<sub>x</sub> &ne; W<sub>y.</sub></em></p>\n<p>The interpretation systems also output a set of goals to be used to bring about the world state and a set of goal sets which could altenatively be used to bring it about. Going back to our rising sea water example, even if W<sub>x</sub> = W<sub>y</sub>, these are only partially specified world views and hence do not determine whether every aspect of the AI's actions would produce outcomes that we want. This means that the subgoals used to get to a goal may still be undesirable. We can now specify the problem of innate drives as:</p>\n<p><em>System I<sub>x</sub> suffers from the weak problem of innate drives from the perspective of system I<sub>y</sub> if C<sub>x</sub> &ne; C<sub>y</sub>. It suffers from the strong problem of innate drives if C<sub>x</sub> is not a member of S<sub>x</sub>.</em></p>\n<p>If these definitions stand up, then pseudofriendly AI is certainly more formally specified than Friendly AI. However, even if not, it seems plausible that it is likely to be easier to formalise pseudofriendliness than friendliness. If you buy that, then the questions remaining are:</p>\n<ol>\n<li>Do these definitions stand up, and if not, is it possible to formulate another version.</li>\n<li>What is the solution to the problems of pseudofriendliness.</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P4dxapqPQAJP4i9fD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -4, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "2486", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-16T18:52:42.058Z", "modifiedAt": null, "url": null, "title": "Omega's subcontracting to Alpha", "slug": "omega-s-subcontracting-to-alpha", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.482Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tPCLcRJKBPGxRPwJX/omega-s-subcontracting-to-alpha", "pageUrlRelative": "/posts/tPCLcRJKBPGxRPwJX/omega-s-subcontracting-to-alpha", "linkUrl": "https://www.lesswrong.com/posts/tPCLcRJKBPGxRPwJX/omega-s-subcontracting-to-alpha", "postedAtFormatted": "Tuesday, March 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Omega's%20subcontracting%20to%20Alpha&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOmega's%20subcontracting%20to%20Alpha%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPCLcRJKBPGxRPwJX%2Fomega-s-subcontracting-to-alpha%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Omega's%20subcontracting%20to%20Alpha%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPCLcRJKBPGxRPwJX%2Fomega-s-subcontracting-to-alpha", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtPCLcRJKBPGxRPwJX%2Fomega-s-subcontracting-to-alpha", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p><em>This is a variant built on Gary Drescher's <a href=\"/lw/1qo/a_problem_with_timeless_decision_theory_tdt/\">xor problem</a> for timeless decision theory.</em></p>\n<p>You get an envelope from your good friend Alpha, and are about to open it, when Omega appears in a puff of logic.</p>\n<p>Being completely trustworthy as usual (don't you just hate that?), he explains that Alpha flipped a coin (or looked at the parity of a sufficiently high digit of pi), to decide whether to put &pound;1000 000 in your envelope, or put nothing.</p>\n<p>He, Omega, knows what Alpha decided, has also predicted your own actions, <em>and you know these facts</em>. He hands you a &pound;10 note and says:</p>\n<p style=\"padding-left: 30px;\">\"(I predicted that you will refuse this &pound;10) if and only if (there is &pound;1000 000 in Alpha's envelope).\"</p>\n<p>What to do?</p>\n<p><strong>EDIT</strong>: to clarify, Alpha will send you the envelope anyway, and Omega may choose to appear or not appear as he and his logic deem fit. Nor is Omega stating a mathematical theorem: that one can deduce from the first premise the truth of the second. He is using <a href=\"http://en.wikipedia.org/wiki/XNOR_gate\">XNOR</a>, but using 'if and only if' seems a more understandable formulation. You get to keep the envelope whatever happens, in case that wasn't clear.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tPCLcRJKBPGxRPwJX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 15, "extendedScore": null, "score": 5.696901066859589e-07, "legacy": true, "legacyId": "2495", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NSX8RuD9tQ4uWzkk3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-17T00:56:01.710Z", "modifiedAt": null, "url": null, "title": "Overcoming the mind-killer", "slug": "overcoming-the-mind-killer", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:29.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "woozle", "createdAt": "2009-04-26T14:42:51.459Z", "isAdmin": false, "displayName": "woozle"}, "userId": "28YXNgAMqSHizndKA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mbpLoshnrtcwi8tBT/overcoming-the-mind-killer", "pageUrlRelative": "/posts/mbpLoshnrtcwi8tBT/overcoming-the-mind-killer", "linkUrl": "https://www.lesswrong.com/posts/mbpLoshnrtcwi8tBT/overcoming-the-mind-killer", "postedAtFormatted": "Wednesday, March 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overcoming%20the%20mind-killer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOvercoming%20the%20mind-killer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbpLoshnrtcwi8tBT%2Fovercoming-the-mind-killer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overcoming%20the%20mind-killer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbpLoshnrtcwi8tBT%2Fovercoming-the-mind-killer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbpLoshnrtcwi8tBT%2Fovercoming-the-mind-killer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3110, "htmlBody": "<p>I've been asked to start a thread in order to continue a debate I <a href=\"/lw/1to/what_is_bayesianism/1oro\">started</a> in the comments of <a href=\"/lw/1to/what_is_bayesianism/\">an otherwise-unrelated post</a>. I started to write a post on that topic, found myself introducing my work by way of explanation, and then realized that this was a sub-topic all its own which is of substantial relevance to at least <a href=\"/lw/1to/what_is_bayesianism/1p5m\">one of the replies</a> to my comments in that post -- and a much better topic for a first-ever post/thread .</p>\n<p>So I'm going to write that introductory post first, and then start another thread specifically on the topic under debate.<a id=\"more\"></a></p>\n<p>I run issuepedia.org, a wiki site largely dedicated to the rational analysis of politics.</p>\n<p>As part of that analysis, it covers areas such as logical fallacies (and the larger domain of what I call \"rhetorical deceptions\" and which LessWrong calls \"the dark arts\"), history, people, organizations, and any other topics necessary to understand an issue. Coverage of each area generally includes collecting sources (such as news articles, editorials, and blog posts), essential details to provide a quick overview, and usually an attempt to draw some kind of conclusion<sup>1</sup> about the topic's ethical significance based, as much as possible, on the sources collected. (Readers are, of course, free to use the wiki format to offer counterarguments and additional facts/sources.)</p>\n<p>I started Issuepedia in 2005, largely in an attempt to understand how Bush could possibly have been re-elected (am I deluded, or is half the country deluded? if the latter, how did this happen?). Most of the content is my writing, as I am better at writing than at community-building, but it is all freely copyable under a CC license. I do not receive any money for my work on the site; it does accept donations, but this fact is not heavily advertised and so far there have been no donors. It does not display advertisements, nor have I advertised it (other than linking to it in contexts where its content seems relevant, such as comments on blog entries). I am considering doing the latter at some point when I have sufficient time to give the project the focus it will need in order to grow successfully.</p>\n<h2>Rationality and Politics</h2>\n<p>My main hypothesis<sup>2</sup> in starting Issuepedia is that it is, in fact, possible to be rational about politics, to overcome its \"<a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">mind-killing</a>\" qualities -- if given sufficient \"thinking room\" in which to record and work through all the relevant (and often mind-numbing) details involved in most political issues in a public venue where you can \"show your work\" and others may point out any errors and omissions. I'm trying to use wiki technology as an intelligence-enhancing, bias-overcoming device.</p>\n<p>Politics contains issues within issues within issues. Arriving at a rational conclusion about any given issue will often depend on being able to draw reasonable conclusions about a handful of other issues, each of which may have other sub-issues affecting it, and so on.</p>\n<p>Keeping track of all of these dependencies, however, is somewhat beyond most people's native emotional intuition and working memory capacity (including mine). Even when we consciously try to overcome built-in biases (such as allegiance to our perceived \"tribes\", unexamined beliefs acquired in childhood, etc.), our hind-brains want to take the fine, complex grain of reality and turn it into a simple good-vs.-bad or us-vs.-them moral map drawn with a blunt magic marker -- something we can easily remember and apply.</p>\n<p>On the other hand, many issues really do seem to boil down to such a simple narrative, something best stated in quite stark terms. Individuals who are making an effort to be measured and rational often seem to reject out of hand the possibility that such simple, clearcut conclusions could possibly be valid, leading to the opposite bias -- a sort of systemic \"fallacy of moderation\". This can cause popular acquiescence to beliefs that are essentially wrong, such as the claim that \"the Democrats do it too\" when pointing out evils committed by the latest generation of Republicans. (Yes, they do it too -- but much less often, and much less egregiously overall.)</p>\n<p>I propose that there must exist some set of factual information upon which each question ultimately rests, if followed far enough \"down\". In other words, if you exhaustively and recursively map out the sub-issues for each issue, you must eventually arrive at an issue which can be resolved by reference to facts known or knowable. If no such point can be reached, then the issue cannot possibly have any real-world significance -- because if anyone is in any way affected by the issue, then there is the fact of that dependency which must somehow tie in; the trick is figuring out the actual nature of that dependency.</p>\n<p>My approach in issuepedia is to break each major issue down into sub-issues, each of which has its own page for collecting information and analysis on that particular issue, then do the same to each of those issues until each sub-branch (or \"rootlet\", if you prefer to stay in-metaphor) has encountered the \"bedrock\" of questions which can be determined factually. Once the \"bedrock\" questions have been answered, the issues which rest upon those questions can be resolved, and so on.</p>\n<p>Documenting these connections, and the facts upon which they ultimately rest, ideally allows each reader to reconstruct the line of reasoning behind a given conclusion. If they disagree with that conclusion, then the facts and reasoning are available for them to figure out where the error lies -- and the wiki format makes it easy for them to post corrections; eventually, all rational parties should be able to reach <a href=\"http://wiki.lesswrong.com/wiki/Aumann_agreement\">agreement</a>.</p>\n<p>I won't go so far as to claim that Issuepedia carries out this methodology with any degree of rigor, but it's what I'm <a href=\"http://issuepedia.org/Issuepedia:Structured_Debate\">working towards</a>.</p>\n<p>I'm also aware that recent studies have shown that many people aren't influenced by facts once they've made up their minds (e.g. <a href=\"http://www.livescience.com/strangenews/060124_political_decisions.html\">here</a>). Since I have many times observed myself change my own opinion<sup>3</sup> in response to facts, I am working with the hypothesis that this ability may be a cognitive attribute that some people have and others lack -- in much the same way that (apparently) <a href=\"http://www.brainstages.net/4thr.html\">only 32% of the adult population can reason abstractly</a>. If it turns out that I do not, in fact, possess this ability to a satisfactory degree, then finding some way to improve it will become a priority.</p>\n<h2>Methodology for Determination of Fact</h2>\n<p>The question of how to methodically go about determining fact -- i.e. which assertions may be provisionally treated as true and which should be subjected to further inquiry -- <a title=\"comment on &quot;what is bayesianism?&quot;\" href=\"/lw/1to/what_is_bayesianism/1p5m\">came up</a> in the earlier discussion, and is something which I think is ripe for formalization.</p>\n<h3>flaws in the existing methodologies<br /></h3>\n<p>Up until now, society has depended upon a sort of organic, slow and inefficient but reasonably thorough vetting of new ideas by a number of processes. Those who are more familiar with this area of study should feel free to note any errors or omissions in my understanding, but here is my list of processes (which I'll call \"epistemic arenas\"<sup>4) </sup>by which we have traditionally arrived at societal truths:</p>\n<ul>\n<li><strong>science</strong> (the scientific process -- the scientific method for performing and documenting experiments, peer-review in publications, the practice of replicating previous experiments, and probably other practices generally considered to be part of \"science\")</li>\n<li><strong>government</strong>: the court system, the legislative sausage factory</li>\n<li><strong>social processing</strong>: people (especially friends) discussing their views on the ethics of various items large and small</li>\n<li><strong>media</strong>: newspapers, radio, TV</li>\n</ul>\n<p>The flaws in each of these methodologies have become much clearer due to the ease and speed with which they may now be researched because of the Internet. A brief summary:</p>\n<p><strong>The scientific process</strong> is clearly the best of the lot, but can be gamed and exploited: fake papers with sciencey-looking graphs and formulas (e.g. <a title=\"Nature, Not Human Activity, Rules the Climate\" href=\"http://issuepedia.org/2008-03-02_Nature,_Not_Human_Activity,_Rules_the_Climate\">this</a>) -- sometimes published in fake journals with sciencey-sounding names (e.g. <a title=\"Journal of American Physicians and Surgeons\" href=\"http://issuepedia.org/Journal_of_American_Physicians_and_Surgeons\">JP&amp;S</a>) or backed by sciencey-sounding institutions (<a title=\"Science and Environmental Policy Project\" href=\"http://issuepedia.org/Science_and_Environmental_Policy_Project\">SEPP</a>, <a title=\"Space and Science Research Center\" href=\"http://issuepedia.org/Space_and_Science_Research_Center\">SSRC</a>, <a title=\"Science and Public Policy Institute\" href=\"http://issuepedia.org/Science_and_Public_Policy_Institute\">SPPI</a>, <a title=\"Oregon Institute of Science and Medicine\" href=\"http://issuepedia.org/Oregon_Institute_of_Science_and_Medicine\">OISM</a>) -- in order to promote ideas which have been soundly defeated by the real scientific process. Lists of <a title=\"More Than 650 International Scientists Dissent Over Man-Made Global Warming Claims\" href=\"http://issuepedia.org/2008-12-10_More_Than_650_International_Scientists_Dissent_Over_Man-Made_Global_Warming_Claims\">hundreds of scientists</a> dissenting from the prevailing view may not, in fact, contain any scientists actually qualified to make an authoritative statement (i.e. one deserving credence without having to hear the actual argument) on the subject, and only gain popular credibility because of the use of the word \"scientist\".</p>\n<p>On the other hand, legitimate ideas which for some reason are considered taboo sometimes cannot gain entry to this process, and must publish their findings by other means which can look very similar to the methods used to promote illegitimate ideas. How can we tell the difference? We can, but it takes time -- thus \"a lie can travel around the world while the truth is still putting on its boots\" by exploiting these weaknesses.</p>\n<p>Bringing the machinery of the scientific process to bear on any given issue is also quite expensive and time-consuming; it can be many years (or decades, in the case of significant new ideas) before enough evidence is gathered to overturn prior assumptions. This fact can be exploited in both directions: important but \"inconvenient\" new facts can be drowned in a sea of publicity arguing against them, and well-established facts can be taken out politically by denialist \"sniping\" (repeating well-refuted claims over and over again until more people are familiar with those claims than with the refutations thereof, leading to popular belief that the claims must be true).</p>\n<p>Also, because the public is generally unaware of how the scientific process functions, they are likely to give it far less weight than it deserves (when they correctly identify that a given conclusion truly is scientifically supported, anyway). For example, an attack commonly used by creationists against the theory of evolution by natural selection is that it is \"only a theory\". Such an argument is only convincing to someone lacking an understanding of the degree to which a hypothesis must withstand interrogation before it starts to be cited as a \"theory\" in scientific circles.</p>\n<p>It should be pretty obvious that <strong>government</strong>'s epistemic process is flawed; nonetheless, many bad or outright false ideas become \"facts\" after being enshrined in law or upheld by court decisions. (I could discuss this at length if needed.)</p>\n<p><strong>Social processing</strong> seems to do much better at spotting ethical transgressions (harm and fairness violations), but isolated social groups and communities are vulnerable to memic infection by ideas which become self-reinforcing in the absence of good communication outside the group. Such ideas tend to survive by discouraging such communication and encouraging distrust of outside ideas (e.g. by labeling those outside the community as untrustworthy or tainted in some way), perpetuating the cycle.</p>\n<p><strong>The mainstream media</strong> was, for many decades, the antidote to the problems in the other arenas. Independent newspapers would risk establishment disfavor in exchange for increased circulation -- and although publishing politically inconvenient truth is not the only way to do that, it was certainly one of them.</p>\n<p>Whether deliberately and conspiratorially or simply by many different interests arriving at the same solutions for their problems (and the people with the power to stop it looking the other way as the industry's lobbyists rewrote the laws to encourage and promote those common solutions), <a href=\"http://issuepedia.org/Media_consolidation\">media consolidation</a> has effectively taken the mainstream media out of the game as a voice of dissent.</p>\n<h3>Issuepedia's methodology<br /></h3>\n<p>The basic idea behind Issuepedia's informal epistemic methodology is that truth -- at least on issues where there is no clear experiment which can be performed to resolve the matter -- is best determined by successive approximation from an initial guess, combined with encouragement of dissenting arguments.</p>\n<p>Someone makes a statement -- a \"claim\". Others respond, giving evidence and reasoning either supporting or in contradicting the claim (counterpoint). Those who still agree with the initial statement then defend it from the counterpoints with further evidence and/or reasoning. If there are any counterpoints nobody has been able to reasonably contradict, then the claim fails; otherwise it stands.</p>\n<p>By keeping a record of the objections offered -- in a publicly-editable space, via previously unavailable technology (the internet) -- it becomes unnecessary to rehash the ensuing debate-branch if someone raises the same objection again. They may add new twigs, but once an argument has been answered, the answers will be there every time the same argument is raised. This is an essential tool for defeating <a href=\"http://issuepedia.org/Denialism\">denialism</a>, which I define as the repeated re-use of already-defeated but otherwise persuasive arguments; to argue with a denialist, one simply need refer to the catalogue of arguments against their position, and reuse those arguments until the denialist comes up with a new one. This puts the burden on the denialists (finally!) and takes it off those who are sincerely trying to determine the nature of reality.</p>\n<p>This also makes it possible for large decisions involving many complex factors to be more accurately updated if knowledge of those factors changes significantly. One would never end up in a situation where one is asking \"why do we do things this way?\", much less \"why do we <em>still</em> do things this way, even though it hasn't made sense since X happened 20 years ago?\" because the chain of reasoning would be thoroughly documented.</p>\n<p>At present, the methodology has to be implemented by hand; I am working on <a href=\"http://issuepedia.org/Issuepedia:Structured_Debate\">software</a> to automate the process.</p>\n<h4>criticism of this methodology<br /></h4>\n<blockquote>\n<p>[<a href=\"/lw/1to/what_is_bayesianism/1ozr\">dripgrind</a>] Your standard of verification seems to be the Wikipedia standard - if you can find a \"mainstream\" source saying something, then you are happy to take it as fact (provided it fits your case).</p>\n<p>[<a href=\"/lw/1to/what_is_bayesianism/1p0h\">woozle</a>] I am \"happy to take it as fact\" until I find something contradictory. When that happens, I generally make note of both sources and look for more authoritative information. If you have a better methodology, I am open to suggestions. .. The \"Wikipedia standard\" seems to work pretty well, though -- didn't someone do a study comparing Wikipedia's accuracy with Encyclopedia Britannica's, and they came out about even?</p>\n</blockquote>\n<blockquote>\n<p>[<a href=\"/lw/1to/what_is_bayesianism/1p5m\">dripgrind</a>] So your standard of accepting something as evidence is \"a 'mainstream source' asserted it and I haven't seen someone contradict it\". That seems like you are setting the bar quite low. Especially because we have seen that <em>[a specific claim woozle made]</em> was quickly debunked (or at least, contradicted, which is what prompts you to abandon your belief and look for more authoritative information) by simple googling. Maybe you should, at minimum, try googling all your beliefs and seeing if there is some contradictory information out there.</p>\n</blockquote>\n<p>\"Setting the bar quite low\": yes, the initial bar for accepting a statement is low. This is by design, based on the idea of successive approximation of truth (as outlined above) and my secondary hypothesis \"that it is important for people to share their opinions on things, regardless of how much thought has been put into those opinions.\" (See note 2 below.)</p>\n<p>Certainly this methodology can lead to error if the size of the observing group is insufficiently large and active -- but it only takes one person saying \"wait, that's nonsense!\" to start the corrective process. I don't see that degree of responsiveness in any of the other epistemic arenas, and I don't believe it adds any new weaknesses -- except that there is no easy/quick way to gauge the reliability of a given assertion. That is a weakness which I plan to address via the <a href=\"http://issuepedia.org/Issuepedia:Structured_Debate\">structured debate tool</a> (although I had not until now consciously realized that it was needed).</p>\n<p>If this explanation of the process still seems problematic, I'm quite happy to discuss it further; getting the process right is obviously critical.</p>\n<p>&nbsp;</p>\n<hr />\n<p>I will be posting next on the specific claims we were discussing, i.e. 9/11 \"conspiracy theories\". It will probably take several more days at least. Will update this post with a link when the second one is ready.</p>\n<p>&nbsp;</p>\n<h2>\n<hr />\nNotes</h2>\n<p>1. For example, the article about <a href=\"http://issuepedia.org/Intelligent_design\">Intelligent Design</a> concludes that \"As with <a title=\"Creationism\" href=\"http://issuepedia.org/Creationism\">creationism</a> in its other forms, ID's main purpose was (and remains) to insinuate <a title=\"Religion\" href=\"http://issuepedia.org/Religion\">religion</a> into <a class=\"new\" title=\"Public school education in the United States (page does not exist)\" href=\"http://issuepedia.org/index.php?title=Public_school_education_in_the_United_States&amp;action=edit&amp;redlink=1\">public school education in the United States</a>. It has no real arguments to offer, its support derives exclusively from Christian <a title=\"Ideological protectionism\" href=\"http://issuepedia.org/Ideological_protectionism\">ideological protectionism</a> and <a class=\"mw-redirect\" title=\"Evangelism\" href=\"http://issuepedia.org/Evangelism\">evangelism</a>, and its proponents have no interest in revising their own beliefs in the light of evidence new to them. It is a form of <a title=\"Denialism\" href=\"http://issuepedia.org/Denialism\">denialism</a>.\"</p>\n<p>The idea here is to \"call a spade a spade\": if something is morally wrong (or right), say so -- rather than giving <em>the appearance of impartiality</em> priority over <em>reaching sound conclusions</em> (e.g. \"he-said/she-said journalism\" in the media, or the \"NPOV\" requirement on Wikipedia). You may start out with a lot of wrong statements, but they will be statements which <em>someone</em> believed firmly enough to write -- and when they are refuted, everyone who believed them will have access to the refutation, doing much more towards reducing overall error than if you only recorded known truths.</p>\n<p>2. A secondary hypothesis is that it is important for people to share their opinions on things, regardless of how much thought has been put into those opinions. I have two reasons for this: (1) it helps overcome individual bias by pooling the opinions of many (in an arena where hopefully all priors and reasoning may eventually be discussed and resolved), and (2) there are many terrible things that happen but which we lack the immediate power to change; if we neither do nor say anything about them, others may reasonably assume that we consent to these things. Saying something at least helps prevent the belief that there is no dissent, which otherwise might be used to justify the status quo.</p>\n<p>3. I am hoping that this observation is not itself a self-delusion or some form of akrasia. Toward the end of confirming or ruling out akrasia, I have made a point of <a href=\"http://issuepedia.org/User:Woozle/positions\">posting my positions</a> on many topics, with an open offer to defend any of those positions against rational criticism. If anyone believes, after observing any of the debates I have been involved in, that I am refusing to change my position in response to facts which clearly indicate such a change should take place, then I will add a note to that effect under the position in question.</p>\n<p>4. These have a lot in common with what David Brin calls \"<a href=\"http://www.davidbrin.com/disputationarticle1.html\">disputation arenas</a>\", but they don't seem to be exactly the same thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mbpLoshnrtcwi8tBT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 14, "extendedScore": null, "score": 5.697606444344242e-07, "legacy": true, "legacyId": "2442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've been asked to start a thread in order to continue a debate I <a href=\"/lw/1to/what_is_bayesianism/1oro\">started</a> in the comments of <a href=\"/lw/1to/what_is_bayesianism/\">an otherwise-unrelated post</a>. I started to write a post on that topic, found myself introducing my work by way of explanation, and then realized that this was a sub-topic all its own which is of substantial relevance to at least <a href=\"/lw/1to/what_is_bayesianism/1p5m\">one of the replies</a> to my comments in that post -- and a much better topic for a first-ever post/thread .</p>\n<p>So I'm going to write that introductory post first, and then start another thread specifically on the topic under debate.<a id=\"more\"></a></p>\n<p>I run issuepedia.org, a wiki site largely dedicated to the rational analysis of politics.</p>\n<p>As part of that analysis, it covers areas such as logical fallacies (and the larger domain of what I call \"rhetorical deceptions\" and which LessWrong calls \"the dark arts\"), history, people, organizations, and any other topics necessary to understand an issue. Coverage of each area generally includes collecting sources (such as news articles, editorials, and blog posts), essential details to provide a quick overview, and usually an attempt to draw some kind of conclusion<sup>1</sup> about the topic's ethical significance based, as much as possible, on the sources collected. (Readers are, of course, free to use the wiki format to offer counterarguments and additional facts/sources.)</p>\n<p>I started Issuepedia in 2005, largely in an attempt to understand how Bush could possibly have been re-elected (am I deluded, or is half the country deluded? if the latter, how did this happen?). Most of the content is my writing, as I am better at writing than at community-building, but it is all freely copyable under a CC license. I do not receive any money for my work on the site; it does accept donations, but this fact is not heavily advertised and so far there have been no donors. It does not display advertisements, nor have I advertised it (other than linking to it in contexts where its content seems relevant, such as comments on blog entries). I am considering doing the latter at some point when I have sufficient time to give the project the focus it will need in order to grow successfully.</p>\n<h2 id=\"Rationality_and_Politics\">Rationality and Politics</h2>\n<p>My main hypothesis<sup>2</sup> in starting Issuepedia is that it is, in fact, possible to be rational about politics, to overcome its \"<a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">mind-killing</a>\" qualities -- if given sufficient \"thinking room\" in which to record and work through all the relevant (and often mind-numbing) details involved in most political issues in a public venue where you can \"show your work\" and others may point out any errors and omissions. I'm trying to use wiki technology as an intelligence-enhancing, bias-overcoming device.</p>\n<p>Politics contains issues within issues within issues. Arriving at a rational conclusion about any given issue will often depend on being able to draw reasonable conclusions about a handful of other issues, each of which may have other sub-issues affecting it, and so on.</p>\n<p>Keeping track of all of these dependencies, however, is somewhat beyond most people's native emotional intuition and working memory capacity (including mine). Even when we consciously try to overcome built-in biases (such as allegiance to our perceived \"tribes\", unexamined beliefs acquired in childhood, etc.), our hind-brains want to take the fine, complex grain of reality and turn it into a simple good-vs.-bad or us-vs.-them moral map drawn with a blunt magic marker -- something we can easily remember and apply.</p>\n<p>On the other hand, many issues really do seem to boil down to such a simple narrative, something best stated in quite stark terms. Individuals who are making an effort to be measured and rational often seem to reject out of hand the possibility that such simple, clearcut conclusions could possibly be valid, leading to the opposite bias -- a sort of systemic \"fallacy of moderation\". This can cause popular acquiescence to beliefs that are essentially wrong, such as the claim that \"the Democrats do it too\" when pointing out evils committed by the latest generation of Republicans. (Yes, they do it too -- but much less often, and much less egregiously overall.)</p>\n<p>I propose that there must exist some set of factual information upon which each question ultimately rests, if followed far enough \"down\". In other words, if you exhaustively and recursively map out the sub-issues for each issue, you must eventually arrive at an issue which can be resolved by reference to facts known or knowable. If no such point can be reached, then the issue cannot possibly have any real-world significance -- because if anyone is in any way affected by the issue, then there is the fact of that dependency which must somehow tie in; the trick is figuring out the actual nature of that dependency.</p>\n<p>My approach in issuepedia is to break each major issue down into sub-issues, each of which has its own page for collecting information and analysis on that particular issue, then do the same to each of those issues until each sub-branch (or \"rootlet\", if you prefer to stay in-metaphor) has encountered the \"bedrock\" of questions which can be determined factually. Once the \"bedrock\" questions have been answered, the issues which rest upon those questions can be resolved, and so on.</p>\n<p>Documenting these connections, and the facts upon which they ultimately rest, ideally allows each reader to reconstruct the line of reasoning behind a given conclusion. If they disagree with that conclusion, then the facts and reasoning are available for them to figure out where the error lies -- and the wiki format makes it easy for them to post corrections; eventually, all rational parties should be able to reach <a href=\"http://wiki.lesswrong.com/wiki/Aumann_agreement\">agreement</a>.</p>\n<p>I won't go so far as to claim that Issuepedia carries out this methodology with any degree of rigor, but it's what I'm <a href=\"http://issuepedia.org/Issuepedia:Structured_Debate\">working towards</a>.</p>\n<p>I'm also aware that recent studies have shown that many people aren't influenced by facts once they've made up their minds (e.g. <a href=\"http://www.livescience.com/strangenews/060124_political_decisions.html\">here</a>). Since I have many times observed myself change my own opinion<sup>3</sup> in response to facts, I am working with the hypothesis that this ability may be a cognitive attribute that some people have and others lack -- in much the same way that (apparently) <a href=\"http://www.brainstages.net/4thr.html\">only 32% of the adult population can reason abstractly</a>. If it turns out that I do not, in fact, possess this ability to a satisfactory degree, then finding some way to improve it will become a priority.</p>\n<h2 id=\"Methodology_for_Determination_of_Fact\">Methodology for Determination of Fact</h2>\n<p>The question of how to methodically go about determining fact -- i.e. which assertions may be provisionally treated as true and which should be subjected to further inquiry -- <a title=\"comment on &quot;what is bayesianism?&quot;\" href=\"/lw/1to/what_is_bayesianism/1p5m\">came up</a> in the earlier discussion, and is something which I think is ripe for formalization.</p>\n<h3 id=\"flaws_in_the_existing_methodologies\">flaws in the existing methodologies<br></h3>\n<p>Up until now, society has depended upon a sort of organic, slow and inefficient but reasonably thorough vetting of new ideas by a number of processes. Those who are more familiar with this area of study should feel free to note any errors or omissions in my understanding, but here is my list of processes (which I'll call \"epistemic arenas\"<sup>4) </sup>by which we have traditionally arrived at societal truths:</p>\n<ul>\n<li><strong>science</strong> (the scientific process -- the scientific method for performing and documenting experiments, peer-review in publications, the practice of replicating previous experiments, and probably other practices generally considered to be part of \"science\")</li>\n<li><strong>government</strong>: the court system, the legislative sausage factory</li>\n<li><strong>social processing</strong>: people (especially friends) discussing their views on the ethics of various items large and small</li>\n<li><strong>media</strong>: newspapers, radio, TV</li>\n</ul>\n<p>The flaws in each of these methodologies have become much clearer due to the ease and speed with which they may now be researched because of the Internet. A brief summary:</p>\n<p><strong>The scientific process</strong> is clearly the best of the lot, but can be gamed and exploited: fake papers with sciencey-looking graphs and formulas (e.g. <a title=\"Nature, Not Human Activity, Rules the Climate\" href=\"http://issuepedia.org/2008-03-02_Nature,_Not_Human_Activity,_Rules_the_Climate\">this</a>) -- sometimes published in fake journals with sciencey-sounding names (e.g. <a title=\"Journal of American Physicians and Surgeons\" href=\"http://issuepedia.org/Journal_of_American_Physicians_and_Surgeons\">JP&amp;S</a>) or backed by sciencey-sounding institutions (<a title=\"Science and Environmental Policy Project\" href=\"http://issuepedia.org/Science_and_Environmental_Policy_Project\">SEPP</a>, <a title=\"Space and Science Research Center\" href=\"http://issuepedia.org/Space_and_Science_Research_Center\">SSRC</a>, <a title=\"Science and Public Policy Institute\" href=\"http://issuepedia.org/Science_and_Public_Policy_Institute\">SPPI</a>, <a title=\"Oregon Institute of Science and Medicine\" href=\"http://issuepedia.org/Oregon_Institute_of_Science_and_Medicine\">OISM</a>) -- in order to promote ideas which have been soundly defeated by the real scientific process. Lists of <a title=\"More Than 650 International Scientists Dissent Over Man-Made Global Warming Claims\" href=\"http://issuepedia.org/2008-12-10_More_Than_650_International_Scientists_Dissent_Over_Man-Made_Global_Warming_Claims\">hundreds of scientists</a> dissenting from the prevailing view may not, in fact, contain any scientists actually qualified to make an authoritative statement (i.e. one deserving credence without having to hear the actual argument) on the subject, and only gain popular credibility because of the use of the word \"scientist\".</p>\n<p>On the other hand, legitimate ideas which for some reason are considered taboo sometimes cannot gain entry to this process, and must publish their findings by other means which can look very similar to the methods used to promote illegitimate ideas. How can we tell the difference? We can, but it takes time -- thus \"a lie can travel around the world while the truth is still putting on its boots\" by exploiting these weaknesses.</p>\n<p>Bringing the machinery of the scientific process to bear on any given issue is also quite expensive and time-consuming; it can be many years (or decades, in the case of significant new ideas) before enough evidence is gathered to overturn prior assumptions. This fact can be exploited in both directions: important but \"inconvenient\" new facts can be drowned in a sea of publicity arguing against them, and well-established facts can be taken out politically by denialist \"sniping\" (repeating well-refuted claims over and over again until more people are familiar with those claims than with the refutations thereof, leading to popular belief that the claims must be true).</p>\n<p>Also, because the public is generally unaware of how the scientific process functions, they are likely to give it far less weight than it deserves (when they correctly identify that a given conclusion truly is scientifically supported, anyway). For example, an attack commonly used by creationists against the theory of evolution by natural selection is that it is \"only a theory\". Such an argument is only convincing to someone lacking an understanding of the degree to which a hypothesis must withstand interrogation before it starts to be cited as a \"theory\" in scientific circles.</p>\n<p>It should be pretty obvious that <strong>government</strong>'s epistemic process is flawed; nonetheless, many bad or outright false ideas become \"facts\" after being enshrined in law or upheld by court decisions. (I could discuss this at length if needed.)</p>\n<p><strong>Social processing</strong> seems to do much better at spotting ethical transgressions (harm and fairness violations), but isolated social groups and communities are vulnerable to memic infection by ideas which become self-reinforcing in the absence of good communication outside the group. Such ideas tend to survive by discouraging such communication and encouraging distrust of outside ideas (e.g. by labeling those outside the community as untrustworthy or tainted in some way), perpetuating the cycle.</p>\n<p><strong>The mainstream media</strong> was, for many decades, the antidote to the problems in the other arenas. Independent newspapers would risk establishment disfavor in exchange for increased circulation -- and although publishing politically inconvenient truth is not the only way to do that, it was certainly one of them.</p>\n<p>Whether deliberately and conspiratorially or simply by many different interests arriving at the same solutions for their problems (and the people with the power to stop it looking the other way as the industry's lobbyists rewrote the laws to encourage and promote those common solutions), <a href=\"http://issuepedia.org/Media_consolidation\">media consolidation</a> has effectively taken the mainstream media out of the game as a voice of dissent.</p>\n<h3 id=\"Issuepedia_s_methodology\">Issuepedia's methodology<br></h3>\n<p>The basic idea behind Issuepedia's informal epistemic methodology is that truth -- at least on issues where there is no clear experiment which can be performed to resolve the matter -- is best determined by successive approximation from an initial guess, combined with encouragement of dissenting arguments.</p>\n<p>Someone makes a statement -- a \"claim\". Others respond, giving evidence and reasoning either supporting or in contradicting the claim (counterpoint). Those who still agree with the initial statement then defend it from the counterpoints with further evidence and/or reasoning. If there are any counterpoints nobody has been able to reasonably contradict, then the claim fails; otherwise it stands.</p>\n<p>By keeping a record of the objections offered -- in a publicly-editable space, via previously unavailable technology (the internet) -- it becomes unnecessary to rehash the ensuing debate-branch if someone raises the same objection again. They may add new twigs, but once an argument has been answered, the answers will be there every time the same argument is raised. This is an essential tool for defeating <a href=\"http://issuepedia.org/Denialism\">denialism</a>, which I define as the repeated re-use of already-defeated but otherwise persuasive arguments; to argue with a denialist, one simply need refer to the catalogue of arguments against their position, and reuse those arguments until the denialist comes up with a new one. This puts the burden on the denialists (finally!) and takes it off those who are sincerely trying to determine the nature of reality.</p>\n<p>This also makes it possible for large decisions involving many complex factors to be more accurately updated if knowledge of those factors changes significantly. One would never end up in a situation where one is asking \"why do we do things this way?\", much less \"why do we <em>still</em> do things this way, even though it hasn't made sense since X happened 20 years ago?\" because the chain of reasoning would be thoroughly documented.</p>\n<p>At present, the methodology has to be implemented by hand; I am working on <a href=\"http://issuepedia.org/Issuepedia:Structured_Debate\">software</a> to automate the process.</p>\n<h4 id=\"criticism_of_this_methodology\">criticism of this methodology<br></h4>\n<blockquote>\n<p>[<a href=\"/lw/1to/what_is_bayesianism/1ozr\">dripgrind</a>] Your standard of verification seems to be the Wikipedia standard - if you can find a \"mainstream\" source saying something, then you are happy to take it as fact (provided it fits your case).</p>\n<p>[<a href=\"/lw/1to/what_is_bayesianism/1p0h\">woozle</a>] I am \"happy to take it as fact\" until I find something contradictory. When that happens, I generally make note of both sources and look for more authoritative information. If you have a better methodology, I am open to suggestions. .. The \"Wikipedia standard\" seems to work pretty well, though -- didn't someone do a study comparing Wikipedia's accuracy with Encyclopedia Britannica's, and they came out about even?</p>\n</blockquote>\n<blockquote>\n<p>[<a href=\"/lw/1to/what_is_bayesianism/1p5m\">dripgrind</a>] So your standard of accepting something as evidence is \"a 'mainstream source' asserted it and I haven't seen someone contradict it\". That seems like you are setting the bar quite low. Especially because we have seen that <em>[a specific claim woozle made]</em> was quickly debunked (or at least, contradicted, which is what prompts you to abandon your belief and look for more authoritative information) by simple googling. Maybe you should, at minimum, try googling all your beliefs and seeing if there is some contradictory information out there.</p>\n</blockquote>\n<p>\"Setting the bar quite low\": yes, the initial bar for accepting a statement is low. This is by design, based on the idea of successive approximation of truth (as outlined above) and my secondary hypothesis \"that it is important for people to share their opinions on things, regardless of how much thought has been put into those opinions.\" (See note 2 below.)</p>\n<p>Certainly this methodology can lead to error if the size of the observing group is insufficiently large and active -- but it only takes one person saying \"wait, that's nonsense!\" to start the corrective process. I don't see that degree of responsiveness in any of the other epistemic arenas, and I don't believe it adds any new weaknesses -- except that there is no easy/quick way to gauge the reliability of a given assertion. That is a weakness which I plan to address via the <a href=\"http://issuepedia.org/Issuepedia:Structured_Debate\">structured debate tool</a> (although I had not until now consciously realized that it was needed).</p>\n<p>If this explanation of the process still seems problematic, I'm quite happy to discuss it further; getting the process right is obviously critical.</p>\n<p>&nbsp;</p>\n<hr>\n<p>I will be posting next on the specific claims we were discussing, i.e. 9/11 \"conspiracy theories\". It will probably take several more days at least. Will update this post with a link when the second one is ready.</p>\n<p>&nbsp;</p>\n<h2 id=\"_Notes\">\n<hr>\nNotes</h2>\n<p>1. For example, the article about <a href=\"http://issuepedia.org/Intelligent_design\">Intelligent Design</a> concludes that \"As with <a title=\"Creationism\" href=\"http://issuepedia.org/Creationism\">creationism</a> in its other forms, ID's main purpose was (and remains) to insinuate <a title=\"Religion\" href=\"http://issuepedia.org/Religion\">religion</a> into <a class=\"new\" title=\"Public school education in the United States (page does not exist)\" href=\"http://issuepedia.org/index.php?title=Public_school_education_in_the_United_States&amp;action=edit&amp;redlink=1\">public school education in the United States</a>. It has no real arguments to offer, its support derives exclusively from Christian <a title=\"Ideological protectionism\" href=\"http://issuepedia.org/Ideological_protectionism\">ideological protectionism</a> and <a class=\"mw-redirect\" title=\"Evangelism\" href=\"http://issuepedia.org/Evangelism\">evangelism</a>, and its proponents have no interest in revising their own beliefs in the light of evidence new to them. It is a form of <a title=\"Denialism\" href=\"http://issuepedia.org/Denialism\">denialism</a>.\"</p>\n<p>The idea here is to \"call a spade a spade\": if something is morally wrong (or right), say so -- rather than giving <em>the appearance of impartiality</em> priority over <em>reaching sound conclusions</em> (e.g. \"he-said/she-said journalism\" in the media, or the \"NPOV\" requirement on Wikipedia). You may start out with a lot of wrong statements, but they will be statements which <em>someone</em> believed firmly enough to write -- and when they are refuted, everyone who believed them will have access to the refutation, doing much more towards reducing overall error than if you only recorded known truths.</p>\n<p>2. A secondary hypothesis is that it is important for people to share their opinions on things, regardless of how much thought has been put into those opinions. I have two reasons for this: (1) it helps overcome individual bias by pooling the opinions of many (in an arena where hopefully all priors and reasoning may eventually be discussed and resolved), and (2) there are many terrible things that happen but which we lack the immediate power to change; if we neither do nor say anything about them, others may reasonably assume that we consent to these things. Saying something at least helps prevent the belief that there is no dissent, which otherwise might be used to justify the status quo.</p>\n<p>3. I am hoping that this observation is not itself a self-delusion or some form of akrasia. Toward the end of confirming or ruling out akrasia, I have made a point of <a href=\"http://issuepedia.org/User:Woozle/positions\">posting my positions</a> on many topics, with an open offer to defend any of those positions against rational criticism. If anyone believes, after observing any of the debates I have been involved in, that I am refusing to change my position in response to facts which clearly indicate such a change should take place, then I will add a note to that effect under the position in question.</p>\n<p>4. These have a lot in common with what David Brin calls \"<a href=\"http://www.davidbrin.com/disputationarticle1.html\">disputation arenas</a>\", but they don't seem to be exactly the same thing.</p>", "sections": [{"title": "Rationality and Politics", "anchor": "Rationality_and_Politics", "level": 1}, {"title": "Methodology for Determination of Fact", "anchor": "Methodology_for_Determination_of_Fact", "level": 1}, {"title": "flaws in the existing methodologies", "anchor": "flaws_in_the_existing_methodologies", "level": 2}, {"title": "Issuepedia's methodology", "anchor": "Issuepedia_s_methodology", "level": 2}, {"title": "criticism of this methodology", "anchor": "criticism_of_this_methodology", "level": 3}, {"title": "\nNotes", "anchor": "_Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "128 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 128, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AN2cBr6xKWCB8dRQG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-17T01:14:24.994Z", "modifiedAt": null, "url": null, "title": "Subjective Anticipation and Death", "slug": "subjective-anticipation-and-death", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:22.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "LucasSloan", "createdAt": "2009-05-28T05:04:38.345Z", "isAdmin": false, "displayName": "LucasSloan"}, "userId": "ouo6Fqn5kTNY7LvqM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ansPTzk5vir5puoAS/subjective-anticipation-and-death", "pageUrlRelative": "/posts/ansPTzk5vir5puoAS/subjective-anticipation-and-death", "linkUrl": "https://www.lesswrong.com/posts/ansPTzk5vir5puoAS/subjective-anticipation-and-death", "postedAtFormatted": "Wednesday, March 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Subjective%20Anticipation%20and%20Death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASubjective%20Anticipation%20and%20Death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FansPTzk5vir5puoAS%2Fsubjective-anticipation-and-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Subjective%20Anticipation%20and%20Death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FansPTzk5vir5puoAS%2Fsubjective-anticipation-and-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FansPTzk5vir5puoAS%2Fsubjective-anticipation-and-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 752, "htmlBody": "<p>tldr; It is incoherent to talk about a \"you\" which stretches through time.&nbsp; Instead, we should think of a series of similar mind-moments.</p>\n<p>Once upon a time, there was a little boy, who answered to the name Lucas Sloan and was scared of dying. I too answer to the name Lucas Sloan, and I remember being afraid of dying. Little Lucas wasn't scared of the present state of affairs, but it is fairly obvious that Little Lucas isn't around anymore. By any practical definition, Little Lucas is dead, he only exists as a memory in my mind and more indirectly in the minds of others. Little Lucas did not care that other people remembered him, he cared that he did not <em>die</em>. So what is this death thing, if Little Lucas was scared of it, but was not scared of the present situation?</p>\n<p><a id=\"more\"></a></p>\n<p>I would now like to introduce the term mind-moment. I'm not sure quite how to define that, it may have to mean a single plank-time snapshot of a mind, or it might be as much as a couple weeks. I doubt that what I mean by this is anywhere near the upper bound I just gave, a more likely upper bound might be about a second - about the time it takes to notice something and realize something is going on.</p>\n<p>Confusion about the exact definition of mind-moment aside, I think it is obvious that Little Lucas and I are separate mind-moments. And the fact of the matter is that the mind moment that was Little Lucas no longer exists, that mind-moment is definitely dead, gone, kaput. In fact, if I'm right about what I mean by mind-moment, many mind-moments have ceased exist since I started writing this. And frankly, both of those things are, in fact, good. What would the point be of constantly re-running the same mind-moment over and over again? I certainly don&acute;t want to be caught in a time loop till the end of time, even if I couldn't tell that I was &ndash; that would be as much a waste of the future as converting the stars into orgasmium. I want to experience new things, I want to grow and learn. But the problem is that my use of the word I is incoherent - the \"me\" that experiences those new things, that knows more than I do, is not me. My time is passing, soon enough, I will be the Little Lucas remembered only because he had some interesting ideas.</p>\n<p>It's not that I don't want to die, it's that I want there to, in the future, exist happy, fulfilled mind-moments that remember being me. This model neatly solves the problem of the <a title=\"anthropic trilemma\" href=\"/lw/19d/the_anthropic_trilemma/\" target=\"_self\">anthropic trilemma</a>, you shouldn't anticipate <em>being </em><span style=\"font-style: normal;\">one of the winners or losers of the lottery, you should anticipate that at a certain point there will be x mind-moments who won or lost the lottery and remember being you. However, it does make the morality of death a lot more complicated. We shouldn't talk about killing as the action that breaks the status quo, we should instead say that each mind-moment needs to be created, and that mind-moments, once created have a right to the creation of their successors, each of whom retains that right.<br /></span></p>\n<p><span style=\"font-style: normal;\">This would all be quite simple if each mind-moment had one and only one possible successor. However, this is not the case. In writing this sentence, I could use the verb \"use\" or \"write\" and both choices require a separate mind-moment. Which mind-moment should be created? Are we obligated to create both? What if there are a million possible mind-moment successors? Are we obligated to create all of them? I don't think so. I still believe that the creation of minds is an active choice, so we shouldn't create minds without cause.&nbsp; Recasting life as a series of decisions to create mind moments explains the attractiveness of quantum suicide, you aren't killing yourself, you are refusing to create certain mind-moments.</span></p>\n<p>I still have some questions about life and death.&nbsp; For example, how quickly do we have to create these mind moments?&nbsp; It seems wrong to delay the creation of a subsequent mind-moment until the end of the universe, but as long as the next mind moment hasn't been created it isn't entitled to its successor.&nbsp; Also, are we obligated to create on the best/happiest/most fulfilled successor mind-moments?&nbsp; It might seem so in a classical universe, but in many worlds, wouldn't doing so result in horrendous duplication of effort?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2fa": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ansPTzk5vir5puoAS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 12, "extendedScore": null, "score": 5.697642147642975e-07, "legacy": true, "legacyId": "2498", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y7jZ9BLEeuNTzgAE5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-17T01:17:47.086Z", "modifiedAt": null, "url": null, "title": "Living Luminously", "slug": "living-luminously", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:02.048Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Alicorn", "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9o3Cjjem7AbmmZfBs/living-luminously", "pageUrlRelative": "/posts/9o3Cjjem7AbmmZfBs/living-luminously", "linkUrl": "https://www.lesswrong.com/posts/9o3Cjjem7AbmmZfBs/living-luminously", "postedAtFormatted": "Wednesday, March 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Living%20Luminously&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALiving%20Luminously%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9o3Cjjem7AbmmZfBs%2Fliving-luminously%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Living%20Luminously%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9o3Cjjem7AbmmZfBs%2Fliving-luminously", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9o3Cjjem7AbmmZfBs%2Fliving-luminously", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 888, "htmlBody": "<p><strong>The following posts may be useful background material:</strong> <a href=\"https://www.lesswrong.com/lw/1mu/sorting_out_sticky_brains/\">Sorting Out Sticky Brains</a>; <a href=\"https://www.lesswrong.com/lw/1ty/mental_crystallography/\">Mental Crystallography</a>; <a href=\"https://www.lesswrong.com/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a></p><p></p><p>I took the word &quot;luminosity&quot; from &quot;Knowledge and its Limits&quot; by Timothy Williamson, although I&#x27;m using it in a different sense than he did.  (He referred to &quot;being in a position to know&quot; rather than actually knowing, and in his definition, he doesn&#x27;t quite restrict himself to mental states and events.)  The original ordinary-language sense of &quot;luminous&quot; means &quot;emitting light, especially self-generated light; easily comprehended; clear&quot;, which should put the titles into context.</p><p></p><p>Luminosity, as I&#x27;ll use the term, is self-awareness.  A luminous mental state is one that you have and know that you have.  It could be an emotion, a belief or alief, a disposition, a quale, a memory - anything that might happen or be stored in your brain.  What&#x27;s going on in your head?  What you come up with when you ponder that question - assuming, nontrivially, that you are accurate - is what&#x27;s luminous to you.  Perhaps surprisingly, it&#x27;s hard for a lot of people to tell.  Even if they can identify the occurrence of individual mental events, they have tremendous difficulty modeling their cognition over time, explaining why it unfolds as it does, or observing ways in which it&#x27;s changed.  With sufficient luminosity, you can inspect your own experiences, opinions, and stored thoughts.  You can watch them interact, and discern patterns in how they do that.  This lets you predict what you&#x27;ll think - and in turn, what you&#x27;ll do - in the future under various possible circumstances.</p><p></p><p>I&#x27;ve made it a project to increase my luminosity as much as possible over the past several years.  While I am not (yet) perfectly luminous, I have already realized considerable improvements in such subsidiary skills like managing my mood, hacking into some of the systems that cause akrasia and other non-endorsed behavior, and simply being less confused about why I do and feel the things I do and feel.  I have some reason to believe that I am substantially more luminous than average, because I can ask people what seem to me to be perfectly easy questions about what they&#x27;re thinking and find them unable to answer.  Meanwhile, I&#x27;m not trusting my mere impression that I&#x27;m generally right when I come to conclusions about myself.  My models of myself, after I stop tweaking and toying with them and decide they&#x27;re probably about right, are borne out a majority of the time by my ongoing behavior.  Typically, they&#x27;ll also match what other people conclude about me, at least on some level.</p><p></p><p>In this sequence, I hope to share some of the techniques for improving luminosity that I&#x27;ve used.  I&#x27;m optimistic that at least some of them will be useful to at least some people.  However, I may be a walking, talking &quot;results not typical&quot;.  My prior attempts at improving luminosity in others consist of me asking individually-designed questions in real time, and that&#x27;s gone fairly well; it remains to be seen if I can distill the basic idea into a format that&#x27;s generally accessible.</p><p></p><p>I&#x27;ve divided up the sequence into eight posts, not including this one, which serves as introduction and index.  (I&#x27;ll update the titles in the list below with links as each post goes up.)</p><p></p><ul><li>Y<a href=\"https://www.lesswrong.com/lw/1xi/you_are_likely_to_be_eaten_by_a_grue/\">ou Are Likely To Be Eaten By A Grue.</a>  Why do you want to be luminous?  What good does it do, and how does it do it?</li><li><a href=\"https://www.lesswrong.com/lw/1xq/let_there_be_light/\">Let There Be Light.</a>  How do you get your priors when you start to model yourself, when your existing models are probably full of biases?</li><li><a href=\"https://www.lesswrong.com/lw/1y0/the_abcs_of_luminosity/\">The ABC&#x27;s of Luminosity.</a>  The most fundamental step in learning to be luminous is correlating your affect, behavior, and circumstance.</li><li><a href=\"https://www.lesswrong.com/lw/1yb/lights_camera_action/\">Lights, Camera, Action!</a>  Luminosity won&#x27;t happen by itself - you need to practice, and watch out for key mental items.</li><li><a href=\"https://www.lesswrong.com/lw/1za/the_spotlight/\">The Spotlight.</a>  Don&#x27;t keep your introspection interior.  Thoughts are slippery.  Label and organize whatever you find in your mind.</li><li><a href=\"https://www.lesswrong.com/lw/209/highlights_and_shadows/\">Highlights and Shadows.</a>  As you uncover and understand new things about yourself, it&#x27;s useful to endorse and repudiate your sub-components, and then encourage or interrupt them, respectively.</li><li><a href=\"https://www.lesswrong.com/lw/20r/city_of_lights/\">City of Lights.</a>  It&#x27;s a handy trick to represent yourself as multiple agents when dealing with tensions in yourself.</li><li><a href=\"https://www.lesswrong.com/lw/21l/lampshading/\">Lampshading</a>.  When you have models, test them - but rig your experiments!</li><li><u><strong>Bonus posts!</strong></u> </li><ul><li><a href=\"https://www.lesswrong.com/lw/20l/ureshiku_naritai/\">Ureshiku Naritai:</a>  A story of how I used luminosity to raise my happiness set point. </li><li><a href=\"https://www.lesswrong.com/lw/2a5/on_enjoying_disagreeable_company/\">On Enjoying Disagreeable Company</a>: a luminosity-driven model of how to like people on purpose.  </li><li><a href=\"https://www.lesswrong.com/lw/2aw/seven_shiny_stories/\">Seven Shiny Stories</a>: concrete fictional descriptions of luminosity techniques from this sequence in action.  (<strong>NOTE</strong>: Several people remark that SSS dramatically improved their understanding of the sequence.  It may be indicated to read each Shiny Story concurrently with its associated post.  The Shiny Stories each open with links to the relevant segment, and commenter apophenia has cleverly crossposted the stories under the top posts.)</li></ul></ul><p>I have already written all of the posts in this sequence, although I may make edits to later ones in response to feedback on earlier ones, and it&#x27;s not impossible that someone will ask me something that seems to indicate I should write an additional post.  I will dole them out at a pace that responds to community feedback.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "Zwv9eHi7KGg5KA9oM": 2, "HXA9WxPpzZCCEwXHT": 2, "5f5c37ee1b5cdee568cfb253": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9o3Cjjem7AbmmZfBs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 82, "baseScore": 88, "extendedScore": null, "score": 0.000148, "legacy": true, "legacyId": "2501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "you-are-likely-to-be-eaten-by-a-grue", "canonicalPrevPostSlug": "generalizing-from-one-example", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 90, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L4GGomr86sEwxzPvS", "SRaRHemkbsHWzzbPN", "baTWMegR42PAsH9qJ", "r6diXRLvkZBLpSoTf", "Y6TpEEKZq6HXfhWxd", "rLuZ6XrGpgjk9BNpX", "v4ngP587MDZ5rC48Y", "Zstm38omrpeu7iWeS", "tCTmAmAapB37dAz9Y", "vfHRahpgbp9YFPuGQ", "goCfoiQkniQwPryki", "xnPFYBuaGhpq869mY", "nK5jraMp7E4xPvuNv", "9sguwESkteCgqFMbj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-17T01:18:13.672Z", "modifiedAt": null, "url": null, "title": "You Are Likely To Be Eaten By A Grue", "slug": "you-are-likely-to-be-eaten-by-a-grue", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:32.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r6diXRLvkZBLpSoTf/you-are-likely-to-be-eaten-by-a-grue", "pageUrlRelative": "/posts/r6diXRLvkZBLpSoTf/you-are-likely-to-be-eaten-by-a-grue", "linkUrl": "https://www.lesswrong.com/posts/r6diXRLvkZBLpSoTf/you-are-likely-to-be-eaten-by-a-grue", "postedAtFormatted": "Wednesday, March 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20Are%20Likely%20To%20Be%20Eaten%20By%20A%20Grue&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20Are%20Likely%20To%20Be%20Eaten%20By%20A%20Grue%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6diXRLvkZBLpSoTf%2Fyou-are-likely-to-be-eaten-by-a-grue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20Are%20Likely%20To%20Be%20Eaten%20By%20A%20Grue%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6diXRLvkZBLpSoTf%2Fyou-are-likely-to-be-eaten-by-a-grue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr6diXRLvkZBLpSoTf%2Fyou-are-likely-to-be-eaten-by-a-grue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1078, "htmlBody": "<p><strong>Previously in sequence/sequence index:</strong> <a href=\"/lw/1xh/living_luminously/\">Living Luminously</a><br /><strong>Next in sequence:</strong> <a href=\"/lw/1xq/let_there_be_light/\">Let There Be Light</a></p>\n<p><em>Luminosity is fun, useful to others, and important in self-improvement.&nbsp; You should learn about it with this sequence.</em></p>\n<p>Luminosity?&nbsp; Pah!&nbsp; Who needs it?<br /><br />It's a legitimate question.&nbsp; The typical human gets through life with astonishingly little introspection, much less careful, accurate introspection.&nbsp; Our models of ourselves are sometimes even worse than our models of each other - we have more data, but also more biases loading up our reflection with noise.&nbsp; Most of the time, most people act on their emotions and beliefs directly, without the interposition of self-aware deliberation.&nbsp; And this doesn't usually seem to get anyone maimed or killed - when was the last time a gravestone read \"Here Lies Our Dear Taylor, Who Might Be Alive Today With More Internal Clarity About The Nature Of Memory Retrieval\"?&nbsp; Nonsense.&nbsp; If Taylor needs to remember something, it'll present itself, or not, and if there's a chronic problem with the latter then Taylor can export memories to <a href=\"/lw/11f/the_great_brain_is_located_externally/\">the environment</a>.&nbsp; Figuring out how the memories are stored in the first place and tweaking that is not high on the to-do list.<br /><br />Still, I think it's worth investing considerable time and effort into improving your luminosity.&nbsp; I submit three reasons why this is so.<a id=\"more\"></a><br /><br />First, you are a fascinating creature.&nbsp; It's just plain fun and rewarding to delve into your own mind.&nbsp; People in general are among the most complex, intriguing things in the world.&nbsp; You're no less so.&nbsp; You have lived a fair number of observer-moments.&nbsp; Starting with a native architecture that is pretty special all by itself, you've accumulated a complex set of filters by which you interpret your input - remembered past, experienced present, and anticipated future.&nbsp; You like things; you want things; you believe things; you expect things; you feel things.&nbsp; There's a lot of stuff rolled up and tucked into the fissures of your brain.&nbsp; Wouldn't you like to know what it is?&nbsp; Particularly because it's <em>you</em>.&nbsp; Many people find themselves to be their favorite topics.&nbsp; Are you an exception?&nbsp; (There's one way to find out...)<br /><br />Second, an accurate model of yourself can help you help others deal with you in the best possible way.&nbsp; Right now, they're probably using kludgey agglomerations of self-projection, stereotype, and automatically generated guesses that they may not bother to update as they learn more about you.&nbsp; I'm assuming you don't surround yourself with hostile people who would use accurate data about you to hurt and manipulate you, but if you do, certainly be judicious with whatever information your quest for luminosity supplies.&nbsp; As for everyone else, their having a better model of you will avoid a lot of headaches on everyone's parts.&nbsp; I'll present myself as an example: I hate surprises.&nbsp; Knowing this, and being able to tell a complete and credible story about how this works, I can explain to people who might wish to exchange gifts why they should not spring unknown wrapped items on me, and avoid that source of irritation.&nbsp; Most of the people around me choose not to take actions that they know will irritate me; but without a detailed explanation of exactly <em>how</em> my preferences are uncommon, they'll all too easily revert to their base model of a generic person.<br /><br />Third, and most germane to the remaining posts in this sequence: with a better picture of who you are and what your brain is up to, you can find the best low-hanging fruit in terms of hacks to <em>change</em> yourself.&nbsp; If you keep going from point A to point Z, but know nothing about the route in between, then the only way you can avoid a disliked Z is to try to come to a screeching halt right before it happens.&nbsp; If you could monitor the process from the start, and determine what pattern your mind follows along the alphabet, you might find that you can easily intervene at G or Q, and never have to deal with Z again.&nbsp; Similarly, if you try to go from alpha to omega but tend not to wind up at omega, how are you ever going to determine where your obstructions lie unless you pay attention to something other than the bare fact of non-omega?&nbsp; There could be some trivial omicron-related problem that you'd fix in a heartbeat if only you knew it was getting in the way.&nbsp; Additionally, your faulty models of yourself are <em>already changing you</em> through such miraculous means as cognitive dissonance.&nbsp; Unless you find out how it's doing that, you lose the chance to monitor and control the process.<br /><br />An analogy: You're waiting to be picked up at the airport.&nbsp; The designated time comes and goes, and you're sitting by the baggage claim with your suitcases at your feet, your eyes on your watch, and a frown on your face.&nbsp; The person was supposed to pick you up <em>at the airport</em>, and isn't there!&nbsp; A clear failure has occurred!&nbsp; But if you phone the person and start screaming \"The airport, you fool!&nbsp; I'm at the airport!&nbsp; Why aren't you?\" then this will tend not to improve things unless the person never left in the first place out of forgetfulness.&nbsp; If they're stuck in traffic, or were sent out of their way by road construction, or have gotten hopelessly lost, or have been identified by the jackbooted thugs that keep watch at the airport parking lot as a terrorist, reiterating that you had this particular goal in mind won't help.&nbsp; And unless you <em>find out what is keeping them</em>, you can't help.&nbsp; You have to know where they are to tell them what detours to take to avoid rush hour; you have to know what diversions were introduced to tell them how to rejoin their planned route; you have to know what landmarks they can see to know where they've gone missing to; you have to know whether to go make Bambi eyes at the security guards and plead misunderstanding.&nbsp; Without rather specific, sensitive data about what's gone wrong, you can't make it right.<br /><br />In the next posts of this sequence, I'm going to illustrate some methods that have helped me learn more about myself and change what I don't like.&nbsp; With luck, they'll assist you on the project that I've just attempted to convince you to want to undertake.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "Zwv9eHi7KGg5KA9oM": 2, "5f5c37ee1b5cdee568cfb253": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r6diXRLvkZBLpSoTf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 71, "extendedScore": null, "score": 0.000119, "legacy": true, "legacyId": "2502", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "let-there-be-light", "canonicalPrevPostSlug": "living-luminously", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 71, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "Y6TpEEKZq6HXfhWxd", "h7NkpER4Jo8BLWgPD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-17T01:18:15.784Z", "modifiedAt": null, "url": null, "title": "You Are Likely To Be Eaten By A Grue", "slug": "you-are-likely-to-be-eaten-by-a-grue-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cogqRFuqXtYuJyTgf/you-are-likely-to-be-eaten-by-a-grue-0", "pageUrlRelative": "/posts/cogqRFuqXtYuJyTgf/you-are-likely-to-be-eaten-by-a-grue-0", "linkUrl": "https://www.lesswrong.com/posts/cogqRFuqXtYuJyTgf/you-are-likely-to-be-eaten-by-a-grue-0", "postedAtFormatted": "Wednesday, March 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20Are%20Likely%20To%20Be%20Eaten%20By%20A%20Grue&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20Are%20Likely%20To%20Be%20Eaten%20By%20A%20Grue%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcogqRFuqXtYuJyTgf%2Fyou-are-likely-to-be-eaten-by-a-grue-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20Are%20Likely%20To%20Be%20Eaten%20By%20A%20Grue%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcogqRFuqXtYuJyTgf%2Fyou-are-likely-to-be-eaten-by-a-grue-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcogqRFuqXtYuJyTgf%2Fyou-are-likely-to-be-eaten-by-a-grue-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 18, "htmlBody": "<p>This was a mistaken double post.&nbsp; The real post is <a href=\"/lw/1xi/you_are_likely_to_be_eaten_by_a_grue/\">here</a>.&nbsp; Please do not comment on this post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cogqRFuqXtYuJyTgf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "2503", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["r6diXRLvkZBLpSoTf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-17T02:26:45.596Z", "modifiedAt": null, "url": null, "title": "Disconnect between Stated/Implemented Preferences", "slug": "disconnect-between-stated-implemented-preferences", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:51.709Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Strange7", "createdAt": "2010-02-12T08:30:10.267Z", "isAdmin": false, "displayName": "Strange7"}, "userId": "hKxerxxgheQZCxHsR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hbrADEQrAbPkwis95/disconnect-between-stated-implemented-preferences", "pageUrlRelative": "/posts/hbrADEQrAbPkwis95/disconnect-between-stated-implemented-preferences", "linkUrl": "https://www.lesswrong.com/posts/hbrADEQrAbPkwis95/disconnect-between-stated-implemented-preferences", "postedAtFormatted": "Wednesday, March 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Disconnect%20between%20Stated%2FImplemented%20Preferences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADisconnect%20between%20Stated%2FImplemented%20Preferences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhbrADEQrAbPkwis95%2Fdisconnect-between-stated-implemented-preferences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Disconnect%20between%20Stated%2FImplemented%20Preferences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhbrADEQrAbPkwis95%2Fdisconnect-between-stated-implemented-preferences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhbrADEQrAbPkwis95%2Fdisconnect-between-stated-implemented-preferences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>Currently, the comment for which I've received the most positive karma <a href=\"/lw/1ww/undiscriminating_skepticism/1r7x\">by a factor of four</a> is a joke about institutionalized ass-rape. A secondhand joke, effectively a quote with no source cited. Furthermore, the comment had, at best, tangential relevance to the subject of discussion. If anyone were to provide a detailed explanation of why they voted as they did, I predict that I would be appreciative.</p>\n<p>Based on this evidence, which priors need to be adjusted? Discuss.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hbrADEQrAbPkwis95", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": -5, "extendedScore": null, "score": 5.697782617599976e-07, "legacy": true, "legacyId": "2505", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-17T19:35:59.046Z", "modifiedAt": null, "url": null, "title": "Let There Be Light", "slug": "let-there-be-light", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:29.667Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y6TpEEKZq6HXfhWxd/let-there-be-light", "pageUrlRelative": "/posts/Y6TpEEKZq6HXfhWxd/let-there-be-light", "linkUrl": "https://www.lesswrong.com/posts/Y6TpEEKZq6HXfhWxd/let-there-be-light", "postedAtFormatted": "Wednesday, March 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let%20There%20Be%20Light&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet%20There%20Be%20Light%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6TpEEKZq6HXfhWxd%2Flet-there-be-light%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let%20There%20Be%20Light%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6TpEEKZq6HXfhWxd%2Flet-there-be-light", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6TpEEKZq6HXfhWxd%2Flet-there-be-light", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1340, "htmlBody": "<p><strong>Sequence index:</strong> <a href=\"/lw/1xh/living_luminously/\">Living Luminously</a><br /><strong>Previously in sequence:</strong> <a href=\"/lw/1xi/you_are_likely_to_be_eaten_by_a_grue/\">You Are Likely To Be Eaten By A Grue</a><br /><strong>Next in sequence:</strong> <a href=\"/lw/1y0/the_abcs_of_luminosity/\">The ABC's of Luminosity</a></p>\n<p><em>You can start from psych studies, personality tests, and feedback from people you know when you're learning about yourself.&nbsp; Then you can throw out the stuff that sounds off, keep what sounds good, and move on.</em></p>\n<p><em>You may find your understanding of this post significantly improved if you read the first story from <a href=\"/lw/2aw/seven_shiny_stories\">Seven Shiny Stories</a>.<br /></em></p>\n<p>Where do you get your priors, when you start modeling yourself seriously instead of doing it by halfhearted intuition?<br /><br />Well, one thing's for sure: not with the caliber of introspection you're most likely starting with.&nbsp; If you've spent any time on this site at all, you know people are riddled with biases and mechanisms for self-deception that systematically confound us about who we are.&nbsp; (\"I'm splendid and brilliant!&nbsp; The last five hundred times I did non-splendid non-brilliant things were outrageous flukes!\")&nbsp; Humans suck at most things, and obeying the edict \"Know thyself!\" is not a special case.<br /><br />The outside view has gotten a bit of a <a href=\"/lw/1p5/outside_view_as_conversationhalter/\">bad</a> <a href=\"/lw/ri/the_outside_views_domain/\">rap</a>, but I'm going to defend it - as a jumping-off point, anyway - when I fill our luminosity toolbox.&nbsp; There's a major body of literature designed to figure out just what the hell happens inside our skulls: it's called psychology, and they have a rather impressive track record.&nbsp; For instance, learning about <a href=\"http://wiki.lesswrong.com/wiki/Heuristic\">heuristics</a> and <a href=\"http://wiki.lesswrong.com/wiki/Bias\">biases</a> may let you detect them in action in yourself.&nbsp; I can often tell when I'm about to be subject to the <a href=\"http://en.wikipedia.org/wiki/Bystander_effect\">bystander effect</a> (\"There is someone sitting in the middle of the road.&nbsp; Should I call 911?&nbsp; I mean, she's sitting up and everything and there are non-alarmed people looking at her - but gosh, I probably don't look alarmed either...\"), have made some progress in reducing the extent to which I <a href=\"/lw/dr/generalizing_from_one_example/\">generalize from one example</a> (\"How are you not all driven insane by the spatters of oil all over the stove?!\"), and am suspicious when I think I might be <a href=\"http://en.wikipedia.org/wiki/Lake_Wobegon_effect\">above average in some way</a> and have no hard data to back it up (\"Now I can be confident that I am in fact good at this sort of problem: I answered all of these questions and most people can't, according to someone who has no motivation to lie!\").&nbsp; Now, even if you <em>are</em> a <a href=\"/lw/17x/beware_of_weird_psychological_samples/\">standard psych study subject</a>, of course you aren't going to align with <em>every psychological finding ever.</em>&nbsp; They don't even align perfectly with each <em>other</em>.&nbsp; But - controlling for some huge, obvious factors, like if you have a mental illness - it's a good place to start.<a id=\"more\"></a><br /><br />For narrowing things down beyond what's been turned up as typical human reactions to things, you can try personality tests like <a href=\"http://en.wikipedia.org/wiki/Myers-Briggs_Type_Indicator\">Myers-Briggs</a> or <a href=\"http://en.wikipedia.org/wiki/Big_Five_personality_traits\">Big Five</a>.&nbsp; These are not fantastically reliable sources.&nbsp; However, some of them have some ability to track with some parts of reality.&nbsp; Accordingly, <a href=\"/lw/gf/saturation_distillation_improvisation_a_story/\">saturate</a> with all the test data you can stand.&nbsp; Filter it for what sounds right (\"gosh, I guess I <em>do</em> tend to be rather bothered by things out of place in my environment, compared to others\") and dump the rest (\"huh?&nbsp; I'm not open to experience at all!&nbsp; I won't even try escargot!\") - these are rough, first-approximation priors, not posteriors you should actually act on, and you can afford a clumsy process this early in the game.&nbsp; While you're at it, give some thought to your <a href=\"http://skyview.vansd.org/lschmidt/Projects/The%20Nine%20Types%20of%20Intelligence.htm\">intelligence types</a>, categorize your <a href=\"http://en.wikipedia.org/wiki/The_Five_Love_Languages\">love language</a><sup>1</sup> - anything that carves up person-space and puts you in a bit of it.<br /><br />Additionally, if you have honest friends or relatives, you can ask for their help.&nbsp; Note that even honest ones will probably have a rosy picture of you: they can stand to be around you, so they probably aren't paying excruciatingly close attention to your flaws, and may exaggerate the importance of your virtues relative to a neutral observer's hypothetical opinion.&nbsp; They also aren't around you <em>all the time</em>, which will constrict the circumstances in which their model is tested and skew it towards whatever influence their own presence has on you.&nbsp; Their outside perspective is, however, still valuable.</p>\n<p>(Tips on getting friends/family to provide feedback: I find musing aloud about myself in an obviously tentative manner to be fairly useful at eliciting some domain-specific input. Some of my friends I can ask point-blank, although it helps to ask about specific situations (\"Do you think I'm just tired?\" \"Was I over the line back there?\") rather than general traits that feel more judgmental to discuss (\"Am I a jerk?\" \"Do I use people?\"). When you communicate in text and keep logs, you can send people pastes of entire conversations (when this is permissible to your original interlocutor) and ask what your consultant thinks of that. If you do not remember some event, or are willing to pretend not to remember the event, then you can get whoever was with you at the time to recount it from their perspective - this process will automatically paint what you did during the event in the light of outside scrutiny.)</p>\n<p>If during your prior-hunting something turns up that seems <em>wrong</em> to you, whether it's a whole test result or some specific supposed feature of people in a group that seems otherwise generally fitting, that's great!&nbsp; Now you can rule something out.&nbsp; Think: what makes the model wrong?&nbsp; When have you done something that falsified it?&nbsp; (\"That one time last week\" is more promising than \"back in eighty-nine I think it might have been January\".)&nbsp; What are the smallest things you could change to make it sit right?&nbsp; (\"Change the word \"rapid\" to \"meticulous\" and that's me to a tee!\")&nbsp; If it helps, take in the information you gather in small chunks.&nbsp; That way you can inspect them one at a time, instead of only holistically accepting or rejecting what a given test tells you.<br /><br />If something sounds <em>right</em> to you, that's also great!&nbsp; Ask: what predictions does this idea let you make about your cognition and behavior?&nbsp; (\"Should you happen to meet a tall, dark stranger, you will make rapid assumptions about his character based on his body language.\")&nbsp; How could you test them, and refine the model?&nbsp; (Where do the tall, dark strangers hang out?)&nbsp; If you've behaved in ways inconsistent with this model in the past, what exceptions to the rule does that imply and how can you most concisely, <a href=\"/lw/jp/occams_razor/\">Occam-esque-ly</a> summarize them?&nbsp; (\"That one tall, dark stranger was wearing a very cool t-shirt which occluded posture data.\")</p>\n<p>Nota bene: you may be tempted to throw out things because they sound bad (\"I can't be a narcissist!&nbsp; That wouldn't be in keeping with the story I tell about myself!\"), rather than because they sound wrong, and to keep things because they sound good (\"ooh!&nbsp; I'm funny and smart!\"), rather than because they sound right.&nbsp; Recite the Litany of Tarski a few times, if that helps: if you have a trait, you desire to believe that you have the trait.&nbsp; If you do not have a trait, you desire to believe that you do not have the trait.&nbsp; May you not become attached to beliefs you may not want.&nbsp; If you have bad features, knowing about them won't make them worse - and might let you fix, work around, or mitigate them.&nbsp; If you lack good features, deluding yourself about them won't make them appear - and might cost you opportunities to develop them for real.&nbsp; If you <em>can't answer</em> the questions \"when have you done something that falsified this model?\" or \"list some examples of times when you've behaved in accordance with this model\" - second guess.&nbsp; Try again.&nbsp; Think harder.&nbsp; You are not guaranteed to be right, and being right should be the aim here.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>It looks cheesy, but I've found it remarkably useful as a first-pass approximation of how to deal with people when I've gotten them to answer the question.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "fkABsGCJZ6y9qConW": 1, "Zwv9eHi7KGg5KA9oM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y6TpEEKZq6HXfhWxd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 55, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "2510", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "the-abc-s-of-luminosity", "canonicalPrevPostSlug": "you-are-likely-to-be-eaten-by-a-grue", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "r6diXRLvkZBLpSoTf", "rLuZ6XrGpgjk9BNpX", "9sguwESkteCgqFMbj", "FsfnDfADftGDYeG4c", "pqoxE3AGMbse68dvb", "baTWMegR42PAsH9qJ", "33YYcoWwtmqzAq9QR", "MT85svcEweuryr2sn", "f4txACqDWithRi7hs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-18T05:22:48.488Z", "modifiedAt": null, "url": null, "title": " Sequential Organization of Thinking: \"Six Thinking Hats\"", "slug": "sequential-organization-of-thinking-six-thinking-hats", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:43.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JustinShovelain", "createdAt": "2009-06-10T00:56:47.112Z", "isAdmin": false, "displayName": "JustinShovelain"}, "userId": "LEeresErqn3BpWrwG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Skc9JZLy9HAzifsXu/sequential-organization-of-thinking-six-thinking-hats", "pageUrlRelative": "/posts/Skc9JZLy9HAzifsXu/sequential-organization-of-thinking-six-thinking-hats", "linkUrl": "https://www.lesswrong.com/posts/Skc9JZLy9HAzifsXu/sequential-organization-of-thinking-six-thinking-hats", "postedAtFormatted": "Thursday, March 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Sequential%20Organization%20of%20Thinking%3A%20%22Six%20Thinking%20Hats%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Sequential%20Organization%20of%20Thinking%3A%20%22Six%20Thinking%20Hats%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkc9JZLy9HAzifsXu%2Fsequential-organization-of-thinking-six-thinking-hats%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Sequential%20Organization%20of%20Thinking%3A%20%22Six%20Thinking%20Hats%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkc9JZLy9HAzifsXu%2Fsequential-organization-of-thinking-six-thinking-hats", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkc9JZLy9HAzifsXu%2Fsequential-organization-of-thinking-six-thinking-hats", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 894, "htmlBody": "<p>Many people move chaotically from thought to thought without explicit structure. Inappropriate structuring may leave blind spots or cause the gears of thought to grind to a halt, but the advantages of appropriate structuring are immense:</p>\n<p>Correct thought structuring ensures that you examine all relevant facets of an issue, idea, or fact.</p>\n<ul>\n<li>It ensures you know what to do next at every stage and are not frustrated or crippled by akrasia between moments of choice; the next action is always obvious.</li>\n<li>It minimizes the overhead of task switching: you are in control and do not dither between possibilities.</li>\n<li>It may be used in a social context so that potentially challenging issues and thoughts may be brought up in a non-threatening manner (let's look at the positive aspects, now let's focus purely on the negative...).</li>\n</ul>\n<p><br />To illustrate thought structuring, I use the example of Edward de Bono's \"<a href=\"http://en.wikipedia.org/wiki/Six_Thinking_Hats\">six thinking hats</a>\" mnemonic.&nbsp; With Edward de Bono's \"six thinking hats\" method you metaphorically put on various colored \"hats\" (perspectives) and switch \"hats\" depending on the task. I will use the somewhat controversial issue of cryonics as my running example.<sup>1</sup></p>\n<p><a id=\"more\"></a><br /><strong>Gather the inputs:</strong><br /><span style=\"color: #c0c0c0;\">White hat</span> - Facts and information<br />This is the perspective where you focus on gathering all the information relevant to the situation by deducing facts, remembering, asking colleagues, reviewing the literature, and conducting experiments.<br />Concrete declarative facts:</p>\n<ul>\n<li>Most of the information is retained when someone is cryogenically frozen.</li>\n<li>No one has been revived yet.</li>\n<li>Bacteria, seeds, and human embryos may be frozen and revived.</li>\n</ul>\n<p><span style=\"color: #ff0000;\">Red hat</span> - Feelings and emotions<br />This is the perspective where you think about or convey vague intuitions. These are rules of thumb, abstracted probabilities, impressions, and things in your procedural understanding. This is also the time to focus on anything that might be interfering with your objectivity.<br />Intuitions and vague inputs:</p>\n<ul>\n<li>The technology will exist in the future to revive the cryogenically frozen.</li>\n<li>People in the future will revive us if they can.</li>\n<li>Family relations will be saddened by choosing cryonics.</li>\n<li>Life will be better in the future than in the present.&nbsp;</li>\n</ul>\n<p><strong>Invention and problem solving:</strong><br /><span style=\"color: #00ff00;\">Green hat</span> - New ideas<br />Going into this perspective you have gathered the evidence and intuitions. Now you focus on using these to solve the problem or invent new approaches. At this point the invented ideas do not have to be very good; your ideas are criticised and evaluated with the other hats.<br />New ideas:</p>\n<ul>\n<li>How about we use something like hibernation instead of cryonics? </li>\n<li>How about we find some sort of chemical concoction that stops the molecular processes and yet works at room temperature?&nbsp;</li>\n</ul>\n<p><strong>Weigh the evidence:</strong><br /><strong>Black hat</strong> - Critical judgment<br />Here you specialize, looking for the flaws in the argument, design, or concept. If you are the originator of a concept or otherwise have positive affect around one, the habit of using this perspective ensures that you look for flaws.<br />Flaws:</p>\n<ul>\n<li>There are many possible future histories where the cryopreserved are not revived.</li>\n<li>Spending money on cryonics means we cannot spend it elsewhere and the resources are locked in.</li>\n</ul>\n<p><span style=\"color: #ffcc00;\">Yellow hat</span> - Positive aspects<br />With this perspective, you look for the arguments for a position or come up with various uses you can put something to. If you are critical of a concept, this step ensures you look at its positive aspects.<br />Strengths and additional purposes:</p>\n<ul>\n<li>Your life may be saved.</li>\n<li>Believing that you will be revived gives you a near mode reason to care for the distant future.</li>\n</ul>\n<p><strong>Monitoring, directing, and deciding:</strong><br /><span style=\"color: #0000ff;\">Blue hat</span> - The big picture<br />This is the perspective where you figure out how valuable the various options are, consider opportunity costs, and choose. Here you also monitor your thoughts and interrupt the flow if something unexpected occurs internally or externally.<br />Monitor and choose:</p>\n<ul>\n<li>If you are on your deathbed or in a risky occupation, making a decision now increases in importance.</li>\n<li>If you are looking for criticisms (or positive aspects) and you mentally flinch, this warns you of possible bias and points out where you need to watch your step.</li>\n<li>At some point, opportunity costs force you to decide one way or another. Recall that the absence of making a decision is a decision.&nbsp;</li>\n</ul>\n<p>As the example shows, Edward de Bono's six thinking hats method is useful for structuring thought, but it is admittedly limited:</p>\n<ul>\n<li>There are many types of thought not covered or de-emphasized by the method (motivation, comparison, memorization, recall, doing, sensing,...)</li>\n<li>The viewpoints overlap.</li>\n<li>It doesn't tell you exactly how you should sequence and time the viewpoints only that you should consider them all, and it doesn't break each viewpoint into even smaller, atomic, components.</li>\n</ul>\n<p>Nevertheless, I find a kind of useful simplicity and beauty in the method (or maybe I just love colors...).<br />What do you think of the method? Can you suggest other ways of \"structuring thought?\"</p>\n<p>1. Disclaimer: I am pro-cryonics, but am using it solely as an example and do not intend to be comprehensive or have the feelings and analysis particularly resemble my own.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "4Man2iP6ftuTPze9K": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Skc9JZLy9HAzifsXu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 30, "extendedScore": null, "score": 5.700922118984131e-07, "legacy": true, "legacyId": "2471", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-18T19:39:40.860Z", "modifiedAt": null, "url": null, "title": "\"Life Experience\" as a Conversation-Halter", "slug": "life-experience-as-a-conversation-halter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.925Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Seth_Goldin", "createdAt": "2009-07-11T05:58:19.756Z", "isAdmin": false, "displayName": "Seth_Goldin"}, "userId": "mSZEFfrYmaiX8QArH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QDJeRcGgyZNcskr7o/life-experience-as-a-conversation-halter", "pageUrlRelative": "/posts/QDJeRcGgyZNcskr7o/life-experience-as-a-conversation-halter", "linkUrl": "https://www.lesswrong.com/posts/QDJeRcGgyZNcskr7o/life-experience-as-a-conversation-halter", "postedAtFormatted": "Thursday, March 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Life%20Experience%22%20as%20a%20Conversation-Halter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Life%20Experience%22%20as%20a%20Conversation-Halter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQDJeRcGgyZNcskr7o%2Flife-experience-as-a-conversation-halter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Life%20Experience%22%20as%20a%20Conversation-Halter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQDJeRcGgyZNcskr7o%2Flife-experience-as-a-conversation-halter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQDJeRcGgyZNcskr7o%2Flife-experience-as-a-conversation-halter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 319, "htmlBody": "<p>Sometimes in an argument, an older opponent might claim that perhaps as I grow older, my opinions will change, or that I'll come around on the topic.&nbsp; Implicit in this claim is the assumption that age or quantity of experience is a proxy for legitimate authority.&nbsp; In and of itself, such \"life experience\" is necessary for an informed rational worldview, but it is not sufficient.</p>\n<p>The claim that more \"life experience\" will completely reverse an opinion indicates that the person making such a claim believes that opinions from others are based primarily on accumulating anecdotes, perhaps derived from extensive <a href=\"http://en.wikipedia.org/wiki/Availability_heuristic\">availability bias</a>.&nbsp; It actually is a pretty decent assumption that other people aren't Bayesian, because for the most part, they aren't.&nbsp; Many can confirm this, including Haidt, Kahneman, and Tversky.<br /><br />When an opponent appeals to more \"life experience,\" it's a last resort, and it's a <a href=\"/lw/1p2/conversation_halters/\">conversation halter</a>.&nbsp; This tactic is used when an opponent is cornered.&nbsp; The claim is nearly an outright acknowledgment of moving to exit the realm of rational debate.&nbsp; Why stick to rational discourse when you can shift to trading anecdotes?&nbsp; It levels the playing field, because anecdotes, while <a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">Bayesian evidence</a>, are easily abused, especially for complex moral, social, and political claims.&nbsp; As rhetoric, this is frustratingly effective, but it's <a href=\"/lw/1p1/logical_rudeness/\">logically rude</a>.</p>\n<p>Although it might be rude and rhetorically weak, it would be authoritatively appropriate for a Bayesian to be condescending to a non-Bayesian in an argument.&nbsp; Conversely, it can be downright maddening for a non-Bayesian to be condescending to a Bayesian, because the non-Bayesian lacks the epistemological authority to warrant such condescension.&nbsp; E.T. Jaynes wrote in <em>Probability Theory</em> about the arrogance of the uninformed, \"The semiliterate on the next bar stool will tell you with absolute, arrogant assurance just how to solve the world's problems; while the scholar who has spent a lifetime studying their causes is not at all sure how to do this.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QDJeRcGgyZNcskr7o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 10, "extendedScore": null, "score": 5.702588010183471e-07, "legacy": true, "legacyId": "2518", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqmmv6NraYv4Xoeyj", "fhojYBGGiYAFcryHZ", "srge9MCLHSiwzaX6r"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-18T21:47:05.648Z", "modifiedAt": null, "url": null, "title": "The ABC's of Luminosity", "slug": "the-abc-s-of-luminosity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:24.540Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rLuZ6XrGpgjk9BNpX/the-abc-s-of-luminosity", "pageUrlRelative": "/posts/rLuZ6XrGpgjk9BNpX/the-abc-s-of-luminosity", "linkUrl": "https://www.lesswrong.com/posts/rLuZ6XrGpgjk9BNpX/the-abc-s-of-luminosity", "postedAtFormatted": "Thursday, March 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20ABC's%20of%20Luminosity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20ABC's%20of%20Luminosity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLuZ6XrGpgjk9BNpX%2Fthe-abc-s-of-luminosity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20ABC's%20of%20Luminosity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLuZ6XrGpgjk9BNpX%2Fthe-abc-s-of-luminosity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLuZ6XrGpgjk9BNpX%2Fthe-abc-s-of-luminosity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1094, "htmlBody": "<p><strong>Sequence index:</strong> <a href=\"/lw/1xh/living_luminously/\">Living Luminously</a><br /><strong>Previously in sequence:</strong> <a href=\"/lw/1xq/let_there_be_light/\">Let There Be Light</a><br /><strong>Next in sequence:</strong> <a href=\"/lw/1yb/lights_camera_action/\">Lights, Camera, Action!</a></p>\n<p><em>Affect, behavior, and circumstance interact with each other.&nbsp; These interactions constitute informative patterns that you should identify and use in your luminosity project.</em></p>\n<p><em>You may find your understanding of this post significantly improved  if you read the second story from <a href=\"/lw/2aw/seven_shiny_stories\">Seven Shiny  Stories</a>.</em></p>\n<p>The single most effective thing you can do when seeking luminosity is to learn to correlate your ABC's, collecting data about how three interrelated items interact and appear together or separately.<br /><br />A stands for \"affect\".&nbsp; Affect is how you feel and what's on your mind.&nbsp; It can be far more complicated than \"enh, I'm fine\" or \"today I'm sad\".&nbsp; You have room for plenty of simultaneous emotions, and different ones can be directed at different things - being on a generally even keel about two different things isn't the same as being nervous about one and cheerful about the other, and neither state is the same as being entirely focused on one subject that thrills you to pieces.&nbsp; If you're nervous about your performance evaluation but tickled pink that you just bought a shiny new consumer good and looking forward to visiting your cousin next week yet irritated that you just stubbed your toe, all while being amused by the funny song on the radio, that's this.&nbsp; For the sake of the alphabet, I'm lumping in less emotionally laden cognition here, too - what thoughts occur to you, what chains of reasoning you follow, what parts of the environment catch your attention.<br /><br />B stands for \"behavior\".&nbsp; Behavior here means <em>what you actually do</em>.&nbsp; Include as a dramatically lower-weighted category those things that you <em>fully intended to do</em>, and <em>actually moved to do</em>, but were then prevented from without from doing, or changed your mind about due to <em>new, unanticipated information</em>.&nbsp; This is critical.&nbsp; Fleeting designs and intentions cross our minds continually, and if you don't firmly and definitively place your evidential weight on the things that ultimately result in action, you will get subconsciously cherry-picked subsets of those incomplete plan-wisps.&nbsp; This is particularly problematic because weaker intentions will be dissuaded by minor environmental complications at a much higher rate.&nbsp; Don't worry overmuch about \"real\" plans that this filtering process discards.&nbsp; You're trying to know yourself <em>in toto</em>, not yourself at your best time-slices when you valiantly meant to do good thing X and were buffetted by circumstance: if those dismissed real plans represent typical dispositions you have, then they'll have their share of the cohort of actual behavior.&nbsp; Trust the law of averages.<br /><br />C stands for \"circumstance\".&nbsp; This is what's going on around you (what time is it?&nbsp; what's going on in your life now and recently and in the near future - major events, minor upheavals, plans for later, what people say to you?&nbsp; where are you: is it warm, cold, bright, dim, windy, calm, quiet, noisy, aromatic, odorless, featureless, busy, colorful, drab, natural, artificial, pretty, ugly, spacious, cozy, damp, dry, deserted, crowded, formal, informal, familiar, new, cluttered, or tidy?).&nbsp; It also covers what you're doing and things inside you that are generally conceptualized as merely physical (are you exhausted, jetlagged, drugged, thirsty, hungry, sore, ill, drunk, energetic, itchy, limber, wired, shivering?&nbsp; are you draped over a recliner, hiding in a cellar, hangliding or dancing or hiking or drumming or hoeing or diving?)&nbsp; Circumstances are a bit easier to observe than affect and behavior.&nbsp; If you have trouble telling where you are and what you're up to, your first priority shouldn't be luminosity.&nbsp; And while we often have <em>some</em> trouble distinguishing between various physical ailments, there are strong pressures on our species to be able to tell when we're hungry or in <a href=\"/lw/14n/pain/\">pain</a>.&nbsp; Don't neglect circumstance when performing correlative exercises just because it doesn't seem as \"the contents of your skull\"-y.&nbsp; <a href=\"http://en.wikipedia.org/wiki/Seasonal_affective_disorder\">SAD</a> should be evidence enough that our environments <em>can</em> profoundly influence our feelings.&nbsp; And wouldn't it be <em>weird</em>, after all, if you felt and acted just the same while ballroom dancing, and while setting the timer on your microwave oven to reheat soup, and while crouching on the floor after having been taken hostage at the bank?<a id=\"more\"></a><br /><br />All of these things are interdependent:</p>\n<ul>\n<li>A -&gt; B: Your affect influences your behavior most directly - affect, after all, captures what you're thinking and feeling, and apart from purely reflexive actions, you're going to act in response to that.</li>\n<li>C -&gt; B: Circumstance also feeds very obviously into behavior.&nbsp; You cannot step on a gas pedal if there isn't one in front of you; you can't take a free sample of tapenade on a cracker if there aren't any to be had; and I've found it dreadfully tricky to start twirling my skirt around when I'm wearing yoga pants.</li>\n<li>A -&gt; C: Affect can change your circumstances via your behavior, but also by altering what happens to your body's condition (we should all be familiar with how stress, for instance, can make you feel physically) and through your fellow human beings by virtue of nonverbal visibility.</li>\n<li>B -&gt; C: Your behavior influences your circumstances, obviously - smash a window, and behold, there is a draft.&nbsp; Say something and the people around you will probably hear it and react.</li>\n<li>B -&gt; A: Behavior can feed back into affect through hardwired two-way connections (smile and your emotions smile with you!) and through things like consistency effects, which make you become more like the person you seem to behaviorally emulate.</li>\n<li>C -&gt; A: Your circumstances mess with your affect both consciously, through perception and knowledge (\"it's my birthday!&nbsp; yay!\"), and subconsciously, via physical effects (if you are operating on excessive sleep debt you will not be pleased with the results).</li>\n</ul>\n<p><br />So don't <em>just</em> correlate how they appear together: also note cause and effect relationships.&nbsp; Until you've developed enough luminosity to detect these things directly, you may have to fall back on a little post-hoc guesswork for connections more complicated than \"I was hungry and thinking about cheese, so then I ate some cheese\".&nbsp; Additionally, take note of any interesting <em>absences</em>.&nbsp; If something generally considered sad has happened to you, and you can detect no sadness in your affect or telltale physical side effects, that's highly relevant data.<br /><br />These correlations will form the building blocks of your first pass of model refinement, proceeding from the priors you extracted from <a href=\"/lw/1xq/let_there_be_light/\">external sources</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwv9eHi7KGg5KA9oM": 2, "Ng8Gice9KNkncxqcj": 2, "5f5c37ee1b5cdee568cfb253": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rLuZ6XrGpgjk9BNpX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 46, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "2520", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "lights-camera-action", "canonicalPrevPostSlug": "let-there-be-light", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "Y6TpEEKZq6HXfhWxd", "v4ngP587MDZ5rC48Y", "9sguwESkteCgqFMbj", "TcJKD2E4uE9XLNxBP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-19T03:14:04.793Z", "modifiedAt": null, "url": null, "title": "Open Thread: March 2010, part 3", "slug": "open-thread-march-2010-part-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:34.989Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lrsu2YWrjhvAkBHLD/open-thread-march-2010-part-3", "pageUrlRelative": "/posts/Lrsu2YWrjhvAkBHLD/open-thread-march-2010-part-3", "linkUrl": "https://www.lesswrong.com/posts/Lrsu2YWrjhvAkBHLD/open-thread-march-2010-part-3", "postedAtFormatted": "Friday, March 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20March%202010%2C%20part%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20March%202010%2C%20part%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrsu2YWrjhvAkBHLD%2Fopen-thread-march-2010-part-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20March%202010%2C%20part%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrsu2YWrjhvAkBHLD%2Fopen-thread-march-2010-part-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLrsu2YWrjhvAkBHLD%2Fopen-thread-march-2010-part-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p>The previous open thread has now exceeded <a href=\"/lw/1wc/spring_2010_meta_thread/1qmk\">300 comments</a> &ndash; new Open Thread posts may be made here.</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px;\">\n<div id=\"entry_t3_1wd\" class=\"content clear\">\n<div class=\"md\" style=\"font-size: small;\">\n<div style=\"margin-bottom: 1em;\">\n<div id=\"entry_t3_1s4\" class=\"content clear\" style=\"margin-bottom: 1em;\">\n<div class=\"md\" style=\"font-size: small; margin-bottom: 1em;\">\n<div style=\"margin-bottom: 1em;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><em style=\"font-style: italic;\">This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lrsu2YWrjhvAkBHLD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 5.703471780113363e-07, "legacy": true, "legacyId": "2522", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 258, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-19T22:21:12.297Z", "modifiedAt": null, "url": null, "title": "Think Before You Speak (And Signal It)", "slug": "think-before-you-speak-and-signal-it", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:44.683Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XCsKmLw8L55ZyYjnK/think-before-you-speak-and-signal-it", "pageUrlRelative": "/posts/XCsKmLw8L55ZyYjnK/think-before-you-speak-and-signal-it", "linkUrl": "https://www.lesswrong.com/posts/XCsKmLw8L55ZyYjnK/think-before-you-speak-and-signal-it", "postedAtFormatted": "Friday, March 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Think%20Before%20You%20Speak%20(And%20Signal%20It)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThink%20Before%20You%20Speak%20(And%20Signal%20It)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCsKmLw8L55ZyYjnK%2Fthink-before-you-speak-and-signal-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Think%20Before%20You%20Speak%20(And%20Signal%20It)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCsKmLw8L55ZyYjnK%2Fthink-before-you-speak-and-signal-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCsKmLw8L55ZyYjnK%2Fthink-before-you-speak-and-signal-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 489, "htmlBody": "<p>In deciding whether to pay attention to an idea, a big clue, if it were readily available, would be how many people have checked it over for correctness, and for how long. Most new ideas that human beings come up with are wrong, and if someone just thought of something five seconds ago and excitedly wants to tell you about it, probably the only benefit of listening is not offending the person.</p>\n<p>But it seems quite rare for this important piece of metadata to be straightforwardly declared, perhaps because such declarations can't be trusted in general. Instead, we usually have to infer it from various other clues, like the speaker's personality (how long do they typically think before they speak?), formality of the language employed to express the idea, the presence of spelling and grammar mistakes, the venue where the idea is presented or published, etc.</p>\n<p>Unfortunately, such inferences can be imprecise or error-prone. For example, the same speaker may sometimes think a lot before speaking, and other times think little before speaking. Using costly signals like formal language is also wasteful compared to everyone simply telling the truth (but can still be a second-best solution in low-trust groups). In a community like ours, where most of us are striving to build reputations for being (or at least trying to be) rational and cooperative, and therefore there is a level of trust higher than usual, it might be worth experimenting with a norm of declaring how long we've thought about each new idea when presenting it. This may be either in addition to or as an alternative to other ways of communicating how confident we are about our ideas.</p>\n<p>To follow my own advice, I'll say that I've thought about this topic off and on for about two weeks, and then spent about three hours writing and reviewing this post. I first started thinking about it at the SIAI decision theory workshop, which was the first time I ever worked with a large group of people on a complex problem in real time. I noticed that the variance in the amount of time different people spend thinking through new ideas before they speak is quite high. I was surprised to discover, for example, that Gary Drescher has been working on decision theory for many years and has considered and discarded about a dozen possible solutions.</p>\n<p>The trigger for actually writing this post is yesterday's Overcoming Bias post <a href=\"http://www.overcomingbias.com/2010/03/twin-conspiracies.html\">Twin Conspiracies</a>, which Robin seemed to have spent much less time thinking through than usual, but which has no overt indications of this. (An obvious objection that he apparently failed to consider is, wouldn't corporations actively recruit twins to be co-CEOs if they are so productive? Several OB commenters also pointed this out.) A blogger may not want to spend days poring over every post, but why not make it easier for the reader to distinguish the serious, carefully thought out ideas from the throwaway ones?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 1, "Ng8Gice9KNkncxqcj": 1, "Q6P8jLn8hH7kbuXRr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XCsKmLw8L55ZyYjnK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 35, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "2523", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-20T00:34:09.078Z", "modifiedAt": null, "url": null, "title": "Information theory and the symmetry of updating beliefs", "slug": "information-theory-and-the-symmetry-of-updating-beliefs", "viewCount": null, "lastCommentedAt": "2019-03-29T22:24:49.738Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SEZqJcSm25XpQMhzr/information-theory-and-the-symmetry-of-updating-beliefs", "pageUrlRelative": "/posts/SEZqJcSm25XpQMhzr/information-theory-and-the-symmetry-of-updating-beliefs", "linkUrl": "https://www.lesswrong.com/posts/SEZqJcSm25XpQMhzr/information-theory-and-the-symmetry-of-updating-beliefs", "postedAtFormatted": "Saturday, March 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Information%20theory%20and%20the%20symmetry%20of%20updating%20beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInformation%20theory%20and%20the%20symmetry%20of%20updating%20beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEZqJcSm25XpQMhzr%2Finformation-theory-and-the-symmetry-of-updating-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Information%20theory%20and%20the%20symmetry%20of%20updating%20beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEZqJcSm25XpQMhzr%2Finformation-theory-and-the-symmetry-of-updating-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEZqJcSm25XpQMhzr%2Finformation-theory-and-the-symmetry-of-updating-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2391, "htmlBody": "<p>Contents:</p>\n<p>1.&nbsp;&nbsp;<a href=\"#symmetry\">The beautiful symmetry of Bayesian updating</a> <br />2.&nbsp;&nbsp;<a href=\"#logodds\">Odds and log odds: a short comparison</a> <br />3.&nbsp;&nbsp;<a href=\"#further\">Further discussion of information</a></p>\n<p>Rationality is all about handling this thing called \"information\".&nbsp; Fortunately, we live in an era after the rigorous formulation of <em>Information Theory</em> by C.E.&nbsp;Shannon in 1948, a basic understanding of which can <em>actually help you think</em> about your beliefs, in a way similar but complementary to probability theory.&nbsp;Indeed, it has flourished as an area of research exactly because it helps people in many areas of science to describe the world.&nbsp; We should take advantage of this!</p>\n<p>The information theory of <em>events,</em> which I'm about to explain, is about as difficult as high school probability.&nbsp; It is certainly easier than the information theory of <em>multiple random variables </em>(which right now is explained on <a href=\"http://en.wikipedia.org/wiki/Information_theory\">Wikipedia</a>), even though the equations look very similar.&nbsp; If you already know it, this can be a linkable source of explanations to save you writing time :)</p>\n<p>So!&nbsp; To get started, what better way to motivate information theory than to answer a question about Bayesianism?</p>\n<h2><a name=\"symmetry\"></a><strong>The beautiful symmetry of Bayesian updating</strong></h2>\n<p>The factor by which observing A increases the probability of B <em>is the same</em> as the factor by which observing B increases the probability of A.&nbsp; This factor is P(A and B)/(P(A)&middot;P(B)), which I'll denote by pev(A,B) for reasons to come. &nbsp;It can vary from&nbsp;0 to +infinity, and&nbsp;allows us to write <a href=\"http://wiki.lesswrong.com/wiki/Bayes_theorem\" target=\"_blank\">Bayes' Theorem</a> succinctly in both directions:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; P(A|B)=P(A)&middot;pev(A,B), &nbsp; and &nbsp; P(B|A)=P(B)&middot;pev(A,B)</p>\n<p><em>What does this symmetry mean, and how should it affect the way we think?</em></p>\n<p>A great way to think of pev(A,B) is as a multiplicative measure of mutual evidence, which I'll call mutual <em>probabilistic evidence</em> to be specific.&nbsp; If pev=1 if they're independent, if pev&gt;1 they make each other more likely, and if pev&lt;1 if they make each other less likely.</p>\n<p>But two ways to think are better than one, so I will offer a second explanation, in terms of <em>information</em>, which I often find quite helpful in analyzing my own beliefs:</p>\n<p><a id=\"more\"></a></p>\n<p><strong>Probabilistic evidence is related to \"mutual information\"</strong></p>\n<p>Lets examine a simple example, and work our way up to illustrating what I mean:</p>\n<p>Say I flip four fair coins with faces \"0\" and \"1\" to generate a 4-bit binary string, X.&nbsp; If I tell you that \"X=1???\", meaning that the first coin reads \"1\", this reduces the number of possibilities by &frac12;.&nbsp; We'd like to say here that you've gained \"1 bit of information\".&nbsp; Suppose instead that I say \"X begins with 01 or 10\".&nbsp; This has <em>quantitatively</em> the same effect, in that it reduces the number of possibilities by &frac12;, so it should also be called \"1 bit of information\".&nbsp; You might call the first statement an \"explicit bit\" in that it explicitly specifies a 1 or 0 in the sequence, but this is merely a <em>qualitative</em> distinction.&nbsp; For once, we're interested in quantity, not quality.</p>\n<p>Now, let A be the event \"X=111?\" (the event that the first three bits come up \"1\", and the last bit can be anything), which has probability P(A)=2<sup>-3</sup>.&nbsp; If A is true but you don't know it, you need to observe exactly 3 independent bits (e.g.&nbsp;the first 3 coins) to confirm it.&nbsp; Intuitively, this is how <em>uncertain</em> A is, because it tells us how far away we are\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nfrom\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nconfirming A.&nbsp; On the other hand, if I tell you A is true, you now only need 1 more independent bit to specify X=111?, so we can say A has \"provided 3 bits\" of \"information\".&nbsp; Intuitively, this is how <em>informative</em> A is.&nbsp; These vague ideas nudge us toward the following definition:</p>\n<p><strong>The information value of an event</strong></p>\n<p>We denote and define the <em>information value</em>&nbsp;of an event A (aka \"surprisal\" or \"self-information\", but not in this post) by the formula</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>inf(A) := log<sub>&frac12;</sub>(P(A)) = -log<sub>2</sub>(P(A))</strong></p>\n<p>which in our example is -log<sub>2</sub>(2<sup>-3</sup>)= 3 bits, just as we'd like.&nbsp; As was suggested, this quantity has two different intuitive meanings, which by the miracle of logic correspond to the same number inf(A), measured in bits:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 1) Uncertainty: How many independent bits are <em>required</em> to confirm that A is true.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 2) Informativity: How many independent bits are <em>gained</em> if we are told that A is true.</p>\n<p>Caution: information value is not \"data\", but rather it is a <em>number</em> that can tell you <em>how uncertain</em> or <em>how informative</em> the data is.&nbsp; Be on the lookout for when \"information\" means \"data\", and when it means \"information value.\"</p>\n<p><strong>Mutual information = informational evidence</strong></p>\n<p>Next, let B be the event \"X=??11\", so P(B)=2<sup>-2</sup>., and recall that A is the event<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px;\">&nbsp;</span>\"X=??11\".&nbsp; Both A and B tell us that the third position reads \"1\", which is independent from the other explicit bits they specify.&nbsp; In this sense, there is 1 bit of \"redundancy\" in observing both A and B.&nbsp; Notice that A provides 3 bits, B provides 2 bits, but \"A and B\" together specify that \"X=1111\" which is only 4 bits, and 3+2-4=1.&nbsp; Thus, we can calculate \"redundancy\" as</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; inf(A) + inf(B) - inf(A and B),</p>\n<p>which is why this expression is called the <em>mutual information</em> of A and B.&nbsp; But wait...&nbsp;taking -log<sub>2</sub> of <em>probabilistic evidence</em> pev(A,B)=P(A and B)/(P(A)&middot;P(B)) yields exactly the same expression!&nbsp; So I'll also call it <em>informational evidence</em>, and write</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>iev(A,B) := -log<sub>2</sub>pev(A,B) = inf(A) + inf(B) - inf(A and B)</strong></p>\n<p>While we're at it, lets just take -log<sub>2</sub> of the rest of Bayes' theorem and see what we get.&nbsp; We can define <em>conditional information value</em> by letting</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; inf(A|B) := -log<sub>2</sub> P(A|B) = inf(A and B) - inf(B),</p>\n<p>and now Bayes' theorem attains the following form:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>inf(A|B) = inf(A) - iev(A,B) &nbsp; &larr; &nbsp; information theoretic Bayes' Theorem</strong></p>\n<p>In Bayesian updating, A hasn't happened yet, so here let's use our \"uncertainty\" interpretation of information value.&nbsp; As you can see from the equation, if iev(A,B) is positive, the uncertainty of A <em>decreases</em> upon observing B, meaning A becomes <em>more likely</em>.&nbsp; If it is negative, the uncertainty of A <em>increases</em>, so A becomes <em>less likely.</em> It ranges from -infinity to +infinity according as A and B completely contradict or completely confirm each other.&nbsp; In summary:</p>\n<p><strong><span style=\"color: red;\">Bayesian updating = subtracting <em>mutual evidence</em> from <em>uncertainty</em>.</span></strong></p>\n<p>This is my other favorite way to think about updating.&nbsp; The fact that evidence can also be thought of as a kind of redundancy or <em>mutual information</em> gives a concrete interpretation for the symmetry of belief updating.&nbsp; As well, since \"N bits of information\" is so easy to conceptualize as a precise quantity, it gives a <em>quantitative intutive meaning</em> to \"how much A and B support each other\".&nbsp; In fact, noticing this is what got me interested in information theory in the first place, and I hope it has piqued your interest, too!</p>\n<p>What <em>kept</em> me interested is the simple fact that informational evidence behaves so nicely:</p>\n<p>&nbsp; &nbsp; &nbsp;(Symmetry) &nbsp;iev(A,B) = iev(B,A)</p>\n<p>More examples and discussion to boost your familiarity with information value is provided in <a href=\"#further\">Section 3</a>, but for now, lets break for a comparison with two other methods to describe Bayesian updating.&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2><a name=\"logodds\"></a>Odds and log odds: a short comparison.</h2>\n<p>(I think these deserve a special mention, because they have already <a href=\"/lw/mp/0_and_1_are_not_probabilities/\" target=\"_blank\">been</a> <a href=\"/lw/1to/what_is_bayesianism/1oqd\" target=\"_blank\">discussed</a> on LessWrong.com.)</p>\n<p>Bayes' theorem can also be expressed fairly neatly using <a href=\"http://wiki.lesswrong.com/wiki/Bayes_theorem\" target=\"_blank\">odds with likelihood ratios</a>, and <a href=\"http://en.wikipedia.org/wiki/Logit\" target=\"_blank\">log odds</a> with log likelihood-ratios.&nbsp; One shortcoming with using odds when updating are that the likelihood-ratio K(B|A)=P(B|A)/P(B|&not;A), sometimes called the Bayes factor, is not symmetric, so it does not make the symmetry of updating obvious.&nbsp; Likewise,&nbsp;log likelihood-ratios are not symmetric either. &nbsp;</p>\n<p>But odds and log odds <a href=\"http://en.wikipedia.org/wiki/Logit#Uses_and_properties\" target=\"_blank\">have their advantages</a>. For example, if B<sub>1</sub> and B<sub>2</sub> are conditionally independent given A <em>and</em> conditionally independent given &not;A, then K(B<sub>1</sub>&nbsp;and B<sub>2</sub>, A) = K(B<sub>1</sub>|A)&middot;K(B<sub>2</sub>|A), and similarly for any number of B's. &nbsp;These conditions are met naturally when&nbsp;B<sub>1</sub>&nbsp;and B<sub>2</sub>&nbsp;are causal consequences of A which do not causally influence each other. &nbsp;By contrast, in causal systems, it is usually&nbsp;<strong>not</strong>&nbsp;the case that&nbsp;pev(A, B<sub>1</sub>&nbsp;and B<sub>2</sub>) = pev(A,B<sub>1</sub>)&middot;pev(A,B<sub>2</sub>). &nbsp;(Reading Pearl's \"Causality: Models, Reasoning, and Inference\" clarified this for me once and for all, by making precise what a \"causal system\" is.) &nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2><a name=\"further\"></a>Further discussion of information</h2>\n<p>In our excitement to get to an information theoretic Bayes' theorem, we glossed over a lot of opportunities to stop and reflect, so lets do some more of that here.</p>\n<p><strong>Information vs \"data\" or \"knowledge\"</strong></p>\n<p>C.E.&nbsp;Shannon originally used the full phrase \"information value\", but nowadays it is often shortened to \"information\".&nbsp; As mentioned, information is not a synonym for \"data\" or \"knowledge\" when used in this way.</p>\n<p>It may be help to analogize this with how \"mass\" is not \"matter\".&nbsp; If I place 2 grams of matter on the left side of a balance scale, and 3 grams on the right, it will tip to the right, because 3g-2g=1g&gt;0g.&nbsp; Where is this 1 gram of matter?&nbsp; Which \"1 gram of matter\" is the matter that tips the scales?&nbsp; The question is meaningless, because the 1g doesn't refer to any matter in particular, just a difference in total amounts.&nbsp; But you <em>can</em> ask \"how much mass does this matter have?\", and likewise \"how much information does this data have?\".</p>\n<p><strong>Why \"<em>the</em> redundant information\" doesn't make sense</strong></p>\n<p>When iev(A,B) is positive, we spoke of the mutual information of A and B as \"redundancy\".&nbsp; But what is this redundant information?&nbsp; What does it say?&nbsp; Again, this is the \"information value is data\" fallacy making ill-posed questions.&nbsp; It's somewhat like asking which gram of matter should be removed from the scales above in order to balance it. To illustrate more precisely, suppose again that A says \"X=111?\" and B says \"X=??11\".&nbsp; If R is the event \"X=??1?\", it is tempting to call R \"<em>the</em> mutual information\" of A and B.&nbsp; Indeed, if we first observe R, then A and B become independent, so there is no more redundancy.&nbsp; <em>But this R is not unique.</em> Any list of 8 outcomes that include the A outcomes and B outcomes would also work this way.&nbsp; For example, we could take R to say \"X is one of 0011, 0111, 1011, 1110, 1111, 1000, 0100, 0001\".&nbsp;</p>\n<p><strong>To infinity and...&nbsp;well, just to infinity.</strong></p>\n<p>We saw that information value inf(A) ranges from 0 to +infinity, and can be interpreted either as informativity or uncertainty, depending on whether the event has happened or not.&nbsp; Let's think a little about the extremes of this scale:</p>\n<p>That 0 information value corresponds to a 100%-likely event means:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 1) 0 informativity: you don't gain any information from observing an event that you already knew was certain (ignoring 0%-likely discrepancies), and</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 2) 0 uncertainty: you don't require any information to verify an event that is certain to occur , and</p>\n<p>That +infinity information value corresponds to a 0%-likely event means:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 1) infinite uncertainty: no finite amount of information can convince you of a 0%-likely event (though perhaps an infinite series of tests could bring you arbitrarily close), and</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 2) infinite informativity: if you observe a 0%-likely event, you might win a Nobel prize (it means someone messed up by having a prior belief of 0% somewhere when they shouldn't have).</p>\n<p>For the values in between, <em>more likely = less uncertain = less informative</em>, and <em>less likely = more uncertain = more informative.</em></p>\n<p><strong>What other cool stuff can happen?</strong></p>\n<p>To get more comfortable with how information values work, let's return to our random 4-bit string X, generated by flipping four coins:</p>\n<p>&bull; <em>Encoding.&nbsp; </em>Let C be the event \"X contains exactly one 1\", i.e.&nbsp; X=1000, 0100, 0010, or 0001.&nbsp; This happens with probability 4/16=1/4=2<sup>-2</sup>, so inf(C) = 2 bits.&nbsp; If C is true, it provides 2 bits of information about X, and using <em>an additional</em> 2 bits we could <em>encode</em> the position of the \"1\" by writing \"first\"=00, \"second\"=01, \"third\"=10, and \"fourth\"=11.&nbsp; Thus we end up using 4 bits in total to specify or \"encode\" X, as we'd expect.&nbsp; In general, there are theorems characterizing information entirely in terms of encoding/decoding, which is part of what makes it so useful in applications.</p>\n<p>&bull; <em>Negative evidence.&nbsp;</em>Let D be the event \"X starts with 1\", which one sees directly as specifying inf(D) = 1 bit of information.&nbsp; It is easy to see that P(D)=1/2 and P(D|C)=1/4, so we know C makes D <em>less likely</em> (and vice versa, by update symmetry!), but lets practice thinking in terms of information.&nbsp; Together, \"C and D\" just means X=1000, so inf(C and D) = 4 bits: it completely determines X.&nbsp; On the other hand, we saw that inf(C)=2 bits, and inf(D)=1 bit, so iev(C,D) = 2+1-4 = -1, confirming that either of them would present negative evidence for the other.</p>\n<p>&bull; <em>Non-integer information values.&nbsp;</em>Being defined as a logarithm, in real life information values are usually not integers, just like probabilities are not usually simple fractions.&nbsp; This is not actually a problem, but reflects a <em>flexibility</em> of the definition.&nbsp; For example, consider the event &not;B: \"X does not start with 11\", which has probability 3/4, hence inf(&not;B)=-log<sub>2</sub>(3/4) = 0.415.&nbsp; If we also knew \"X does not end with 11\", that would give us another 0.415 bits of information (since it's independent!).&nbsp; All of our formulae work just fine with non-integer information values, so we can add these to conclude we have 0.830 bits.&nbsp; This being less than 1 means we still haven't constrained the number possibilities as much as knowing a single bit for certain (i.e.&nbsp;50%).&nbsp; Indeed, 9 out of the 16 possibilities neither start nor end with 11.&nbsp;&nbsp;</p>\n<p><strong>Okay, but is this anything more than a cool trick with logarithms?</strong></p>\n<p>Yes!&nbsp; This definition of information has loads of real-world applications that legitimize it as a scientific <em>quantity of interest</em>:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; *Communication (bandwidth = information per second),</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; *Data compression (information = how 'incompressible' the output of a data source is),</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; *Statistical mechanics and physics (entropy = average uncertainty = expected informativity of observing a system),</p>\n<p>and of course, artificial intelligence.</p>\n<p><strong>Where can I read more?</strong></p>\n<p>Eliezer has written a <a href=\"/lw/o1/entropy_and_short_codes/\" target=\"_blank\">number</a> <a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\" target=\"_blank\">of</a> <a href=\"/lw/o8/conditional_independence_and_naive_bayes/\" target=\"_blank\">posts</a> which involve the information theory of handling multiple random variables at once.&nbsp; So, if you want to learn more about it, <a href=\"http://en.wikipedia.org/wiki/Information_theory\" target=\"_blank\">Wikipedia</a> is currently a decent source.&nbsp; The general philosophy is to take expected values of the quantities defined here to obtain analogues for random variables, so you're already half-way there.&nbsp;</p>\n<p>For something more coherent and in-depth, <a href=\"http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html\" target=\"_blank\">A Mathematical Theory of Communication, Shannon (1948)</a>, which is credited with pioneering modern information theory, impressively remains a fantastic introduction to the subject.&nbsp; Way to go, Shannon!&nbsp;</p>\n<p>There's lots more good stuff to learn in that paper alone, but I'll end this post here.&nbsp; What I think is most relevant to LessWrong readers is the awareness of a precise definition of information, and that it can <em>help you think</em> about beliefs and Bayesianism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DCN2zNscbMZp5aatL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SEZqJcSm25XpQMhzr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 63, "extendedScore": null, "score": 0.00011878835536250649, "legacy": true, "legacyId": "2529", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Contents:</p>\n<p>1.&nbsp;&nbsp;<a href=\"#symmetry\">The beautiful symmetry of Bayesian updating</a> <br>2.&nbsp;&nbsp;<a href=\"#logodds\">Odds and log odds: a short comparison</a> <br>3.&nbsp;&nbsp;<a href=\"#further\">Further discussion of information</a></p>\n<p>Rationality is all about handling this thing called \"information\".&nbsp; Fortunately, we live in an era after the rigorous formulation of <em>Information Theory</em> by C.E.&nbsp;Shannon in 1948, a basic understanding of which can <em>actually help you think</em> about your beliefs, in a way similar but complementary to probability theory.&nbsp;Indeed, it has flourished as an area of research exactly because it helps people in many areas of science to describe the world.&nbsp; We should take advantage of this!</p>\n<p>The information theory of <em>events,</em> which I'm about to explain, is about as difficult as high school probability.&nbsp; It is certainly easier than the information theory of <em>multiple random variables </em>(which right now is explained on <a href=\"http://en.wikipedia.org/wiki/Information_theory\">Wikipedia</a>), even though the equations look very similar.&nbsp; If you already know it, this can be a linkable source of explanations to save you writing time :)</p>\n<p>So!&nbsp; To get started, what better way to motivate information theory than to answer a question about Bayesianism?</p>\n<h2 id=\"The_beautiful_symmetry_of_Bayesian_updating\"><a name=\"symmetry\"></a><strong>The beautiful symmetry of Bayesian updating</strong></h2>\n<p>The factor by which observing A increases the probability of B <em>is the same</em> as the factor by which observing B increases the probability of A.&nbsp; This factor is P(A and B)/(P(A)\u00b7P(B)), which I'll denote by pev(A,B) for reasons to come. &nbsp;It can vary from&nbsp;0 to +infinity, and&nbsp;allows us to write <a href=\"http://wiki.lesswrong.com/wiki/Bayes_theorem\" target=\"_blank\">Bayes' Theorem</a> succinctly in both directions:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; P(A|B)=P(A)\u00b7pev(A,B), &nbsp; and &nbsp; P(B|A)=P(B)\u00b7pev(A,B)</p>\n<p><em>What does this symmetry mean, and how should it affect the way we think?</em></p>\n<p>A great way to think of pev(A,B) is as a multiplicative measure of mutual evidence, which I'll call mutual <em>probabilistic evidence</em> to be specific.&nbsp; If pev=1 if they're independent, if pev&gt;1 they make each other more likely, and if pev&lt;1 if they make each other less likely.</p>\n<p>But two ways to think are better than one, so I will offer a second explanation, in terms of <em>information</em>, which I often find quite helpful in analyzing my own beliefs:</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"Probabilistic_evidence_is_related_to__mutual_information_\">Probabilistic evidence is related to \"mutual information\"</strong></p>\n<p>Lets examine a simple example, and work our way up to illustrating what I mean:</p>\n<p>Say I flip four fair coins with faces \"0\" and \"1\" to generate a 4-bit binary string, X.&nbsp; If I tell you that \"X=1???\", meaning that the first coin reads \"1\", this reduces the number of possibilities by \u00bd.&nbsp; We'd like to say here that you've gained \"1 bit of information\".&nbsp; Suppose instead that I say \"X begins with 01 or 10\".&nbsp; This has <em>quantitatively</em> the same effect, in that it reduces the number of possibilities by \u00bd, so it should also be called \"1 bit of information\".&nbsp; You might call the first statement an \"explicit bit\" in that it explicitly specifies a 1 or 0 in the sequence, but this is merely a <em>qualitative</em> distinction.&nbsp; For once, we're interested in quantity, not quality.</p>\n<p>Now, let A be the event \"X=111?\" (the event that the first three bits come up \"1\", and the last bit can be anything), which has probability P(A)=2<sup>-3</sup>.&nbsp; If A is true but you don't know it, you need to observe exactly 3 independent bits (e.g.&nbsp;the first 3 coins) to confirm it.&nbsp; Intuitively, this is how <em>uncertain</em> A is, because it tells us how far away we are\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nfrom\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nconfirming A.&nbsp; On the other hand, if I tell you A is true, you now only need 1 more independent bit to specify X=111?, so we can say A has \"provided 3 bits\" of \"information\".&nbsp; Intuitively, this is how <em>informative</em> A is.&nbsp; These vague ideas nudge us toward the following definition:</p>\n<p><strong id=\"The_information_value_of_an_event\">The information value of an event</strong></p>\n<p>We denote and define the <em>information value</em>&nbsp;of an event A (aka \"surprisal\" or \"self-information\", but not in this post) by the formula</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>inf(A) := log<sub>\u00bd</sub>(P(A)) = -log<sub>2</sub>(P(A))</strong></p>\n<p>which in our example is -log<sub>2</sub>(2<sup>-3</sup>)= 3 bits, just as we'd like.&nbsp; As was suggested, this quantity has two different intuitive meanings, which by the miracle of logic correspond to the same number inf(A), measured in bits:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 1) Uncertainty: How many independent bits are <em>required</em> to confirm that A is true.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 2) Informativity: How many independent bits are <em>gained</em> if we are told that A is true.</p>\n<p>Caution: information value is not \"data\", but rather it is a <em>number</em> that can tell you <em>how uncertain</em> or <em>how informative</em> the data is.&nbsp; Be on the lookout for when \"information\" means \"data\", and when it means \"information value.\"</p>\n<p><strong id=\"Mutual_information___informational_evidence\">Mutual information = informational evidence</strong></p>\n<p>Next, let B be the event \"X=??11\", so P(B)=2<sup>-2</sup>., and recall that A is the event<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px;\">&nbsp;</span>\"X=??11\".&nbsp; Both A and B tell us that the third position reads \"1\", which is independent from the other explicit bits they specify.&nbsp; In this sense, there is 1 bit of \"redundancy\" in observing both A and B.&nbsp; Notice that A provides 3 bits, B provides 2 bits, but \"A and B\" together specify that \"X=1111\" which is only 4 bits, and 3+2-4=1.&nbsp; Thus, we can calculate \"redundancy\" as</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; inf(A) + inf(B) - inf(A and B),</p>\n<p>which is why this expression is called the <em>mutual information</em> of A and B.&nbsp; But wait...&nbsp;taking -log<sub>2</sub> of <em>probabilistic evidence</em> pev(A,B)=P(A and B)/(P(A)\u00b7P(B)) yields exactly the same expression!&nbsp; So I'll also call it <em>informational evidence</em>, and write</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>iev(A,B) := -log<sub>2</sub>pev(A,B) = inf(A) + inf(B) - inf(A and B)</strong></p>\n<p>While we're at it, lets just take -log<sub>2</sub> of the rest of Bayes' theorem and see what we get.&nbsp; We can define <em>conditional information value</em> by letting</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; inf(A|B) := -log<sub>2</sub> P(A|B) = inf(A and B) - inf(B),</p>\n<p>and now Bayes' theorem attains the following form:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>inf(A|B) = inf(A) - iev(A,B) &nbsp; \u2190 &nbsp; information theoretic Bayes' Theorem</strong></p>\n<p>In Bayesian updating, A hasn't happened yet, so here let's use our \"uncertainty\" interpretation of information value.&nbsp; As you can see from the equation, if iev(A,B) is positive, the uncertainty of A <em>decreases</em> upon observing B, meaning A becomes <em>more likely</em>.&nbsp; If it is negative, the uncertainty of A <em>increases</em>, so A becomes <em>less likely.</em> It ranges from -infinity to +infinity according as A and B completely contradict or completely confirm each other.&nbsp; In summary:</p>\n<p><strong id=\"Bayesian_updating___subtracting_mutual_evidence_from_uncertainty_\"><span style=\"color: red;\">Bayesian updating = subtracting <em>mutual evidence</em> from <em>uncertainty</em>.</span></strong></p>\n<p>This is my other favorite way to think about updating.&nbsp; The fact that evidence can also be thought of as a kind of redundancy or <em>mutual information</em> gives a concrete interpretation for the symmetry of belief updating.&nbsp; As well, since \"N bits of information\" is so easy to conceptualize as a precise quantity, it gives a <em>quantitative intutive meaning</em> to \"how much A and B support each other\".&nbsp; In fact, noticing this is what got me interested in information theory in the first place, and I hope it has piqued your interest, too!</p>\n<p>What <em>kept</em> me interested is the simple fact that informational evidence behaves so nicely:</p>\n<p>&nbsp; &nbsp; &nbsp;(Symmetry) &nbsp;iev(A,B) = iev(B,A)</p>\n<p>More examples and discussion to boost your familiarity with information value is provided in <a href=\"#further\">Section 3</a>, but for now, lets break for a comparison with two other methods to describe Bayesian updating.&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h2 id=\"Odds_and_log_odds__a_short_comparison_\"><a name=\"logodds\"></a>Odds and log odds: a short comparison.</h2>\n<p>(I think these deserve a special mention, because they have already <a href=\"/lw/mp/0_and_1_are_not_probabilities/\" target=\"_blank\">been</a> <a href=\"/lw/1to/what_is_bayesianism/1oqd\" target=\"_blank\">discussed</a> on LessWrong.com.)</p>\n<p>Bayes' theorem can also be expressed fairly neatly using <a href=\"http://wiki.lesswrong.com/wiki/Bayes_theorem\" target=\"_blank\">odds with likelihood ratios</a>, and <a href=\"http://en.wikipedia.org/wiki/Logit\" target=\"_blank\">log odds</a> with log likelihood-ratios.&nbsp; One shortcoming with using odds when updating are that the likelihood-ratio K(B|A)=P(B|A)/P(B|\u00acA), sometimes called the Bayes factor, is not symmetric, so it does not make the symmetry of updating obvious.&nbsp; Likewise,&nbsp;log likelihood-ratios are not symmetric either. &nbsp;</p>\n<p>But odds and log odds <a href=\"http://en.wikipedia.org/wiki/Logit#Uses_and_properties\" target=\"_blank\">have their advantages</a>. For example, if B<sub>1</sub> and B<sub>2</sub> are conditionally independent given A <em>and</em> conditionally independent given \u00acA, then K(B<sub>1</sub>&nbsp;and B<sub>2</sub>, A) = K(B<sub>1</sub>|A)\u00b7K(B<sub>2</sub>|A), and similarly for any number of B's. &nbsp;These conditions are met naturally when&nbsp;B<sub>1</sub>&nbsp;and B<sub>2</sub>&nbsp;are causal consequences of A which do not causally influence each other. &nbsp;By contrast, in causal systems, it is usually&nbsp;<strong>not</strong>&nbsp;the case that&nbsp;pev(A, B<sub>1</sub>&nbsp;and B<sub>2</sub>) = pev(A,B<sub>1</sub>)\u00b7pev(A,B<sub>2</sub>). &nbsp;(Reading Pearl's \"Causality: Models, Reasoning, and Inference\" clarified this for me once and for all, by making precise what a \"causal system\" is.) &nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h2 id=\"Further_discussion_of_information\"><a name=\"further\"></a>Further discussion of information</h2>\n<p>In our excitement to get to an information theoretic Bayes' theorem, we glossed over a lot of opportunities to stop and reflect, so lets do some more of that here.</p>\n<p><strong id=\"Information_vs__data__or__knowledge_\">Information vs \"data\" or \"knowledge\"</strong></p>\n<p>C.E.&nbsp;Shannon originally used the full phrase \"information value\", but nowadays it is often shortened to \"information\".&nbsp; As mentioned, information is not a synonym for \"data\" or \"knowledge\" when used in this way.</p>\n<p>It may be help to analogize this with how \"mass\" is not \"matter\".&nbsp; If I place 2 grams of matter on the left side of a balance scale, and 3 grams on the right, it will tip to the right, because 3g-2g=1g&gt;0g.&nbsp; Where is this 1 gram of matter?&nbsp; Which \"1 gram of matter\" is the matter that tips the scales?&nbsp; The question is meaningless, because the 1g doesn't refer to any matter in particular, just a difference in total amounts.&nbsp; But you <em>can</em> ask \"how much mass does this matter have?\", and likewise \"how much information does this data have?\".</p>\n<p><strong id=\"Why__the_redundant_information__doesn_t_make_sense\">Why \"<em>the</em> redundant information\" doesn't make sense</strong></p>\n<p>When iev(A,B) is positive, we spoke of the mutual information of A and B as \"redundancy\".&nbsp; But what is this redundant information?&nbsp; What does it say?&nbsp; Again, this is the \"information value is data\" fallacy making ill-posed questions.&nbsp; It's somewhat like asking which gram of matter should be removed from the scales above in order to balance it. To illustrate more precisely, suppose again that A says \"X=111?\" and B says \"X=??11\".&nbsp; If R is the event \"X=??1?\", it is tempting to call R \"<em>the</em> mutual information\" of A and B.&nbsp; Indeed, if we first observe R, then A and B become independent, so there is no more redundancy.&nbsp; <em>But this R is not unique.</em> Any list of 8 outcomes that include the A outcomes and B outcomes would also work this way.&nbsp; For example, we could take R to say \"X is one of 0011, 0111, 1011, 1110, 1111, 1000, 0100, 0001\".&nbsp;</p>\n<p><strong id=\"To_infinity_and____well__just_to_infinity_\">To infinity and...&nbsp;well, just to infinity.</strong></p>\n<p>We saw that information value inf(A) ranges from 0 to +infinity, and can be interpreted either as informativity or uncertainty, depending on whether the event has happened or not.&nbsp; Let's think a little about the extremes of this scale:</p>\n<p>That 0 information value corresponds to a 100%-likely event means:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 1) 0 informativity: you don't gain any information from observing an event that you already knew was certain (ignoring 0%-likely discrepancies), and</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 2) 0 uncertainty: you don't require any information to verify an event that is certain to occur , and</p>\n<p>That +infinity information value corresponds to a 0%-likely event means:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 1) infinite uncertainty: no finite amount of information can convince you of a 0%-likely event (though perhaps an infinite series of tests could bring you arbitrarily close), and</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; 2) infinite informativity: if you observe a 0%-likely event, you might win a Nobel prize (it means someone messed up by having a prior belief of 0% somewhere when they shouldn't have).</p>\n<p>For the values in between, <em>more likely = less uncertain = less informative</em>, and <em>less likely = more uncertain = more informative.</em></p>\n<p><strong id=\"What_other_cool_stuff_can_happen_\">What other cool stuff can happen?</strong></p>\n<p>To get more comfortable with how information values work, let's return to our random 4-bit string X, generated by flipping four coins:</p>\n<p>\u2022 <em>Encoding.&nbsp; </em>Let C be the event \"X contains exactly one 1\", i.e.&nbsp; X=1000, 0100, 0010, or 0001.&nbsp; This happens with probability 4/16=1/4=2<sup>-2</sup>, so inf(C) = 2 bits.&nbsp; If C is true, it provides 2 bits of information about X, and using <em>an additional</em> 2 bits we could <em>encode</em> the position of the \"1\" by writing \"first\"=00, \"second\"=01, \"third\"=10, and \"fourth\"=11.&nbsp; Thus we end up using 4 bits in total to specify or \"encode\" X, as we'd expect.&nbsp; In general, there are theorems characterizing information entirely in terms of encoding/decoding, which is part of what makes it so useful in applications.</p>\n<p>\u2022 <em>Negative evidence.&nbsp;</em>Let D be the event \"X starts with 1\", which one sees directly as specifying inf(D) = 1 bit of information.&nbsp; It is easy to see that P(D)=1/2 and P(D|C)=1/4, so we know C makes D <em>less likely</em> (and vice versa, by update symmetry!), but lets practice thinking in terms of information.&nbsp; Together, \"C and D\" just means X=1000, so inf(C and D) = 4 bits: it completely determines X.&nbsp; On the other hand, we saw that inf(C)=2 bits, and inf(D)=1 bit, so iev(C,D) = 2+1-4 = -1, confirming that either of them would present negative evidence for the other.</p>\n<p>\u2022 <em>Non-integer information values.&nbsp;</em>Being defined as a logarithm, in real life information values are usually not integers, just like probabilities are not usually simple fractions.&nbsp; This is not actually a problem, but reflects a <em>flexibility</em> of the definition.&nbsp; For example, consider the event \u00acB: \"X does not start with 11\", which has probability 3/4, hence inf(\u00acB)=-log<sub>2</sub>(3/4) = 0.415.&nbsp; If we also knew \"X does not end with 11\", that would give us another 0.415 bits of information (since it's independent!).&nbsp; All of our formulae work just fine with non-integer information values, so we can add these to conclude we have 0.830 bits.&nbsp; This being less than 1 means we still haven't constrained the number possibilities as much as knowing a single bit for certain (i.e.&nbsp;50%).&nbsp; Indeed, 9 out of the 16 possibilities neither start nor end with 11.&nbsp;&nbsp;</p>\n<p><strong id=\"Okay__but_is_this_anything_more_than_a_cool_trick_with_logarithms_\">Okay, but is this anything more than a cool trick with logarithms?</strong></p>\n<p>Yes!&nbsp; This definition of information has loads of real-world applications that legitimize it as a scientific <em>quantity of interest</em>:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; *Communication (bandwidth = information per second),</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; *Data compression (information = how 'incompressible' the output of a data source is),</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; *Statistical mechanics and physics (entropy = average uncertainty = expected informativity of observing a system),</p>\n<p>and of course, artificial intelligence.</p>\n<p><strong id=\"Where_can_I_read_more_\">Where can I read more?</strong></p>\n<p>Eliezer has written a <a href=\"/lw/o1/entropy_and_short_codes/\" target=\"_blank\">number</a> <a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\" target=\"_blank\">of</a> <a href=\"/lw/o8/conditional_independence_and_naive_bayes/\" target=\"_blank\">posts</a> which involve the information theory of handling multiple random variables at once.&nbsp; So, if you want to learn more about it, <a href=\"http://en.wikipedia.org/wiki/Information_theory\" target=\"_blank\">Wikipedia</a> is currently a decent source.&nbsp; The general philosophy is to take expected values of the quantities defined here to obtain analogues for random variables, so you're already half-way there.&nbsp;</p>\n<p>For something more coherent and in-depth, <a href=\"http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html\" target=\"_blank\">A Mathematical Theory of Communication, Shannon (1948)</a>, which is credited with pioneering modern information theory, impressively remains a fantastic introduction to the subject.&nbsp; Way to go, Shannon!&nbsp;</p>\n<p>There's lots more good stuff to learn in that paper alone, but I'll end this post here.&nbsp; What I think is most relevant to LessWrong readers is the awareness of a precise definition of information, and that it can <em>help you think</em> about beliefs and Bayesianism.</p>", "sections": [{"title": "The beautiful symmetry of Bayesian updating", "anchor": "The_beautiful_symmetry_of_Bayesian_updating", "level": 1}, {"title": "Probabilistic evidence is related to \"mutual information\"", "anchor": "Probabilistic_evidence_is_related_to__mutual_information_", "level": 2}, {"title": "The information value of an event", "anchor": "The_information_value_of_an_event", "level": 2}, {"title": "Mutual information = informational evidence", "anchor": "Mutual_information___informational_evidence", "level": 2}, {"title": "Bayesian updating = subtracting mutual evidence from uncertainty.", "anchor": "Bayesian_updating___subtracting_mutual_evidence_from_uncertainty_", "level": 2}, {"title": "Odds and log odds: a short comparison.", "anchor": "Odds_and_log_odds__a_short_comparison_", "level": 1}, {"title": "Further discussion of information", "anchor": "Further_discussion_of_information", "level": 1}, {"title": "Information vs \"data\" or \"knowledge\"", "anchor": "Information_vs__data__or__knowledge_", "level": 2}, {"title": "Why \"the redundant information\" doesn't make sense", "anchor": "Why__the_redundant_information__doesn_t_make_sense", "level": 2}, {"title": "To infinity and...\u00a0well, just to infinity.", "anchor": "To_infinity_and____well__just_to_infinity_", "level": 2}, {"title": "What other cool stuff can happen?", "anchor": "What_other_cool_stuff_can_happen_", "level": 2}, {"title": "Okay, but is this anything more than a cool trick with logarithms?", "anchor": "Okay__but_is_this_anything_more_than_a_cool_trick_with_logarithms_", "level": 2}, {"title": "Where can I read more?", "anchor": "Where_can_I_read_more_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 15}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QGkYCwyC7wTDyt3yT", "soQX8yXLbKy7cFvy8", "yLcuygFfMfrfK8KjF", "gDWvLicHhcMfGmwaK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-20T05:29:21.102Z", "modifiedAt": null, "url": null, "title": "Lights, Camera, Action!", "slug": "lights-camera-action", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:32.615Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v4ngP587MDZ5rC48Y/lights-camera-action", "pageUrlRelative": "/posts/v4ngP587MDZ5rC48Y/lights-camera-action", "linkUrl": "https://www.lesswrong.com/posts/v4ngP587MDZ5rC48Y/lights-camera-action", "postedAtFormatted": "Saturday, March 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lights%2C%20Camera%2C%20Action!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALights%2C%20Camera%2C%20Action!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4ngP587MDZ5rC48Y%2Flights-camera-action%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lights%2C%20Camera%2C%20Action!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4ngP587MDZ5rC48Y%2Flights-camera-action", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv4ngP587MDZ5rC48Y%2Flights-camera-action", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 788, "htmlBody": "<p><strong>Sequence index:</strong> <a href=\"/lw/1xh/living_luminously/\">Living Luminously</a><br /><strong>Previously in sequence:</strong> <a href=\"/lw/1y0/the_abcs_of_luminosity/\">The ABC's of Luminosity</a><br /><strong>Next in sequence:</strong> <a href=\"/lw/1za/the_spotlight/\">The Spotlight</a></p>\n<p><em>You should pay attention to key mental events, on a regular and frequent basis, because important thoughts can happen very briefly or very occasionally and you need to catch them.</em></p>\n<p><em>You may find your understanding of this post significantly improved  if you read the third story from <a href=\"/lw/2aw/seven_shiny_stories\">Seven Shiny  Stories</a>.</em></p>\n<p>Luminosity is hard and you are complicated.&nbsp; You can't meditate on yourself for ten minutes over a smoothie and then announce your self-transparency.&nbsp; You have to keep working at it over a long period of time, not least because some effects don't work over the short term.&nbsp; If your affect varies with the seasons, or with major life events, then you'll need to keep up the <em>first</em> phase of work through a full year or a major life event, and it turns out those don't happen every alternate Thursday.&nbsp; Additionally, you can't cobble together the best quality models from snippets of introspection that are each five seconds long; extended strings of cognition are important, too, and can take quite a long time to unravel fully.<br /><br />Sadly, looking at what you are thinking inevitably changes it.&nbsp; With enough introspection, this wouldn't influence your accuracy about your overall self - there's no reason in principle why you couldn't spend all your waking hours noting your own thoughts and forming meta-thoughts in real time - but practically speaking that's not going to happen.&nbsp; Therefore, some of your data will have to come from memory.&nbsp; To minimize the error introduction that comes of retrieving things from storage, it's best to arrange to reflect on very recent thoughts.&nbsp; It may be worth your while to set up an external reminder system to periodically prompt you to look inward, both in the moment and retrospectively over the last brief segment of time.&nbsp; This can be a specifically purposed system (i.e. set a timer to go off every half hour or so), or you can tie it to convenient promptings from the world as-is, like being asked \"What's up?\" or \"Penny for your thoughts\".<a id=\"more\"></a><br /><br />When you introspect, there is a lot to keep track of.&nbsp; For instance, consider the following:</p>\n<ul>\n<li>What were you thinking about?&nbsp; (This could be more than one thing.&nbsp; You are a massively parallel system.)&nbsp; Was it a concept, image, sensation, desire, belief, person, object, word, place, emotion, plan, memory...?</li>\n<li>How tightly were you focused on it?&nbsp; (Is the topic itself narrow or disparate?)&nbsp; What other items (sensory, cognitive, emotional) seemed to intrude on your concentration, if any, and how did you react to this incursion?</li>\n<li>How did you feel about the subject of the thought?&nbsp; This includes not only emotional reactions like \"this is depressing\" or \"yay!\", but also what you felt inclined to do about the topic (if anything), and how important or interesting your thought seemed.</li>\n<li>How does thinking, in general, feel to you?&nbsp; (I conducted an informal survey of this and got no two answers the same.&nbsp; Anecdotally, it may be rather key to determining how you are different from others, and so in refining your model of yourself relative to the fairly generic <a href=\"/lw/1xq/let_there_be_light/\">priors</a> we're starting with.)&nbsp; Coming up with a good way to conceptualize your style of thinking can help you interpret introspective data, although be sure to abandon a metaphor that looks about to snap.&nbsp; You might have different answers when you're \"actively\" thinking something through - i.e. when novel information is generated in your mind - and when you're thinking \"passively\", as when you read or listen to some information and absorb its content as it comes.</li>\n<li>What memories did the thought dredge up, if any - parallel situations from the past, apparently unrelated anecdotes that floated by for no reason, events where you learned concepts key to the topic of your thought?&nbsp; Did the thought generate anticipations for the future - a plan, a fear, a hope, an expectation, a worry?</li>\n<li>What sensory input were you receiving at the time?&nbsp; Include not only sight, sound, smell, touch, and taste, but also things like temperature, proprioception, and internal symptoms like hunger or nausea.&nbsp; Can you determine how, if at all, that interacted with the thought?</li>\n</ul>\n<p><br />You cannot have too much data.&nbsp; (You probably can have too much data in one situation relative to how much you have in another, though - that'll overbalance your models - so make a concerted effort to diversify your times and situations for introspection.)&nbsp; When you acquire the data, <a href=\"/lw/1y0/the_abcs_of_luminosity/\">correlate it</a> to learn more about what might bring various aspects of your thought into being.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwv9eHi7KGg5KA9oM": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v4ngP587MDZ5rC48Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 42, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "2531", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "the-spotlight", "canonicalPrevPostSlug": "the-abc-s-of-luminosity", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "rLuZ6XrGpgjk9BNpX", "Zstm38omrpeu7iWeS", "9sguwESkteCgqFMbj", "Y6TpEEKZq6HXfhWxd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-20T09:40:12.278Z", "modifiedAt": null, "url": null, "title": "The Price of Life", "slug": "the-price-of-life", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:49.816Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BenAlbahari", "createdAt": "2010-01-04T10:39:05.080Z", "isAdmin": false, "displayName": "BenAlbahari"}, "userId": "yHHBZZDtuWZw92SAf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PmQoBTjFNMDB6Nu9r/the-price-of-life", "pageUrlRelative": "/posts/PmQoBTjFNMDB6Nu9r/the-price-of-life", "linkUrl": "https://www.lesswrong.com/posts/PmQoBTjFNMDB6Nu9r/the-price-of-life", "postedAtFormatted": "Saturday, March 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Price%20of%20Life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Price%20of%20Life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmQoBTjFNMDB6Nu9r%2Fthe-price-of-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Price%20of%20Life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmQoBTjFNMDB6Nu9r%2Fthe-price-of-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmQoBTjFNMDB6Nu9r%2Fthe-price-of-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 520, "htmlBody": "<p>Less Wrong readers are familiar with the idea you can and should put a price on life. Unfortunately the Big Lie that you can't and shouldn't has big consequences in the current health care debate. Here's some articles on it:</p>\n<p>Yvain's blog post <a href=\"http://squid314.livejournal.com/260949.html\">here</a>&nbsp;(HT: Vladimir Nesov).<br />Peter Singer's article on rationing health care&nbsp;<a href=\"http://www.nytimes.com/2009/07/19/magazine/19healthcare-t.html\">here</a>.<br />Wikipedia <a href=\"http://en.wikipedia.org/wiki/Value_of_life\">here</a>.<br />Experts and policy makers who debate this issue <a href=\"http://www.takeonit.com/question/342.aspx\">here</a>.</p>\n<p>For those new to Less Wrong, here's the crux of Peter Singer's reasoning as to why you can put a price on life:</p>\n<p><em><a id=\"more\"></a></em></p>\n<blockquote>\n<p>The dollar value that bureaucrats place on a generic human life is intended to reflect social values, as revealed in our behavior. It is the answer to the question: \"How much are you willing to pay to save your life?\" &mdash; except that, of course, if you asked that question of people who were facing death, they would be prepared to pay almost anything to save their lives. So instead, economists note how much people are prepared to pay to reduce the risk that they will die. How much will people&nbsp;pay for air bags in a car, for instance? Once you know how much they will pay for a specified reduction in risk, you multiply the amount that people are willing to pay by how much the risk has been reduced, and then you know, or so the theory goes, what value people place on their lives. Suppose that there is a 1 in 100,000 chance that an air bag in my car will save my life, and that I would pay $50 &mdash; but no more than that &mdash; for an air bag. Then it looks as if I value my life at $50 x 100,000, or $5 million.</p>\n<p>The theory sounds good, but in practice it has problems. We are not good at taking account of differences between very small risks, so if we are asked how much we would pay to reduce a risk of dying from 1 in 1,000,000 to 1 in 10,000,000, we may give the same answer as we would if asked how much we would pay to reduce the risk from 1 in 500,000 to 1 in 10,000,000. Hence multiplying what we would pay to reduce the risk of death by the reduction in risk lends an apparent mathematical precision to the outcome of the calculation &mdash; the supposed value of a human life &mdash; that our intuitive responses to the questions cannot support. Nevertheless, this approach to setting a value on a human life is at least closer to what we really believe &mdash; and to what we should believe &mdash; than dramatic pronouncements about the infinite value of every human life, or the suggestion that we cannot distinguish between the value of a single human life and the value of a million human lives, or even of the rest of the world. Though such feel-good claims may have some symbolic value in particular circumstances, to take them seriously and apply them &mdash; for instance, by leaving it to chance whether we save one life or a billion &mdash; would be deeply unethical.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PmQoBTjFNMDB6Nu9r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 4, "extendedScore": null, "score": 5.707025893372363e-07, "legacy": true, "legacyId": "2535", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-21T07:08:28.304Z", "modifiedAt": null, "url": null, "title": "The scourge of perverse-mindedness", "slug": "the-scourge-of-perverse-mindedness", "viewCount": null, "lastCommentedAt": "2021-10-12T19:45:22.461Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "simplicio", "createdAt": "2010-03-06T04:03:43.272Z", "isAdmin": false, "displayName": "simplicio"}, "userId": "fDQ7ty7YmE3PxgqNh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cmrtpfG7hGEL9Zh9f/the-scourge-of-perverse-mindedness", "pageUrlRelative": "/posts/cmrtpfG7hGEL9Zh9f/the-scourge-of-perverse-mindedness", "linkUrl": "https://www.lesswrong.com/posts/cmrtpfG7hGEL9Zh9f/the-scourge-of-perverse-mindedness", "postedAtFormatted": "Sunday, March 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20scourge%20of%20perverse-mindedness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20scourge%20of%20perverse-mindedness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcmrtpfG7hGEL9Zh9f%2Fthe-scourge-of-perverse-mindedness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20scourge%20of%20perverse-mindedness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcmrtpfG7hGEL9Zh9f%2Fthe-scourge-of-perverse-mindedness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcmrtpfG7hGEL9Zh9f%2Fthe-scourge-of-perverse-mindedness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1410, "htmlBody": "<p>This website is devoted to the art of rationality, and as such, is a wonderful corrective to wrong facts and, more importantly, wrong <em>procedures</em> for finding out facts.<br /><br />There is, however, another type of cognitive phenomenon that I&rsquo;ve come to consider particularly troublesome, because it militates against rationality in the irrationalist, and fights against contentment and curiousity in the rationalist. For lack of a better word, I&rsquo;ll call it perverse-mindedness.<br /><br />The perverse-minded do not necessarily disagree with you about any fact questions. Rather, they feel the wrong emotions about fact questions, usually because they haven&rsquo;t worked out all the corollaries.<br /><br />Let&rsquo;s make this less abstract. I think the following quote is preaching to the choir on a site like LW:<br /><br />&ldquo;The universe that we observe has precisely the properties we should expect if there is, at bottom, no design, no purpose, no evil, no good, nothing but pitiless indifference.&rdquo;<br />-Richard Dawkins, \"God's Utility Function,\" Scientific American (November, 1995).<br /><br />Am I posting that quote to disagree with it? No. Every jot and tittle of it is correct. But allow me to quote another point of view on this question.<br /><br />&ldquo;We are not born into this world, but grow out of it; for in the same way an apple tree apples, the Earth peoples.&rdquo;<a id=\"more\"></a><br /><br />This quote came from an ingenious and misguided man named Alan Watts. You will not find him the paragon of rationality, to put it mildly. And yet, let&rsquo;s consider this particular statement on its own. What exactly is wrong with it? Sure, you can pick some trivial holes in it &ndash; life would not have arisen without the sun, for example, and Homo sapiens was not inevitable in any way. But the basic idea &ndash; that <strong>life and consciousness is a natural and possibly inevitable consequence of the way the universe works</strong> &ndash; is indisputably correct.<br /><br />So why would I be surprised to hear a rationalist say something like this? Note that it is <em>empirically indistinguishable</em> from the more common view of &ldquo;mankind confronted by a hostile universe.&rdquo; This is the message of the present post: it is not only our knowledge that matters, but also our attitude to that knowledge. I believe I share a desire with most others here to seek truth naively, swallowing the hard pills when it becomes necessary. However, there is no need to turn every single truth into a hard pill. Moreover, sometimes the hard pills also come in chewable form.<br /><br />What other fact questions might people regard in a perverse way?<br /><br />How about materialism, the view that reality consists, at bottom, in the interplay of matter and energy? This, to my mind, is the biggie. To come to facilely gloomy conclusions based on materialism seems to be practically a cottage industry among Christian apologists and New Agers alike. Since the claims are all so similar to each other, I will address them collectively.<br /><br />&ldquo;If we are nothing but matter in motion, mere chemicals, then:</p>\n<ol>\n<li>Life has no meaning;</li>\n<li>Morality has no basis;</li>\n<li>Love is an illusion;</li>\n<li>Everything is futile (there is no immortality);</li>\n<li>Our actions are determined; we have no free will;</li>\n<li>et</li>\n<li>cetera.&rdquo;</li>\n</ol>\n<p><br />The usual response from materialists is to say that an argument from consequences isn&rsquo;t valid &ndash; if you don&rsquo;t like the fact that X is just matter in motion, that doesn&rsquo;t make it false. While eminently true, as a rhetorical strategy for convincing people who aren&rsquo;t already on board with our programme, it&rsquo;s borderline suicidal.<br /><br />I have already hinted at what I think the response ought to be. It is not necessarily a point-by-point refutation of each of these issues individually. The simple fact is, not only is materialism true, but it <em>shouldn&rsquo;t</em> bother anyone who isn&rsquo;t being perverse about it, and it <em>wouldn&rsquo;t</em> bother us if it had always been the standard view.<br /><br />There are multiple levels of analysis in the lives of human beings. We can speak of societies, move to individual psychology, thence to biology, then chemistry&hellip; this is such a trope that I needn&rsquo;t even finish the sentence.<br /><br />However, the concerns of, say, human psychology (as distinct from neuroscience), or morality, or politics, or love, are not directly informed by physics. Some concepts only work meaningfully on one level of analysis. If you were trying to predict the weather, would you start by modeling quarks? Reductionism <em>in principle</em> I will argue for until the second coming (i.e., forever). Reductionism <em>in practice</em> is not always useful. This is the difference between proximate and ultimate causation. The perverse-mindedness I speak of consists in leaping straight from behaviour or phenomenon X to its ultimate cause in physics or chemistry. Then &ndash; here&rsquo;s the &ldquo;ingenious&rdquo; part &ndash; declaring that, since the ultimate level is devoid of meaning, morality, and general warm-and-fuzziness, so too must be all the higher levels.<br /><br />What can we make of someone who says that materialism implies meaninglessness? I can only conclude that if I took them to see Seurat&rsquo;s painting &ldquo;<a href=\"http://en.wikipedia.org/wiki/A_Sunday_Afternoon_on_the_Island_of_La_Grande_Jatte\" target=\"_blank\">A Sunday Afternoon on the Island of La Grande Jatte</a>,\" they would earnestly ask me what on earth the purpose of all the little dots was. Matter is what we&rsquo;re made of, in the same way as a painting is made of dried pigments on canvas. Big deal! What would you prefer to be made of, if not matter?<br /><br />It is only by the contrived unfavourable contrast of matter with something that doesn&rsquo;t actually exist &ndash; soul or spirit or &eacute;lan vital or whatever &ndash; that somebody can pull off the astounding trick of spoiling your experience of a perfectly good reality, one that you should feel lucky to inhabit.<br /><br />I worry that some rationalists, while rejecting wooly dualist ideas about ghosts in the machine, have tacitly accepted the dualists&rsquo; baseless assumptions about the gloomy consequences of materialism. <strong>There really is no hard pill to swallow.</strong><br /><br />What are some other examples of perversity? <a href=\"/lw/j3/science_as_curiositystopper/\" target=\"_blank\">Eliezer</a> has written extensively on another important one, which we might call the disappointment of explicability. &ldquo;A rainbow is just light refracting.&rdquo; &ldquo;The aurora is only a bunch of protons hitting the earth&rsquo;s magnetic field.&rdquo; Rationalists are, sadly, not immune to this nasty little meme. It can be easily spotted by tuning your ears to the words &ldquo;just&rdquo; and &ldquo;merely.&rdquo; By saying, for example, that sexual attraction is &ldquo;merely&rdquo; biochemistry, you are telling the truth and deceiving at the same time. You are making a (more or less) correct factual statement, while Trojan-horsing an extraneous value judgment into your listener&rsquo;s mind as well: &ldquo;chemicals are unworthy.&rdquo; On behalf of chemicals everywhere, I say: Screw you! Where would you be without us?<br /><br />What about the final fate of the universe, to take another example? Many of us probably remember the opening scene of Annie Hall, where little Alfie tells the family doctor he&rsquo;s become depressed because everything will end in expansion and heat death. &ldquo;He doesn&rsquo;t do his homework!&rdquo; cries his mother. &ldquo;What&rsquo;s the point?&rdquo; asks Alfie.<br /><br />Although I found that scene hilarious, I have actually heard several smart people po-facedly lament the fact that the universe will end with a whimper. If this seriously bothers you psychologically, then your psychology is severely divorced from the reality that you inhabit. By all means, be depressed about your chronic indigestion or the Liberal Media or teenagers on your lawn, but <strong>not</strong> about an event that will happen in 10<sup>14</sup> years, involving a <em>dramatis personae</em> of burnt-out star remnants. Puh-lease. There is infinitely more tragedy happening every second in a cup of buttermilk.<br /><br />The art of not being perverse consists in seeing the same reality as others and agreeing about facts, but perceiving more in an aesthetic sense. It is the joy of learning something that&rsquo;s been known for centuries; it is appreciating the consilience of knowledge without moaning about reductionism; it is accepting nature on her own terms, without fatuous navel-gazing about how unimportant you are on the cosmic scale. If there is a fact question at stake, take no prisoners; but you don&rsquo;t get extra points for unnecessary angst.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cmrtpfG7hGEL9Zh9f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 112, "baseScore": 122, "extendedScore": null, "score": 0.000203, "legacy": true, "legacyId": "2538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 122, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 257, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L22jhyY9ocXQNLqyE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-22T20:18:21.388Z", "modifiedAt": null, "url": null, "title": "What would you do if blood glucose theory of willpower was true?", "slug": "what-would-you-do-if-blood-glucose-theory-of-willpower-was", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:36.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mf5LS5pxAy6WxCFNW/what-would-you-do-if-blood-glucose-theory-of-willpower-was", "pageUrlRelative": "/posts/mf5LS5pxAy6WxCFNW/what-would-you-do-if-blood-glucose-theory-of-willpower-was", "linkUrl": "https://www.lesswrong.com/posts/mf5LS5pxAy6WxCFNW/what-would-you-do-if-blood-glucose-theory-of-willpower-was", "postedAtFormatted": "Monday, March 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20would%20you%20do%20if%20blood%20glucose%20theory%20of%20willpower%20was%20true%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20would%20you%20do%20if%20blood%20glucose%20theory%20of%20willpower%20was%20true%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmf5LS5pxAy6WxCFNW%2Fwhat-would-you-do-if-blood-glucose-theory-of-willpower-was%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20would%20you%20do%20if%20blood%20glucose%20theory%20of%20willpower%20was%20true%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmf5LS5pxAy6WxCFNW%2Fwhat-would-you-do-if-blood-glucose-theory-of-willpower-was", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmf5LS5pxAy6WxCFNW%2Fwhat-would-you-do-if-blood-glucose-theory-of-willpower-was", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>There's considerable amount of evidence that willpower is severely diminished if blood glucose get down, and <a href=\"http://www.miller-mccune.com/health/dogs-offer-clues-to-self-control-10928/\">this effect is not limited to humans</a>. And a small sugary drink at the right time is enough to restore it.</p>\n<p>We're talking really small numbers. Total <a href=\"http://en.wikipedia.org/wiki/Blood_sugar\">blood glucose</a> of a healthy adult is about 5g and it varies within fairly limited range. Then there's maybe 45g in total body waters. Then there's about 100g of glycogen in liver, plus yet larger amount in muscles and other organs, but which doesn't seem to take part in sugar level regulation. For comparison a small can of coke <span style=\"font-family: Arial;\">contains 33g - a really small amounts at appropriate times can make a big difference.</span></p>\n<p><span style=\"font-family: Arial;\">This leads to two issues. First,</span><span style=\"font-family: Arial;\"> is blood glucose a good explanation for willpower deficiency and therefore akrasia? I'd say there's significant amount of evidence that some effect exists, but is it really the most important factor? Humans are complicated, science knows very little about how we work, and probably half of what it \"knows\" is false or at best only half-true. Caution is definitely warranted.<br /></span></p>\n<p>And the second issue - if this theory was true - and by manipulating blood glucose levels you could achieve far greater willpower whenever you wanted, what would you do? It seems that exploiting it isn't that easy, and I'd love to hear if any of you tried it before.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mf5LS5pxAy6WxCFNW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 18, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "2543", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-22T22:33:23.315Z", "modifiedAt": null, "url": null, "title": "Understanding your understanding", "slug": "understanding-your-understanding", "viewCount": null, "lastCommentedAt": "2020-11-27T15:47:28.350Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "SilasBarta", "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4gevjbK77NQS6hybY/understanding-your-understanding", "pageUrlRelative": "/posts/4gevjbK77NQS6hybY/understanding-your-understanding", "linkUrl": "https://www.lesswrong.com/posts/4gevjbK77NQS6hybY/understanding-your-understanding", "postedAtFormatted": "Monday, March 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Understanding%20your%20understanding&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnderstanding%20your%20understanding%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4gevjbK77NQS6hybY%2Funderstanding-your-understanding%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Understanding%20your%20understanding%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4gevjbK77NQS6hybY%2Funderstanding-your-understanding", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4gevjbK77NQS6hybY%2Funderstanding-your-understanding", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 947, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/la/truly_part_of_you/\">Truly Part of You</a>, <a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a> </p><p>Partly because of LessWrong discussions about what <em>really</em> counts as understanding (<a href=\"/lw/1m2/hypotheses_for_dualism/1fvr\">some</a> <a href=\"/lw/1v3/the_graviton_as_aether/1pvc\">typical</a> <a href=\"/lw/1xy/life_experience_as_a_conversationhalter/1six\">examples</a>), I came up with a scheme to classify different levels of understanding so that posters can be more precise about what they mean when they claim to understand -- or fail to understand -- a particular phenomenon or domain.</p><p>Each level has a description so that you know if you meet it, and tells you what to watch out for when you&#x27;re at or close to that level. I have taken the liberty of naming them after the LW articles that describe what such a level is like.</p><h2><strong>Level 0:</strong> The &quot;<a href=\"/lw/iq/guessing_the_teachers_password/\">Guessing the Teacher&#x27;s Password</a>&quot; Stage </h2><p><strong>Summary:</strong> You have no understanding, because you don&#x27;t see how any outcome is more or less likely than any other.</p><p><strong>Description:</strong> This level is only included for comparison -- to show something that is <em>not</em> understanding. At this point, you have, a best, <em>labels</em> that other people use when describing the phenomenon. Maybe you can even generate the appearance of understanding on the topic. However, you actually have a maximum entropy probability distribution. In other words, nothing would surprise you, no event is more or less likely to happen, and everything is consistent with what you &quot;know&quot; about it. No rationalist should count this as an understanding, though it may involve knowledge of the labels that a domain uses.</p><p><strong>Things to watch out for:</strong> Scientific-sounding terms in your vocabulary that don&#x27;t correspond to an actual predictive model; your inability to say what you expect to see, and what you would be surprised by. </p><h2><strong>Level 1:</strong> The &quot;<a href=\"/lw/q5/quantum_nonrealism/\">Shut up and Calculate</a>&quot; Stage</h2><p><strong>Summary:</strong> You can successfully predict the phenomenon, but see it as an independent, compartmentalized domain. </p><p><strong>Description:</strong> This is where you can predict the phenomenon, using a generative model that tells you what to expect. You are capable of being surprised, as certain observations are assigned low probability. It may even be tremendously complicated, but it <em>works</em>.</p><p>Though low on the hierarchy, it&#x27;s actually a big accomplishment in itself. However, when you are at this stage, you see its dynamics as being unrelated to anything else, belonging to its own domain, following its own rules. While it might have parallels to things you do understand, you see no reason why the parallel must hold, and therefore can&#x27;t reason about how extensive that relationship is.</p><p><strong>Things to watch out for:</strong> Going from &quot;It just works, I don&#x27;t know what it means&quot; to &quot;it doesn&#x27;t mean anything!&quot; Also, becoming proud of your ignorance of its relationship to the rest of the world.</p><h2><strong>Level 2:</strong> The &quot;<a href=\"/lw/uw/entangled_truths_contagious_lies/\">Entangled Truths</a>&quot; Stage. (Alternate name: &quot;<a href=\"/lw/hq/universal_fire/\">Universal Fire</a>&quot;.)</h2><p><strong>Summary:</strong> Your accurate model in this domain has deep connections to the rest of your models (whether inferential or causal); inferences can flow between the two.</p><p><strong>Description:</strong> At this stage, your model of the phenomenon is also deeply connected to your model of everything else. Instead of the phenomenon being something with its own set of rules, you see how its dynamics interface with the dynamics of everything else in your understanding. You can derive parameters in this domain from your knowledge in another domain; you can explain how they are related.</p><p>Note the regression here: you meet this stage when your model for the new phenomenon connects to your model for &quot;everything else&quot;. So what about the <em>first </em>&quot;everything else&quot; you understood (which could be called your &quot;primitively understood&quot; part of reality)? This would be the instinctive model of the world that you are born with: the &quot;folk physics&quot;, &quot;folk psychology&quot;, etc. Its existence is revealed in such experiments as when babies are <a href=\"http://scienceblogs.com/cognitivedaily/2006/08/babies_and_eye_tracking.php\">confused</a> by rolling balls that suddenly violate the laws of physics.</p><p>This &quot;Level 2&quot; understanding therefore ultimately connects everything back to your direct, raw experiences (&quot;qualia&quot;) of the world, but, importantly, is not subordinate to them \u2013 optical illusions shouldn&#x27;t override the stronger evidence that proves to you it&#x27;s an illusion.</p><p><strong>Things to watch out for:</strong> Assuming that similar behavior in different domains (&quot;<a href=\"/lw/rj/surface_analogies_and_deep_causes/\">surface analogies</a>&quot;) is enough to explain their relationship. Also, using one intersection between multiple domains as a reason to immediately collapse them together.</p><h2><strong>Level 3:</strong> The &quot;<a href=\"/lw/la/truly_part_of_you/\">Truly Part of You</a>&quot; Stage</h2><p><strong>Summary:</strong> Your models are such that you would re-discover them, for the right reasons, even they were deleted from your memory.</p><p><strong>Description:</strong> At this stage, not only do you have good, well-<em>connected </em>models of reality, but they are so well-<em>grounded</em>, that they &quot;regenerate&quot; when &quot;damaged&quot;. That is, you weren&#x27;t merely fed these wonderful models outright by some other Really Smart Being (though initially you might have been), but rather, you also consistently use a reliable method for gaining knowledge, and this method would eventually stumble upon the same model you have now, no matter how much knowledge is stripped away from it.</p><p>This capability arises because your high understanding makes much of your knowledge redundant: knowing something in one domain has implications in quite distant domains, leading you to recognize what was lost \u2013 and your reliable methods of inference tell you what, if anything, you need to do to recover it.</p><p>This stage should be the goal of all rationalists.</p><p><strong>Things to watch out for:</strong> Hindsight bias: you may <em>think</em> you would have made the same inferences at a previous epistemic state, but that might just be due to already knowing the answers. Also, if you&#x27;re really at this stage, you should have what amounts to a &quot;fountain of knowledge&quot; \u2013 are you learning all you can from it?</p><p><strong>In conclusion: </strong>In trying to enhance your own, or someone else&#x27;s, understanding of a topic, I recommend identifying which level you both are at to see if you have something to learn from each other, or are simply using different standards. </p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 4, "fR7QfYx4JA3BnptT9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4gevjbK77NQS6hybY", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 85, "baseScore": 97, "extendedScore": null, "score": 0.000162, "legacy": true, "legacyId": "2546", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 97, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related to:</strong> <a href=\"/lw/la/truly_part_of_you/\">Truly Part of You</a>, <a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a> </p><p>Partly because of LessWrong discussions about what <em>really</em> counts as understanding (<a href=\"/lw/1m2/hypotheses_for_dualism/1fvr\">some</a> <a href=\"/lw/1v3/the_graviton_as_aether/1pvc\">typical</a> <a href=\"/lw/1xy/life_experience_as_a_conversationhalter/1six\">examples</a>), I came up with a scheme to classify different levels of understanding so that posters can be more precise about what they mean when they claim to understand -- or fail to understand -- a particular phenomenon or domain.</p><p>Each level has a description so that you know if you meet it, and tells you what to watch out for when you're at or close to that level. I have taken the liberty of naming them after the LW articles that describe what such a level is like.</p><h2 id=\"Level_0__The__Guessing_the_Teacher_s_Password__Stage_\"><strong>Level 0:</strong> The \"<a href=\"/lw/iq/guessing_the_teachers_password/\">Guessing the Teacher's Password</a>\" Stage </h2><p><strong>Summary:</strong> You have no understanding, because you don't see how any outcome is more or less likely than any other.</p><p><strong>Description:</strong> This level is only included for comparison -- to show something that is <em>not</em> understanding. At this point, you have, a best, <em>labels</em> that other people use when describing the phenomenon. Maybe you can even generate the appearance of understanding on the topic. However, you actually have a maximum entropy probability distribution. In other words, nothing would surprise you, no event is more or less likely to happen, and everything is consistent with what you \"know\" about it. No rationalist should count this as an understanding, though it may involve knowledge of the labels that a domain uses.</p><p><strong>Things to watch out for:</strong> Scientific-sounding terms in your vocabulary that don't correspond to an actual predictive model; your inability to say what you expect to see, and what you would be surprised by. </p><h2 id=\"Level_1__The__Shut_up_and_Calculate__Stage\"><strong>Level 1:</strong> The \"<a href=\"/lw/q5/quantum_nonrealism/\">Shut up and Calculate</a>\" Stage</h2><p><strong>Summary:</strong> You can successfully predict the phenomenon, but see it as an independent, compartmentalized domain. </p><p><strong>Description:</strong> This is where you can predict the phenomenon, using a generative model that tells you what to expect. You are capable of being surprised, as certain observations are assigned low probability. It may even be tremendously complicated, but it <em>works</em>.</p><p>Though low on the hierarchy, it's actually a big accomplishment in itself. However, when you are at this stage, you see its dynamics as being unrelated to anything else, belonging to its own domain, following its own rules. While it might have parallels to things you do understand, you see no reason why the parallel must hold, and therefore can't reason about how extensive that relationship is.</p><p><strong>Things to watch out for:</strong> Going from \"It just works, I don't know what it means\" to \"it doesn't mean anything!\" Also, becoming proud of your ignorance of its relationship to the rest of the world.</p><h2 id=\"Level_2__The__Entangled_Truths__Stage___Alternate_name___Universal_Fire___\"><strong>Level 2:</strong> The \"<a href=\"/lw/uw/entangled_truths_contagious_lies/\">Entangled Truths</a>\" Stage. (Alternate name: \"<a href=\"/lw/hq/universal_fire/\">Universal Fire</a>\".)</h2><p><strong>Summary:</strong> Your accurate model in this domain has deep connections to the rest of your models (whether inferential or causal); inferences can flow between the two.</p><p><strong>Description:</strong> At this stage, your model of the phenomenon is also deeply connected to your model of everything else. Instead of the phenomenon being something with its own set of rules, you see how its dynamics interface with the dynamics of everything else in your understanding. You can derive parameters in this domain from your knowledge in another domain; you can explain how they are related.</p><p>Note the regression here: you meet this stage when your model for the new phenomenon connects to your model for \"everything else\". So what about the <em>first </em>\"everything else\" you understood (which could be called your \"primitively understood\" part of reality)? This would be the instinctive model of the world that you are born with: the \"folk physics\", \"folk psychology\", etc. Its existence is revealed in such experiments as when babies are <a href=\"http://scienceblogs.com/cognitivedaily/2006/08/babies_and_eye_tracking.php\">confused</a> by rolling balls that suddenly violate the laws of physics.</p><p>This \"Level 2\" understanding therefore ultimately connects everything back to your direct, raw experiences (\"qualia\") of the world, but, importantly, is not subordinate to them \u2013 optical illusions shouldn't override the stronger evidence that proves to you it's an illusion.</p><p><strong>Things to watch out for:</strong> Assuming that similar behavior in different domains (\"<a href=\"/lw/rj/surface_analogies_and_deep_causes/\">surface analogies</a>\") is enough to explain their relationship. Also, using one intersection between multiple domains as a reason to immediately collapse them together.</p><h2 id=\"Level_3__The__Truly_Part_of_You__Stage\"><strong>Level 3:</strong> The \"<a href=\"/lw/la/truly_part_of_you/\">Truly Part of You</a>\" Stage</h2><p><strong>Summary:</strong> Your models are such that you would re-discover them, for the right reasons, even they were deleted from your memory.</p><p><strong>Description:</strong> At this stage, not only do you have good, well-<em>connected </em>models of reality, but they are so well-<em>grounded</em>, that they \"regenerate\" when \"damaged\". That is, you weren't merely fed these wonderful models outright by some other Really Smart Being (though initially you might have been), but rather, you also consistently use a reliable method for gaining knowledge, and this method would eventually stumble upon the same model you have now, no matter how much knowledge is stripped away from it.</p><p>This capability arises because your high understanding makes much of your knowledge redundant: knowing something in one domain has implications in quite distant domains, leading you to recognize what was lost \u2013 and your reliable methods of inference tell you what, if anything, you need to do to recover it.</p><p>This stage should be the goal of all rationalists.</p><p><strong>Things to watch out for:</strong> Hindsight bias: you may <em>think</em> you would have made the same inferences at a previous epistemic state, but that might just be due to already knowing the answers. Also, if you're really at this stage, you should have what amounts to a \"fountain of knowledge\" \u2013 are you learning all you can from it?</p><p><strong>In conclusion: </strong>In trying to enhance your own, or someone else's, understanding of a topic, I recommend identifying which level you both are at to see if you have something to learn from each other, or are simply using different standards. </p>", "sections": [{"title": "Level 0: The \"Guessing the Teacher's Password\" Stage ", "anchor": "Level_0__The__Guessing_the_Teacher_s_Password__Stage_", "level": 1}, {"title": "Level 1: The \"Shut up and Calculate\" Stage", "anchor": "Level_1__The__Shut_up_and_Calculate__Stage", "level": 1}, {"title": "Level 2: The \"Entangled Truths\" Stage. (Alternate name: \"Universal Fire\".)", "anchor": "Level_2__The__Entangled_Truths__Stage___Alternate_name___Universal_Fire___", "level": 1}, {"title": "Level 3: The \"Truly Part of You\" Stage", "anchor": "Level_3__The__Truly_Part_of_You__Stage", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "80 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["fg9fXrHpeaDD6pEPL", "NMoLJuDJEms7Ku9XS", "k3823vuarnmL5Pqin", "wyyfFfaRar2jEdeQK", "LaM5aTcXvXzwQSC2Q", "6ByPxcGDhmx74gPSm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-23T15:49:35.282Z", "modifiedAt": null, "url": null, "title": "Subtext is not invariant under linear transformations", "slug": "subtext-is-not-invariant-under-linear-transformations", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:45.741Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zKf7LNzjrR5QofgW2/subtext-is-not-invariant-under-linear-transformations", "pageUrlRelative": "/posts/zKf7LNzjrR5QofgW2/subtext-is-not-invariant-under-linear-transformations", "linkUrl": "https://www.lesswrong.com/posts/zKf7LNzjrR5QofgW2/subtext-is-not-invariant-under-linear-transformations", "postedAtFormatted": "Tuesday, March 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Subtext%20is%20not%20invariant%20under%20linear%20transformations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASubtext%20is%20not%20invariant%20under%20linear%20transformations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKf7LNzjrR5QofgW2%2Fsubtext-is-not-invariant-under-linear-transformations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Subtext%20is%20not%20invariant%20under%20linear%20transformations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKf7LNzjrR5QofgW2%2Fsubtext-is-not-invariant-under-linear-transformations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKf7LNzjrR5QofgW2%2Fsubtext-is-not-invariant-under-linear-transformations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 345, "htmlBody": "<p>You can download the audio and PDFs from the 2007 Cognitive Aging Summit in Washington DC <a href=\"http://www.health.ufl.edu/brain/summit/index.htm\">here</a>; they're good listening.&nbsp; But I want to draw your attention to the graphs on page 6 of Archana Singh-Manoux's <a href=\"http://www.health.ufl.edu/brain/summit/pdf/05_singh-manoux.pdf\">presentation</a>.&nbsp; It shows the \"social gradient\" of intelligence.&nbsp; The X-axis is decreasing socioeconomic status (SES); the Y-axis is increasing performance on tests of reasoning, memory, phonemic fluency, and vocabulary.&nbsp; Each graph shows a line sloping from the upper left (high SES, high performance) downwards and to the right.</p>\n<p>Does anything leap out at you as strange about these graphs?<a id=\"more\"></a></p>\n<p>What leapt out at me was, \"Why the hell would anybody make a graph with their independent variable <em>decreasing</em> along the X-axis?\"</p>\n<p>Socio-economic status (SES) basically means income.&nbsp; It has a natural zero.&nbsp; The obvious thing to do would be to put it on the X-axis with zero towards the left, increasing towards the right.&nbsp; These graphs have zero off somewhere on the right, with income increasing towards the left.&nbsp; That's so weird that it couldn't happen by accident.&nbsp; It would be like \"accidentally\" drawing the graph with the Y-axis flipped.</p>\n<p>What could be the intent behind flipping the X-axis when presenting the data?</p>\n<p>If you drew the data the normal way, it would suggest that there's a natural zero-level to both SES and cognition; and that increasing SES increases cognition, possibly without limit.</p>\n<p>But when you flip the X-axis, you're limited.&nbsp; You can't go too far to the right, or you'd hit zero.&nbsp; And you can't go off to the left, because we don't think that way in the West.&nbsp; We start at the left and move right.</p>\n<p>By flipping the X-axis, the presenter has communicated that SES and intelligence have natural bounds.&nbsp; Instead of communicating the idea that higher SES is a good thing that leads to higher intelligence, this presentation of the data suggests that the leftmost point on each graph (the anchoring point) is \"normal\", and a <em>lack</em> of wealth has caused a deficiency in people of lower SES.</p>\n<p>(And, of course, rotating around the line Y=X would suggest that intelligence makes you rich.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zKf7LNzjrR5QofgW2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 47, "extendedScore": null, "score": 9.7e-05, "legacy": true, "legacyId": "2551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-23T17:11:03.256Z", "modifiedAt": null, "url": null, "title": "Necessary, But Not Sufficient", "slug": "necessary-but-not-sufficient", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:32.853Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pjeby", "createdAt": "2009-02-27T23:51:22.854Z", "isAdmin": false, "displayName": "pjeby"}, "userId": "Zzxr5JZpkitaNxL4Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4DLinGRkYKX2Lovtv/necessary-but-not-sufficient", "pageUrlRelative": "/posts/4DLinGRkYKX2Lovtv/necessary-but-not-sufficient", "linkUrl": "https://www.lesswrong.com/posts/4DLinGRkYKX2Lovtv/necessary-but-not-sufficient", "postedAtFormatted": "Tuesday, March 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Necessary%2C%20But%20Not%20Sufficient&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANecessary%2C%20But%20Not%20Sufficient%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DLinGRkYKX2Lovtv%2Fnecessary-but-not-sufficient%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Necessary%2C%20But%20Not%20Sufficient%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DLinGRkYKX2Lovtv%2Fnecessary-but-not-sufficient", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DLinGRkYKX2Lovtv%2Fnecessary-but-not-sufficient", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 654, "htmlBody": "<p>There seems to be something odd about how people reason in relation to themselves, compared to the way they examine problems in other domains.</p>\n<p>In mechanical domains, we seem to have little problem with the idea that things can be \"necessary, but not sufficient\".&nbsp; For example, if your car fails to start, you will likely know that several things are <em>necessary</em> for the car to start, but not <em>sufficient</em> for it to do so.&nbsp; It has to have fuel, ignition, and compression, and oxygen...&nbsp; each of which in turn has further necessary conditions, such as an operating fuel pump, electricity for the spark plugs, electricity for the starter, and so on.</p>\n<p>And usually, we don't go around claiming that \"fuel\" is a magic bullet for fixing the problem of car-not-startia, or argue that if we increase the amount of electricity in the system, the car will necessarily run faster or better.</p>\n<p>For some reason, however, we don't seem to apply this sort of necessary-but-not-sufficient thinking to systems above a certain level of complexity...&nbsp; such as <em>ourselves</em>.</p>\n<p>When I wrote my previous post about <a href=\"/lw/1tu/improving_the_akrasia_hypothesis/\">the akrasia hypothesis</a>, I mentioned that there was something bothering me about the way people seemed to be reasoning about akrasia and other complex problems.&nbsp; And recently, with taw's post about <a href=\"/lw/1yn/what_would_you_do_if_blood_glucose_theory_of/\">blood sugar and akrasia</a>, I've realized that the specific thing bothering me is the absence of causal-chain reasoning there.<a id=\"more\"></a></p>\n<p>When I was a kid, I remember reading once about a scientist saying that the problem with locating brain functions by what's impaired when somebody has brain damage there, is that it's like opening up a TV set and taking out a resistor.&nbsp; If the picture goes bad, you might then conclude that the resistor is the \"source of pictureness\", when all you have really proved is that the resistor (or brain part) is <em>necessary</em> for pictureness.</p>\n<p>Not that it's <strong>sufficient</strong>.</p>\n<p>And so, in every case where an akrasia technique works for you -- whether it's glucose or turning off your internet -- all you have <em>really</em> done, is the equivalent of putting the missing resistor back into the TV set.</p>\n<p>This is why \"different things work for different people\", in different circumstances.&nbsp; And it's why \"magic bullets\" are possible, like vitamin C as a cure for scurvy.&nbsp; When you fix a <em>deficiency</em> (as long as it's the <em>only</em> deficiency present) then it seems like a \"magic\" fix.</p>\n<p>But, just because some specific deficiency creates scurvy, akrasia, or no-picture-on-the-TV-ia, this doesn't mean the resistor you replaced is therefore the ultimate, one true source of \"pictureness\"!</p>\n<p>Even if you've successfully removed and replaced that resistor repeatedly, in multiple televisions under laboratory conditions.</p>\n<p>Unfortunately, it seems that thinking in terms of causal chains like this is not really a \"natural\" feature of human brains.&nbsp; And upon reflection, I realize that I only learned to think this way because I studied the <a href=\"http://en.wikipedia.org/wiki/Theory_of_Constraints\">Theory of Constraints</a> (ToC) about 13 years ago, and I also had a mentor who drilled me in some aspects of its practice, even before I knew what it was called.</p>\n<p>But, if you are going to reason about complex problems, it's a very good tool to have in your rationalist toolkit.</p>\n<p>Because, what the Theory of Constraints teaches us about problem solving, is that if you can reason well enough about a system to identify <em>which</em> necessary-but-not-sufficient conditions are currently deficient (or underpowered relative to the whole), then you will be able to systematically create your <em>own</em> \"magic bullets\".</p>\n<p>So, I encourage you to challenge any fuzzy thinking you see here (or anywhere) about \"magic bullets\", because a magic bullet is <em>only</em> effective in cases where it applies to the <strong>only</strong> insufficiency present in the system under consideration.&nbsp; And having found one magic bullet, is not equivalent to actually <em>understanding</em> the problem, let alone understanding the system as a whole.&nbsp; (Which, by the way, is also why self-help advice is so divergent: it reflects a vast array of possible deficiencies in a very complex system.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "BzghQYM9GnkMHxZKb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4DLinGRkYKX2Lovtv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 58, "extendedScore": null, "score": 9.7e-05, "legacy": true, "legacyId": "2552", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uKoqrgnRoWjhneDvM", "mf5LS5pxAy6WxCFNW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-23T20:36:38.120Z", "modifiedAt": null, "url": null, "title": "Co-operative Utilitarianism", "slug": "co-operative-utilitarianism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:44.471Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardChappell", "createdAt": "2009-03-22T04:29:28.935Z", "isAdmin": false, "displayName": "RichardChappell"}, "userId": "rW2qN9f2r5DP9urFF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fNrWB2w7n42BQZFGx/co-operative-utilitarianism", "pageUrlRelative": "/posts/fNrWB2w7n42BQZFGx/co-operative-utilitarianism", "linkUrl": "https://www.lesswrong.com/posts/fNrWB2w7n42BQZFGx/co-operative-utilitarianism", "postedAtFormatted": "Tuesday, March 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Co-operative%20Utilitarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACo-operative%20Utilitarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfNrWB2w7n42BQZFGx%2Fco-operative-utilitarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Co-operative%20Utilitarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfNrWB2w7n42BQZFGx%2Fco-operative-utilitarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfNrWB2w7n42BQZFGx%2Fco-operative-utilitarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 587, "htmlBody": "<p>Donald Regan's masterful (1980) <em>Utilitarianism and Co-operation</em>&nbsp;raises a problem for traditional moral theories, which conceive of agents as choosing between external options like 'push' or 'not-push' (options that are specifiable independently of the motive from which they are performed). He proves that no such traditional theory T is&nbsp;<em>adaptable</em>, in the sense that \"<em>the agents who satisfy T, whoever and however numerous they may be, are guaranteed to produce the best consequences possible [from among their options] as a group, given the behaviour of everyone else</em>.\" (p.6) It's easy to see that various forms of rule or collective consequentialism fail when you're the only agent satisfying the theory -- doing what&nbsp;<em>would be</em>&nbsp;best if everyone played their part is not necessarily to do what's&nbsp;<em>actually</em>&nbsp;best. What's more interesting is that even Act Utilitarianism can fail to beat co-ordination problems like the following:</p>\n<p><span style=\"font-family: Verdana, Arial, sans-serif; font-size: 13px; color: #333333; line-height: 16px;\"> \n<table border=\"2\" cellpadding=\"4\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>Poof: push</td>\n<td>Not-push</td>\n</tr>\n<tr>\n<td>Whiff: push</td>\n<td>10</td>\n<td>0</td>\n</tr>\n<tr>\n<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Not-push</td>\n<td>0</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n<a name=\"more\"></a><br />Here the best result is obviously for Whiff and Poof to both push. But this isn't guaranteed by the mere fact that each agent does as AU says they ought. Why not? Well, what each ought to do depends on what the other does. If Poof doesn't push then neither should Whiff (that way he can at least secure 6 utils, which is better than 0). And vice versa. So, if Whiff and Poof&nbsp;<em>both</em>&nbsp;happen to not-push, then both have satisfied AU. Each, considered individually, has picked the best option available. But clearly this is insufficient: the two of them together have fallen into a bad equilibrium, and hence not done as well as they (collectively) could have.<br /><br />Regan's solution is build a certain decision-procedure into the objective requirements of the theory:<br /> </span></p>\n<blockquote style=\"margin-top: 1em; margin-right: 0px; margin-bottom: 0px; margin-left: 1em; padding-left: 1em; border-left-width: 3px; border-left-style: solid; border-left-color: #cccccc; color: #667799;\">The basic idea is that each agent should proceed in two steps: First he should identify the other agents who are willing and able to co-operate in the production of the best possible consequences. Then he should do his part in the best plan of behaviour for the group consisting of himself and the others so identified, in view of the behaviour of non-members of the group. (p.x)</blockquote>\n<p>&nbsp;</p>\n<p>This theory, which Regan calls 'Co-operative Utilitarianism', secures the property of adaptability. &nbsp;(You can read Regan for the technical details; here I'm simply aiming to convey the rough idea.) &nbsp;To illustrate with our previous example: suppose Poof is a non-cooperator, and so decides on outside grounds to not-push. Then Whiff should (i) determine that Poof is not available to cooperate, and hence (ii) make the best of a bad situation by likewise not-pushing. In this case, only Whiff satisfies CU, and hence the agents who satisfy the theory (namely, Whiff alone) collectively achieve the best results available to them in the circumstances.</p>\n<p>If&nbsp;<em>both</em>&nbsp;agents satisfied the theory, then they would first recognize the other as a cooperator, and then each would push, as that is what is required for them to \"do their part\" to achieve the best outcome available to the actual cooperators.</p>\n<p>* * *</p>\n<p>[Originally posted to <a href=\"http://www.philosophyetc.net/2010/03/co-operative-utilitarianism.html\">Philosophy, etc.</a>&nbsp;&nbsp;Reproduced here as an experiment of sorts: despite discussing philosophical topics, LW doesn't tend to engage much with the extant philosophical literature, which seems like a lost opportunity. &nbsp;I chose this piece because of the possible connections between Regan's view of cooperative games and the dominant LW view of competitive games: that one should be disposed to co-operate if and only if dealing with another co-operator. &nbsp;In any case, I'll be interested to see whether others find this at all helpful or interesting -- naturally that'll influence whether I attempt this sort of thing again.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fNrWB2w7n42BQZFGx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 5.716731238536738e-07, "legacy": true, "legacyId": "2554", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-23T21:32:25.903Z", "modifiedAt": null, "url": null, "title": "Levels of communication", "slug": "levels-of-communication", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:54.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gs8bZCmaWqDaus7Dr/levels-of-communication", "pageUrlRelative": "/posts/gs8bZCmaWqDaus7Dr/levels-of-communication", "linkUrl": "https://www.lesswrong.com/posts/gs8bZCmaWqDaus7Dr/levels-of-communication", "postedAtFormatted": "Tuesday, March 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Levels%20of%20communication&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALevels%20of%20communication%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgs8bZCmaWqDaus7Dr%2Flevels-of-communication%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Levels%20of%20communication%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgs8bZCmaWqDaus7Dr%2Flevels-of-communication", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgs8bZCmaWqDaus7Dr%2Flevels-of-communication", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 918, "htmlBody": "<p>Communication fails when the participants in a conversation aren't talking about the same thing. This can be something as subtle as having <a href=\"/lw/1ty/mental_crystallography/1oot\">slightly differing mappings of verbal space to conceptual space</a>, or it can be a question of being on entirely different&nbsp;<em>levels of conversation</em>. There are at least four such levels: the level of facts, the level of status, the level of values, and the level of socialization. I suspect that many people with rationalist tendencies tend to operate primarily on the fact level and assume others to be doing so as well, which might lead to plenty of frustration.</p>\r\n<p><strong>The level of facts.&nbsp;</strong>This is the most straightforward one. When everyone is operating on the level of facts, they are detachedly trying to discover the truth about a certain subject. Pretty much nothing else than the facts matter.</p>\r\n<p><strong>The level of status. </strong>Probably the best way of explaining what happens when everyone is operating on the level of status is the following passage, <a href=\"http://www.thestage.co.uk/connect/acblack/improkj.php\">originally found</a> in <a href=\"http://www.amazon.com/Impro-Improvisation-Theatre-Keith-Johnstone/dp/0878301178/\">Keith Johnstone's Impro</a>:&nbsp;<a id=\"more\"></a></p>\r\n<blockquote>\r\n<p><strong>MRS X:</strong> I had a nasty turn last week. I was standing in a queue waiting for my turn to go into the cinema when I felt ever so queer. Really, I thought I should faint or something.<br /><br /><em>[Mrs X is attempting to raise her status by having an interesting medical problem. Mrs Y immediately outdoes her.]<br /></em></p>\r\n<p><strong>MRS Y:</strong> You're lucky to have been going to a cinema. If I thought I could go to a cinema I should think I had nothing to complain of at all.<br /><br /><em>[Mrs Z now blocks Mrs Y.]</em><br /><br /><strong>MRS Z: </strong>I know what Mrs X means. I feel just like that myself, only I should have had to leave the queue.<br /><br /><em>[Mrs Z is very talented in that she supports Mrs X against Mrs Y while at the same time claiming to be more worthy of interest, her condition more severe. Mr A now intervenes to lower them all by making their condition seem very ordinary.]</em><br /><br /><strong>MR A:</strong> Have you tried stooping down? That makes the blood come back to your head. I expect you were feeling faint.<br /><br /><em>[Mrs X defends herself.]</em><br /><br /><strong>MRS X:</strong> It's not really faint.<br /><br /><strong>MRS Y: </strong>I always find it does a lot of good to try exercises. I don't know if that's what Mr A means.<br /><br /><em>[She seems to be joining forces with Mr A, but implies that he was unable to say what he meant. She doesn't say 'Is that what you mean?' but protects herself by her typically high-status circumlocution. Mrs Z now lowers everybody, and immediately lowers herself to avoid counterattack.]</em><br /><br /><strong>MRS Z: </strong>I think you have to use your will-power. That's what worries me--I haven't got any.<br /><br /><em>[Mr B then intervenes, I suspect in a low-status way, or rather trying to be high-status but failing. It's impossible to be sure from just the words.]</em><br /><br /><strong>MR B: </strong>I had something similar happen to me last week, only I wasn't standing in a queue. I was sitting at home quietly when...<br /><br /><em>[Mr C demolishes him.]</em><br /><br /><strong>MR C: </strong>You were lucky to be sitting at home quietly. If I was able to do that I shouldn't think I had anything to grumble about. If you can't sit at home why don't you go to the cinema or something?</p>\r\n</blockquote>\r\n<p><strong>The level of values.</strong> Here the participants of a discussion are primarily attempting to signal their values. Any statements that on the surface refer to facts actually refer to values. For instance, \"men and women are equally intelligent\" might actually mean \"men and women should be given equal treatment\" while \"there are differences in the intelligence of men and women\" is taken to mean \"it's justified to treat men and women unequally\".</p>\r\n<p><strong>The </strong><strong>level of socialization</strong>, also known as small talk.&nbsp;You aren't really talking about anything, but instead just enjoying the other's company. If the group is seeking to mainly operate on this level, someone trying to operate on the level of facts might get slapped down for perceived aggression if they insist on getting things factually correct.</p>\r\n<p>For rationalists to succeed in spreading our ideas, we need to learn to recognize which level of conversation the discussion is operating on. One person acting on the level of facts and another on the level of values is a conversation that's certain to go nowhere. Also, it took me a while to realize that there have been occasions on which I was consciously <em>trying</em> to act on the level of facts, but my subconscious was operating on the level of status and got very defensive whenever my facts were challenged.</p>\r\n<p>Usually what rationalists would want to do is to move the conversation to the level of facts. Unfortunately, if a person is operating on the level of values, they might perceive this as an underhanded attempt to undermine their values. I'm uncertain of what, exactly, would be the right approach in this kind of a situation. Defusing the level of status seems easier, as people will frequently find their unconscious jockeying for status silly once it's been brought to their conscious attention.</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gHCNhqxuJq2bZ2akb": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gs8bZCmaWqDaus7Dr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 64, "baseScore": 59, "extendedScore": null, "score": 0.00013123544422403462, "legacy": true, "legacyId": "2555", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-24T00:38:44.899Z", "modifiedAt": null, "url": null, "title": "There just has to be something more, you know?", "slug": "there-just-has-to-be-something-more-you-know", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:17.186Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Edq3ZanR22Xtft2x8/there-just-has-to-be-something-more-you-know", "pageUrlRelative": "/posts/Edq3ZanR22Xtft2x8/there-just-has-to-be-something-more-you-know", "linkUrl": "https://www.lesswrong.com/posts/Edq3ZanR22Xtft2x8/there-just-has-to-be-something-more-you-know", "postedAtFormatted": "Wednesday, March 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20There%20just%20has%20to%20be%20something%20more%2C%20you%20know%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThere%20just%20has%20to%20be%20something%20more%2C%20you%20know%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdq3ZanR22Xtft2x8%2Fthere-just-has-to-be-something-more-you-know%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=There%20just%20has%20to%20be%20something%20more%2C%20you%20know%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdq3ZanR22Xtft2x8%2Fthere-just-has-to-be-something-more-you-know", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdq3ZanR22Xtft2x8%2Fthere-just-has-to-be-something-more-you-know", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 807, "htmlBody": "<!-- There just has to be something more, you know? -->\n<p><em>A non-materialist thought experiment.</em></p>\n<p>Okay, so you don't exactly believe in the God of the Abrahamic scriptures verbatim who punishes and sets things on fire and lives in the sky.&nbsp;  But still, there just has to be something more than just matter and energy, doesn't there?&nbsp;  You just feel it.&nbsp;  If you don't, try to remember when you did, or at least empathize with someone you know who does.&nbsp;  After all, you have a mind, you think, you feel &mdash; you <em>feel</em> for crying out loud &mdash; and you must realize <em>that</em> can't be made entirely of things like carbon and hydrogen atoms, which are basically just dots with other dots swirling around them.&nbsp;  Okay, maybe they're waves, but at least sometimes they act like dots.&nbsp;  Start with a few swirling dots&hellip; now add more&hellip; keep going, until it equals love.&nbsp;  It just doesn't seem to capture it.</p>\n<p>In fact, now that you think about it, you <em>know</em> your mind exists.&nbsp;  It's right there: it's you.&nbsp;  Your \"experiencing self\".&nbsp;  Maybe you call it a spirit or soul; I don't want to fix too rigid a description in case it wouldn't quite match your own.&nbsp;  But cogito-ergo-sum, it's definitely there!&nbsp; By contrast, this particle business is just a mathematical concept &mdash; a very smart one, of course &mdash; thought of by scientists to explain and predict a bunch of carefully designed and important measurements.&nbsp;  Yes, it does that extremely well, and you're not downplaying that.&nbsp;  But that doesn't explain how you see blue, or taste strawberry &mdash; something you have <em>direct access</em> to.&nbsp;  Particles might not even exist, if that means anything to say.&nbsp;  It might just be that <em>observation itself</em> follows a mathematical pattern that we can <em>understand better</em> by visualizing dots and waves.&nbsp;  They might not be real.</p>\n<p>So actually, your mind or spirit &mdash; that thing you feel, that <em>you</em> &mdash; is much more certain an extant than scientific \"matter\".&nbsp;  That must be something very important to understand!&nbsp;  Certainly you can tell your mind has different parts to it: hearing, seeing, reasoning, moving, remembering, empathizing, picturing, yearning&hellip;  When you think of all the things you can remember alone &mdash; or <em>could</em> remember &mdash; the complexity of all that data is mindbogglingly vast.&nbsp;  Imagine the task of actually having to take it all apart and describe it completely&hellip; it could take aeons&hellip;   <a id=\"more\"></a></p>\n<p>Imagine then, for a moment, that you could isolate just <em>one</em> part: some relatively insignificant portion of your vast mind or spirit.&nbsp;  Let's say a single, second-long experience of walking with a friend; certainly minute compared to the entirety of your life.&nbsp;  But still, an extremely complex object.&nbsp;  Think about all you are perceiving in that second...&nbsp; your mind is incredible!&nbsp;  No, I'm not talking about your brain, I'm talking about your <em>experiencing self</em>, your <em>mind</em>, your <em>essence</em>, however you might think about that experiencing entity.&nbsp;  Now imagine isolating some small aspect of that memory with your friend, discarding the massively detailed experiences that are your vision, your sense of balance, how hungry you are for nachos&hellip; Say, a concerned awareness of your friend's emotional state at that instant.&nbsp;  This too is a highly complex object, so it too has parts, which I may not be able to describe in finer granularity, but they're there.&nbsp;  Now let's say you're some kind of super-introspective savant, who can sense the conceptual fragments of still finer, sharper aspects of this&hellip;</p>\n<p>I'm doing my best here to approach what \"a tiny piece of your soul\" might mean.&nbsp;  But no matter; perhaps you have a better idea of what that is.&nbsp;  In any case, suppose you somehow isolated this tiny fraction of a mind or spirit, and took it out of the context of all the countless other details we didn't look at.&nbsp;  Now it's disconnected from all that other stuff: vision, balance, nachos, nuances of empathy&hellip;</p>\n<p>Suppose you managed to somehow <em>look</em> at this object, by which I mean observe it in some way &mdash; It is part of <em>your</em> mind, after all &mdash; and consider a possible outcome.  So that we're picturing roughly the same thing, imagine that as you observed it, this piece of your soul is not writhing and thrashing about spasmodically, but appears in fact calm, and focused on its task.&nbsp; Suppose it moved regularly, like maybe in a circle, for example.&nbsp; How curious it could turn out to be!&nbsp; What would we call this tiny, almost infinitesimal speck of your mind?</p>\n<p>I say we call it \"electron\".</p>\n<hr />\n<p>Like many readers of this blog, I am a materialist.&nbsp; Like many still, I was not always.&nbsp;  Long ago, the now-rhetorical ponderings in the preceding post in fact <em>delivered the fatal blow</em> to my nagging suspicion that, somehow, materialism just isn't enough&hellip;</p>\n<p><strong>Finish reading in:</strong> <a href=\"/lw/1z6/the_two_insights_of_materialism/\">The two insights of materialism</a></p>\n<p>(these were originally a single post, so some comments below refer to the sequel.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Edq3ZanR22Xtft2x8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 16, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "2556", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SQoz2pb2ut2x4ZJWo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-24T14:47:12.969Z", "modifiedAt": null, "url": null, "title": "The two insights of materialism", "slug": "the-two-insights-of-materialism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:48.043Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SQoz2pb2ut2x4ZJWo/the-two-insights-of-materialism", "pageUrlRelative": "/posts/SQoz2pb2ut2x4ZJWo/the-two-insights-of-materialism", "linkUrl": "https://www.lesswrong.com/posts/SQoz2pb2ut2x4ZJWo/the-two-insights-of-materialism", "postedAtFormatted": "Wednesday, March 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20two%20insights%20of%20materialism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20two%20insights%20of%20materialism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQoz2pb2ut2x4ZJWo%2Fthe-two-insights-of-materialism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20two%20insights%20of%20materialism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQoz2pb2ut2x4ZJWo%2Fthe-two-insights-of-materialism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQoz2pb2ut2x4ZJWo%2Fthe-two-insights-of-materialism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2223, "htmlBody": "<!-- The two insights of materialism -->\n<p><strong>Preceded by:</strong>&nbsp; <a href=\"/lw/1z0/there_just_has_to_be_something_more_you_know/\">There just has to be something more, you know?</a>&nbsp;  <strong>Followed by:</strong>&nbsp; <a href=\"/lw/29d/physicalism_consciousness_as_the_last_sense/\">Physicalism: consciousness as the last sense.</a></p>\n<p><strong>Contents:</strong>&nbsp; 1. <a href=\"#difficulty\">An epistemic difficulty</a>&nbsp;  2. <a href=\"#how_and_why\">How and why to be a materialist</a></p>\n<h3><a name=\"difficulty\"></a>An epistemic difficulty</h3>\n<p>Like many readers of this blog, I am a materialist.&nbsp; Like many still, I was not always.&nbsp;  Long ago, the now-rhetorical ponderings in the preceding post in fact delivered the fatal blow to my nagging suspicion that <em>somehow, materialism just isn't enough</em>.</p>\n<p>By materialism, I mean the belief that the world and people are composed entirely of something called matter (a.k.a.&nbsp; energy), which physics currently best understands as consisting of particles (a.k.a.&nbsp; waves).&nbsp;  If physics reformulates these notions, materialism can adjust with it, leading some to prefer the term \"physicalism\".</p>\n<p>Now, I encounter people all the time who, because of education or disillusionment, have abandoned most aspects of religion, yet still believe in more than one than one kind of reality.&nbsp;  It's often called \"being spiritual\".&nbsp;  People often think it feels better than the alternative (see <a href=\"/lw/or/joy_in_the_merely_real/\" target=\"_blank\">Joy in the merely real</a>), but it also persists for what people experience as an  epistemic concern:</p>\n<p><strong>The inability to reconcile the \"experiencing self\" concept with one's notion of physical reality.</strong> <a id=\"more\"></a></p>\n<p>This is among the the most common epistemic discomforts with materialism (I only say \"discomfort\", because a blank spot on your map <a href=\"http://wiki.lesswrong.com/wiki/Mind_projection_fallacy\" target=\"_blank\">does not correspond to a blank territory</a>).&nbsp;  The inside view &mdash; introspection &mdash; shows us something people call a \"mind\" or \"spirit\", and the outside view &mdash; our eyes &mdash; shows us something we call a \"brain\", which looks nothing at all the same.&nbsp;  But the perceived distance between these concepts signals that connecting them would be extremely meaningful, the way superficially unrelated hypotheses and conclusions make for a very powerful theorem.&nbsp;  For the connection to start making sense, one must realize that \"you are made of matter\" is as much a statement about matter as a statment about you&hellip;</p>\n<p><strong>The two insights of materialism: That the reconciliation of mind and matter &ndash;</strong></p>\n<ol>\n<li>is not misinformation about mind, but <em>extra information about matter</em>, and </li>\n<li>is not misinformation about matter, but <em>extra information about mind</em>. </li>\n</ol>\n<p>These are really two insights, and underusing one of them leaves a sense of \"doesn't quite capture it\" in the psyche.&nbsp;  See, the way most people think or learn about physics, a particle is a tiny dot, with some attributes like charge specified by numbers, obeying certain laws of motion.&nbsp; But in fact, this is a <em>model</em> of a particle.&nbsp; As a conviction, physics need not claim that \"dots and waves are <em>all there is</em>\", but rather, that <em>all there is</em> can be <em>described on analogy</em> with dots and waves.&nbsp; Science is about modelling &mdash; a <a href=\"http://wiki.lesswrong.com/wiki/Rationality\" target=\"_blank\">map that matches the territory</a> &mdash; and \"truth\" is just how well it matches up.</p>\n<p>And given modern science, there is <em>something more</em> you can say about a particle besides the geometry and equations that describe it, something which connects it to the direct, cogito-ergo-sum style knowledge we all enjoy: whatever it is, a particle is a one thousand-trillion-trillionth of a <em>you.&nbsp; </em>Yes, you, in your entirety.&nbsp;  If part of that includes something you call a \"soul\", then yes, science can now model the quantitative aspects, in more or less complete detail, of a one thousand-trillion-trillionth of a \"soul\".&nbsp;  Is that too much?&nbsp;  Too incredible?&nbsp;  A song by <em>The Books</em> that I like almost says it perfectly:</p>\n<p><strong>You are something that the whole world is made of.</strong></p>\n<p>This moots the debate.&nbsp; The first step is not to \"reduce\" the introspective view to the extrospective view, but to realize that they're looking at the same object.&nbsp;  The assertion is not that \"mind is just particles\", but rather that \"a tiny fraction of a mind\" and \"a tiny fraction of matter\" happen to refer to the same object, and we should agree to call that object \"particle\".&nbsp;  Depending on how you use the word \"conscious\", this does not necessarily say that a particle is conscious in the way that you are; an octant of a sphere is not a sphere.&nbsp;  But assembled correctly, it is certainly one-eighth of a sphere!</p>\n<p>I've learned that some people call this view \"neutral monism\", but I prefer to still call it materialism as an emphasis that the extrospective view \"science\" really has a larger <a target=\"_blank\">quantity of information</a> at this point in human history.&nbsp;  This is <em>different information about reality</em> than provided by introspection, and to ignore it is <em>detrimental</em> to one's world view!&nbsp;</p>\n<p>So, to help non-materialists in attaining this reconciliation of mind and matter, I've written the following rough path of ideas that one can follow:</p>\n<h3><a name=\"how_and_why\"></a>How and why to be a materialist</h3>\n<ol>\n<li>\n<p><strong>Accepting materialism is saying \"the rest of the world is made of whatever I am\"</strong>, not just \"I am made of whatever the rest of the world is\".&nbsp;  And why not?&nbsp; In the eyes of science, these are both the same, true statement.&nbsp;  Semantically, the first one tells you something <em>qualitative</em> about matter, and the second one tells you something <em>extremely quantitative</em> about your mind!&nbsp;  It means modern neuroscience and biology can be used to help you understand yourself.&nbsp;  Awesome!</p>\n</li>\n<li>\n<p><strong>Accepting physics is accepting that your \"spirit\" might consist of parts which, sufficiently divided and removed from context, might behave in a regular fashion.</strong> Then you might as well call the parts \"particles\" and call your spirit \"brain\", and look at all the amazing data we have about them that help describe how you work.</p>\n</li>\n<li>\n<p><strong>Beware of the works-how-it-feels bias</strong>, the fallacious additional assumption that <em>the world works the way you feel about it</em>.&nbsp;  (See <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\" target=\"_blank\">How an algorithm feels from the inside</a>.)  These pieces of your mind/spirit called particles are extremely tiny; in order of magnitude, they are more than twice as small as your deepest introspection, so you can't judge them very well based on instinct (a neuron is about a 10<sup>11</sup>th of your mind, and an atom is about a 10<sup>14</sup>th of a neuron).&nbsp;  And because they're so tiny and numerous, they can be put together to form things vastly different from yourself in form and function, like plants and stars.</p>\n</li>\n<li><strong>Your instinct that the laws of physics <em>don't fully describe you</em> is correct!</strong> You are the way you are because of two things: \n<ul>\n<li> the laws that describe your soul-pieces or particles, whatever those laws may be, and </li>\n<li> the way they're put together, </li>\n</ul>\n<p>and the latter is almost <em>unimaginably</em> more significant!&nbsp;  One way to see this is to look around at all the things that are not you.&nbsp;  <em>Saying how the tiny bits of your soul behave independently does not describe how to put them together,</em> just like describing an octant of a sphere doesn't explain say how to turn eight of them into a whole sphere.&nbsp;  Plus, even after your initial construction as a baby, a whole lot of growth and experience has configured what you are today.</p>\n<p>Only to put this into perspective, consider that the all the most fundamental laws of physics know can certainly be written down, without evidence or much explanation, in a text file of less than 1 megabyte.&nbsp;  The <a href=\"/lw/1y9/information_theory_and_the_symmetry_of_updating/\" target=\"_blank\">information content</a> of the human genome, which so far seems necessary to construct a sustainable brain, is about 640 MB (<a href=\"http://en.wikipedia.org/wiki/Huffman_coding\" target=\"_blank\">efficiently encoded</a>, that's 1.7 bits per nucleotide pair).&nbsp;  Don't be fooled at how \"small\" 640 is: it means the number of possible states of that data is at least 8<sup>640</sup> times larger than the number of the states of our text file describing all of physics!&nbsp;  Next, the brain itself stores information as you develop, with a capacity of at least 1 terrabyte by the most conservative estimates, which means it has at least around 8<sup>1500</sup> times the number of possible states of the DNA sequence that first built it.</p>\n<p>So being a desk is different from being a human, not because it's made of different stuff, but because the stuff is put together <em>extremely differently</em>, more differently than we can fully imagine.&nbsp;  When people say form determines function, they should say FORM in BIG CAPITAL LETTERS.&nbsp;  No wonder you thought particle physics \"just doesn't seem to capture it\"!</p>\n</li>\n<li><strong>Your perceived distance between the <em>concepts</em> of \"mind\" and \"particles\" is also correct!</strong> As JanetK says, \"There is no shortcut from electrons to thoughts\".&nbsp;  Continuing the connection/theorem analogy, a theorem with superficially unrelated hypotheses and conclusions is not only liable to be very useful, but to have a difficult proof as well.&nbsp;  The analogue of the <em>difficult proof</em> is that, distinct from the discovery of particles themselves, massive amounts of technological progress and research have been required to establish the connection between: \n<ul>\n<li>how particles work and how you look from the outside (neurochemistry/neurobiology), and </li>\n<li>how you look from the outside and how you look from the inside (neuropsychology). </li>\n</ul>\nIndeed, the perceptual distance between the second pair is why people use the concepts \"brain\" and \"mind\" separately: \"brain\" is a model for the outside view, and \"mind\" is a model for the inside view.&nbsp;  The analogue of the theorem's <em>usefulness</em> is how much neurology can say and do about our minds:  \n<ul>\n<li>treat mental illness, </li>\n<li>restore lost memories, </li>\n<li>design brain surgery, </li>\n<li>explain cognitive biases, </li>\n<li>physically relate our emotions to each other&hellip; </li>\n</ul>\n</li>\n<li>\n<p><strong>Adjusting emotionally is extremely important</strong> as you bring materialism under consideration, not only to accomodate changing your beliefs, but to cope with them when they do change.&nbsp;  You may need to <em>redescribe</em> morality, what makes you happy, and why you want to be alive, but none of these things needs to be <em>revoked</em>, and LessWrong is by far the best community I've ever seen to help with this transition.&nbsp;  For example, Eliezer has written a chronological account of his <a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky's_coming_of_age\" target=\"_blank\">Coming of Age</a> as a rationalist, and he has certainly maintained a sense of morality and life-worth.&nbsp;  I recommend building an appropriate emotional safety net <em>while</em> you consider materialism, not just to combat the bias of fear, but so you're ready when one day you realize <em>oh my gosh I'm a materialist!</em></p>\n</li>\n</ol>\n<p>I have a friend who says that instead of classifying people as believing in the material or the supernatural, he classifies them by whether they think more than one of those things exists and are different.&nbsp;   Roughly speaking, dualists and non-dualists.&nbsp;  I think he's got the right idea.&nbsp;  Why bother believing in more than one kind of thing?&nbsp;  Why believe in separate \"soul\" and \"material\" if the world can just as well be made of tiny specks of regularly-behaved \"spirit\"?&nbsp; It's the same theory, and watching out for works-how-it-feels bias, you gain a lot of tangible insight about yourself if you <em>realize</em> they're the same.</p>\n<p>So do what's right.&nbsp;  Right for you, right for your loved ones, and right for rightness itself if that matters to you.&nbsp;  You probably already know what it means to be a good person, and your good intentions just won't work if you use poor judgement.&nbsp;  Start thinking about materialism so you can know more, and make better, well-informed decisions.&nbsp;</p>\n<p><em>Who believes in the supernatural is simply underestimating the natural.</em></p>\n<p>There is no something more, because there is no something less&hellip; but there certainly and most definitely is <em>you</em>.&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p><strong>Follow up</strong> to comments:</p>\n<p>One can only get so far from dualism in a single sitting, and what this article includes is a much a function of my time as of its validity.&nbsp;   For now I'll leave it up to others to argue stronger positions than those presented here, but to acknowledge, some important issues I did not address include:</p>\n<p>Whatever stuff or process the world comprises, is it merely <em>accessible</em> to physics, or can physics describe its nature entirely?&nbsp;  And supposing it can, is consciousness an entirely mathematical phenomenon that is unaffected by how it is physically implemented?&nbsp;  That is, if we made a neural network computationally isomorphic to the human brain, but in a different physical arrangement (e.g.&nbsp; a silicon based computer), should you be as certain of its consciousness as of the consciousness of other humans?&nbsp;  And more questions...</p>\n<p>A rough outline of some stances on the questions above is as follows: (to avoid debate I'll omit the term <a href=\"http://plato.stanford.edu/entries/naturalism/\" target=\"_blank\">naturalism</a>, though I do approve of its normative use)</p>\n<p><a href=\"http://plato.stanford.edu/entries/monism/\" target=\"_blank\">Monism</a>: the world comprises just one genre of stuff or process (no natural/supernatural distinction).</p>\n<p><span style=\"text-decoration: underline;\">This article</span>: this stuff or process is <em>physically accessible</em>, and is therefore amenable to study by the natural sciences.</p>\n<p><a href=\"http://plato.stanford.edu/entries/physicalism/\" target=\"_blank\">Physicalism</a>: the stuff or process is <em>no more extensive</em> than its description in terms of physics.</p>\n<p><a href=\"http://plato.stanford.edu/entries/computational-mind/\" target=\"_blank\">Computationalism</a>: consciousness is a <em>mathematical phenomenon</em>, unaffected by how it is physically represented or implemented.</p>\n<p><em>And</em> of course it is also important to question whether these distinctions are practical, meaningful, or merely illusory.&nbsp;  It all needs to be cleaned and carefully disected.&nbsp;  Have at it, LessWrong!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SQoz2pb2ut2x4ZJWo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 23, "extendedScore": null, "score": 5.718862058540839e-07, "legacy": true, "legacyId": "2562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<!-- The two insights of materialism -->\n<p><strong>Preceded by:</strong>&nbsp; <a href=\"/lw/1z0/there_just_has_to_be_something_more_you_know/\">There just has to be something more, you know?</a>&nbsp;  <strong>Followed by:</strong>&nbsp; <a href=\"/lw/29d/physicalism_consciousness_as_the_last_sense/\">Physicalism: consciousness as the last sense.</a></p>\n<p><strong>Contents:</strong>&nbsp; 1. <a href=\"#difficulty\">An epistemic difficulty</a>&nbsp;  2. <a href=\"#how_and_why\">How and why to be a materialist</a></p>\n<h3 id=\"An_epistemic_difficulty\"><a name=\"difficulty\"></a>An epistemic difficulty</h3>\n<p>Like many readers of this blog, I am a materialist.&nbsp; Like many still, I was not always.&nbsp;  Long ago, the now-rhetorical ponderings in the preceding post in fact delivered the fatal blow to my nagging suspicion that <em>somehow, materialism just isn't enough</em>.</p>\n<p>By materialism, I mean the belief that the world and people are composed entirely of something called matter (a.k.a.&nbsp; energy), which physics currently best understands as consisting of particles (a.k.a.&nbsp; waves).&nbsp;  If physics reformulates these notions, materialism can adjust with it, leading some to prefer the term \"physicalism\".</p>\n<p>Now, I encounter people all the time who, because of education or disillusionment, have abandoned most aspects of religion, yet still believe in more than one than one kind of reality.&nbsp;  It's often called \"being spiritual\".&nbsp;  People often think it feels better than the alternative (see <a href=\"/lw/or/joy_in_the_merely_real/\" target=\"_blank\">Joy in the merely real</a>), but it also persists for what people experience as an  epistemic concern:</p>\n<p><strong>The inability to reconcile the \"experiencing self\" concept with one's notion of physical reality.</strong> <a id=\"more\"></a></p>\n<p>This is among the the most common epistemic discomforts with materialism (I only say \"discomfort\", because a blank spot on your map <a href=\"http://wiki.lesswrong.com/wiki/Mind_projection_fallacy\" target=\"_blank\">does not correspond to a blank territory</a>).&nbsp;  The inside view \u2014 introspection \u2014 shows us something people call a \"mind\" or \"spirit\", and the outside view \u2014 our eyes \u2014 shows us something we call a \"brain\", which looks nothing at all the same.&nbsp;  But the perceived distance between these concepts signals that connecting them would be extremely meaningful, the way superficially unrelated hypotheses and conclusions make for a very powerful theorem.&nbsp;  For the connection to start making sense, one must realize that \"you are made of matter\" is as much a statement about matter as a statment about you\u2026</p>\n<p><strong id=\"The_two_insights_of_materialism__That_the_reconciliation_of_mind_and_matter__\">The two insights of materialism: That the reconciliation of mind and matter \u2013</strong></p>\n<ol>\n<li>is not misinformation about mind, but <em>extra information about matter</em>, and </li>\n<li>is not misinformation about matter, but <em>extra information about mind</em>. </li>\n</ol>\n<p>These are really two insights, and underusing one of them leaves a sense of \"doesn't quite capture it\" in the psyche.&nbsp;  See, the way most people think or learn about physics, a particle is a tiny dot, with some attributes like charge specified by numbers, obeying certain laws of motion.&nbsp; But in fact, this is a <em>model</em> of a particle.&nbsp; As a conviction, physics need not claim that \"dots and waves are <em>all there is</em>\", but rather, that <em>all there is</em> can be <em>described on analogy</em> with dots and waves.&nbsp; Science is about modelling \u2014 a <a href=\"http://wiki.lesswrong.com/wiki/Rationality\" target=\"_blank\">map that matches the territory</a> \u2014 and \"truth\" is just how well it matches up.</p>\n<p>And given modern science, there is <em>something more</em> you can say about a particle besides the geometry and equations that describe it, something which connects it to the direct, cogito-ergo-sum style knowledge we all enjoy: whatever it is, a particle is a one thousand-trillion-trillionth of a <em>you.&nbsp; </em>Yes, you, in your entirety.&nbsp;  If part of that includes something you call a \"soul\", then yes, science can now model the quantitative aspects, in more or less complete detail, of a one thousand-trillion-trillionth of a \"soul\".&nbsp;  Is that too much?&nbsp;  Too incredible?&nbsp;  A song by <em>The Books</em> that I like almost says it perfectly:</p>\n<p><strong id=\"You_are_something_that_the_whole_world_is_made_of_\">You are something that the whole world is made of.</strong></p>\n<p>This moots the debate.&nbsp; The first step is not to \"reduce\" the introspective view to the extrospective view, but to realize that they're looking at the same object.&nbsp;  The assertion is not that \"mind is just particles\", but rather that \"a tiny fraction of a mind\" and \"a tiny fraction of matter\" happen to refer to the same object, and we should agree to call that object \"particle\".&nbsp;  Depending on how you use the word \"conscious\", this does not necessarily say that a particle is conscious in the way that you are; an octant of a sphere is not a sphere.&nbsp;  But assembled correctly, it is certainly one-eighth of a sphere!</p>\n<p>I've learned that some people call this view \"neutral monism\", but I prefer to still call it materialism as an emphasis that the extrospective view \"science\" really has a larger <a target=\"_blank\">quantity of information</a> at this point in human history.&nbsp;  This is <em>different information about reality</em> than provided by introspection, and to ignore it is <em>detrimental</em> to one's world view!&nbsp;</p>\n<p>So, to help non-materialists in attaining this reconciliation of mind and matter, I've written the following rough path of ideas that one can follow:</p>\n<h3 id=\"How_and_why_to_be_a_materialist\"><a name=\"how_and_why\"></a>How and why to be a materialist</h3>\n<ol>\n<li>\n<p><strong>Accepting materialism is saying \"the rest of the world is made of whatever I am\"</strong>, not just \"I am made of whatever the rest of the world is\".&nbsp;  And why not?&nbsp; In the eyes of science, these are both the same, true statement.&nbsp;  Semantically, the first one tells you something <em>qualitative</em> about matter, and the second one tells you something <em>extremely quantitative</em> about your mind!&nbsp;  It means modern neuroscience and biology can be used to help you understand yourself.&nbsp;  Awesome!</p>\n</li>\n<li>\n<p><strong>Accepting physics is accepting that your \"spirit\" might consist of parts which, sufficiently divided and removed from context, might behave in a regular fashion.</strong> Then you might as well call the parts \"particles\" and call your spirit \"brain\", and look at all the amazing data we have about them that help describe how you work.</p>\n</li>\n<li>\n<p><strong>Beware of the works-how-it-feels bias</strong>, the fallacious additional assumption that <em>the world works the way you feel about it</em>.&nbsp;  (See <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\" target=\"_blank\">How an algorithm feels from the inside</a>.)  These pieces of your mind/spirit called particles are extremely tiny; in order of magnitude, they are more than twice as small as your deepest introspection, so you can't judge them very well based on instinct (a neuron is about a 10<sup>11</sup>th of your mind, and an atom is about a 10<sup>14</sup>th of a neuron).&nbsp;  And because they're so tiny and numerous, they can be put together to form things vastly different from yourself in form and function, like plants and stars.</p>\n</li>\n<li><strong>Your instinct that the laws of physics <em>don't fully describe you</em> is correct!</strong> You are the way you are because of two things: \n<ul>\n<li> the laws that describe your soul-pieces or particles, whatever those laws may be, and </li>\n<li> the way they're put together, </li>\n</ul>\n<p>and the latter is almost <em>unimaginably</em> more significant!&nbsp;  One way to see this is to look around at all the things that are not you.&nbsp;  <em>Saying how the tiny bits of your soul behave independently does not describe how to put them together,</em> just like describing an octant of a sphere doesn't explain say how to turn eight of them into a whole sphere.&nbsp;  Plus, even after your initial construction as a baby, a whole lot of growth and experience has configured what you are today.</p>\n<p>Only to put this into perspective, consider that the all the most fundamental laws of physics know can certainly be written down, without evidence or much explanation, in a text file of less than 1 megabyte.&nbsp;  The <a href=\"/lw/1y9/information_theory_and_the_symmetry_of_updating/\" target=\"_blank\">information content</a> of the human genome, which so far seems necessary to construct a sustainable brain, is about 640 MB (<a href=\"http://en.wikipedia.org/wiki/Huffman_coding\" target=\"_blank\">efficiently encoded</a>, that's 1.7 bits per nucleotide pair).&nbsp;  Don't be fooled at how \"small\" 640 is: it means the number of possible states of that data is at least 8<sup>640</sup> times larger than the number of the states of our text file describing all of physics!&nbsp;  Next, the brain itself stores information as you develop, with a capacity of at least 1 terrabyte by the most conservative estimates, which means it has at least around 8<sup>1500</sup> times the number of possible states of the DNA sequence that first built it.</p>\n<p>So being a desk is different from being a human, not because it's made of different stuff, but because the stuff is put together <em>extremely differently</em>, more differently than we can fully imagine.&nbsp;  When people say form determines function, they should say FORM in BIG CAPITAL LETTERS.&nbsp;  No wonder you thought particle physics \"just doesn't seem to capture it\"!</p>\n</li>\n<li><strong>Your perceived distance between the <em>concepts</em> of \"mind\" and \"particles\" is also correct!</strong> As JanetK says, \"There is no shortcut from electrons to thoughts\".&nbsp;  Continuing the connection/theorem analogy, a theorem with superficially unrelated hypotheses and conclusions is not only liable to be very useful, but to have a difficult proof as well.&nbsp;  The analogue of the <em>difficult proof</em> is that, distinct from the discovery of particles themselves, massive amounts of technological progress and research have been required to establish the connection between: \n<ul>\n<li>how particles work and how you look from the outside (neurochemistry/neurobiology), and </li>\n<li>how you look from the outside and how you look from the inside (neuropsychology). </li>\n</ul>\nIndeed, the perceptual distance between the second pair is why people use the concepts \"brain\" and \"mind\" separately: \"brain\" is a model for the outside view, and \"mind\" is a model for the inside view.&nbsp;  The analogue of the theorem's <em>usefulness</em> is how much neurology can say and do about our minds:  \n<ul>\n<li>treat mental illness, </li>\n<li>restore lost memories, </li>\n<li>design brain surgery, </li>\n<li>explain cognitive biases, </li>\n<li>physically relate our emotions to each other\u2026 </li>\n</ul>\n</li>\n<li>\n<p><strong>Adjusting emotionally is extremely important</strong> as you bring materialism under consideration, not only to accomodate changing your beliefs, but to cope with them when they do change.&nbsp;  You may need to <em>redescribe</em> morality, what makes you happy, and why you want to be alive, but none of these things needs to be <em>revoked</em>, and LessWrong is by far the best community I've ever seen to help with this transition.&nbsp;  For example, Eliezer has written a chronological account of his <a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky's_coming_of_age\" target=\"_blank\">Coming of Age</a> as a rationalist, and he has certainly maintained a sense of morality and life-worth.&nbsp;  I recommend building an appropriate emotional safety net <em>while</em> you consider materialism, not just to combat the bias of fear, but so you're ready when one day you realize <em>oh my gosh I'm a materialist!</em></p>\n</li>\n</ol>\n<p>I have a friend who says that instead of classifying people as believing in the material or the supernatural, he classifies them by whether they think more than one of those things exists and are different.&nbsp;   Roughly speaking, dualists and non-dualists.&nbsp;  I think he's got the right idea.&nbsp;  Why bother believing in more than one kind of thing?&nbsp;  Why believe in separate \"soul\" and \"material\" if the world can just as well be made of tiny specks of regularly-behaved \"spirit\"?&nbsp; It's the same theory, and watching out for works-how-it-feels bias, you gain a lot of tangible insight about yourself if you <em>realize</em> they're the same.</p>\n<p>So do what's right.&nbsp;  Right for you, right for your loved ones, and right for rightness itself if that matters to you.&nbsp;  You probably already know what it means to be a good person, and your good intentions just won't work if you use poor judgement.&nbsp;  Start thinking about materialism so you can know more, and make better, well-informed decisions.&nbsp;</p>\n<p><em>Who believes in the supernatural is simply underestimating the natural.</em></p>\n<p>There is no something more, because there is no something less\u2026 but there certainly and most definitely is <em>you</em>.&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p><strong>Follow up</strong> to comments:</p>\n<p>One can only get so far from dualism in a single sitting, and what this article includes is a much a function of my time as of its validity.&nbsp;   For now I'll leave it up to others to argue stronger positions than those presented here, but to acknowledge, some important issues I did not address include:</p>\n<p>Whatever stuff or process the world comprises, is it merely <em>accessible</em> to physics, or can physics describe its nature entirely?&nbsp;  And supposing it can, is consciousness an entirely mathematical phenomenon that is unaffected by how it is physically implemented?&nbsp;  That is, if we made a neural network computationally isomorphic to the human brain, but in a different physical arrangement (e.g.&nbsp; a silicon based computer), should you be as certain of its consciousness as of the consciousness of other humans?&nbsp;  And more questions...</p>\n<p>A rough outline of some stances on the questions above is as follows: (to avoid debate I'll omit the term <a href=\"http://plato.stanford.edu/entries/naturalism/\" target=\"_blank\">naturalism</a>, though I do approve of its normative use)</p>\n<p><a href=\"http://plato.stanford.edu/entries/monism/\" target=\"_blank\">Monism</a>: the world comprises just one genre of stuff or process (no natural/supernatural distinction).</p>\n<p><span style=\"text-decoration: underline;\">This article</span>: this stuff or process is <em>physically accessible</em>, and is therefore amenable to study by the natural sciences.</p>\n<p><a href=\"http://plato.stanford.edu/entries/physicalism/\" target=\"_blank\">Physicalism</a>: the stuff or process is <em>no more extensive</em> than its description in terms of physics.</p>\n<p><a href=\"http://plato.stanford.edu/entries/computational-mind/\" target=\"_blank\">Computationalism</a>: consciousness is a <em>mathematical phenomenon</em>, unaffected by how it is physically represented or implemented.</p>\n<p><em>And</em> of course it is also important to question whether these distinctions are practical, meaningful, or merely illusory.&nbsp;  It all needs to be cleaned and carefully disected.&nbsp;  Have at it, LessWrong!</p>", "sections": [{"title": "An epistemic difficulty", "anchor": "An_epistemic_difficulty", "level": 1}, {"title": "The two insights of materialism: That the reconciliation of mind and matter \u2013", "anchor": "The_two_insights_of_materialism__That_the_reconciliation_of_mind_and_matter__", "level": 2}, {"title": "You are something that the whole world is made of.", "anchor": "You_are_something_that_the_whole_world_is_made_of_", "level": 2}, {"title": "How and why to be a materialist", "anchor": "How_and_why_to_be_a_materialist", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "134 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 134, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Edq3ZanR22Xtft2x8", "r9vpZQWQzrWFkuHx9", "x4dG4GhpZH2hgz59x", "yA4gF5KrboK2m2Xu7", "SEZqJcSm25XpQMhzr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-24T17:38:09.105Z", "modifiedAt": null, "url": null, "title": "The \"show, don't tell\" nature of argument", "slug": "the-show-don-t-tell-nature-of-argument", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:56.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gfkhZWzWrr4hQdxJY/the-show-don-t-tell-nature-of-argument", "pageUrlRelative": "/posts/gfkhZWzWrr4hQdxJY/the-show-don-t-tell-nature-of-argument", "linkUrl": "https://www.lesswrong.com/posts/gfkhZWzWrr4hQdxJY/the-show-don-t-tell-nature-of-argument", "postedAtFormatted": "Wednesday, March 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20%22show%2C%20don't%20tell%22%20nature%20of%20argument&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20%22show%2C%20don't%20tell%22%20nature%20of%20argument%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfkhZWzWrr4hQdxJY%2Fthe-show-don-t-tell-nature-of-argument%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20%22show%2C%20don't%20tell%22%20nature%20of%20argument%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfkhZWzWrr4hQdxJY%2Fthe-show-don-t-tell-nature-of-argument", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfkhZWzWrr4hQdxJY%2Fthe-show-don-t-tell-nature-of-argument", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 663, "htmlBody": "<p>Consider a statement of the form \"based on knowledge and common sense and estimating the probabilities of alternative hypotheses, I believe that X\". Or, perhaps, \"based on Bayesian reasoning I have come to the conclusion X\". How much, when an interlocutor of yours uses such a statement, should it affect your credence in X?</p>\n<p>I claim that the answer is \"negligibly\", in most cases. (I discuss exceptions below.)</p>\n<p>The most appropriate way of conducting an argument is in most cases to <strong>simply state your belief</strong> X. If you are going to adduce evidence and argument in support of your belief X, simply state these. If you've derived probability estimates, simply provide them, perhaps with the method of derivation. The \"meta\" observation by itself <em>has no place in an argument</em>.</p>\n<p>(Isn't that obvious, you might ask? Not to everyone, as this statement form turns out to be <a href=\"/lw/1ww/undiscriminating_skepticism/1t2j\">actually used</a> in discussions here. So, in the interest of contributing even a little to a possible <a href=\"/lw/1wu/reasoning_isnt_about_logic_its_about_arguing/1r43\">theory of argumentation</a>, I develop some further observations on this pattern.)</p>\n<p><a id=\"more\"></a></p>\n<p>The assertion \"my opinions on X are soundly arrived at\" has an equivalent structure to saying \"I am a truthful person\": it cannot be verified by an interlocutor, other than (in the latter case) by looking at what statements you utter and independently verifying they are truthful, or (in the first) by looking at what arguments and evidence you adduce in support of your opinions on topic X, and independently assessing whether the evidence is credible and the arguments are individually sound.</p>\n<p><em>Claiming to be a truthful person</em> doesn't mean much. I know parents who are constantly telling their kids \"oh, you shouldn't lie, it's bad\", but who actually lie to their kids quite often. My own approach has been to tell my kids the truth (and to answer pretty much all their questions), while almost never mentioning \"truth\" as a moral topic. Empirically, I observe that my kids are growing to be truthful and trustworthy persons, more so than many of e.g. my neighbours' kids.</p>\n<p><em>Claiming that you use sound methods</em> in arriving at a conclusion of which you want to convince your interlocutor similarly has little content. It is the sort of thing that can only be assessed by looking at your justifications for belief. The claim may (if stated in an authoritative tone) actually manage to nudge your interlocutor a little in the direction of accepting your conclusions. On reflection, it shouldn't, and your argumentative skills would benefit from refusing to accept such statements as justification.</p>\n<p>Rather than say \"there is evidence\", point to evidence. Rather than say, \"it can be argued that\", simply argue it. If there are allowable exceptions, they concern cases where you say something in addition to the bare assertion of soundness: for instance if you say \"there is lots of evidence, readily obtainable from everyday sources\". Your claim is above and beyond a claim of soundness.</p>\n<p>You should be prepared, if challenged, to back up your observation about the abundance and availability of the evidence. If you say of something that it's \"<a href=\"/lw/1yi/the_scourge_of_perversemindedness/1sky\">obvious</a>\", it had better <em>be</em> obvious.</p>\n<p>Sometimes too, the use of a given methodology to arrive at a belief X is <em>surprising information in and of itself</em>. It makes a difference to an interlocutor to know that your belief has that origin. It is hard to think of examples where it could possibly make a difference to say that in very vague and general terms, but I can think of cases where the use of a specific method is new and surprising. For instance, if you say \"I believe in X and Bayesian reasoning (as opposed to other methodologies) demonstrates X in a particularly convincing manner\". In this case the claim is not only about X, it is also a reasonably interesting claim about the method itself. (Which claim might or might not stand up to examination -&nbsp; cf. the ongoing argument about frequentism vs. bayesianism.)</p>\n<p>These exceptions aside, sound argumentation shares one precept with fiction: <strong>show, don't tell</strong>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gfkhZWzWrr4hQdxJY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 22, "extendedScore": null, "score": 5.719196166667292e-07, "legacy": true, "legacyId": "2563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-24T18:44:43.628Z", "modifiedAt": null, "url": null, "title": "An empirical test of anthropic principle / great filter reasoning", "slug": "an-empirical-test-of-anthropic-principle-great-filter", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:46.038Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SwwCYwzWf7id7gjZm/an-empirical-test-of-anthropic-principle-great-filter", "pageUrlRelative": "/posts/SwwCYwzWf7id7gjZm/an-empirical-test-of-anthropic-principle-great-filter", "linkUrl": "https://www.lesswrong.com/posts/SwwCYwzWf7id7gjZm/an-empirical-test-of-anthropic-principle-great-filter", "postedAtFormatted": "Wednesday, March 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20empirical%20test%20of%20anthropic%20principle%20%2F%20great%20filter%20reasoning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20empirical%20test%20of%20anthropic%20principle%20%2F%20great%20filter%20reasoning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSwwCYwzWf7id7gjZm%2Fan-empirical-test-of-anthropic-principle-great-filter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20empirical%20test%20of%20anthropic%20principle%20%2F%20great%20filter%20reasoning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSwwCYwzWf7id7gjZm%2Fan-empirical-test-of-anthropic-principle-great-filter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSwwCYwzWf7id7gjZm%2Fan-empirical-test-of-anthropic-principle-great-filter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 498, "htmlBody": "<p>If our civilization doesn&rsquo;t collapse then in 50 to 1000 years humanity will almost certainly start colonizing the galaxy.&nbsp; This seems inconsistent with the fact that there are a huge number of other planets in our galaxy but we have not yet found evidence of extraterrestrial life.&nbsp; Drawing from this paradox, Robin Hanson writes that there should be some <a href=\"http://hanson.gmu.edu/greatfilter.html\">great filter</a> &ldquo;between death and expanding lasting life, and humanity faces the ominous question: how far along this filter are we?&rdquo;</p>\r\n<p><a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">Katja Grace reasons</a> that this filter probably lies in our future and so we are likely doomed.&nbsp;&nbsp; (<a href=\"http://www.overcomingbias.com/2010/03/very-bad-news.html\">Hanson agrees</a> with Grace&rsquo;s argument.)&nbsp; Please read <a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">Grace&rsquo;s post</a> before reading the rest of this post.</p>\r\n<p>&nbsp;</p>\r\n<p>Small groups of humans have been in situations similar to that currently faced by our entire species.&nbsp; To see this imagine you live in a prehistoric hunter gatherer tribe of 200 people.&nbsp; Your tribe lives on a large, mostly uninhabited island.&nbsp; Your grandparents, along with a few other people came over to the island about 30 years ago.&nbsp; Since arriving on the island no one in your tribe has encountered any other humans.&nbsp;&nbsp; You figure that if your civilization doesn&rsquo;t get wiped out then in 100 or so years your people will multiply in number and spread throughout the island.&nbsp; If your civilization does so spread, then any new immigrants to the island would quickly encounter your tribe.</p>\r\n<p>Why, you wonder, have you not seen other groups of humans on your island?&nbsp; You figure that it&rsquo;s either because your tribe was the first to arrive on the island, or because your island is prone to extinction disasters that periodically wipes out all its human inhabitance.&nbsp; Using anthropic reasoning similar to that used by Katja Grace you postulate the existence of four types of islands:</p>\r\n<p>1)&nbsp; Islands that are easy to reach and are not prone to disaster.<br />2)&nbsp; Islands that are hard to reach and are not prone to disaster.<br />3)&nbsp; Islands that are easy to reach and are prone to disaster.<br />4)&nbsp; Islands that are hard to reach and are prone to disaster.</p>\r\n<p>You conclude that your island almost certainly isn&rsquo;t type (1) because if it were you would have almost certainly seen other humans on the island.&nbsp;&nbsp; You also figure that on type (3) islands lots of civilizations will start and then be destroyed.&nbsp; Consequently, most of the civilizations that find themselves on types (2), (3) or (4) islands are in fact on type (3) islands.&nbsp; Your tribe, you therefore reason, will probably be wiped out by a disaster.</p>\r\n<p>I believe that the argument for why you are probably on a type (3) island is analogous to that of why the great filter probably lies in our future because both come about from updating <a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">&ldquo;on your own existence by weighting possible worlds as more likely the more observers they contain.&rdquo;</a></p>\r\n<p>If, therefore, Grace&rsquo;s anthropic reasoning is correct then most of the time when a prehistoric group of humans settled a large, uninhabited island, that group went extinct.</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2, "25oxqHiadqM6Hf7Gn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SwwCYwzWf7id7gjZm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 5.719324387320137e-07, "legacy": true, "legacyId": "2564", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-24T23:43:52.719Z", "modifiedAt": null, "url": null, "title": "The Spotlight", "slug": "the-spotlight", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:12.770Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zstm38omrpeu7iWeS/the-spotlight", "pageUrlRelative": "/posts/Zstm38omrpeu7iWeS/the-spotlight", "linkUrl": "https://www.lesswrong.com/posts/Zstm38omrpeu7iWeS/the-spotlight", "postedAtFormatted": "Wednesday, March 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Spotlight&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Spotlight%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZstm38omrpeu7iWeS%2Fthe-spotlight%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Spotlight%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZstm38omrpeu7iWeS%2Fthe-spotlight", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZstm38omrpeu7iWeS%2Fthe-spotlight", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 870, "htmlBody": "<p><strong>Sequence index:</strong> <a href=\"/lw/1xh/living_luminously/\">Living Luminously</a><br /><strong>Previously in sequence:</strong> <a href=\"/lw/1yb/lights_camera_action/\">Lights, Camera, Action</a><br /><strong>Next in sequence:</strong> <a href=\"/lw/209/highlights_and_shadows/\">Highlights and Shadows</a><br /><br /><em>Inspecting thoughts is easier and more accurate if they aren't in your head.&nbsp; Look at them in another form from the outside, like they belonged to someone else.</em></p>\n<p><em>You may find your understanding of this post significantly improved  if you read the fourth story from <a href=\"/lw/2aw/seven_shiny_stories\">Seven Shiny  Stories</a>.</em><br /><br />One problem with introspection is that the conclusions you draw about your thoughts are themselves thoughts.&nbsp; Thoughts, of course, can change or disappear before you can extract information about yourself from them.&nbsp; If a flash of unreasonable anger crosses my mind, this might stick around long enough to make me lash out, but then vanish before I discover how unreasonable it was.&nbsp; If thoughts weren't slippery like this, luminosity wouldn't be much of a project.&nbsp; So of course, if you're serious about luminosity, you need a way to pin down your thoughts into a concrete format that will hold still.<br /><br />You have to pry your thoughts out of your brain.<br /><br />Writing is the obvious way to do this - for me, anyway.&nbsp; You don't have to publicize what you extract, so it doesn't have to be aesthetic or skillful, just serviceable for your own reference.&nbsp; The key is to get it down in a form that you can look at without having to continue to introspect.&nbsp; Whether this means sketching or scribing or singing, dump your brain out into the environment and have a peek.&nbsp; It's easy to fool yourself into thinking that a given idea makes sense; it's harder to fool someone else.&nbsp; Writing down an idea automatically engages the mechanisms we use to communicate to others, helping you hold your self-analysis to a higher standard.<a id=\"more\"></a><br /><br />To turn your thoughts into non-thoughts, use labels to represent them.&nbsp; Put them in reference classes, so that you can notice when the same quale, habit of inference, or thread of cognition repeats. That way, you can detect patterns: \"Hey, the last time I felt like this, I said something I really regretted; I'd better watch it.\"&nbsp; If you can tell when something has happened twice, you can tell when it hasn't - and <em>new</em> moods or dispositions are potentially very important.&nbsp; They mean that you or something around you has changed, and that could be a valuable resource or a tricky hazard.<br /><br />Your labels can map onto traditional terms or not - if you want to call the feeling of having just dropped your ice cream on the sidewalk \"blortrath\", no one will stop you.&nbsp; (It can be useful, later when you're trying to share your conclusions about yourself with others, to have a vocabulary of emotion that overlaps significantly with theirs; but you can always set up an idiolect-to-dialect dictionary later.)&nbsp; I do recommend identifying labeled items as being more or less similar to each other (e.g. annoyance is more like fury than it is like glee) and having a way to account for that in your symbolism.&nbsp; Similarities like that will make it more obvious how you can generalize strategies from one thing to another.<br /><br />Especially if you don't think in words, you might find it challenging to turn your thoughts into something in the world that represents them.&nbsp; Maybe, for instance, you think in pictures but aren't at all good at drawing.&nbsp; This is one of the steps in luminosity that I think is potentially dispensible, so if you honestly cannot think of any way to jot down the dance of your mind for later inspection, you can just work on thinking very carefully such that if something were to be out of place the next time you came back to your thought, you'd notice it.&nbsp; I do recommend spending at least five to ten minutes trying to write, diagram, draw, mutter, or interpretive-dance your mental activity before you give it up as untenable for you, however.<br /><br />Once you have produced a visible or audible translation of your thoughts, <em>analyze it the way you would if someone else had written it</em>.&nbsp; (Except inasmuch as it's in a code that's uniquely understandable to you and you shouldn't pretend to do cryptanalysis on it.)&nbsp; What would you think of the person described if you didn't know anything else?&nbsp; How would you explain these thoughts?&nbsp; What threads of reasoning seem to run in the background from one belief to another, or from a perception to a belief, or from a desire to an intention?&nbsp; What do you expect this person to do next?&nbsp; What's your next best guess after that?&nbsp; And: what more do you want to know?&nbsp; If you met the person described, how could you satisfy your curiosity without relying on the bias-laden answer you'd get in response to a verbal inquiry?&nbsp; Try it now - in a comment under this post, if you like: note what you're thinking, as much of it as you can grab and get down.&nbsp; Turn on the anti-kibitzer and pretend someone else said it: what must be going on in the mind behind this writing?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwv9eHi7KGg5KA9oM": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zstm38omrpeu7iWeS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 46, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "2566", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "highlights-and-shadows", "canonicalPrevPostSlug": "lights-camera-action", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "v4ngP587MDZ5rC48Y", "tCTmAmAapB37dAz9Y", "9sguwESkteCgqFMbj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-25T01:39:12.829Z", "modifiedAt": null, "url": null, "title": "More thoughts on assertions", "slug": "more-thoughts-on-assertions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:46.702Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TaPr4YSBbiakeKdwX/more-thoughts-on-assertions", "pageUrlRelative": "/posts/TaPr4YSBbiakeKdwX/more-thoughts-on-assertions", "linkUrl": "https://www.lesswrong.com/posts/TaPr4YSBbiakeKdwX/more-thoughts-on-assertions", "postedAtFormatted": "Thursday, March 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20thoughts%20on%20assertions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20thoughts%20on%20assertions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTaPr4YSBbiakeKdwX%2Fmore-thoughts-on-assertions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20thoughts%20on%20assertions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTaPr4YSBbiakeKdwX%2Fmore-thoughts-on-assertions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTaPr4YSBbiakeKdwX%2Fmore-thoughts-on-assertions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1881, "htmlBody": "<p><strong>Response to: </strong><a href=\"/lw/1z7/the_show_dont_tell_nature_of_argument/\">The \"show, don't tell\" nature of argument</a></p>\n<p>Morendil says not to trust simple assertions. He's right, for the certain class of simple assertions he's talking about. But in order to see why, let's look at different types of assertions&nbsp; and see how useful it is to believe them.</p>\n<p><em>Summary:</em><br /> - Hearing an assertion can be strong evidence if you know nothing else about the proposition in question.<br /> - Hearing an assertion is not useful evidence if you already have a reasonable estimate of how many people do or don't believe the proposition.<br /> - An assertion by a leading authority is stronger than an assertion by someone else.<br /> - An assertion plus an assertion that there is evidence makes no factual difference, but is a valuable signal.<a id=\"more\"></a></p>\n<p><strong>Unsupported assertions about non-controversial topics</strong></p>\n<p>Consider my assertion: \"The Wikipedia featured article today is on Uriel Sebree\". Even if you haven't checked Wikipedia today and have no evidence on this topic, you're likely to believe me. Why would I be lying? <br /><br />This can be nicely modeled in Bayesian terms - you start with a prior evenly distributed across Wikipedia topics, the probability of me saying this conditional on it being false is pretty low, and the probability of me saying it conditional on it being true is pretty high. So noting that I said it nicely concentrates probability mass in the worlds where it's true. You're totally justified in believing it. The key here is that you have no reason to believe there's a large group of people who go around talking about Uriel Sebree being on Wikipedia regardless of whether or not he really is.<br /><br /><strong>Unsupported assertions about controversial topics</strong></p>\n<p>The example given in Morendil's post is that some races are biologically less intelligent than others. Let's say you have no knowledge of this whatsoever. You're so naive you don't even realize it might be controversial. In this case, someone who asserts \"some races are biologically less intelligent than others\" is no less believable than someone who asserts \"some races have slightly different frequencies of pancreatic cancer than others.\" You'd accept the second as the sort of boring but reliable biological fact that no one is particularly prone to lie about, and you'd do the same with the first.<br /><br />Now let's say you're familiar with controversies in sociology and genetics, you already know that some people believe some races are biologically more intelligent, and other people don't. Let's say you gauge the people around you and find that about 25% of people agree with the statement and 75% disagree.<br /><br />This survey could be useful. You have to ask yourself - is this statement about race and genetics more likely to have the support of a majority of people in a world where it's true than in a world where it's false? \"No\" is a perfectly valid answer here - you might think people are so interested in signalling that they're not racist that they'll completely suspend their rational faculties. But \"yes\" is also a valid answer here if you think that the people around you have reasonably intelligent opinions on the issue. This would be a good time to increase your probability that it's true.<br /><br />Now I, a perfectly average member of the human race, make the assertion that I believe that statement. But from your survey, you already have information that negates any evidence from my belief - that given that the statement is false and there's a 25% belief rate, there's a 25% chance I would agree with it, and given that the statement is true and there's a 25% belief rate, there's a 25% chance I would agree with it. If you've already updated on your survey, my assertion is equally likely in both conditions and doesn't shift probability one way or the other.</p>\n<p><strong>Unsupported assertions on extremely unusual topics</strong></p>\n<p>There is a case, I think, in which a single person asserting ze believes something can increase your probability. Imagine that I say, truthfully, that I believe that a race of otter-people from Neptune secretly controls the World Cup soccer tournament. If you've never heard this particular insane theory before, your estimate of the number of people who believed it was probably either zero, or so low that you wouldn't expect anyone you actually meet (even for values of \"meet\" including online forums) to endorse it. My endorsing it actually raises your estimate of the percent of the human race who endorse it, and this should raise your probability of it being true. Clearly, it should not raise it very <em>much</em>, and it need not necessarily raise it at all to the degree that you can prove that I have reasons other than truth for making the assertion (in this case, most of the probability mass generated by the assertion would leak off into the proposition that I was insane) but it can raise it a little bit.</p>\n<p><strong>Unsupported assertions by important authorities</strong></p>\n<p>This effect becomes more important when the person involved has impressive credentials. If someone with a Ph.D in biology says that race plays a part in intelligence, this could shift your estimate. In particular, it would shift it if you previously thought the race-intelligence connection was such a fringe theory that they would be unlikely to get even one good biologist on their side. But if you already knew that this theory was somewhat mainstream and had at least a tiny bit of support from the scientific community, it would be giving no extra information. Consider this the Robin Hanson Effect, because a lot of the good Robin Hanson does comes from being a well-credentialed guy with a Ph.D willing to endorse theories that formerly sounded so crazy that people would not have expected even one Ph.D to endorse them.<br /><br />In cases of the Hanson Effect, the way you found out about the credentialled supporter is actually pretty important. If you Googled \"Ph.D who supports transhumanism\" and found Robin's name, then all it tells you is that there is at least one Ph.D who supports transhumanism. But if you were at a bar, and you found out the person next to you was a Ph.D, and you asked zir out of the blue if ze supported transhumanism, and ze said yes, then you know that there are enough Ph.Ds who support transhumanism that randomly running into one at the bar is not that uncommon an event.<br /><br />An extreme case of the Hanson Effect is hearing that the world's top expert supports something. If there's only one World's Top Expert, then that person's opinion is always meaningful. This is why it was such a big deal when Watson came out in favor of a connection between race and intelligence. Now, I don't know if Watson actually knows anything about human genetic variation. He could have just had one clever insight about biochemistry way back when, and be completely clueless around the rest of the field. But if we imagine he really is the way his celebrity status makes him seem - the World's Top Expert in the field of genetics - then his opinion carries special weight for two reasons: first of all, it's the only data point we have in the field of \"what the World's Top Expert thinks\", and second, it suggests that a large percentage of the rest of the scientific community agrees with him (his status as World's Top Expert makes him something of a randomly chosen data point, and it would be very odd if we randomly pick the only data point that shares this opinion).</p>\n<p><strong>Assertions supported by unsupported claims of \"evidence\"</strong></p>\n<p>So much for completely unsupported assertions. Seeing as most people are pretty good at making up \"evidence\" that backs their pet beliefs, does it add anything to say \"...and I arrived at this conclusion using evidence\" if you refuse to say what the evidence is?<br /><br />Well, it's a good signal for sanity. Instead of telling you only that at least one person believes in this hypothesis, you now know that at least one person who is smart enough to understand that ideas require evidence believes it.<br /><br />This is less useful than it sounds. Disappointingly, there are not too many ideas that are believed solely by stupid people. As mentioned before, even creationism can muster a list of Ph.Ds who support it. When I was much younger, I was once quite impressed to hear that there were creationist Ph.Ds with a long list of scientific accomplishments in various fields. Since then, I learned about <a href=\"/lw/gv/outside_the_laboratory/\">compartmentalization</a>. So all that this \"...and I have evidence for this proposition\" can do on a factual level is highlight the existence of compartmentalization for people who weren't already aware of it.<br /><br />But on a nonfactual level...again, it signals sanity. The difference betwee \"I believe some races are less intelligent than others\" and \"I believe some races are less intelligent than others, and I arrived at this conclusion using evidence\" is that the second person is trying to convince you ze's not some random racist with an axe to grind, ze's an amateur geneticist addressing an interesting biological question. I don't evaluate the credibility of the two statements any differently, but I'd much rather hang out with the person who made the second one (assuming ze wasn't lying or trying to hide real racism behind a scientific veneer).<br /><br />Keep in mind that most <a href=\"/lw/1yz/levels_of_communication/\">communication</a> is done not to convince anyone of anything, but to <a href=\"/lw/1kr/that_other_kind_of_status/\">signal the character</a> of the person arguing (source: I arrived at this conclusion using evidence). One character signal may interfere with other character signals, and \"I arrived at this belief through evidence\" can be a powerful backup. I have a friend who's a physics Ph.D, an evangelical Christian with an strong interest in theology, and an American living abroad. If he tries to signal that he's an evangelical Christian, he's very likely to get shoved into the \"redneck American with ten guns and a Huckabee bumper sticker\" box unless he immediately adds something like \"and I base this belief on sound reasoning.\" That is one very useful signal there, and if he hadn't given it, I probably would have never bothered talking to him further. It's not a signal that his beliefs are actually based on sound reasoning, but it's a signal that he's the kind of guy who realizes beliefs should be based on that sort of thing and is probably pretty smart.<br /><br />You can also take this the opposite way. There's a great Dilbert cartoon where Dilbert's date says something like \"I know there's no scientific evidence that crystals can heal people, but it's my point of view that they do.\" This is a different signal; something along the lines of \"I'd like to signal my support for New Agey crystal medicine, but don't dock me points for ignoring the scientific evidence against it.\" This is more of a status-preserving manuever than the status-claiming \"I have evidence for this\" one, but astoundingly it seems to work pretty well (except on Dilbert, who responded, \"When did ignorance become a point of view?\")</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "ZXFpyQWPB5ideFbEG": 1, "Q6P8jLn8hH7kbuXRr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TaPr4YSBbiakeKdwX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 28, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "2567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Response to: </strong><a href=\"/lw/1z7/the_show_dont_tell_nature_of_argument/\">The \"show, don't tell\" nature of argument</a></p>\n<p>Morendil says not to trust simple assertions. He's right, for the certain class of simple assertions he's talking about. But in order to see why, let's look at different types of assertions&nbsp; and see how useful it is to believe them.</p>\n<p><em>Summary:</em><br> - Hearing an assertion can be strong evidence if you know nothing else about the proposition in question.<br> - Hearing an assertion is not useful evidence if you already have a reasonable estimate of how many people do or don't believe the proposition.<br> - An assertion by a leading authority is stronger than an assertion by someone else.<br> - An assertion plus an assertion that there is evidence makes no factual difference, but is a valuable signal.<a id=\"more\"></a></p>\n<p><strong id=\"Unsupported_assertions_about_non_controversial_topics\">Unsupported assertions about non-controversial topics</strong></p>\n<p>Consider my assertion: \"The Wikipedia featured article today is on Uriel Sebree\". Even if you haven't checked Wikipedia today and have no evidence on this topic, you're likely to believe me. Why would I be lying? <br><br>This can be nicely modeled in Bayesian terms - you start with a prior evenly distributed across Wikipedia topics, the probability of me saying this conditional on it being false is pretty low, and the probability of me saying it conditional on it being true is pretty high. So noting that I said it nicely concentrates probability mass in the worlds where it's true. You're totally justified in believing it. The key here is that you have no reason to believe there's a large group of people who go around talking about Uriel Sebree being on Wikipedia regardless of whether or not he really is.<br><br><strong>Unsupported assertions about controversial topics</strong></p>\n<p>The example given in Morendil's post is that some races are biologically less intelligent than others. Let's say you have no knowledge of this whatsoever. You're so naive you don't even realize it might be controversial. In this case, someone who asserts \"some races are biologically less intelligent than others\" is no less believable than someone who asserts \"some races have slightly different frequencies of pancreatic cancer than others.\" You'd accept the second as the sort of boring but reliable biological fact that no one is particularly prone to lie about, and you'd do the same with the first.<br><br>Now let's say you're familiar with controversies in sociology and genetics, you already know that some people believe some races are biologically more intelligent, and other people don't. Let's say you gauge the people around you and find that about 25% of people agree with the statement and 75% disagree.<br><br>This survey could be useful. You have to ask yourself - is this statement about race and genetics more likely to have the support of a majority of people in a world where it's true than in a world where it's false? \"No\" is a perfectly valid answer here - you might think people are so interested in signalling that they're not racist that they'll completely suspend their rational faculties. But \"yes\" is also a valid answer here if you think that the people around you have reasonably intelligent opinions on the issue. This would be a good time to increase your probability that it's true.<br><br>Now I, a perfectly average member of the human race, make the assertion that I believe that statement. But from your survey, you already have information that negates any evidence from my belief - that given that the statement is false and there's a 25% belief rate, there's a 25% chance I would agree with it, and given that the statement is true and there's a 25% belief rate, there's a 25% chance I would agree with it. If you've already updated on your survey, my assertion is equally likely in both conditions and doesn't shift probability one way or the other.</p>\n<p><strong id=\"Unsupported_assertions_on_extremely_unusual_topics\">Unsupported assertions on extremely unusual topics</strong></p>\n<p>There is a case, I think, in which a single person asserting ze believes something can increase your probability. Imagine that I say, truthfully, that I believe that a race of otter-people from Neptune secretly controls the World Cup soccer tournament. If you've never heard this particular insane theory before, your estimate of the number of people who believed it was probably either zero, or so low that you wouldn't expect anyone you actually meet (even for values of \"meet\" including online forums) to endorse it. My endorsing it actually raises your estimate of the percent of the human race who endorse it, and this should raise your probability of it being true. Clearly, it should not raise it very <em>much</em>, and it need not necessarily raise it at all to the degree that you can prove that I have reasons other than truth for making the assertion (in this case, most of the probability mass generated by the assertion would leak off into the proposition that I was insane) but it can raise it a little bit.</p>\n<p><strong id=\"Unsupported_assertions_by_important_authorities\">Unsupported assertions by important authorities</strong></p>\n<p>This effect becomes more important when the person involved has impressive credentials. If someone with a Ph.D in biology says that race plays a part in intelligence, this could shift your estimate. In particular, it would shift it if you previously thought the race-intelligence connection was such a fringe theory that they would be unlikely to get even one good biologist on their side. But if you already knew that this theory was somewhat mainstream and had at least a tiny bit of support from the scientific community, it would be giving no extra information. Consider this the Robin Hanson Effect, because a lot of the good Robin Hanson does comes from being a well-credentialed guy with a Ph.D willing to endorse theories that formerly sounded so crazy that people would not have expected even one Ph.D to endorse them.<br><br>In cases of the Hanson Effect, the way you found out about the credentialled supporter is actually pretty important. If you Googled \"Ph.D who supports transhumanism\" and found Robin's name, then all it tells you is that there is at least one Ph.D who supports transhumanism. But if you were at a bar, and you found out the person next to you was a Ph.D, and you asked zir out of the blue if ze supported transhumanism, and ze said yes, then you know that there are enough Ph.Ds who support transhumanism that randomly running into one at the bar is not that uncommon an event.<br><br>An extreme case of the Hanson Effect is hearing that the world's top expert supports something. If there's only one World's Top Expert, then that person's opinion is always meaningful. This is why it was such a big deal when Watson came out in favor of a connection between race and intelligence. Now, I don't know if Watson actually knows anything about human genetic variation. He could have just had one clever insight about biochemistry way back when, and be completely clueless around the rest of the field. But if we imagine he really is the way his celebrity status makes him seem - the World's Top Expert in the field of genetics - then his opinion carries special weight for two reasons: first of all, it's the only data point we have in the field of \"what the World's Top Expert thinks\", and second, it suggests that a large percentage of the rest of the scientific community agrees with him (his status as World's Top Expert makes him something of a randomly chosen data point, and it would be very odd if we randomly pick the only data point that shares this opinion).</p>\n<p><strong id=\"Assertions_supported_by_unsupported_claims_of__evidence_\">Assertions supported by unsupported claims of \"evidence\"</strong></p>\n<p>So much for completely unsupported assertions. Seeing as most people are pretty good at making up \"evidence\" that backs their pet beliefs, does it add anything to say \"...and I arrived at this conclusion using evidence\" if you refuse to say what the evidence is?<br><br>Well, it's a good signal for sanity. Instead of telling you only that at least one person believes in this hypothesis, you now know that at least one person who is smart enough to understand that ideas require evidence believes it.<br><br>This is less useful than it sounds. Disappointingly, there are not too many ideas that are believed solely by stupid people. As mentioned before, even creationism can muster a list of Ph.Ds who support it. When I was much younger, I was once quite impressed to hear that there were creationist Ph.Ds with a long list of scientific accomplishments in various fields. Since then, I learned about <a href=\"/lw/gv/outside_the_laboratory/\">compartmentalization</a>. So all that this \"...and I have evidence for this proposition\" can do on a factual level is highlight the existence of compartmentalization for people who weren't already aware of it.<br><br>But on a nonfactual level...again, it signals sanity. The difference betwee \"I believe some races are less intelligent than others\" and \"I believe some races are less intelligent than others, and I arrived at this conclusion using evidence\" is that the second person is trying to convince you ze's not some random racist with an axe to grind, ze's an amateur geneticist addressing an interesting biological question. I don't evaluate the credibility of the two statements any differently, but I'd much rather hang out with the person who made the second one (assuming ze wasn't lying or trying to hide real racism behind a scientific veneer).<br><br>Keep in mind that most <a href=\"/lw/1yz/levels_of_communication/\">communication</a> is done not to convince anyone of anything, but to <a href=\"/lw/1kr/that_other_kind_of_status/\">signal the character</a> of the person arguing (source: I arrived at this conclusion using evidence). One character signal may interfere with other character signals, and \"I arrived at this belief through evidence\" can be a powerful backup. I have a friend who's a physics Ph.D, an evangelical Christian with an strong interest in theology, and an American living abroad. If he tries to signal that he's an evangelical Christian, he's very likely to get shoved into the \"redneck American with ten guns and a Huckabee bumper sticker\" box unless he immediately adds something like \"and I base this belief on sound reasoning.\" That is one very useful signal there, and if he hadn't given it, I probably would have never bothered talking to him further. It's not a signal that his beliefs are actually based on sound reasoning, but it's a signal that he's the kind of guy who realizes beliefs should be based on that sort of thing and is probably pretty smart.<br><br>You can also take this the opposite way. There's a great Dilbert cartoon where Dilbert's date says something like \"I know there's no scientific evidence that crystals can heal people, but it's my point of view that they do.\" This is a different signal; something along the lines of \"I'd like to signal my support for New Agey crystal medicine, but don't dock me points for ignoring the scientific evidence against it.\" This is more of a status-preserving manuever than the status-claiming \"I have evidence for this\" one, but astoundingly it seems to work pretty well (except on Dilbert, who responded, \"When did ignorance become a point of view?\")</p>", "sections": [{"title": "Unsupported assertions about non-controversial topics", "anchor": "Unsupported_assertions_about_non_controversial_topics", "level": 1}, {"title": "Unsupported assertions on extremely unusual topics", "anchor": "Unsupported_assertions_on_extremely_unusual_topics", "level": 1}, {"title": "Unsupported assertions by important authorities", "anchor": "Unsupported_assertions_by_important_authorities", "level": 1}, {"title": "Assertions supported by unsupported claims of \"evidence\"", "anchor": "Assertions_supported_by_unsupported_claims_of__evidence_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "31 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gfkhZWzWrr4hQdxJY", "N2pENnTPB75sfc9kb", "gs8bZCmaWqDaus7Dr", "qjSHfbjmSyMnGR9DS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-25T17:43:06.467Z", "modifiedAt": null, "url": null, "title": "SIA won't doom you", "slug": "sia-won-t-doom-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vaZYAs7tsDriNkoAP/sia-won-t-doom-you", "pageUrlRelative": "/posts/vaZYAs7tsDriNkoAP/sia-won-t-doom-you", "linkUrl": "https://www.lesswrong.com/posts/vaZYAs7tsDriNkoAP/sia-won-t-doom-you", "postedAtFormatted": "Thursday, March 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SIA%20won't%20doom%20you&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASIA%20won't%20doom%20you%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaZYAs7tsDriNkoAP%2Fsia-won-t-doom-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SIA%20won't%20doom%20you%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaZYAs7tsDriNkoAP%2Fsia-won-t-doom-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvaZYAs7tsDriNkoAP%2Fsia-won-t-doom-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 938, "htmlBody": "<p>Katja Grace has just presented an ingenious <a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">model</a>, claiming that SIA combined with the <a href=\"http://en.wikipedia.org/wiki/Great_filter\">great filter</a> generates its own variant of the doomsday argument. Robin <a href=\"http://www.overcomingbias.com/2010/03/very-bad-news.html\">echoed</a> this on Overcoming Bias. We met soon after Katja had come up with the model, and I signed up to it, saying that I could see no flaw in the argument.</p>\n<p>Unfortunately, I erred. The argument does not work in the form presented.</p>\n<p>First of all, there is the issue of time dependence. We are not just a human level civilization drifting through the void in blissful ignorance about our position in the universe. We know (approximately) the age of our galaxy, and the time elapsed since the big bang.</p>\n<p>How is this relevant? It is relevant because all arguments about the great filter are time-dependent. Imagine we had just reached consciousness and human-level civilization, by some fluke, two thousand years after the creation of our galaxy, by an evolutionary process that took two thousand years. We see no aliens around us. In this situation, we have no reason to suspect any great filter; if we asked ourselves \"are we likely to be the first civilization to reach this stage?\" then the answer is probably yes. No evidence for a filter.</p>\n<p>Imagine, instead, that we had reached consciousness a trillion years into the life of our galaxy, again via an evolutionary process that took two thousand years, and we see no aliens or traces of aliens. Then the evidence for a filter is overwhelming; something must have stopped all those previous likely civilizations from emerging into the galactic plane.</p>\n<p>So neither of these civilizations can be included in our reference class (indeed, the second one can only exist if we ourselves are filtered!). So the correct reference class to use is not \"the class of all potential civilizations in our galaxy that have reached our level of technological advancement and seen no aliens\", but \"the class of all potential civilizations in our galaxy that have reached our level of technological advancement <em>at around the same time as us</em> and seen no aliens\". Indeed, SIA, once we update on the present, cannot tell us anything about the future.</p>\n<p>But there's more. <a id=\"more\"></a>Let us lay aside, for the moment, the issue of time dependence. Let us instead consider the diagrams in Katja's <a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">post</a> as if the vertical axis were time: all potential civilizations start at the same point, and progress at the same rate. Is there still a role for SIA?</p>\n<p>The answer is... it depends. It depends entirely on your choice of prior. To illustrate this, consider this pair of early-filter worlds:</p>\n<p><img src=\"http://images.lesswrong.com/t3_1zj_0.png\" alt=\"\" width=\"661\" height=\"157\" /></p>\n<p>To simplify, I've flattened the diagram, and now consider only two states: human civilizations and basic lifeforms. And here are some late filter worlds:</p>\n<p><img src=\"http://images.lesswrong.com/t3_1zj_1.png\" alt=\"\" width=\"367\" height=\"157\" /></p>\n<p>Assign an equal prior of (1/4) to each one of these world. Then the prior probability of living in a late filter world is (1/4+1/4)=1/2, and the same holds for early filter worlds.</p>\n<p>Let us now apply SIA. These boost the probability of Y and B at the expense of A and X. Y and B end up having a probability 1/3, while A and X end up having a probability 1/6. The postiori probability of living in a late filter world is (1/3+1/6)=1/2, and the same goes for early filter worlds. Applying SIA has not changed the odds of late versus early filters.</p>\n<p>But people might feel this is unfair; that I have loaded the dice, especially by giving world Y the same prior as the others. It has too many primitive lifeforms; it's too unlikely. Fine then; let us give prior probabilities as follows:</p>\n<p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">X<br /></th><th style=\"cursor: text;\">Y<br /></th><th style=\"cursor: text;\">A<br /></th><th style=\"cursor: text;\">B</th>\n</tr>\n<tr>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">2/30<br /></td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1/30</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">18/30</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">9/30<br /></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>This world does not exactly over-weight the chance of human survival! The prior probability of a late filter is (18/30+9/30)=9/10, while that of an early filter is 1/10. But now let us consider how SIA changes those odds: Y and B are weighted by a factor of two, while X and A are weighted by a factor of one. The postiori probabilities are thus:</p>\n<p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">X<br /></th><th style=\"cursor: text;\">Y<br /></th><th style=\"cursor: text;\">A<br /></th><th style=\"cursor: text;\">B</th>\n</tr>\n<tr>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1/20<br /></td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1/20</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">9/20</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">9/20<br /></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>The postiori probability of a late filter is (9/20+9/20)=9/10, same as before: again SIA has not changed the probability of where the filter is. But it gets worse; if, for instance, we had started with the priors:</p>\n<p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">X<br /></th><th style=\"cursor: text;\">Y<br /></th><th style=\"cursor: text;\">A<br /></th><th style=\"cursor: text;\">B</th>\n</tr>\n<tr>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">1/30<br /></td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">2/30</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">18/30</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">9/30<br /></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>This is the same as before, but with X and Y inversed. The early filter still has only one chance in ten, a priori. But now if we apply SIA, the postiori odds of X and Y are 1/41 and 4/41, totalling of 5/41 &gt; 1/10. Here applying SIA has <em>increased </em>our chances of survival!</p>\n<p>In general there are a lot of reasonable priors over possible worlds were SIA makes little or no difference to the odds of the great filter, either way.</p>\n<p>&nbsp;</p>\n<p><strong>Conclusion</strong>: Do I believe that this has demonstrated that the SIA/great filter argument is nonsense? No, not at all. I think there is a lot to be gained from analysing the argument, and I hope that Katja or Robin or someone else - maybe myself, when I get some spare time, one of these centuries - sits down and goes through various scenarios, looks at classes of reasonable priors and evidence, and comes up with a conclusion about what exactly SIA says about the great filter, the strength of the effect, and how sensitive it is to prior changes. I suspect that when the dust settles, SIA will still slightly increase the chance of doom, but that the effect will be minor.</p>\n<p>Having just saved humanity, I will now return to more relaxing pursuits.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vaZYAs7tsDriNkoAP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 5.722021836269306e-07, "legacy": true, "legacyId": "2575", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-25T17:58:56.809Z", "modifiedAt": null, "url": null, "title": "Over-encapsulation", "slug": "over-encapsulation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:47.254Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nitfKbhkM5xkLnHkQ/over-encapsulation", "pageUrlRelative": "/posts/nitfKbhkM5xkLnHkQ/over-encapsulation", "linkUrl": "https://www.lesswrong.com/posts/nitfKbhkM5xkLnHkQ/over-encapsulation", "postedAtFormatted": "Thursday, March 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Over-encapsulation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOver-encapsulation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnitfKbhkM5xkLnHkQ%2Fover-encapsulation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Over-encapsulation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnitfKbhkM5xkLnHkQ%2Fover-encapsulation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnitfKbhkM5xkLnHkQ%2Fover-encapsulation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1006, "htmlBody": "<p>Take a look at <a href=\"http://www.sciencemag.org/cgi/content/abstract/325/5936/87\">\"Role of Layer 6 of V2 Visual Cortex in Object-Recognition Memory\"</a>, <em>Science</em> 3 July 2009:<br />Vol. 325. no. 5936, pp. 87 - 89.&nbsp; The article has some good points, but I'm going to pick on some of its tests.</p>\n<p>The experimenters believed they could enhance object-recognition memory (ORM) by using a lentivirus to insert a gene into area V2 of visual cortex.&nbsp; They tested the ORM of rats by putting an object in a field with a rat, and then putting either the same object (\"old\"), or a different object (\"new\"), in the field 30, 45, or 60 minutes later.&nbsp; The standard assumption is that rats spend more time investigating unfamiliar than familiar objects.</p>\n<p>They chose this test:&nbsp; For each condition, measure the difference in mean time spent investigating the old object vs. the new object.&nbsp; If the latter is more than the former, and the difference is statistically-significant, conclude that the rats recognized the old object.</p>\n<p>Figure 1 Graph A (below the article summary cutoff) shows how much time normal rats spent investigating an object.&nbsp; Here it is in HTML table form: How much time the rats spent exploring old and new objects:</p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>Minutes after first exposure</td>\n<td>30</td>\n<td>45</td>\n<td>60</td>\n</tr>\n<tr>\n<td>Old</td>\n<td>8</td>\n<td>12</td>\n<td>14</td>\n</tr>\n<tr>\n<td>New</td>\n<td>17</td>\n<td>28</td>\n<td>14</td>\n</tr>\n</tbody>\n</table>\n<p>The black bars (new) are significantly longer than the white bars (old) after 30 and 45 minutes, but not after 60 minutes.&nbsp; Therefore, the normal rats recognized the old objects after 30 and 45 minutes, but not after 60 minutes.</p>\n<p>Figure 3 Graph D (also below the article-summary cutoff) shows how much time different types of rats spent exploring old and new objects.&nbsp; The \"RGS\" group is rats given the gene therapy, but in parietal cortex rather than in V2.</p>\n<p>Here it is in HTML form: How much time the rats spent exploring old and new objects, by rat type:</p>\n<table border=\"1\">\n<tbody>\n<tr>\n<td>Rat type</td>\n<td>Normal (after 45 min)</td>\n<td>Parietal RGS (after 60 min)</td>\n</tr>\n<tr>\n<td>Old</td>\n<td>10</td>\n<td>11</td>\n</tr>\n<tr>\n<td>New</td>\n<td>27</td>\n<td>12</td>\n</tr>\n</tbody>\n</table>\n<p>Parietal RGS rats displayed no difference in time spent exploring old and new objects after 60 minutes; therefore, this gene therapy to parietal cortex does not improve ORM.</p>\n<p>To recap:</p>\n<ol>\n<li>We conclude that rats no longer recognize an old object if they spend about the same time investigating it as investigating a new object.</li>\n<li>Normal rats spend the same time investigating old and new objects 60 minutes after first exposure to the old object.</li>\n<li>Parietal RGS rats also spend the same time investigating old and new objects after 60 minutes.</li>\n<li>Therefore, normal rats and parietal RGS rats both lose ORM by 60 minutes.</li>\n</ol>\n<p>So why don't I buy it?</p>\n<p><a id=\"more\"></a></p>\n<p>Figure 1 (look at A).</p>\n<p><img src=\"http://liveweb.archive.org/http://image.bayimg.com/palegaaci.jpg\" alt=\"Fig. 1 Animal performance on ORM task.\" width=\"440\" height=\"321\" /></p>\n<p>Figure 3 (look at D):</p>\n<p><img src=\"http://liveweb.archive.org/http://image.bayimg.com/palejaaci.jpg\" alt=\"Fig. 3 Ox7-SAP injection in layer 6 and determination of ORM.\" width=\"440\" height=\"363\" /></p>\n<p>(Original image is <a href=\"http://www.sciencemag.org/content/vol325/issue5936/images/medium/325_87_F3.gif\">here</a>.)</p>\n<p>The investigators were trying to determine when rats recognized an old object.&nbsp; So what's most relevant is how much time they spent investigating the <em>old</em> object.&nbsp; The time spent investigating new objects is probably supposed to control for variations in their testing procedure.</p>\n<p>But in both of the graphs, we see that they are claiming that rats failed to recognize an old object in the 60-minute condition, <em>even though they spent the same amount of time investigating it as in the other conditions.</em>&nbsp; The difference was only in their response to <em>new</em> objects.&nbsp; The test methodology assumes that the response to new objects is always the same.</p>\n<p>Look at the error bars on those graphs.&nbsp; The black bars are supposed to all be the same height (except in 1B and 1C).&nbsp; Yet we see they differ across conditions by what looks like about 10 standard deviations in several cases.</p>\n<p>When you regularly get 10 standard deviations of difference in your control variable across cases, you shouldn't say, \"Gee, lucky thing I used that control variable!&nbsp; Otherwise I never would have noticed the large, significant difference between the test and control cases.\"&nbsp; No; you say, \"Gee, something is wrong with my experimental procedure.\"</p>\n<p>A couple of other things to notice, in addition to the comments above:</p>\n<ul>\n<li>The leftmost two sets of bars in 1B contrast the time spent examining old and new objects 60 minutes after exposure to the old objects, in normal and treated rats.&nbsp; Note that, again, there is no difference between the time spent looking at the old objects between normal (control) and treated rats; yet they concluded that the treated rats remembered them, and the normal rats did not, because the treated rats spent more time looking at new objects than untreated rats did.</li>\n<li>1D is supposed to show that normal rats could remember only 2 objects, while treated rats could remember 6 objects.&nbsp; But, again, this conclusion was reached because the normal rats spent less time looking at the <em>new</em> objects when exposed to 4 new objects than when exposed to 2 new objects.&nbsp; There was no difference in the time they spent looking at the old objects with either type of rat under any of the conditions.</li>\n</ul>\n<p>One subtle type of error is committed disproportionately by scientists, because it's a natural by-product of the scientific process of abstracting a theory into a testable hypothesis.&nbsp; A scientist is supposed to formulate a test before performing the test, to avoid introducing bias into the test formulation in order to get the desired results.&nbsp; Over-encapsulation is when the scientist performs the test, and examines the results according to the previously-established criteria, without noticing that the test results invalidate the assumptions used to formulate the test.&nbsp; I call it \"over-encapsulation\" because the scientist has tried to encapsulate the reasoning process in a box, and put data into the box and get decisions out of it; and the journey into and out of the box strips off relevant but unanticipated information.</p>\n<p>Over-encapsulation is especially tricky when you're reasoning about decision theory.&nbsp; It's possible to construct a formally-valid evaluation of the probabilities of different cases; and then take those probabilities and choose an action based on them using some decision theory, without noticing that some of the cases are inconsistent with the assumptions used in your decision theory.&nbsp; I hope to write another, more controversial post on this someday.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 1, "ZpG9rheyAkgCoEQea": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nitfKbhkM5xkLnHkQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 29, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "2576", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-25T20:53:55.266Z", "modifiedAt": null, "url": null, "title": "Newcomb's problem happened to me", "slug": "newcomb-s-problem-happened-to-me-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RrNvJsZr9MfedtnZF/newcomb-s-problem-happened-to-me-0", "pageUrlRelative": "/posts/RrNvJsZr9MfedtnZF/newcomb-s-problem-happened-to-me-0", "linkUrl": "https://www.lesswrong.com/posts/RrNvJsZr9MfedtnZF/newcomb-s-problem-happened-to-me-0", "postedAtFormatted": "Thursday, March 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Newcomb's%20problem%20happened%20to%20me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANewcomb's%20problem%20happened%20to%20me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrNvJsZr9MfedtnZF%2Fnewcomb-s-problem-happened-to-me-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Newcomb's%20problem%20happened%20to%20me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrNvJsZr9MfedtnZF%2Fnewcomb-s-problem-happened-to-me-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrNvJsZr9MfedtnZF%2Fnewcomb-s-problem-happened-to-me-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 836, "htmlBody": "<!-- Newcomb's problem happened to me -->\n<p>Okay, maybe not me, but someone I know, and that's what the title would be if he wrote it.&nbsp;    <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\" target=\"_blank\">Newcomb's problem</a> and <a href=\"http://en.wikipedia.org/wiki/Kavka's_toxin_puzzle\" target=\"_blank\">Kavka's toxin puzzle</a> are more than just curiosities.&nbsp;  Like a lot of thought experiments, they <em>approximately happen</em>.&nbsp;  They make the issues with <a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\" target=\"_blank\">causal decision theory</a> relevant, not only to designing artificial intelligence, but to our everyday lives as well.&nbsp;</p>\n<p>Yet somehow it isn't mainstream knowledge that these are more than merely abstract linguistic issues, as evidenced by <a href=\"/lw/7i/rationality_is_systematized_winning/52q?context=1#52q\" target=\"_blank\">this comment thread</a> (please no Karma sniping of the comments, they are a valuable record).&nbsp;  Scenarios involving brain scanning, decision simulation, etc., can establish their validy and future relevance, but not that they are already commonplace.&nbsp;  I want to provide an already-happened, real-life account that captures the Newcomb essence.</p>\n<p>So let's say my friend is named Joe.&nbsp;  In his account, Joe is very much in love with this girl named Omega&hellip; er&hellip; Kate, and he wants to get married.&nbsp;  Kate is somewhat traditional, and won't marry him unless he <em>proposes</em>, not only in the sense of explicitly asking her, but also <em>expressing certainty that he will never try to leave her if they do marry</em>.&nbsp;</p>\n<p>At this point, many of you could easily make up a simple conclusion to this post.&nbsp;  As such, I want to convey the actual account, in which Joe's beliefs are roughly schematized as follows:&nbsp;</p>\n<ol>\n<li>if he <em>proposes sincerely</em>, she is effectively sure to <em>believe it</em>. </li>\n<li>if he <em>proposes insincerely</em>, she will 50% likely <em>believe it</em>. </li>\n<li>if she <em>believes his proposal</em>, she will 80% likely <em>say yes</em>. </li>\n<li>if she <em>doesn't believe his proposal</em>, she will surely <em>say no</em>, but will not be significantly upset in comparison to the significance of marriage. </li>\n<li>if they marry, Joe will 90% likely be <em>happy</em>, and will 10% likely be <em>unhappy</em>. </li>\n</ol>\n<p>He roughly values the happy and unhappy outcomes oppositely:</p>\n<ol>\n<li>being happily married to Kate:&nbsp; 125 megautilons </li>\n<li>being unhapily married to Kate:&nbsp; -125 megautilons. </li>\n</ol>\n<p>So what should he do?&nbsp;  <em>What should this real person have actually done?</em><a href=\"#1\"><sup>1</sup></a>&nbsp;  Well, as in Newcomb, these beliefs and utilities present an interesting and quantifiable problem&hellip;  <a id=\"more\"></a></p>\n<ul>\n<li>EU(marriage) = 90%&middot;125 - 10%&middot;125 = 100, </li>\n<li>EU(sincere proposal) = 80%&middot;100 = 80, and </li>\n<li>EU(insincere proposal) = 50%&middot;80%&middot;100 = 40. </li>\n</ul>\n<p>No surprise here, sincere proposal comes out on top.&nbsp; That's the important thing, not the particular numbers.&nbsp; In fact, in real life Joe's utility function assigned negative moral value to insincerity, broadening the gap.&nbsp;  But no matter; this did not <em>make</em> him sincere.&nbsp;  The problem is that Joe was a <a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\" target=\"_blank\"><em>causal decision theorist</em></a>, and he believed that if circumstances changed to render him unhappily married, he would necessarily try to leave her.&nbsp;  Because of this possibility, he could not propose sincerely in the sense she desired.</p>\n<p>This feels strikingly similar to Newcomb's problem, and in fact it is: if we change some probabilities to 0 and 1, it's essentially isomorphic:</p>\n<ol>\n<li>If he proposes sincerely, she will say yes. </li>\n<li>If he proposes insincerely, she will say no and break up with him forever. </li>\n<li>If they marry, he is 90% likely to be very happy, and 10% likely to be very unhappy. </li>\n</ol>\n<p>The analogue of the two boxes are marriage (opaque) and the option of leaving (transparent).&nbsp;  Given marriage, the option of leaving has a small marginal utility of 10%&middot;125 = 12.5 utilons.&nbsp;  So \"clearly\" he should \"just take both\"?&nbsp;  The problem is that he <em>can't just take both</em>.&nbsp;  The proposed payout matrix would be:</p>\n<p>\n<table style=\"cursor: default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor: text;\">Joe \\ Kate<br /></th><th style=\"cursor: text;\">Say yes<br /></th><th style=\"cursor: text;\">Say no<br /></th>\n</tr>\n<tr>\n<th style=\"cursor: text;\">Propose sincerely<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">Marriage</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">Nothing significant</td>\n</tr>\n<tr>\n<th style=\"cursor: text;\">Propose insincerely<br /></th>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">Marriage + option to leave</td>\n<td style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 15px; cursor: text; margin: 8px;\" align=\"center\">Nothing significant<br /></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>The \"principal of (weak<a href=\"#2\"><sup>2</sup></a>) dominance\" would say the second row is the better \"option\", and that therefore \"clearly\" Joe should propose insincerely.&nbsp;  But in Newcomb some of the outcomes are declared logically impossible.&nbsp;  If he tries to take both boxes, there will be nothing in the marriage box.&nbsp;  The analogue in real life is simply that <em>the four outcomes need not be equally likely</em>.&nbsp;</p>\n<p>So there you have it.&nbsp;  Newcomb happens.&nbsp;  Newcomb <em>happened</em>.&nbsp;  You might be wondering, what did Joe actually do?&nbsp;</p>\n<p>In real life, Joe became a <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">timeless decision theorist</a>, and noting his 90% certainty, <a href=\"http://en.wikipedia.org/wiki/Self-modifying_code\">self-modified</a> by adopting a moral <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">pre-commitment</a> to never leaving Kate should they marry, proposed to her sincerely, and the rest is history.&nbsp;  No joke!&nbsp; That's if Joe's account is accurate, mind you.</p>\n<p>&nbsp;</p>\n<hr />\n<p>Footnotes:</p>\n<p><a name=\"1\"><sup>1</sup></a> This is not a social commentary, but an illustration that probabilistic Newcomblike scenarios <em>can and do exist</em>.&nbsp;  Although this also does not hinge on whether you believe Joe's account, I have provided it as-is nonetheless.&nbsp; I would hope that there are other similar accounts written down somewhere, but I haven't seen them, so I've provided his.&nbsp;</p>\n<p><a name=\"2\"><sup>2</sup></a> Newcomb involves \"strong\" dominance, with the second row always strictly better, but that's not essential to this post.&nbsp;  In any case, I could exhibit strong dominance by removing \"if they do get married\" from Kate's proposal requirement, but I decided against it, favoring instead the actual account of events.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RrNvJsZr9MfedtnZF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "2577", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["c3wWnvgzdbRhNnNbQ", "6ddcsdA2c2XpNpE5x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-26T04:39:17.551Z", "modifiedAt": null, "url": null, "title": "Maximise Expected Utility, not Expected Perception of Utility", "slug": "maximise-expected-utility-not-expected-perception-of-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:53.624Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cJSCTtyJmykHofkGm/maximise-expected-utility-not-expected-perception-of-utility", "pageUrlRelative": "/posts/cJSCTtyJmykHofkGm/maximise-expected-utility-not-expected-perception-of-utility", "linkUrl": "https://www.lesswrong.com/posts/cJSCTtyJmykHofkGm/maximise-expected-utility-not-expected-perception-of-utility", "postedAtFormatted": "Friday, March 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Maximise%20Expected%20Utility%2C%20not%20Expected%20Perception%20of%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaximise%20Expected%20Utility%2C%20not%20Expected%20Perception%20of%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJSCTtyJmykHofkGm%2Fmaximise-expected-utility-not-expected-perception-of-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Maximise%20Expected%20Utility%2C%20not%20Expected%20Perception%20of%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJSCTtyJmykHofkGm%2Fmaximise-expected-utility-not-expected-perception-of-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJSCTtyJmykHofkGm%2Fmaximise-expected-utility-not-expected-perception-of-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>Suppose we are building an agent, and we have a particular utility function U over states of the universe that we want the agent to optimize for. So we program into this agent a function CalculateUtility that computes the value of U given its current knowledge. Then we can program it to make decisions by searching through its available actions for the one that maximizes its expectation for its result of running CalculateUtility. But wait, how will an agent with this programming behave?</p>\n<p>Suppose the agent has the opportunity (option A) to arrange to falsely believe the universe is in a state that is worth utility u<sub>FA</sub> but this action really leads to a different state worth utility u<sub>TA</sub>, and a competing opportunity (option B) to actually achieve a state of the universe that has utility u<sub>B</sub>, with u<sub>TA</sub> &lt; u<sub>B</sub> &lt; u<sub>FA</sub>. Then the agent will expect that if it takes option A that its CalculateUtility function will return uFA, and if it takes option B that its CalculateUtility function will return u<sub>B</sub>. u<sub>FA</sub> &gt; u<sub>B</sub>, so the agent takes option A, and achieves a states of the universe with utility u<sub>TA</sub> which is worse than the utility u<sub>B</sub> it could have achieved if it had taken option B. This agent is not a very effective optimization process<sup>1</sup>. It would rather falsely believe that it has achieved its goals than actually achieve its goals. This sort of problem<sup>2</sup> is known as <a href=\"http://wiki.lesswrong.com/wiki/Wireheading\" target=\"_blank\">wireheading</a>.</p>\n<p>Let us back up a step, and instead program our agent to make decisions by searching through its available actions for the one whose expected results maximizes its current calculation of CalculateUtility. Then, the agent would calculate that option A gives it expected utility u<sub>TA</sub> and option B gives it expected utility u<sub>B</sub>. u<sub>B</sub> &gt; u<sub>TA</sub>, so it chooses option B and actually optimizes the universe. That is much better.</p>\n<p>So, if you care about states of the universe, and not just your personal experience of maximizing your utility function, you should make choices that maximize your expected utility, not choices that maximize your expectation of perceived utility.</p>\n<p>&nbsp;</p>\n<hr />\n<p>1. We might have expected this to work, because <a title=\"The Importance of Goodhart's Law\" href=\"/lw/1ws/the_importance_of_goodharts_law/\">we built our agent to have beliefs that correspond to the actual state of the world</a>.</p>\n<p>&nbsp;</p>\n<p>2. A similar problem occurs if the agent has the opportunity to modify its CalculateUtility function, so it returns large values for states of the universe that would have occurred anyways (or any state of the universe).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cJSCTtyJmykHofkGm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 14, "extendedScore": null, "score": 5.723305847500408e-07, "legacy": true, "legacyId": "2581", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YtvZxRpZjcFNwJecS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-26T09:26:15.631Z", "modifiedAt": null, "url": null, "title": "The mathematical universe: the map that is the territory", "slug": "the-mathematical-universe-the-map-that-is-the-territory", "viewCount": null, "lastCommentedAt": "2021-04-24T03:53:24.699Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fZJRxYLtNNzpbWZAA/the-mathematical-universe-the-map-that-is-the-territory", "pageUrlRelative": "/posts/fZJRxYLtNNzpbWZAA/the-mathematical-universe-the-map-that-is-the-territory", "linkUrl": "https://www.lesswrong.com/posts/fZJRxYLtNNzpbWZAA/the-mathematical-universe-the-map-that-is-the-territory", "postedAtFormatted": "Friday, March 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20mathematical%20universe%3A%20the%20map%20that%20is%20the%20territory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20mathematical%20universe%3A%20the%20map%20that%20is%20the%20territory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfZJRxYLtNNzpbWZAA%2Fthe-mathematical-universe-the-map-that-is-the-territory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20mathematical%20universe%3A%20the%20map%20that%20is%20the%20territory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfZJRxYLtNNzpbWZAA%2Fthe-mathematical-universe-the-map-that-is-the-territory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfZJRxYLtNNzpbWZAA%2Fthe-mathematical-universe-the-map-that-is-the-territory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3186, "htmlBody": "<p style=\"padding-left: 30px; \"><em>This post is for people who are not familiar with the Level IV Multiverse/Ultimate Ensemble/Mathematical Universe Hypothesis, people who are not convinced that there&rsquo;s any reason to believe it, and people to whom it appears believable or useful but not satisfactory as an actual explanation for anything.</em></p>\n<p style=\"padding-left: 30px; \"><em>I&rsquo;ve found that while it&rsquo;s fairly easy to understand what this idea asserts, it is more difficult to get to the point where it actually seems convincing and intuitively correct, until you independently invent it for yourself. Doing so can be fun, but for those who want to skip that part, I&rsquo;ve tried to write this post as a kind of <a href=\"http://en.wikipedia.org/wiki/Intuition_pump\">intuition pump</a> (of the variety, I hope, that deserves the non-derogatory use of that term) with the goal of leading you along the same line of thinking that I followed, but in a few minutes rather than a few years.</em></p>\n<hr />\n<p>Once upon a time, I was reading some Wikipedia articles on physics, <a href=\"http://xkcd.com/214/\">clicking links aimlessly</a>, when I happened upon a page then titled &ldquo;Ultimate Ensemble&rdquo;. It described a multiverse of all internally-consistent mathematical structures, thereby allegedly explaining our own universe &mdash;&nbsp;it&rsquo;s mathematically possible, so it exists along with every other possible structure.</p>\n<p>Now, I was certainly interested in the question it was attempting to answer. It&rsquo;s one that most young aspiring deep thinkers (and many very successful deep thinkers) end up at eventually: why is there a universe at all? A friend of mine calls himself an agnostic because, he says, &ldquo;Who created God?&rdquo; and &ldquo;What caused the Big Bang?&rdquo; are the same question. Of course, they&rsquo;re not <em>quite</em> the same,&nbsp;but the fundamental point is valid: although nothing happened &ldquo;before&rdquo; the Big Bang (as a more na&iuml;ve version of this query might ask), saying that it caused the universe to exist still requires us to explain what brought about the laws and circumstances allowing the Big Bang to happen. There are some hypotheses that try to explain this universe in terms of a more general multiverse, but all of them seemed to lead to another question: &ldquo;Okay, fine, then what caused <em>that</em> to be the case?&rdquo;</p>\n<p>The Ultimate Ensemble, although interesting, looked like yet another one of those non-explanations to me. &ldquo;Alright, so every mathematical structure &lsquo;exists&rsquo;. Why? Where? If there are all these mathematical structures floating around in some multiverse, what are the laws of this multiverse, and what caused <em>those</em> laws? What&rsquo;s the evidence for it?&rdquo; It seemed like every explanation would lead to an infinite regress of multiverses to explain, or a <a href=\"/lw/it/semantic_stopsigns/\">stopsign</a> like &ldquo;God did it&rdquo; or &ldquo;it just exists because it exists and that&rsquo;s the end of it&rdquo; (I&rsquo;ve seen that from&nbsp;several atheists trying to convince themselves or others that this is a non-issue) or &ldquo;science can never know what lies beyond this point&rdquo; or &ldquo;here be dragons&rdquo;. This was deeply vexing to my 15-year-old self, and after a completely secular upbringing, I suffered a mild bout of spirituality over the following year or so. Fortunately I made a full recovery, but I gave in and decided that Stephen Hawking was right that &ldquo;Why does the universe bother to exist?&rdquo; would remain permanently unanswerable.</p>\n<p>Last year, I found myself thinking about this question again &mdash;&nbsp;but only after unexpectedly making my way back to it while thinking about the idea of an AI being conscious. And the path I took actually suggested an answer this time.&nbsp;<a id=\"more\"></a>As I worked on writing it up, I noticed that it sounded familiar. After I remembered what that Wikipedia article was called, and after actually looking up Max Tegmark&rsquo;s papers on it this time, I confirmed that it was indeed the same essential idea. (Don&rsquo;t you hate/love it when you find out that your big amazing groundbreaking idea has already been advocated by someone smarter and more important than you? It&rsquo;s so disappointing/validating.) One of the papers briefly explores reasoning similar to that which I had accidentally used to convince myself of it,&nbsp;but it&rsquo;s an argument that I haven&rsquo;t seen emphasized in any discussions of it hereabouts, and it&rsquo;s one which seems inescapable with no assumptions outside of ordinary materialism and reductionism.</p>\n<p>I shall now get to the point.</p>\n<hr />\n<p>Suppose this universe is a computer simulation.</p>\n<p>It isn&rsquo;t, but we&rsquo;ll imagine for the next few paragraphs that it is.</p>\n<p>Suppose everything we see &mdash;&nbsp;and all of the Many Worlds that we don&rsquo;t see, and everything in this World that is too distant for us to ever see &mdash;&nbsp;is the product of a precise simulation being performed by some amazing supercomputer. Let&rsquo;s call it the Grand Order Deducer, or G.O.D. for short.</p>\n<p>Actually, let&rsquo;s say that G.O.D. is not an amazing supercomputer, but a 386 with an insanely large hard drive. Obviously, we wouldn&rsquo;t notice the slowness from the inside, any more than the characters in a movie would notice that your DVD player is being choppy.</p>\n<p>Clearly, then, if G.O.D. were turned off for a billion years, and then reactivated at the point where it left off, we wouldn&rsquo;t notice anything either. How about if the state of the simulation were copied to a very different kind of computer (say, a prototypical tape-based universal Turing machine, or an immortal person doing lambda calculus operations by hand) and continued? If our universe&rsquo;s physics turns out to be fundamentally time-symmetrical, then if G.O.D. started from the end of the universe and simulated backwards, would we experience our lives backwards? If it saved a copy of the universe at the beginning of your life and repeatedly ran the simulation from there until your death (if any), would it mean anything to say that you are experiencing your life multiple times? If the state of the simulation were copied onto a million identical computers, and continued thence on all of them, would we feel a million times as real (or would there be a million &ldquo;more&rdquo; of each of us in any meaningful sense), and would the implausibly humanlike agent who hypothetically created this simulation feel a million times more culpable for any suffering taking place within it? It would be hard to argue that any of this should be the case without resorting to some truly ridiculous metaphysics. Every computer is calculating the same thing, even the ones that don&rsquo;t seem plausible as universe-containers under our intuitions about what a simulation would look like.</p>\n<p>But what, then, makes us feel real? What if, after G.O.D. has been turned off for a billion years&hellip; it stays off? If we can feel real while being simulated by a hundred computers, and no less real while being simulated by <a href=\"/lw/1hg/the_moral_status_of_independent_identical_copies/\">one computer</a>, how about if we&rsquo;re being simulated by zero computers? More concretely, and perhaps more disturbingly, if <a href=\"/lw/1pz/the_ai_in_a_box_boxes_you/\">torturing a million identical simulations</a> is the same thing as torturing one (I&rsquo;d argue that it is), is torturing one the same as torturing zero?</p>\n<p>2 + 2 will always be 4 whether somebody is computing it or not. (No Platonism is necessary here; only the Simple Truth that taking the string &ldquo;2 + 2&rdquo; and applying certain rules of inference to it always results in the string &ldquo;4&rdquo;.) Similarly, even if this universe is nothing but a hypothetical, not being computed by anyone, not existing in anything larger, there are certain things that are necessarily true <em>about</em> the hypothetical, including facts about the subjective mental states of us self-aware substructures. Nothing magical happens when a simulation runs. Most of us agree that consciousness is probably purely mechanistic, and that we could therefore create a conscious AI or emulate an uploaded brain, and that it would be just as conscious as we are; that if we could simulate Descartes, we&rsquo;d hear him make the usual arguments about the duality of the material body and the extraphysical mind, and if we could simulate Chalmers, he&rsquo;d come to the same familiar nonsensical conclusions about qualia and zombies. But the fact remains that it&rsquo;s just a computer doing what computers always do, with no special EXIST or FEEL opcodes added to its instruction set. If a mind, from the outside, can be a self-contained and timeless structure, and the full structure can be calculated (within given finite limits) from some initial state by a normal computer, then its consciousness is a property of the structure itself, not of the computer or the program &mdash; the program is not causing it, it&rsquo;s just letting someone notice it. So deep runs the dualist intuition that even when we have reduced spirits and consciousness and free will to normal physical causality, there&rsquo;s still sometimes a tendency to think as though turning on a sufficiently advanced calculator causes something to mysteriously blink into existence or awareness, when all it is doing is reporting facts about some very large numbers that would be true one way or the other.</p>\n<p>G.O.D. is doing the very same thing, just with numbers that are even more unimaginably huge: a universe instead of an individual mind. The distilled and generalized argument is thus: <em>If we can feel real inside a non-magical computer simulation, then our feeling of reality must be due to necessary properties of the information being computed, because such properties do not exist in the abstract process of computing, and those properties will not cease to be true about the underlying information if the simulation is stopped or is never created in the first place.</em> This is identically true about every other possible reality.</p>\n<p>By Occam&rsquo;s Razor, I conclude that if a universe <em>can</em> exist in this way &mdash;&nbsp;as one giant subjunctive &mdash;&nbsp;then we must accept that that is how and why our universe <em>does</em> exist; even if we are being simulated on a computer in some outer universe, or if we were created by an actual deity (which, from a non-intervening deity&rsquo;s perspective, would probably look about the same as running a simulation anyway), or if there is some other explanation for this particular universe, we now see that this would not actually be the cause of our existence. Existence is what mathematical possibility feels like from the inside. Turn off G.O.D., and we&rsquo;ll go on with our lives, not noticing that anything has changed. Because the only thing that <em>has</em> changed is that the people who were running the simulation won&rsquo;t get to find out what happens next.</p>\n<hr />\n<p>Tegmark has described this as a &ldquo;theory of everything&rdquo;. I&rsquo;d discourage that use, merely as a matter of consistency with common usage; conventionally, &ldquo;theory of everything&rdquo; refers to the underlying laws that define the regularities of <em>this</em> universe, and whatever heroic physicists eventually discover those laws should retain the honour of having their theory known as such. As a metaphysical theory (less arbitrary than conventional metaphysics, but metaphysical nonetheless), this does not fit that description; it gives us almost no useful information about our own universe. It is a theory of more than everything, and a theory of nothing (in the same way that a program that prints out every possible bit string will eventually print out any given piece of information, while its actual information content is near zero).</p>\n<p>That said, this theory and the argument I presented are not <em>entirely</em> free of implications about and practical applications within this particular universe. Here are some of them.</p>\n<ul>\n<li>\n<p>The <a href=\"http://www.simulation-argument.com/\">simulation argument</a> is dissolved. At this point, the idea of &ldquo;living in a computer simulation&rdquo; is meaningless. Simulating a universe should properly be viewed as comparable more to looking in a window than building the house. (Most of Robin Hanson&rsquo;s <a href=\"http://www.jetpress.org/volume7/simulation.htm\">thoughts about metaethics and self-preservation within a simulation</a> are similarly dissolved, since a reality doesn&rsquo;t pop out of existence when people stop simulating it; the only relevant part is the section about &ldquo;If our descendants sometimes play parts in their simulations&rdquo;, and this doesn&rsquo;t seem to be the case anyway.)</p>\n</li>\n<li>\n<p>As I mentioned, this significantly changes the dynamics of thought experiments like <a href=\"/lw/1pz/the_ai_in_a_box_boxes_you/\">The AI In A Box Boxes You</a>. Torturing a thousand identical simulations is the same as torturing one, and torturing one is the same as torturing zero &mdash;&nbsp;<em>if and only if</em> the structure within the simulation(s) is not being causally influenced by any ongoing circumstances in <em>this</em> universe. If it is, then the two realities are entangled to the point where they are essentially different parts of the same structure, and it is worth thinking about how much we should care about each one.</p>\n</li>\n<li>\n<p>That leads me to a more general point about metaethics: although there are other realities out there where there are very sentient and very intelligent beings experiencing suffering literally 3^^^3 times greater than anything we can imagine, and others where there are beings experiencing bliss in the same proportions,&nbsp;we must resist the urge to feel (respectively) sorry for them or jealous of them. Your intuitive sense of what &ldquo;really exists&rdquo; should remain limited to this universe.</p>\n<p>Perhaps this caution only applies to me in the first place. I am, admittedly, the only person I know who has to leave the room when people are playing <em>The Sims</em> because I can&rsquo;t stand to watch those little nowhere-near-sentient structures being put in torturous or even merely uncomfortable situations, so maybe it&rsquo;s only my own empathy that&rsquo;s a bit overactive. However, when we&rsquo;re talking about sentient, sapient structures, we really do need to think about where to draw the line. I&rsquo;d draw it at the point where a simulation starts to interact with this universe, in both directions &mdash;&nbsp;of course it will affect our universe if we are observing the simulation and reacting based on it, but we should only start caring about its feelings if we have designed the software such that <em>it</em> is affected by <em>our actions</em> beyond our choices for its initial conditions. That&rsquo;s what I referred to as entanglement earlier. Once there&rsquo;s that bilateral feedback, it&rsquo;s no longer one structure observing another; they are both part of the same reality. (Take that as a practicality, not as a statement of an alleged metaphysical law. We&rsquo;re trying to eliminate the need for metaphysical laws here.)</p>\n</li>\n<li>\n<p>This theory results in a variation on the <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Boltzmann brain scenario</a>: regardless of <em>this</em> universe&rsquo;s ability to create Boltzmann brains, there&rsquo;s also the possibility (and, therefore, necessity) of disembodied mind-structures hallucinating their own realities. My best guess as to the solution to this problem (if we&rsquo;re to take it as a problem) is that any mind-structure that contains enough information to reliably hallucinate an orderly, mechanistic reality must be isomorphic to that reality.</p>\n</li>\n<li>\n<p>It raises other strange anthropic questions too. The one that comes most immediately to my mind is this: If every possible mathematical structure is real in the same way that this universe is, then isn&rsquo;t there only an infinitesimal probability that this universe will turn out to be ruled entirely by simple regularities? Given a universe governed by a small set of uniformly applied laws, there will be an infinity of universes governed by the same laws plus arbitrary variations, possibly affecting the internally observable structure only at very specific points in space and time. This results in a sort of anti&ndash;Occam&rsquo;s Razor (Macco&rsquo;s Rozar? Occam&rsquo;s Beard Tonic?), where the larger the irregularity, the more likely it becomes over the space of all possible universes, because there are that many more ways for it to happen. (For example, there is a universe &mdash;&nbsp;actually, a huge number (possibly infinity) of barely different universes &mdash;&nbsp;identical to this one, except that, for no reason explainable by the usual laws of quantum mechanics, but not ruled out as a logically possible law unto itself, your head will explode as soon as you finish reading this post. I hope that possibility does not dissuade you from doing so, but I accept no responsibility if this <em>does</em> turn out to be one of those universes.)</p>\n<p>From the outside, this would appear to be a non-issue. Consider people in some other reality simulating this one (assuming that this one really <em>is</em> as simple and consistent as it appears). By some extraordinary luck, they&rsquo;ve zoomed in on this exact planet in this exact Everett branch, and from there, they&rsquo;ve even zoomed in on me writing this. &ldquo;What does this guy mean,&rdquo; they ask themselves, &ldquo;wondering what the probability is that this particular reality will have the laws that it does? It&rsquo;s not like anyone had any choice in the matter.&rdquo; Yes, there will be versions of the universe that really are that orderly, and if this is one of them, than that would be why this universe&rsquo;s version of me is wondering about the apparent astronomical unlikelihood of being in this universe. But from the inside, this seems terribly unsatisfying &mdash; if these slightly-irregular universes are possible, then we don&rsquo;t know for sure what kind we&rsquo;re in, so <em>should</em> we expect to find such irregularities? Perhaps such exceptions would constitute such a departure from quantum mechanics that they couldn&rsquo;t be made consistent with it even as a special case. (Tegmark makes a related point in one paper: the hypothesis &ldquo;does certainly <em>not</em> imply that all imaginable universes exist. We humans can imagine many things that are mathematically undefined and hence do not correspond to mathematical structures.&rdquo;) Or perhaps the infinity of universes where such irregularities exist in places we&rsquo;ll never observe (outside our light cone, in vast areas of empty space, etc.) is a much larger infinity (in probability density, not cardinality) than that of those universes where any of those irregularities will actually affect us. I&rsquo;m leaning toward that explanation, but maybe a simpler one is that I&rsquo;m reasoning about this incorrectly &mdash;&nbsp;after a conversation about this with Justin Shovelain, I&rsquo;m reconsidering whether it&rsquo;s actually correct to use probabilities to reason about an infinite space of apparently equally-likely items &mdash; or maybe this reasoning is correct and it observationally refutes the hypothesis. We&rsquo;ll see.</p>\n</li>\n</ul>\n<hr />\n<p>One last comment: some people I&rsquo;ve discussed this with have actually taken it as a <em>reductio ad absurdum</em> against the idea that a being within a simulation <em>could</em> feel real. As we say, one person&rsquo;s <em>modus ponens</em> is another person&rsquo;s <em>modus tollens</em>. Since the conclusion I&rsquo;m arguing for is merely unusual, not inconsistent (as far as I can tell), that takes out the <em>absurdum</em>; therefore, in the apparent absence of any specific alternatives at all, you can weigh the probability of this hypothesis against the stand-in alternatives that there <em>is</em> something extraphysical about our own existence, something noncomputable about consciousness, or something metaphysically significant about processes equivalent to universal computation (or any other alternatives that I&rsquo;ve neglected to think of).</p>\n<p>Finally, as I mentioned, the main goal of this post was to serve as an intuition pump for the&nbsp;Level IV Multiverse idea (and to point out some of the rationality-related questions it raises, so we&rsquo;ll have something apropos to discuss here), not to explore it in depth.&nbsp;So if this was your first exposure to it, you should probably read Max Tegmark&rsquo;s <a href=\"http://arxiv.org/abs/0704.0646\">The Mathematical Universe</a> now.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 1, "6nS8oYmSMuFMaiowF": 1, "22z6XpWKqw3bNv4oR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fZJRxYLtNNzpbWZAA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 96, "baseScore": 97, "extendedScore": null, "score": 0.000163, "legacy": true, "legacyId": "2585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 98, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FWMfQKG3RpZx6irjm", "DNyMJmLf5o26seqvX", "c5GHf2kMGhA4Tsj4g", "LubwxZHKKvCivYGzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-26T13:51:08.199Z", "modifiedAt": null, "url": null, "title": "Compartmentalization as a passive phenomenon", "slug": "compartmentalization-as-a-passive-phenomenon", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:06.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kD8uzcmjKwSaTHnQJ/compartmentalization-as-a-passive-phenomenon", "pageUrlRelative": "/posts/kD8uzcmjKwSaTHnQJ/compartmentalization-as-a-passive-phenomenon", "linkUrl": "https://www.lesswrong.com/posts/kD8uzcmjKwSaTHnQJ/compartmentalization-as-a-passive-phenomenon", "postedAtFormatted": "Friday, March 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Compartmentalization%20as%20a%20passive%20phenomenon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACompartmentalization%20as%20a%20passive%20phenomenon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkD8uzcmjKwSaTHnQJ%2Fcompartmentalization-as-a-passive-phenomenon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Compartmentalization%20as%20a%20passive%20phenomenon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkD8uzcmjKwSaTHnQJ%2Fcompartmentalization-as-a-passive-phenomenon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkD8uzcmjKwSaTHnQJ%2Fcompartmentalization-as-a-passive-phenomenon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 920, "htmlBody": "<p>We commonly discuss compartmentalization as if it were an active process, something you <em>do</em>. Eliezer suspected <a href=\"/lw/yu/formative_youth/\">his altruism</a>, as well as <a href=\"/lw/1mh/what_is_in_that_click/\">some people's \"clicking\"</a>, was due to a \"failure to compartmentalize\". Morendil discussed compartmentalization as <a href=\"/lw/1v4/the_fallacy_of_worklife_compartmentalization/\">something to avoid</a>. But I suspect compartmentalization might actually be the <em>natural</em> state, the one that requires effort to overcome.</p>\n<p>I started thinking about this when I encountered an article claiming that the <a href=\"http://www.falstad.com/gravity.html\">average American does not know the answer</a> to the following question:</p>\n<blockquote>\n<p>If a pen is dropped on a moon, will it:<br /> A) Float away<br /> B) Float where it is<br /> C) Fall to the surface of the moon</p>\n</blockquote>\n<p>Now, I have to admit that the correct answer <em>wasn't obvious </em>to me at first. I thought about it for a moment, and almost settled on B - after all, there isn't much gravity on the moon, and a pen is so light that it might just be unaffected. It was only then that I remembered that the astronauts had walked on the surface of the moon without trouble. Once I remembered that piece of knowledge, I was able to deduce that the pen quite probably would fall.</p>\n<p>A link on that page brought me to <a href=\"http://www.milk.com/wall-o-shame/heavy_boots.html\">another article</a>. This one described two students randomly calling 30 people and asking them the question above. 47 percent of them got the question correct, but what was interesting was that those who got it wrong were asked a follow-up question: \"You've seen films of the APOLLO astronauts walking around on the Moon, why didn't they fall off?\" Of those who heard it, about 20 percent changed their answer, but about half confidently replied, \"Because they were wearing heavy boots\".</p>\n<p>While these articles were totally unscientific surveys, it doesn't seem to me like this would be the result of an <em>active</em> process of compartmentalization. I don't think my mind first knew that pens would fall down because of gravity, but quickly hid that knowledge from my conscious awareness until I was able to overcome the block. What would be the point in that? Rather, it seems to indicate that my \"compartmentalization\" was simply a lack of a connection, and that such connections are <em>much harder to draw</em> than we might assume.</p>\n<p><a id=\"more\"></a>The world is a complicated place. One of the reasons we don't have AI yet is because we haven't found very many reliable cross-domain reasoning rules. Reasoning algorithms in general are quickly subject to a combinatorial explosion: the reasoning system might know which potential inferences are valid ones, but not which ones are meaningful in any useful sense. Most current-day AI systems need to be more or less fine-tuned or rebuilt entirely when they're made to reason in a domain they weren't originally built for.</p>\n<p>For humans, it can be even worse than that. Many of the basic tenets in a variety of fields are counter-intuitive, or are intuitive but have counter-intuitive consequences. The universe <a href=\"/lw/hq/universal_fire/\">isn't actually fully arbitrary</a>, but for somebody who doesn't know how all the rules add up, it might as well be. Think of all the times when somebody has tried to reason using <a href=\"/lw/rj/surface_analogies_and_deep_causes/\">surface analogies</a>, mistaking them for deep causes; or dismissed a deep cause, mistaking it for a surface analogy. Somebody might present us with a connection between two domains, but we have no sure way of testing the validity of that connection.</p>\n<p>Much of our reasoning, I suspect, is actually pattern recognition. We initially have no idea of the connection between X and Y, but then we see X and Y occur frequently together, and we begin to think of the connection as an \"obvious\" one. For those well-versed in physics, it seems mind-numbingly bizarre to hear someone claim that the Moon's gravity isn't enough to affect a pen, but is enough to affect people wearing heavy boots. But as for some hypothetical person who hasn't studied much physics... or screw the hypotheticals - for <em>me</em>, this sounds wrong but not <em>obviously and completely </em>wrong. I mean, \"the pen has less mass, so there's less stuff for gravity to affect\" sounds intuitively sorta-plausible for me, because I haven't had enough exposure to formal physics to hammer in the right intuition.</p>\n<p>I suspect that often when we say \"(s)he's compartmentalizing!\", we're operating in a domain that's more familiar to us, and thus it feels like an <em>active attempt</em> to keep things separate must be the cause. After all, how could they not see it, were they not actively keeping it compartmentalized?</p>\n<p>So my theory is that much of compartmentalization is simply because the search space is so large that people don't end up seeing that there might be a connection between two domains. Even if they do see the potential, or if it's explicitly pointed out to them, they might still not know enough about the domain in question (such as in the example of heavy boots), or they might find the proposed connection implausible. If you don't know which cross-domain rules and reasoning patterns are valid, then building up a separate set of rules for each domain is the safe approach. Discarding as much of your previous knowledge as possible when learning about a new thing is slow, but it at least guarantees that you're not polluted by existing incorrect information. Build your theories primarily on evidence found from a single domain, and they will be true within that domain. While there can certainly also be situations calling for an active process of compartmentalization, that might only happen in a minority of the cases.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KWFhr6A2dHEb6wmWJ": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kD8uzcmjKwSaTHnQJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 60, "extendedScore": null, "score": 0.000107, "legacy": true, "legacyId": "2586", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hwbopYqniG9iDqGDH", "R3ATEWWmBhMhbY2AL", "ZouugGbM4SqTEQBZW", "LaM5aTcXvXzwQSC2Q", "6ByPxcGDhmx74gPSm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-26T16:16:55.362Z", "modifiedAt": null, "url": null, "title": "The Shabbos goy", "slug": "the-shabbos-goy", "viewCount": null, "lastCommentedAt": "2019-12-06T07:21:26.668Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cPJ9WhRBbT9PASiZ7/the-shabbos-goy", "pageUrlRelative": "/posts/cPJ9WhRBbT9PASiZ7/the-shabbos-goy", "linkUrl": "https://www.lesswrong.com/posts/cPJ9WhRBbT9PASiZ7/the-shabbos-goy", "postedAtFormatted": "Friday, March 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Shabbos%20goy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Shabbos%20goy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcPJ9WhRBbT9PASiZ7%2Fthe-shabbos-goy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Shabbos%20goy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcPJ9WhRBbT9PASiZ7%2Fthe-shabbos-goy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcPJ9WhRBbT9PASiZ7%2Fthe-shabbos-goy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 661, "htmlBody": "<p>Exodus 22:25, Leviticus 25:36, and Deuteronomy 23:20-21 forbid Jews from charging interest on loans to \"your brother\" (other Jews).&nbsp; (This is to me the most convincing argument against Judaism and Christianity, because it's too simple to argue around.&nbsp; That proscription is just wrong, in exactly the way you would expect laws written by uneducated tribal people to be wrong.)</p>\n<p>Roman Catholics believe they must follow the Old Testament laws, except for the ones they don't have to follow; but during much of the middle ages in Western Europe, this was one of the ones they had to follow.&nbsp; They interpreted \"your brother\" as meaning \"brother Christians\".&nbsp; So Jews could lend to Christians with interest (and, presumably, Christians could lend to Jews).&nbsp; This was convenient for everyone.&nbsp; The Jews were necessary to work around an irrational moral prohibition of the Christians.</p>\n<p>Of course, the Jews had to take on the guilt of violating the moral code, even though it was for the benefit of the Christians.&nbsp; (This was also convenient; it meant that after some Jews had loaned you an especially large amount of money, you could kill or expel them instead of paying them back, as the Spanish monarchy did in 1492).</p>\n<p>Later on, some orthodox Jews hired goyim to turn lightswitches and other electric devices on and off for them on the Sabbath.&nbsp; They're called Shabbos goy, the Sabbath goy (thanks, Alicorn!).</p>\n<p>JCVI is considering moving from an on-site hardware grid, to cloud computing.&nbsp; There are lots of reasons to do this.&nbsp; One is so that Amazon can be our Shabbos goy.</p>\n<p><a id=\"more\"></a></p>\n<p>We develop lots of bioinformatics software that we're supposed to, and would like to, give out to anyone who wants it.&nbsp; But if you don't have 800 computers at home, connected using the Sun Grid Engine with a VICS interface and using a Sybase database, with exactly the same versions of C++ and Perl and every C++ and Perl library that we do, you're going to have a hard time running the software.</p>\n<p>We can't put up a web service and let anybody send their jobs to our computers, because then some professor is going to say to their freshman class of 200 students, \"Today, class, your assignment is to assemble a genome using JCVI's free genome assembly web service.\"</p>\n<p>If we could charge users just a little bit of money, just a fraction of the cost of running their programs, we could probably do this.&nbsp; Then people wouldn't be so cavalier about running a program repeatedly that takes 500 CPU hours each time you run it.</p>\n<p>But we can't, because we're an academic institution.&nbsp; So that would be evil.</p>\n<p>So we need a Shabbos goy.&nbsp; That's Amazon.&nbsp; We can release our software and tell users, \"All you have to do to run this is to get an account on the Amazon cloud and run it there.&nbsp; Of course, they'll charge you for it.&nbsp; They're evil.\"</p>\n<p>(The Amazon cloud is evil, BTW.&nbsp; They charged me for 21G of RAM and then only gave me 12, and charged me for 24 1GHz processors and gave me about 1/4 of that.&nbsp; I spent over $100 and was never able to run my program; and they told me to stuff it when I complained.&nbsp; But that's another story.)</p>\n<h4>Summary</h4>\n<div id=\"body_t1_1til\" class=\"comment-content\">\n<div class=\"md\">\n<ul>\n<li>People would rather patch around failings in their reasoning than fix them.</li>\n<li>If your morals require you to use the services of someone not adhering to your morals, you may be in error.</li>\n</ul>\n</div>\n<div class=\"md\">P.S. - My morals may require me to use the services of someone not adhering to my morals.&nbsp; I believe in a moral ecosystem:&nbsp; You don't hold your dog to your moral standard; and you don't remedy this by adopting your dog's moral standard.&nbsp; But AFAIK I'm the only one.&nbsp; People who believe in one universal moral code shouldn't use Shabbos goyim.&nbsp; (I don't think that orthodox Jews believe gentiles are supposed to obey the Torah, so their use of Shabbos goyim may be logically consistent.)<br /></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "nSHiKwWyMZFdZg5qt": 1, "HFou6RHqFagkyrKkW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cPJ9WhRBbT9PASiZ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 46, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "2587", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-26T18:31:43.355Z", "modifiedAt": null, "url": null, "title": "Newcomb's problem happened to me", "slug": "newcomb-s-problem-happened-to-me", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:01.312Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6fvzjL4duMsWXswKf/newcomb-s-problem-happened-to-me", "pageUrlRelative": "/posts/6fvzjL4duMsWXswKf/newcomb-s-problem-happened-to-me", "linkUrl": "https://www.lesswrong.com/posts/6fvzjL4duMsWXswKf/newcomb-s-problem-happened-to-me", "postedAtFormatted": "Friday, March 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Newcomb's%20problem%20happened%20to%20me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANewcomb's%20problem%20happened%20to%20me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fvzjL4duMsWXswKf%2Fnewcomb-s-problem-happened-to-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Newcomb's%20problem%20happened%20to%20me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fvzjL4duMsWXswKf%2Fnewcomb-s-problem-happened-to-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fvzjL4duMsWXswKf%2Fnewcomb-s-problem-happened-to-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 966, "htmlBody": "<!-- Newcomb's problem happened to me -->\n<p>Okay, maybe not me, but someone I know, and that's what the title would be if he wrote it.&nbsp;    <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\" target=\"_blank\">Newcomb's problem</a> and <a href=\"http://en.wikipedia.org/wiki/Kavka's_toxin_puzzle\" target=\"_blank\">Kavka's toxin puzzle</a> are more than just curiosities relevant to artificial intelligence theory.&nbsp;  Like a lot of thought experiments, they <em>approximately happen</em>.&nbsp;  They illustrate <em>robust issues</em> with <a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\" target=\"_blank\">causal decision theory</a> that can <em>deeply affect</em> our everyday lives.</p>\n<p>Yet somehow it isn't mainstream knowledge that these are more than merely abstract linguistic issues, as evidenced by <a href=\"/lw/7i/rationality_is_systematized_winning/52q?context=1#52q\" target=\"_blank\">this comment thread</a> (please no Karma sniping of the comments, they are a valuable record).&nbsp;  Scenarios involving brain scanning, decision simulation, etc., can establish their validy and future relevance, but not that they are already commonplace.&nbsp;  For the record, I want to provide an already-happened, real-life account that captures the Newcomb essence and <em>explicitly describes how</em>.</p>\n<p>So let's say my friend is named Joe.&nbsp;  In his account, Joe is very much in love with this girl named Omega&hellip; er&hellip; Kate, and he wants to get married.&nbsp;  Kate is somewhat traditional, and won't marry him unless he <em>proposes</em>, not only in the sense of explicitly asking her, but also <em>expressing certainty that he will never try to leave her if they do marry</em>.&nbsp;</p>\n<p>Now, I don't want to make up the ending here.&nbsp;  I want to convey the actual account, in which Joe's beliefs are roughly schematized as follows:&nbsp;</p>\n<ol>\n<li>if he <em>proposes sincerely</em>, she is effectively sure to <em>believe it</em>. </li>\n<li>if he <em>proposes insincerely</em>, she will 50% likely <em>believe it</em>. </li>\n<li>if she <em>believes his proposal</em>, she will 80% likely <em>say yes</em>. </li>\n<li>if she <em>doesn't believe his proposal</em>, she will surely <em>say no</em>, but will not be significantly upset in comparison to the significance of marriage. </li>\n<li>if they marry, Joe will 90% likely be <em>happy</em>, and will 10% likely be <em>unhappy</em>. </li>\n</ol>\n<p>He roughly values the happy and unhappy outcomes oppositely:</p>\n<ol>\n<li>being happily married to Kate:&nbsp; 125 megautilons </li>\n<li>being unhapily married to Kate:&nbsp; -125 megautilons. </li>\n</ol>\n<p>So what should he do?&nbsp;  <em>What should this real person have actually done?</em><a href=\"#1\"><sup>1</sup></a>&nbsp;  Well, as in Newcomb, these beliefs and utilities present an interesting and quantifiable problem&hellip; <a id=\"more\"></a></p>\n<ul>\n<li>ExpectedValue(<strong>marriage</strong>) = 90%&middot;125 - 10%&middot;125 = <strong>100</strong>, </li>\n<li>ExpectedValue(<strong>sincere proposal</strong>) = 80%&middot;100 = <strong>80</strong>, </li>\n<li>ExpectedValue(<strong>insincere proposal</strong>) = 50%&middot;80%&middot;100 = <strong>40</strong>. </li>\n</ul>\n<p>No surprise here, sincere proposal comes out on top.&nbsp; That's the important thing, not the particular numbers.&nbsp;  In fact, in real life Joe's utility function assigned negative moral value to insincerity, broadening the gap.&nbsp;  But no matter; this did not <em>make</em> him sincere.&nbsp;  The problem is that Joe was a <a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\" target=\"_blank\"><em>classical causal decision theorist</em></a>, and he believed that if circumstances changed to render him unhappily married, he would necessarily try to leave her.&nbsp;  Because of this possibility, he could not propose sincerely in the sense she desired.&nbsp; He could even <em>appease himself</em> by speculating causes<a href=\"#2\"><sup>2</sup></a> for how Kate can detect his uncertainty and constrain his options, but that <em>still wouldn't make him sincere</em>.&nbsp;</p>\n<p>Seeing expected value computations with adjustable probabilities for the problem can really help <em>feel its robustness</em>.&nbsp;  It's not about to disappear.&nbsp;  Certainties can be replaced with 95%'s and it all still works the same.&nbsp;  It's a whole <em>parametrized family</em> of problems, not just one.&nbsp;</p>\n<p>Joe's scenario feels strikingly similar to Newcomb's problem, and in fact it is:&nbsp; if we change some probabilities to 0 and 1, it's essentially isomorphic:&nbsp;</p>\n<ol>\n<li>If he proposes sincerely, she will say yes. </li>\n<li>If he proposes insincerely, she will say no and break up with him forever. </li>\n<li>If they marry, he is 90% likely to be happy, and 10% likely to be unhappy. </li>\n</ol>\n<p>The analogue of the two boxes are marriage (opaque) and the option of leaving (transparent).&nbsp;  Given marriage, the option of leaving has a small marginal utility of 10%&middot;125 = 12.5 utilons.&nbsp;  So \"clearly\" he should \"just take both\"?&nbsp;  The problem is that he <em>can't just take both</em>.&nbsp;  The proposed payout matrix would be:</p>\n<p>\n<table style=\"cursor:  default;\" border=\"3\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"cursor:  text;\">Joe \\ Kate<br /></th><th style=\"cursor:  text;\">Say yes<br /></th><th style=\"cursor:  text;\">Say no<br /></th>\n</tr>\n<tr>\n<th style=\"cursor:  text;\">Propose sincerely<br /></th>\n<td style=\"color:  #000000; font-family:  Verdana, Arial, Helvetica, sans-serif; font-size:  15px; cursor:  text; margin:  8px;\" align=\"center\">Marriage</td>\n<td style=\"color:  #000000; font-family:  Verdana, Arial, Helvetica, sans-serif; font-size:  15px; cursor:  text; margin:  8px;\" align=\"center\">Nothing significant</td>\n</tr>\n<tr>\n<th style=\"cursor:  text;\">Propose insincerely<br /></th>\n<td style=\"color:  #000000; font-family:  Verdana, Arial, Helvetica, sans-serif; font-size:  15px; cursor:  text; margin:  8px;\" align=\"center\">Marriage + option to leave</td>\n<td style=\"color:  #000000; font-family:  Verdana, Arial, Helvetica, sans-serif; font-size:  15px; cursor:  text; margin:  8px;\" align=\"center\">Nothing significant<br /></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>The \"principal of (weak<a href=\"#3\"><sup>3</sup></a>) dominance\" would say the second row is the better \"option\", and that therefore \"clearly\" Joe should propose insincerely.&nbsp;  But in Newcomb some of the outcomes are declared logically impossible.&nbsp;  If he tries to take both boxes, there will be nothing in the marriage box.&nbsp;  The analogue in real life is simply that <em>the four outcomes need not be equally likely</em>.&nbsp;</p>\n<p>So there you have it.&nbsp;  Newcomb happens.&nbsp;  Newcomb <em>happened</em>.&nbsp;  You might be wondering, what did the real Joe <em>do</em>?&nbsp;</p>\n<p>In real life, Joe actually recognized the similarity to Newcomb's problem, realizing for the first time that he must become <a href=\"/lw/15m/my_timeless_decision_theory/\">updateless decision agent</a>, and noting his 90% certainty, he <a href=\"http://en.wikipedia.org/wiki/Self-modifying_code\">self-modified</a> by adopting a moral <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">pre-commitment</a> to never leaving Kate should they marry, proposed to her sincerely, and the rest is history.&nbsp;  No joke!&nbsp;  That's if Joe's account is accurate, mind you.</p>\n<p>&nbsp;</p>\n<hr />\n<p>Footnotes:</p>\n<p><a name=\"1\"><sup>1</sup></a> This is not a social commentary, but an illustration that probabilistic Newcomblike scenarios <em>can and do exist</em>.&nbsp;  Although this also does not hinge on whether you believe Joe's account, I have provided it as-is nonetheless.&nbsp;</p>\n<p><a name=\"2\"><sup>2</sup></a> If you care about causal reasoning, the other half of what's supposed to make Newcomb confusing, then Joe's problem is more like Kavka's (so this post accidentally shows how Kavka and Newcomb are similar).&nbsp;  But the distinction is instrumentally irrelevant:&nbsp; the point is that he can benefit from decision mechanisms that are <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\">evidential</a> and time-invariant, and you don't need \"unreasonable certainties\" or \"paradoxes of causality\" for this to come up.&nbsp;</p>\n<p><a name=\"3\"><sup>3</sup></a> Newcomb involves \"strong\" dominance, with the second row always strictly better, but that's not essential to this post.&nbsp;  In any case, I could exhibit strong dominance by removing \"if they do get married\" from Kate's proposal requirement, but I decided against it, favoring instead the actual account of events.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "fihKHQuS5WZBJgkRm": 3, "mip7tdAN87Jarkcew": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6fvzjL4duMsWXswKf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 53, "extendedScore": null, "score": 9e-05, "legacy": true, "legacyId": "2588", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["de3xjFaACCAk6imzv", "6ddcsdA2c2XpNpE5x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-26T23:02:28.384Z", "modifiedAt": null, "url": null, "title": "Addresses in the Multiverse", "slug": "addresses-in-the-multiverse", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:46.835Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xLqmthhc2H5EzTMv5/addresses-in-the-multiverse", "pageUrlRelative": "/posts/xLqmthhc2H5EzTMv5/addresses-in-the-multiverse", "linkUrl": "https://www.lesswrong.com/posts/xLqmthhc2H5EzTMv5/addresses-in-the-multiverse", "postedAtFormatted": "Friday, March 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Addresses%20in%20the%20Multiverse&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAddresses%20in%20the%20Multiverse%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLqmthhc2H5EzTMv5%2Faddresses-in-the-multiverse%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Addresses%20in%20the%20Multiverse%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLqmthhc2H5EzTMv5%2Faddresses-in-the-multiverse", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLqmthhc2H5EzTMv5%2Faddresses-in-the-multiverse", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1253, "htmlBody": "<p><em>Abstract: If we assume that any universe can be modeled as a computer program which has been running for finitely many steps, then we can assign a multiverse-address to every event by combining its world-program with the number of steps into the world-program where it occurs. We define a probability distribution over multiverse-addresses called a Finite Occamian Multiverse (FOM). FOMs assign negligible probability mass to being a Boltzmann brain or to being in a universes that implements the Many Worlds Interpretation of quantum mechanics.</em></p>\n<p>One explanation of existence is the Tegmark level 4 multiverse, the idea that all coherent mathematical structures exist, and our universe is one of them. To make this meaningful, we must add a probability distribution over mathematical structures, effectively assigning each a degree of existence. Assume that the universe we live in can be fully modeled as a computer program, and that that program, and the number of steps it's been running for, are both finite. (Note that it's not clear whether our universe is finite or infinite; our universe is either spatially infinite, or expanding outwards at a rate greater than or equal to the speed of light, but there's no observation we could make inside the universe that would distinguish these two possibilities.) Call the program that implements our universe a world-program, W.&nbsp; This could be implemented in any programming language - it doesn't really matter which, since we can translate between languages by prepending some stuff to translate.</p>\n<p>Now, suppose we choose a <em>particular event</em> in the universe - an atom emitting a photon, say - and we want to find a corresponding operation in the world-program. We could, in principle, run W until it starts working on the part of spacetime we care about, and count the steps. Call the number of steps leading up to this event T. Taken together, the pair (W,T) uniquely identifies a place, not just in the universe, but in the space of all possible universes. Call any such pair (W,T) a multiverse-address.<br /><br />Now, suppose we observe an event. What should be our prior probability distribution over multiverse-addresses for that event? That is, for a given event (W,T), what is P(W=X and T=Y)?<br /><a id=\"more\"></a></p>\n<p>For this question, our best (and pretty much only possible) tool is Occam's Razor. We're after a <em>prior</em> probability distribution, so we aren't going to bother including all the things we know about W from observation, except that we have good reason to believe that W is short - what we know of physics seems to indicate that at the most basic level, the rules are simple. So, first factor out W and apply Occam's Razor to it:<br /><br />&nbsp;&nbsp;&nbsp; P(W=X and T=Y) = P(W=X) * P(T=Y|W=X)<br />&nbsp;&nbsp;&nbsp; P(W=X and T=Y) = exp(-len(W)) * P(T=Y|W=X)<br /><br />Now assume independence between T and W. This isn't entirely correct (some world-programs are structured in such a way that all the events we might be looking for happen on even time-steps, for example), but that kind of entanglement isn't important for our purposes. Then apply Occam's Razor to T, getting<br /><br />&nbsp;&nbsp;&nbsp; P(W=X and T=Y) = exp(-len(W)-len(T))</p>\n<p>Now, applying Occam's razor to T requires some explanation, and there is one detail we have glossed over; we referred to the <em>length</em> of W and T (that is, their logarithm), when we should have referred to their <em>Kolmogorov complexity</em> - that is, their length after compression. For example, a world-program that contains 10^10 random instructions is much less likely than one that contains 10^10 copies of the same instruction. Suppose we resolve this by requiring W to be fully compressed, and give it an initialization stage where it unpacks itself before we start counting steps for T.<br /><br />This lets us transfer bits of complexity from T to W, by having W run itself for awhile during the initialization stage. We can also transfer complexity from W to T, by writing W in such a way that it runs a class of programs in order, and T determines which of them it's running. Since we can transfer complexity back and forth between W and T, we can't justify applying Occam's Razor to one but not the other, so it makes sense to apply it to T. This also means that we should also treat T as compressible; it is more likely that the universe is 3^^^3 steps old than that is 207798236098322674 steps old.<br /><br />To recap - we started by assuming that the universe is a computer program, W. We chose an event in W, corresponding to a computation that occurs after W has preformed T operations. We assume that W and T are both finite. Occam's Razor tells us that W, if fully compressed, should be short. We can trade off complexity between W and T, so we should also apply Occam's Razor to T and expect that T, if fully compressed, should also be short. We had to assume that the universe behaves like a computer program, that that program is finite, and that the probability distribution which Occam's Razor gives us is actually meaningful here.</p>\n<p>We then got P(W=X and T=Y) = exp(-len(W)-len(T)). Call this probability distribution a Finite Occamian Multiverse (FOM). We can define this in terms of different programming languages, reweighting the probabilities of different universe-addresses somewhat, but all FOMs share some interesting properties.</p>\n<p>A Finite Occamian Multiverse avoids the Boltzmann brain problem. A Boltzmann brain is a brain that, rather than living in a simulation with stable physics that allow it to continue to exist as the simulation advances, arises by chance out of randomly arranged particles or other simulation-components, and merely thinks (contains a representation of the claim that) it lives in a universe with stable physics. If you live in a FOM, then the probability that you are a Boltzmann brain is negligible because Boltzmann brains must have extremely complex multiverse-addresses, while evolved brains can have multiverse-addresses that are simple.<br /><br />If we are in a Finite Occamian Multiverse, then the Many Worlds interpretation of quantum mechanics must be false, because if it were true, then any multiverse address would have to contain the complete branching history of the universe, so its length would be proportional to the mass of the universe times the age of the universe. On the other hand, if branches were selected according to a pseudo-random process, then multiverse-addresses would be short. This sort of pseudo-random process would slightly increase the length of W, but drastically decrease the length of T. In other words, in this type of multiverse, worldeaters eat more complexity than they contain.<br /><br />If we are in a Finite Occamian Multiverse, then we might also expect certain quantities, such as the age and volume of the universe, to have much less entropy than otherwise expected. If, for example, we discovered that the universe had been running for exactly 3^^^3+725 time steps, then we could be reasonably certain that we were inside such a multiverse.<br /><br />This kind of multiverse also sets an upper bound on the total amount of entropy (number of fully independent random bits) that can be gathered in one place, equal to the total complexity of that place's multiverse-address, since it would be possible to generate all of those bits from the multiverse-address by simulating the universe. However, since simulating the universe is intractible, the universe can still act as a very-strong cryptographic pseudorandom number generator.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xLqmthhc2H5EzTMv5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 5.725465681873756e-07, "legacy": true, "legacyId": "2589", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-27T14:32:52.050Z", "modifiedAt": null, "url": null, "title": "It's not like anything to be a bat", "slug": "it-s-not-like-anything-to-be-a-bat", "viewCount": null, "lastCommentedAt": "2021-08-10T15:01:53.055Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K3hFLRn7MvYacL466/it-s-not-like-anything-to-be-a-bat", "pageUrlRelative": "/posts/K3hFLRn7MvYacL466/it-s-not-like-anything-to-be-a-bat", "linkUrl": "https://www.lesswrong.com/posts/K3hFLRn7MvYacL466/it-s-not-like-anything-to-be-a-bat", "postedAtFormatted": "Saturday, March 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20It's%20not%20like%20anything%20to%20be%20a%20bat&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIt's%20not%20like%20anything%20to%20be%20a%20bat%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK3hFLRn7MvYacL466%2Fit-s-not-like-anything-to-be-a-bat%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=It's%20not%20like%20anything%20to%20be%20a%20bat%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK3hFLRn7MvYacL466%2Fit-s-not-like-anything-to-be-a-bat", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK3hFLRn7MvYacL466%2Fit-s-not-like-anything-to-be-a-bat", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 695, "htmlBody": "<p>...at least not if you accept a certain line of anthropic argument.<br /><br />Thomas Nagel famously challenged the philosophical world to come to terms with qualia in his essay \"What is it Like to Be a Bat?\". Bats, with sensory systems so completely different from those of humans, must have exotic bat qualia that we could never imagine. Even if we deduce all the physical principles behind echolocation, even if we could specify the movement of every atom in a bat's senses and nervous system that represents its knowledge of where an echolocated insect is, we still have no idea what it's like to feel a subjective echolocation quale.<br /><br />Anthropic reasoning is the idea that you can reason conditioning on your own existence. For example, the Doomsday Argument says that you would be more likely to exist in the present day if the overall number of future humans was medium-sized instead of humongous, therefore since you exist in the present day, there must be only a medium-sized number of future humans, and the apocalypse must be nigh, for values of nigh equal to \"within a few hundred years or so\".<br /><br />The Buddhists have a parable to motivate young seekers after enlightenment. They say - there are zillions upon zillions of insects, trillions upon trillions of lesser animals, and only a relative handful of human beings. For a reincarnating soul to be born as a human being, then, is a rare and precious gift, and an opportunity that should be seized with great enthusiasm, as it will be endless eons before it comes around again.<br /><br />Whatever one thinks of reincarnation, the parable raises an interesting point. Considering the vast number of non-human animals compared to humans, the probability of being a human is vanishingly low. Therefore, chances are that if I could be an animal, I would be. This makes a strong anthropic argument that it is impossible for me to be an animal.<a id=\"more\"></a><br /><br />The phrase \"for me to be an animal\" may sound nonsensical, but \"why am I me, rather than an animal?\" is not obviously sillier than \"why am I me, rather than a person from the far future?\". If the doomsday argument is sufficient to prove that some catastrophe is preventing me from being one of a trillion spacefaring citizens of the colonized galaxy, this argument hints that something is preventing me from being one of a trillion bats or birds or insects.<br /><br />And this could be that animals lack subjective experience. This would explain quite nicely why I'm not an animal: because you can't <em>be </em>an animal, any more than you can <em>be</em> a toaster. So Thomas Nagel can stop worrying about what it's like to be a bat, and the rest of us can eat veal and foie gras guilt-free.<br /><br />But before we break out the dolphin sausages - this is a pretty weird conclusion. It suggests there's a qualitative and discontinuous difference between the nervous system of other beings and our own, not just in what capacities they have but in the way they cause experience. It should make dualists a little bit happier and materialists a little bit more confused (though it's far from knockout proof of either).<br /><br />The most significant objection I can think of is that it is significant not that we are beings with experiences, but that we know we are beings with experiences and can self-identify as conscious - a distinction that applies only to humans and maybe to some species like apes and dolphins who are rare enough not to throw off the numbers. But why can't we use the reference class of conscious beings if we want to? One might as well consider it significant only that we are beings who make anthropic arguments, and imagine there will be no Doomsday but that anthropic reasoning will fall out of favor in a few decades.<br /><br />But I still don't fully accept this argument, and I'd be pretty happy if someone could find a more substantial flaw in it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 1, "8e9e8fzXuW5gGBS3F": 1, "XSryTypw5Hszpa4TS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K3hFLRn7MvYacL466", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 25, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "2597", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 192, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-28T15:55:40.902Z", "modifiedAt": null, "url": null, "title": "Mental Models", "slug": "mental-models", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:47.235Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s3xNWtE3GxgSXwNtt/mental-models", "pageUrlRelative": "/posts/s3xNWtE3GxgSXwNtt/mental-models", "linkUrl": "https://www.lesswrong.com/posts/s3xNWtE3GxgSXwNtt/mental-models", "postedAtFormatted": "Sunday, March 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mental%20Models&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMental%20Models%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3xNWtE3GxgSXwNtt%2Fmental-models%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mental%20Models%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3xNWtE3GxgSXwNtt%2Fmental-models", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3xNWtE3GxgSXwNtt%2Fmental-models", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1104, "htmlBody": "<p><strong>Related</strong>: <a href=\"/lw/ip/fake_explanations/\">Fake explanation</a>, <a href=\"/lw/iq/guessing_the_teachers_password/\">Guessing the teachers password</a>, <a href=\"/lw/1yq/understanding_your_understanding/\">Understanding your understanding</a>, <em>many more </em></p>\n<p>The mental model concept gets used so frequently and seems so intuitively obvious that I debated whether to bother writing this. But beyond the basic value that comes from unpacking our intuitions, it turns out that the concept allows a pretty impressive integration and streamlining of a wide range of mental phenomena.</p>\n<div>The basics: a mental model falls under the heading of mental representations, ways that the brain stores information. It's a specific sort of mental representation - one who's conceptual structure matches some corresponding structure in reality. In short, mental models are how we think something works.</div>\n<div>A mental model begins life as something like an explanatory black box - a mere correlation between items, without any understanding of the mechanism at work. \"Flick switch -&gt; lamp turns on\" for example. &nbsp;But a mere correlation doesn't give you much clue as to what's actually happening. If something stops working - if you hit the switch and the light doesn't go on - you don't have many clues as to why. This pre-model stage lacks the most important and useful portion; moving parts.</div>\n<div><a id=\"more\"></a></div>\n<div>The real power of mental models comes from putting something inside this black box &nbsp;- moving parts that you can fiddle with to give you an idea of how something actually works. My basic lamp model will be improved quite a bit if I add the concept of a circuit to it, for instance. Once I've done that, the model becomes \"Flick switch -&gt; switch completes circuit -&gt; electricity flows through lightbulb-&gt; lamp turns on\". Now if the light&nbsp;<em>doesn't&nbsp;</em>go on, I can play with my model to see what might cause that, finding that either the circuit is broken or no electricity is being provided. We learn from models the same way we learn from reality, by moving the parts around and seeing the results. &nbsp;</div>\n<div>It usually doesn't take much detail, many moving parts, for something to \"click\" and make sense. For instance, I had a difficult time grasping the essence of imaginary numbers until I saw them modeled as a&nbsp;<a href=\"http://betterexplained.com/articles/a-visual-intuitive-guide-to-imaginary-numbers\">rotation</a>, which instantly made all the bits and pieces I had gleaned about them fall into place. &nbsp;A great deal of understanding rests in getting a few small details right. And once the basics are right, additional knowledge often changes very little. After you have the basic understanding of a circuit, learning about resistance and capacitors and alternating vs direct current won't change much about your lamp model. Because of this, the key to understanding something is often getting the basic model right - I suspect bursts of insight, a-ha moments, and magical \"clicks\" are often new mental models suddenly taking shape.</div>\n<div>Now let's really open this concept up, and see what it can do. For starters, the reason analogies and metaphors are so damn useful (and can be so damn misleading) is that they're little more than pre-assembled mental models for something. Diagrams provide their explanatory mechanism through essentially the same principle. Phillip Johnson-Laird has formulated the processes of induction and deduction in terms of adjustments made to mental models. And building from the scenario concept used by Kahneman and Tversky, he's formulated a method of probabilistic thinking with them as well. Much of the work on heuristics and biases, in fact, either dovetails very nicely with mental models or can be explained by them directly.</div>\n<div>For example, the brain seems to have a strong bias towards modifying an existing model vs. replacing it with a new one. Often in real life \"updating\" means \"changing your underlying model\", and the fact that we prefer not to causes us to make systematic errors. You see this writ large all the time with (among other things) people endlessly tweaking a theory that fails to explain the data, rather than throwing it out. Ptomely's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Epicycles\">epicycles</a>&nbsp;would be the prototypical example. Confirmation bias, various attribution biases and various data neglect biases can all be interpreted as favoring the models we already have.&nbsp;</div>\n<div>The brain's favorite method for building models is to take parts from something else it already understands. Our best and most extensive experience is with objects moving in the physical world, so our models are often expressed in terms of physical objects moving about. They are, essentially, acting as&nbsp;<a href=\"http://en.wikipedia.org/wiki/Intuition_pump\">intuition pumps</a>. Of course, all models are wrong, but some are useful - as Dennett points out, converting problems to examples of something more familiar often allows us to solve problems&nbsp;<a href=\"http://en.wikipedia.org/wiki/Wason_selection_test\">much more easily</a>.</div>\n<div>One of the major design flaws of using mental models (aside from the biases they induce) is that our mental models always tend to&nbsp;<em>feel&nbsp;</em>like understanding, regardless of how many moving parts they have. So, for example, if the teacher asks \"why does fire burn\" and I answer \"because it's hot\", it feels like a real explanation, even if there's not any moving parts that might explain what 'burn' or 'hot' actually mean. I suspect a bias towards short causal chains may be involved here. Of course, if the model stops working, or you find yourself needing to explain yourself, it becomes quite obvious that you do not, in fact, have the understanding that you thought you did. And unpacking what turns out to be an empty box is a fantastic way to trigger cognitive dissonance, which can have the nasty effect of burrowing your flawed model in even deeper.</div>\n<div>So how can we maximize our use of mental models? Johnson-Laird tells us that \"any factor that makes it easier for individuals to flesh out explicit models of the premises should improve performance.\" Making clear what the moving parts are and couching it in terms of something already understood is going to help us build a better model, and a better model is equivalent to a better understanding. Again, this is not particularly groundbreaking - any problem solving technique will likely have the&nbsp;<a href=\"http://en.wikipedia.org/wiki/How_to_Solve_It\">same insights</a>.</div>\n<div>Ultimately, the mental model concept is itself just a model. I'm not familiar enough with the psychological literature to know if mental models are really the correct way to explain mental functions, or if it's merely another in a long list of similar concepts - belief, schema, framework, cognitive map, etc. But the fact that it's intuitively obvious and that it explains a large swath of brain function (without being too general) suggests that it's a useful concept to carry around, so I'll continue to do so until I have evidence that it's wrong.</div>\n<div>-</div>\n<div>Sources:</div>\n<div><a href=\"http://mentalmodels.princeton.edu/papers/1994probabilistic.pdf\">Mental models and probabilistic thinking - Johnson-Laird (1994)</a></div>\n<div><a href=\"https://www.wpi.edu/Images/CMS/SSPS/06.pdf\">Mental models concepts for system dynamics research - Doyle and Ford (1998)</a></div>\n<div><a href=\"http://www.amazon.com/Design-Everyday-Things-Donald-Norman/dp/0385267746\">The Design of Everyday Things, Norman</a></div>\n<div>Using concept maps to reveal conceptual typologies - Hay and Kinchin (2006)</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s3xNWtE3GxgSXwNtt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 18, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "2599", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fysgqk4CjAwhBgNYT", "NMoLJuDJEms7Ku9XS", "4gevjbK77NQS6hybY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-28T18:13:13.358Z", "modifiedAt": null, "url": null, "title": "The I-Less Eye", "slug": "the-i-less-eye", "viewCount": null, "lastCommentedAt": "2018-07-28T04:23:32.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rwallace", "createdAt": "2009-03-01T16:13:25.493Z", "isAdmin": false, "displayName": "rwallace"}, "userId": "cPhXNeZvnK7LgPMnv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WJ9t6FPPrN6ijBzXF/the-i-less-eye", "pageUrlRelative": "/posts/WJ9t6FPPrN6ijBzXF/the-i-less-eye", "linkUrl": "https://www.lesswrong.com/posts/WJ9t6FPPrN6ijBzXF/the-i-less-eye", "postedAtFormatted": "Sunday, March 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20I-Less%20Eye&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20I-Less%20Eye%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJ9t6FPPrN6ijBzXF%2Fthe-i-less-eye%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20I-Less%20Eye%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJ9t6FPPrN6ijBzXF%2Fthe-i-less-eye", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJ9t6FPPrN6ijBzXF%2Fthe-i-less-eye", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1298, "htmlBody": "<p>or: How I Learned to Stop Worrying and Love <a title=\"the Anthropic Trilemma\" href=\"/lw/19d/the_anthropic_trilemma/\">the Anthropic Trilemma</a></p>\n<p>Imagine you live in a future society where the law allows up to a hundred instances of a person to exist at any one time, but insists that your property belongs to the original you, not to the copies. (Does this sound illogical? I may ask my readers to believe in the potential existence of uploading technology, but I would not insult your intelligence by asking you to believe in the existence of a society where all the laws were logical.)</p>\n<p>So you decide to create your full allowance of 99 copies, and a customer service representative explains how the procedure works: the first copy is made, and informed he is copy number one; then the second copy is made, and informed he is copy number two, etc. That sounds fine until you start thinking about it, whereupon the native hue of resolution is sicklied o'er with the pale cast of thought. The problem lies in your anticipated subjective experience.</p>\n<p>After step one, you have a 50% chance of finding yourself the original; there is nothing controversial about this much. If you are the original, you have a 50% chance of finding yourself still so after step two, and so on. That means after step 99, your subjective probability of still being the original is 0.5^99, in other words as close to zero as makes no difference.</p>\n<p>Assume you prefer existing as a dependent copy to not existing at all, but preferable still would be existing as the original (in the eyes of the law) and therefore still owning your estate. You might reasonably have hoped for a 1% chance of the subjectively best outcome. 0.5^99 sounds entirely unreasonable!<a id=\"more\"></a></p>\n<p>You explain your concerns to the customer service representative, who in turn explains that regulations prohibit making copies from copies (the otherwise obvious solution) due to concerns about accumulated errors (the technical glitches in the early versions of the technology that created occasional errors have long been fixed, but the regulations haven't caught up yet). However, they do have a prototype machine that can make all 99 copies simultaneously, thereby giving you your 1% chance.</p>\n<p>It seems strange that such a minor change in the path leading to the exact same end result could make such a huge difference to what you anticipate, but the philosophical reasoning seems unassailable, and philosophy has a superb track record of predictive accuracy... er, well the reasoning seems unassailable. So you go ahead and authorize the extra payment to use the prototype system, and... your 1% chance comes up! You're still the original.</p>\n<p>\"Simultaneous?\" a friend shakes his head afterwards when you tell the story. \"No such thing. The Planck time is the shortest physically possible interval. Well if their new machine was that precise, it'd be worth the money, but obviously it isn't. I looked up the specs: it takes nearly three milliseconds per copy. That's into the range of timescales in which the human mind operates. Sorry, but your chance of ending up the original was actually 0.5^99, same as mine, and I got the cheap rate.\"</p>\n<p>\"But,\" you reply, \"it's a fuzzy scale. If it was three seconds per copy, that would be one thing. But three milliseconds, that's really too short to perceive, even the entire procedure was down near the lower limit. My probability of ending up the original couldn't have been 0.5^99, that's effectively impossible, less than the probability of hallucinating this whole conversation. Maybe it was some intermediate value, like one in a thousand or one in a million. Also, you don't know the exact data paths in the machine by which the copies are made. Perhaps that makes a difference.\"</p>\n<p>Are you convinced yet there is something wrong with this whole business of subjective anticipation?</p>\n<p><a id=\"more\"></a></p>\n<p>Well in a sense there is nothing wrong with it, it works fine in the kind of situations for which it evolved. I'm not suggesting throwing it out, merely that it is not ontologically fundamental.</p>\n<p>We've been down this road before. Life isn't ontologically fundamental, so we should not expect there to be a unique answer to questions like \"is a virus alive\" or \"is a beehive a single organism or a group\". Mind isn't ontologically fundamental, so we should not expect there to be a unique answer to questions like \"at what point in development does a human become conscious\". Particles aren't ontologically fundamental, so we should not expect there to be a unique answer to questions like \"which slit did the photon go through\". Yet it still seems that I am alive and conscious whereas a rock is not, and the reason it seems that way is because it actually is that way.</p>\n<p>Similarly, subjective experience is not ontologically fundamental, so we should not expect there to be unique answer to questions involving subjective probabilities of outcomes in situations involving things like copying minds (which our intuition was not evolved to handle). That's not a paradox, and it shouldn't give us headaches, any more than we (nowadays) get a headache pondering whether a virus is alive. It's just a consequence of using concepts that are not ontologically fundamental, in situations where they are not well defined. It all has to boil down to normality -- <em>but only in normal situations</em>. In abnormal situations, we just have to accept that our intuitions don't apply.</p>\n<p>How palatable is the bullet I'm biting? Well, the way to answer that is to check whether there are any well-defined questions we still can't answer. Let's have a look at some of the questions we were trying to answer with subjective/anthropic reasoning.</p>\n<p><em>Can I be sure I will not wake up as Britney Spears tomorrow?</em></p>\n<p>Yes. For me to wake up as Britney Spears, would mean the atoms in her brain were rearranged to encode my memories and personality. The probability of this occurring is negligible.</p>\n<p>If that isn't what we mean, then we are presumably referring to a counterfactual world in which every atom is in exactly the same location as in the actual world. That means it is the same world. To claim there is or could be any difference is equivalent to claiming the existence of p-zombies.</p>\n<p><em>Can you win the lottery by methods such as \"Program your computational environment to, if you win, make a trillion copies of yourself, and wake them up for ten seconds, long enough to experience winning the lottery. &nbsp;Then suspend the programs, merge them again, and start the result\"?</em></p>\n<p>No. The end result will still be that you are not the winner in more than one out of several million Everett branches. That is what we <em>mean</em> by 'winning the lottery', to the extent that we mean anything well-defined by it. If we mean something else by it, we are asking a question that is not well-defined, so we are free to make up whatever answer we please.</p>\n<p><em>In the Sleeping Beauty problem, is 1/3 the correct answer?</em></p>\n<p>Yes. 2/3 of Sleeping Beauty's waking moments during the experiment are located in the branch in which she was woken twice. That is what the question means, if it means anything.</p>\n<p><em>Can I be sure I am probably not a Boltzmann brain?</em></p>\n<p>Yes. I am the set of all subpatterns in the Tegmark multiverse that match a certain description. The vast majority of these are embedded in surrounding patterns that gave rise to them by lawful processes. That is what 'probably not a Boltzmann brain' means, if it means anything.</p>\n<p>What we want from a solution to confusing problems like the essence of life, quantum collapse or the anthropic trilemma is for the paradoxes to dissolve, leaving a situation where all well-defined questions have well-defined answers. That's how it worked out for the other problems, and that's how it works out for the anthropic trilemma.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 1, "5f5c37ee1b5cdee568cfb2fa": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WJ9t6FPPrN6ijBzXF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 44, "extendedScore": null, "score": 0.000107, "legacy": true, "legacyId": "2600", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y7jZ9BLEeuNTzgAE5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-28T20:56:19.473Z", "modifiedAt": null, "url": null, "title": "Highlights and Shadows", "slug": "highlights-and-shadows", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:52.888Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tCTmAmAapB37dAz9Y/highlights-and-shadows", "pageUrlRelative": "/posts/tCTmAmAapB37dAz9Y/highlights-and-shadows", "linkUrl": "https://www.lesswrong.com/posts/tCTmAmAapB37dAz9Y/highlights-and-shadows", "postedAtFormatted": "Sunday, March 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Highlights%20and%20Shadows&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHighlights%20and%20Shadows%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCTmAmAapB37dAz9Y%2Fhighlights-and-shadows%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Highlights%20and%20Shadows%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCTmAmAapB37dAz9Y%2Fhighlights-and-shadows", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCTmAmAapB37dAz9Y%2Fhighlights-and-shadows", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 738, "htmlBody": "<p><strong>Sequence index:</strong> <a href=\"/lw/1xh/living_luminously/\">Living Luminously</a><br /><strong>Previously in sequence:</strong> <a href=\"/lw/1za/the_spotlight/\">The Spotlight</a><br /><strong>Next in sequence:</strong> <a href=\"/lw/20r/city_of_lights/\">City of Lights</a><br /><br /><em>Part of a good luminosity endeavor is to decide what parts of yourself you do and don't like.</em></p>\n<p><em>You may find your understanding of this post significantly improved  if you read the fifth story from <a href=\"/lw/2aw/seven_shiny_stories\">Seven Shiny  Stories</a>.</em><br /><br />As you uncover and understand new things about yourself, you might find that you like some of them, but don't like others.&nbsp; While one would hope that you'd be generally pleased with yourself, it's a rare arrogance or a rarer saintliness that would enable unlimited approval.&nbsp; Fortunately, as promised in <a href=\"/lw/1xi/you_are_likely_to_be_eaten_by_a_grue/\">post two</a>, luminosity can let you determine what you'd like to change as well as what's already present.<br /><br />But what to change?<br /><br />An important step in the luminosity project is to sort your thoughts and feelings not only by type, <a href=\"/lw/1y0/the_abcs_of_luminosity/\">correlation</a>, strength, etc, but also by <em>endorsement</em>.&nbsp; You endorse those thoughts that you like, find representative of your favorite traits, prefer to see carried into action, and wish to keep intact (at least for the duration of their useful lives).&nbsp; By contrast, you repudiate those thoughts that you dislike, consider indicative of negative characteristics, want to keep inefficacious, and desire to modify or be rid of entirely.<a id=\"more\"></a><br /><br />Deciding which is which might not be trivial.&nbsp; You might need to sift through several orders of desire before finally figuring out whether you <a href=\"/lw/fv/wanting_to_want/\">want to want</a> cake, or like liking sleep, or prefer your preference for preferentism.&nbsp; A good place to start is with your macro-level goals and theoretical commitments (e.g., when this preference is efficacious, does it serve your Life Purpose&trade;, directly or indirectly?&nbsp; if you have firm metaethical notions of right and wrong, is this tendency you have uncovered in yourself one that impels you to do right things?).<br /><br />As a second pass, you can work with the information you collected when you correlated your ABCs.&nbsp; How does an evaluated desire makes you feel when satisfied or unsatisfied?&nbsp; Does it cripple you when unsatisfied or improve your performance when satisfied?&nbsp; Are you reliably in a position to satisfy it?&nbsp; If you can't typically satisfy it, would it be easier to change the desire or to change the circumstances that prevent its satisfaction?&nbsp; However, this is a second step.&nbsp; You need to know what affect and behavior are preferable to you before you can judge desires (and other mental activity) relative to what they yield in those departments, and judging affect and behavior is itself an exercise in endorsement and repudiation.<br /><br />Knowing what you like and don't like about your mind is a fine thing.&nbsp; Once you have that information, you can put it to direct use immediately - I find it useful to tag many of my expressions of emotion with the words \"endorsed\" or \"non-endorsed\".&nbsp; That way, the people around me can use that categorization rather than having to either assume I approve of everything I feel, or layer their own projections of endorsement on top of me.&nbsp; Either would be unreliable and cause people to have poor models of me: I have not yet managed to excise my every unwanted trait, and my patterns of endorsement do not typically map on to the ones that the people around me have or expect me to have.<br /><br />Additionally, once you know what you like and don't like about your mind, you can begin to make progress in increasing the ratio of liked to unliked characteristics.&nbsp; People often make haphazard lurches towards trying to be \"better people\", but when \"better\" means \"lines up more closely with vaguely defined commonsense intuitions about morality\", this is not the sort of goal we're at all good at pursuing.&nbsp; Specific projects like being generous or more mindful are a step closer, but the greatest marginal benefit in self-revision comes of figuring out what comes in <em>advance</em> of behaving in a non-endorsed way and heading it off at the pass.&nbsp; (More on this in \"Lampshading\".)&nbsp; The odds are low that your brain's patterns align closely with conventional virtues well enough for them to be useful targets.&nbsp; It's a better plan to identify what's already present, then endorse or repudiate these pre-sliced thoughts and work on them as they appear instead of sweeping together an unnatural category.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "Zwv9eHi7KGg5KA9oM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tCTmAmAapB37dAz9Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 25, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "2601", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "city-of-lights", "canonicalPrevPostSlug": "the-spotlight", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "Zstm38omrpeu7iWeS", "vfHRahpgbp9YFPuGQ", "9sguwESkteCgqFMbj", "r6diXRLvkZBLpSoTf", "rLuZ6XrGpgjk9BNpX", "azdqDRbcw3EkrnHNw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-29T16:59:41.839Z", "modifiedAt": null, "url": null, "title": "NYC Rationalist Community", "slug": "nyc-rationalist-community", "viewCount": null, "lastCommentedAt": "2022-02-26T02:57:57.314Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cosmos", "createdAt": "2009-04-26T03:18:01.731Z", "isAdmin": false, "displayName": "Cosmos"}, "userId": "c3Ji9Th6jATRyHLFC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/woB6QBLkuwG7c9FTf/nyc-rationalist-community", "pageUrlRelative": "/posts/woB6QBLkuwG7c9FTf/nyc-rationalist-community", "linkUrl": "https://www.lesswrong.com/posts/woB6QBLkuwG7c9FTf/nyc-rationalist-community", "postedAtFormatted": "Monday, March 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20NYC%20Rationalist%20Community&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANYC%20Rationalist%20Community%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwoB6QBLkuwG7c9FTf%2Fnyc-rationalist-community%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=NYC%20Rationalist%20Community%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwoB6QBLkuwG7c9FTf%2Fnyc-rationalist-community", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwoB6QBLkuwG7c9FTf%2Fnyc-rationalist-community", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>For those who don't yet know, there has been a thriving rationalist community in NYC since April 2009.&nbsp; We've been holding weekly meetups for the past several months now, and often have game nights, focused discussions, etc.&nbsp; For those of you who live in the area, and not yet involved, I highly encourage you to join the following two groups:</p>\n<p><a href=\"http://www.meetup.com/Less-Wrong-Overcoming-Bias-NYC/\" target=\"_blank\">This Meetup group</a> is our public face, which draws new members to the meetups.</p>\n<p><a href=\"http://groups.google.com/group/overcomingbiasnyc\" target=\"_blank\">This Google Group</a> was our original method of coordination, and we still use it for private communication.</p>\n<p>The reason I am posting this is because there has been interest by several members in <a href=\"http://groups.google.com/group/overcomingbiasnyc/browse_thread/thread/89aa057bfdfc6808\" target=\"_blank\">sharing an apartment/loft</a>, or even multiple apartments on a floor of a building if there are enough people.&nbsp; The core interest group is going to be meeting soon to figure out the logistics, so I wanted to extend this opportunity to any aspiring rationalists who either currently or would like to live in NYC.&nbsp; If you are interested, please join the Google Group and let us know, so that we can include you in the planning process.&nbsp; Additionally, if anyone has experience living with other rationalists, or more generally in a community setting, please feel free to share your knowledge with us so we can avoid any common pitfalls.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "woB6QBLkuwG7c9FTf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 24, "extendedScore": null, "score": 5.73322511237563e-07, "legacy": true, "legacyId": "2611", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-29T18:09:47.655Z", "modifiedAt": null, "url": null, "title": "Even if you have a nail, not all hammers are the same", "slug": "even-if-you-have-a-nail-not-all-hammers-are-the-same", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:05.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pWi5WmvDcN4Hn7Bo6/even-if-you-have-a-nail-not-all-hammers-are-the-same", "pageUrlRelative": "/posts/pWi5WmvDcN4Hn7Bo6/even-if-you-have-a-nail-not-all-hammers-are-the-same", "linkUrl": "https://www.lesswrong.com/posts/pWi5WmvDcN4Hn7Bo6/even-if-you-have-a-nail-not-all-hammers-are-the-same", "postedAtFormatted": "Monday, March 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Even%20if%20you%20have%20a%20nail%2C%20not%20all%20hammers%20are%20the%20same&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEven%20if%20you%20have%20a%20nail%2C%20not%20all%20hammers%20are%20the%20same%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpWi5WmvDcN4Hn7Bo6%2Feven-if-you-have-a-nail-not-all-hammers-are-the-same%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Even%20if%20you%20have%20a%20nail%2C%20not%20all%20hammers%20are%20the%20same%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpWi5WmvDcN4Hn7Bo6%2Feven-if-you-have-a-nail-not-all-hammers-are-the-same", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpWi5WmvDcN4Hn7Bo6%2Feven-if-you-have-a-nail-not-all-hammers-are-the-same", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1717, "htmlBody": "<p>(Related to <a href=\"/lw/1zk/overencapsulation/\">Over-ensapsulation</a> and <a href=\"/lw/1yv/subtext_is_not_invariant_under_linear/\">Subtext is not invariant under linear transformation</a>)</p>\n<p>Between 2004 and 2007, Goran Bjelakovic et al. published 3 famous meta-analysis of vitamin supplements, concluding that vitamins don't help people but instead kill people.&nbsp; This is now the accepted dogma; and if you ask your doctor about vitamins, she's likely to tell you not to take them, based on reading either one of these articles, or one of the many summaries of these articles made in secondary sources like The Mayo Clinic Journal.</p>\n<p>The 2007 study claims that beta-carotene and vitamins A and E are positively correlated with death - the more you take, the more likely you are to die. Therefore, vitamins kill.&nbsp; The conclusion on E requires a little explanation, but the data on beta-carotene and A is simple and specific:</p>\n<blockquote>\n<p><span style=\"font-family: verdana,arial,helvetica,sans-serif; font-size: x-small;\">Univariate meta-regression analyses revealed significant influences<sup> </sup>of dose of beta carotene (Relative Risk (RR), 1.004; 95% CI, 1.001-1.007; <em>P</em>&nbsp;=&nbsp;.012),<sup> </sup>dose of vitamin A (RR, 1.000006; 95% CI, 1.000002-1.000009;<sup> </sup><em>P</em>&nbsp;=&nbsp;.003), ... on mortality.</span></p>\n</blockquote>\n<p>This appears to mean that, for each mg of beta carotene that you take, your risk of death increases by a factor (RR) of 1.004; for each IU of vitamin A that you take, by a factor of 1.000006.&nbsp; \"95% CI, 1.001-1.007\" means that the standard deviation of the sample indicates a 95% probability that the true RR lies somewhere between 1.001 and 1.007.&nbsp; \"P = .012\" means that there's only a 1.2% chance that you would be so unlucky as to get a sample giving that result, if in fact the true RR were 1.</p>\n<p>A risk factor of 1.000006 doesn't sound like much; but I'm taking 2,500 IU of vitamin A per day.&nbsp; That gives a 1.5% increase in my chance of death!&nbsp; (Per 3.3 years.)&nbsp; And look at those P-values: .012, .003!</p>\n<p>So why do I still take vitamins?</p>\n<div><a id=\"more\"></a></div>\n<p>What all of these articles do, in excruciating detail with regard to sample selection (though not so much with regard to the math), is to run a linear regression on a lot of data from studies of patients taking vitamins.&nbsp; A linear regression takes a set of data where each datapoint looks like this:</p>\n<p>&nbsp; &nbsp;&nbsp; Y = a<sub>1</sub>X<sub>1</sub> + c</p>\n<p>and a multiple linear regression takes a set of data where each datapoint usually looks like this:<br /><br />&nbsp; &nbsp; &nbsp;Y = a<sub>1</sub>X<sub>1</sub> + a<sub>2</sub>X<sub>2</sub> + ... a<sub>n</sub>X<sub>n</sub> + c<br /><br />where Y and all the X<sub>i</sub>'s are known.&nbsp; In this case, Y is a 1 for someone who died and a 0 for someone who didn't, and each X<sub>i</sub> is the amount of some vitamin taken.&nbsp; In either case, the regression finds the values for a<sub>1</sub>, ... a<sub>n</sub>, c that best fit the data (meaning they minimize the sum, over all data points, of the squared error of the value predicted for Y, (Y - (a<sub>1</sub>X<sub>1</sub> + a<sub>2</sub>X<sub>2</sub> + ... a<sub>n</sub>X<sub>n</sub> + c)<sup>2</sup>).</p>\n<p>Scientists love linear regression.&nbsp; It's simple, fast, and mathematically pure.&nbsp; There are lots of tools available to perform it for you.&nbsp; It's a powerful hammer in a scientists' toolbox.</p>\n<p>But not everything is a nail.&nbsp; And even for a nail, not every hammer is the right hammer.&nbsp; You shouldn't use linear regression just because it's the \"default regression analysis\".&nbsp; When a paper says they performed \"a regression\", beware.</p>\n<p>A linear analysis assumes that if 10 milligrams is good for you, then 100 milligrams is ten times as good for you, and 1000 milligrams is one-hundred times as good for you.</p>\n<p>This is not how vitamins work.&nbsp; Vitamin A is toxic in doses over 15,000 IU/day, and vitamin E is toxic in doses over 400 IU/day (Miller et al. 2004, Meta-Analysis: High-Dosage Vitamin E Supplementation May Increase All-Cause Mortality;&nbsp; Berson et al. 1993, <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/8512476\">Randomized trial of vitamin A and vitamin E supplementation for retinitis pigmentosa</a>.). The RDA for vitamin A is 2500 IU/day for adults. Good dosage levels for vitamin A appear to be under 10,000 IU/day, and for E, less than 300 IU/day. (Sadly, studies rarely discriminate in their conclusions between dosage levels for men and women.&nbsp; Doing so would give more useful results, but make it harder to reach the coveted P &lt; .05 or P &lt; .01.)<br /><br />Quoting from the 2007 JAMA article:</p>\n<blockquote>The dose and regimen of the antioxidant supplements were: beta carotene 1.2 to 50.0 mg (mean, 17.8 mg) , vitamin A 1333 to 200 000 IU (mean, 20 219 IU), vitamin C 60 to 2000 mg (mean, 488 mg), vitamin E 10 to 5000 IU (mean, 569 IU), and selenium 20 to 200 &mu;g (mean 99 &mu;g) daily or on alternate days for 28 days to 12 years (mean 2.7 years).<br /></blockquote>\n<p>The&nbsp; <em>mean</em>&nbsp; values used in the study of both A and E are in ranges known to be toxic. The maximum values used were ten times the known toxic levels, and about 20 times the beneficial levels.<br /><br />17.8 mg of beta-carotene translates to about 30,000 IUs of vitamin A, if it were converted to vitamin A. This is also a toxic value. It is surprising that beta-carotene showed toxicity, though, since common wisdom is that beta-carotene is converted to vitamin A only as needed.<br /><br />Vitamins, like any medicine, have an inverted-J-shaped response curve. If you graph their health effects, with dosage on the horizontal access, and some measure of their effects - say, change to average lifespan - on the vertical axis, you would get an upside-down J. (If you graph the death rate on the vertical axis, as in this study, you would get a rightside-up J.) That is, taking a moderate amount has some good effect; taking a huge a mount has a large bad effect.<br /><br />If you then try to draw a straight line through the J that best-matches the J, you get a line showing detrimental effects increasing gradually with dosage. The results are exactly what we expect. Their conclusion, that \"Treatment with beta carotene, vitamin A, and vitamin E may increase mortality,\" is technically correct. Treatment with anything may increase mortality, if you take ten times the toxic dose.<br /><br />For a headache, some people take 4 200mg tablets of aspirin. 10 tablets of aspirin might be toxic. If you made a study averaging in people who took from 1 to 100 tablets of aspirin for a headache, you would find that \"aspirin increases mortality\".</p>\n<p>(JAMA later published 4 letters criticizing the 2007 article.&nbsp; None of them mentioned the use of linear regression as a problem.&nbsp; They didn't publish my letter - perhaps because I didn't write it until nearly 2 months after the article was published.)</p>\n<p>Anyone reading the study should have been alerted to this by the fact that all of the water-soluble vitamins in the study showed no harmful effects, while all of the fat-soluble vitamins \"showed\" harmful effects. Fat-soluble vitamins are stored in the fat, so they build up to toxic levels when people take too much for a long time.</p>\n<p>A better methodology would have been to use piecewise (or \"hockey-stick\") regression, which assumes the data is broken into 2 sections (typically one sloping downwards and one sloping upwards), and tries to find the right breakpoint, and perform a separate linear regression on each side of the break that meets at the break.&nbsp; (I almost called this \"The case of the missing hockey-stick\", but thought that would give the answer away.)</p>\n<p>Would these articles have been accepted by the most-respected journals in medicine if they evaluated a pharmaceutical in the same way?&nbsp; I doubt it; or else we wouldn't have any pharmaceuticals.&nbsp; Bias against vitamins?&nbsp; You be the judge.</p>\n<h3>Meaningful results have meaningful interpretations</h3>\n<p>The paper states the mortality risk in terms of \"relative risk\" (RR).&nbsp; But&nbsp; <a href=\"http://en.wikipedia.org/wiki/Relative_risk\">relative risk</a>&nbsp; is used for studies of 0/1 conditions, like smoking/no smoking, not for studies that use regression on different dosage levels.&nbsp; How do you interepret the RR value for different dosages?&nbsp; Is it RR x dosage?&nbsp; Or RR<sup>dosage</sup> (each unit multiplies risk by RR)?&nbsp; The difference between these interpretations is trivial for standard dosages.&nbsp; But can you say you understand the paper if you can't interpret the results?</p>\n<p>To answer this question, you have to ask exactly what type of regression the authors used.&nbsp; Even if a linear non-piecewise regression were correct, the best regression analysis to use in this case would be a logistic regression, which estimates the probability of a binary outcome conditioned on the regression variables. The authors didn't consider it necessary to report what type of regression analysis they performed; they reported only the computer program (STATA) and the command (\"metareg\").&nbsp; The&nbsp; <a href=\"http://www.blackwellpublishing.com/medicine/bmj/systreviews/pdfs/chapter18.pdf\">STATA metareg manual</a>&nbsp; is not easy to understand, but three things are clear:</p>\n<ul>\n<li>It doesn't use the word \"logistic\" anywhere, and it doesn't use the logistic function, so it isn't logistic regression.</li>\n<li>It does regression on the log of the risk ratio between two binary cases, a \"treatment\" case and a \"no-treatment\" case; and computes regression coefficients for possibly-correlated continuous treatment variables (such as vitamin doses).</li>\n<li>It doesn't directly give relative risk for the correlated variables.&nbsp; It gives regression coefficients telling the change in log relative risk per unit of (in this case) beta carotene or vitamin A.&nbsp; If anything, the reported RR is probably e<sup>r</sup>, where&nbsp; <em>r</em> is the computed regression coefficient.&nbsp; This means the interpretation is that risk is proportional to RR<sup>dosage</sup>.</li>\n</ul>\n<p>Since there is no \"treatment/no treatment\" case for this study, but only the variables that would be correlated with treatment/no treatment, it would have been impossible to put the data into a form that metareg can use.&nbsp; So what test, exactly, did the authors perform?&nbsp; And what do the results mean?&nbsp; It remains a mystery to me - and, I'm willing to bet, to every other reader of the paper.</p>\n<h3>References</h3>\n<p>Bjelakovic et al. 2007,&nbsp; <a href=\"http://jama.ama-assn.org/cgi/content/abstract/297/8/842\">\"Mortality in randomized trials of antioxidant supplements for primary and secondary prevention: Systematic review and meta-analysis\",</a>&nbsp; Journal of the American Medical Association, Feb. 28 2007. See a commentary on it&nbsp; <a href=\"http://www.playfuls.com/news_005056_Can_Vitamins_Kill_You.html\">here</a>.</p>\n<p>Bjelakovic et al. 2006, \"Meta-analysis: Antioxidant supplements for primary and secondary prevention of colorectal adenoma\", Alimentary Pharmacology &amp; Therapeutics 24, 281-291.<br /><br />Bjelakovic et al. 2004, \"Antioxidant supplements for prevention of gastrointestinal cancers: A systematic review and meta-analysis,\" The Lancet 364, Oct. 2 2004.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1, "bh7uxTTqmsQ8jZJdB": 1, "ZpG9rheyAkgCoEQea": 1, "ksdiAMKfgSyEeKMo6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pWi5WmvDcN4Hn7Bo6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 113, "baseScore": 132, "extendedScore": null, "score": 0.000225, "legacy": true, "legacyId": "2610", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 132, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(Related to <a href=\"/lw/1zk/overencapsulation/\">Over-ensapsulation</a> and <a href=\"/lw/1yv/subtext_is_not_invariant_under_linear/\">Subtext is not invariant under linear transformation</a>)</p>\n<p>Between 2004 and 2007, Goran Bjelakovic et al. published 3 famous meta-analysis of vitamin supplements, concluding that vitamins don't help people but instead kill people.&nbsp; This is now the accepted dogma; and if you ask your doctor about vitamins, she's likely to tell you not to take them, based on reading either one of these articles, or one of the many summaries of these articles made in secondary sources like The Mayo Clinic Journal.</p>\n<p>The 2007 study claims that beta-carotene and vitamins A and E are positively correlated with death - the more you take, the more likely you are to die. Therefore, vitamins kill.&nbsp; The conclusion on E requires a little explanation, but the data on beta-carotene and A is simple and specific:</p>\n<blockquote>\n<p><span style=\"font-family: verdana,arial,helvetica,sans-serif; font-size: x-small;\">Univariate meta-regression analyses revealed significant influences<sup> </sup>of dose of beta carotene (Relative Risk (RR), 1.004; 95% CI, 1.001-1.007; <em>P</em>&nbsp;=&nbsp;.012),<sup> </sup>dose of vitamin A (RR, 1.000006; 95% CI, 1.000002-1.000009;<sup> </sup><em>P</em>&nbsp;=&nbsp;.003), ... on mortality.</span></p>\n</blockquote>\n<p>This appears to mean that, for each mg of beta carotene that you take, your risk of death increases by a factor (RR) of 1.004; for each IU of vitamin A that you take, by a factor of 1.000006.&nbsp; \"95% CI, 1.001-1.007\" means that the standard deviation of the sample indicates a 95% probability that the true RR lies somewhere between 1.001 and 1.007.&nbsp; \"P = .012\" means that there's only a 1.2% chance that you would be so unlucky as to get a sample giving that result, if in fact the true RR were 1.</p>\n<p>A risk factor of 1.000006 doesn't sound like much; but I'm taking 2,500 IU of vitamin A per day.&nbsp; That gives a 1.5% increase in my chance of death!&nbsp; (Per 3.3 years.)&nbsp; And look at those P-values: .012, .003!</p>\n<p>So why do I still take vitamins?</p>\n<div><a id=\"more\"></a></div>\n<p>What all of these articles do, in excruciating detail with regard to sample selection (though not so much with regard to the math), is to run a linear regression on a lot of data from studies of patients taking vitamins.&nbsp; A linear regression takes a set of data where each datapoint looks like this:</p>\n<p>&nbsp; &nbsp;&nbsp; Y = a<sub>1</sub>X<sub>1</sub> + c</p>\n<p>and a multiple linear regression takes a set of data where each datapoint usually looks like this:<br><br>&nbsp; &nbsp; &nbsp;Y = a<sub>1</sub>X<sub>1</sub> + a<sub>2</sub>X<sub>2</sub> + ... a<sub>n</sub>X<sub>n</sub> + c<br><br>where Y and all the X<sub>i</sub>'s are known.&nbsp; In this case, Y is a 1 for someone who died and a 0 for someone who didn't, and each X<sub>i</sub> is the amount of some vitamin taken.&nbsp; In either case, the regression finds the values for a<sub>1</sub>, ... a<sub>n</sub>, c that best fit the data (meaning they minimize the sum, over all data points, of the squared error of the value predicted for Y, (Y - (a<sub>1</sub>X<sub>1</sub> + a<sub>2</sub>X<sub>2</sub> + ... a<sub>n</sub>X<sub>n</sub> + c)<sup>2</sup>).</p>\n<p>Scientists love linear regression.&nbsp; It's simple, fast, and mathematically pure.&nbsp; There are lots of tools available to perform it for you.&nbsp; It's a powerful hammer in a scientists' toolbox.</p>\n<p>But not everything is a nail.&nbsp; And even for a nail, not every hammer is the right hammer.&nbsp; You shouldn't use linear regression just because it's the \"default regression analysis\".&nbsp; When a paper says they performed \"a regression\", beware.</p>\n<p>A linear analysis assumes that if 10 milligrams is good for you, then 100 milligrams is ten times as good for you, and 1000 milligrams is one-hundred times as good for you.</p>\n<p>This is not how vitamins work.&nbsp; Vitamin A is toxic in doses over 15,000 IU/day, and vitamin E is toxic in doses over 400 IU/day (Miller et al. 2004, Meta-Analysis: High-Dosage Vitamin E Supplementation May Increase All-Cause Mortality;&nbsp; Berson et al. 1993, <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/8512476\">Randomized trial of vitamin A and vitamin E supplementation for retinitis pigmentosa</a>.). The RDA for vitamin A is 2500 IU/day for adults. Good dosage levels for vitamin A appear to be under 10,000 IU/day, and for E, less than 300 IU/day. (Sadly, studies rarely discriminate in their conclusions between dosage levels for men and women.&nbsp; Doing so would give more useful results, but make it harder to reach the coveted P &lt; .05 or P &lt; .01.)<br><br>Quoting from the 2007 JAMA article:</p>\n<blockquote>The dose and regimen of the antioxidant supplements were: beta carotene 1.2 to 50.0 mg (mean, 17.8 mg) , vitamin A 1333 to 200 000 IU (mean, 20 219 IU), vitamin C 60 to 2000 mg (mean, 488 mg), vitamin E 10 to 5000 IU (mean, 569 IU), and selenium 20 to 200 \u03bcg (mean 99 \u03bcg) daily or on alternate days for 28 days to 12 years (mean 2.7 years).<br></blockquote>\n<p>The&nbsp; <em>mean</em>&nbsp; values used in the study of both A and E are in ranges known to be toxic. The maximum values used were ten times the known toxic levels, and about 20 times the beneficial levels.<br><br>17.8 mg of beta-carotene translates to about 30,000 IUs of vitamin A, if it were converted to vitamin A. This is also a toxic value. It is surprising that beta-carotene showed toxicity, though, since common wisdom is that beta-carotene is converted to vitamin A only as needed.<br><br>Vitamins, like any medicine, have an inverted-J-shaped response curve. If you graph their health effects, with dosage on the horizontal access, and some measure of their effects - say, change to average lifespan - on the vertical axis, you would get an upside-down J. (If you graph the death rate on the vertical axis, as in this study, you would get a rightside-up J.) That is, taking a moderate amount has some good effect; taking a huge a mount has a large bad effect.<br><br>If you then try to draw a straight line through the J that best-matches the J, you get a line showing detrimental effects increasing gradually with dosage. The results are exactly what we expect. Their conclusion, that \"Treatment with beta carotene, vitamin A, and vitamin E may increase mortality,\" is technically correct. Treatment with anything may increase mortality, if you take ten times the toxic dose.<br><br>For a headache, some people take 4 200mg tablets of aspirin. 10 tablets of aspirin might be toxic. If you made a study averaging in people who took from 1 to 100 tablets of aspirin for a headache, you would find that \"aspirin increases mortality\".</p>\n<p>(JAMA later published 4 letters criticizing the 2007 article.&nbsp; None of them mentioned the use of linear regression as a problem.&nbsp; They didn't publish my letter - perhaps because I didn't write it until nearly 2 months after the article was published.)</p>\n<p>Anyone reading the study should have been alerted to this by the fact that all of the water-soluble vitamins in the study showed no harmful effects, while all of the fat-soluble vitamins \"showed\" harmful effects. Fat-soluble vitamins are stored in the fat, so they build up to toxic levels when people take too much for a long time.</p>\n<p>A better methodology would have been to use piecewise (or \"hockey-stick\") regression, which assumes the data is broken into 2 sections (typically one sloping downwards and one sloping upwards), and tries to find the right breakpoint, and perform a separate linear regression on each side of the break that meets at the break.&nbsp; (I almost called this \"The case of the missing hockey-stick\", but thought that would give the answer away.)</p>\n<p>Would these articles have been accepted by the most-respected journals in medicine if they evaluated a pharmaceutical in the same way?&nbsp; I doubt it; or else we wouldn't have any pharmaceuticals.&nbsp; Bias against vitamins?&nbsp; You be the judge.</p>\n<h3 id=\"Meaningful_results_have_meaningful_interpretations\">Meaningful results have meaningful interpretations</h3>\n<p>The paper states the mortality risk in terms of \"relative risk\" (RR).&nbsp; But&nbsp; <a href=\"http://en.wikipedia.org/wiki/Relative_risk\">relative risk</a>&nbsp; is used for studies of 0/1 conditions, like smoking/no smoking, not for studies that use regression on different dosage levels.&nbsp; How do you interepret the RR value for different dosages?&nbsp; Is it RR x dosage?&nbsp; Or RR<sup>dosage</sup> (each unit multiplies risk by RR)?&nbsp; The difference between these interpretations is trivial for standard dosages.&nbsp; But can you say you understand the paper if you can't interpret the results?</p>\n<p>To answer this question, you have to ask exactly what type of regression the authors used.&nbsp; Even if a linear non-piecewise regression were correct, the best regression analysis to use in this case would be a logistic regression, which estimates the probability of a binary outcome conditioned on the regression variables. The authors didn't consider it necessary to report what type of regression analysis they performed; they reported only the computer program (STATA) and the command (\"metareg\").&nbsp; The&nbsp; <a href=\"http://www.blackwellpublishing.com/medicine/bmj/systreviews/pdfs/chapter18.pdf\">STATA metareg manual</a>&nbsp; is not easy to understand, but three things are clear:</p>\n<ul>\n<li>It doesn't use the word \"logistic\" anywhere, and it doesn't use the logistic function, so it isn't logistic regression.</li>\n<li>It does regression on the log of the risk ratio between two binary cases, a \"treatment\" case and a \"no-treatment\" case; and computes regression coefficients for possibly-correlated continuous treatment variables (such as vitamin doses).</li>\n<li>It doesn't directly give relative risk for the correlated variables.&nbsp; It gives regression coefficients telling the change in log relative risk per unit of (in this case) beta carotene or vitamin A.&nbsp; If anything, the reported RR is probably e<sup>r</sup>, where&nbsp; <em>r</em> is the computed regression coefficient.&nbsp; This means the interpretation is that risk is proportional to RR<sup>dosage</sup>.</li>\n</ul>\n<p>Since there is no \"treatment/no treatment\" case for this study, but only the variables that would be correlated with treatment/no treatment, it would have been impossible to put the data into a form that metareg can use.&nbsp; So what test, exactly, did the authors perform?&nbsp; And what do the results mean?&nbsp; It remains a mystery to me - and, I'm willing to bet, to every other reader of the paper.</p>\n<h3 id=\"References\">References</h3>\n<p>Bjelakovic et al. 2007,&nbsp; <a href=\"http://jama.ama-assn.org/cgi/content/abstract/297/8/842\">\"Mortality in randomized trials of antioxidant supplements for primary and secondary prevention: Systematic review and meta-analysis\",</a>&nbsp; Journal of the American Medical Association, Feb. 28 2007. See a commentary on it&nbsp; <a href=\"http://www.playfuls.com/news_005056_Can_Vitamins_Kill_You.html\">here</a>.</p>\n<p>Bjelakovic et al. 2006, \"Meta-analysis: Antioxidant supplements for primary and secondary prevention of colorectal adenoma\", Alimentary Pharmacology &amp; Therapeutics 24, 281-291.<br><br>Bjelakovic et al. 2004, \"Antioxidant supplements for prevention of gastrointestinal cancers: A systematic review and meta-analysis,\" The Lancet 364, Oct. 2 2004.</p>", "sections": [{"title": "Meaningful results have meaningful interpretations", "anchor": "Meaningful_results_have_meaningful_interpretations", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "126 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 126, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nitfKbhkM5xkLnHkQ", "zKf7LNzjrR5QofgW2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-29T18:14:12.075Z", "modifiedAt": null, "url": null, "title": "Disambiguating Doom", "slug": "disambiguating-doom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:28.658Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zm9S8mknDfavxKStA/disambiguating-doom", "pageUrlRelative": "/posts/zm9S8mknDfavxKStA/disambiguating-doom", "linkUrl": "https://www.lesswrong.com/posts/zm9S8mknDfavxKStA/disambiguating-doom", "postedAtFormatted": "Monday, March 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Disambiguating%20Doom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADisambiguating%20Doom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzm9S8mknDfavxKStA%2Fdisambiguating-doom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Disambiguating%20Doom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzm9S8mknDfavxKStA%2Fdisambiguating-doom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzm9S8mknDfavxKStA%2Fdisambiguating-doom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 533, "htmlBody": "<p>Analysts of humanity's future sometimes use the word \"doom\" rather loosely. (\"Doomsday\" has the further problem that it privileges a particular time scale.) But doom sounds like something important; and when something is important, it's important to be clear about what it is.</p>\n<p>Some properties that could all qualify an event as doom:</p>\n<ol>\n<li><strong>Gigadeath</strong>: Billions of people, or some number roughly comparable to the number of people alive, die.</li>\n<li><strong>Human extinction</strong>: No humans survive afterward. (Or, modified: no human-like life survives, or no sentient life survives, or no intelligent life survives.)</li>\n<li><strong>Existential disaster</strong>: Some significant fraction, perhaps all, of the future's potential moral value is lost. (Coined by Nick Bostrom, who <a href=\"http://www.nickbostrom.com/existential/risks.html\">defines</a> an existential risk as one \"where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential\", which I interpret to mean the same thing.)</li>\n<li>\"<strong>Doomsday argument doomsday</strong>\": The total number of observers (or observer-moments) in existence ends up being small&nbsp;<span style=\"font-family: 'Times New Roman', Times, serif; font-size: 16px;\">&ndash;</span>&nbsp;not much larger than the total that have existed in the past. This is what we should believe if we accept the <a href=\"http://www.anthropic-principle.com/primer1.html\">Doomsday argument</a>.</li>\n<li><strong>Great filter</strong>: Earth ends up <a href=\"http://hanson.gmu.edu/greatfilter.html\">not colonizing</a> the stars, or doing anything else widely visible. If all species are filtered out, this explains the Fermi paradox.</li>\n</ol>\n<p>Examples to illustrate that these properties are fundamentally different:</p>\n<ul>\n<li>If billions die (1), humanity may still recover and not go extinct (2), retain most of its potential future value (3), spawn many future observers (4), and colonize the stars (5). (E.g., nuclear war, but also aging.)&nbsp;</li>\n<li>If cockroaches or Klingon colonists build something even cooler afterward, human extinction (2) isn't an existential disaster (3), and conversely, the creation of an eternal dystopia could be an existential disaster (3) without involving human extinction (2).&nbsp;</li>\n<li>Human extinction (2) doesn't imply few future observers (4) if it happens too late, or if we're not alone; and few future observers (4) doesn't imply human extinction (2) if we all live forever childlessly. (It's harder to find an example of few observer-moments without human extinction, short of p-zombie infestations.)&nbsp;</li>\n<li>If we create an AI that converts the galaxy to paperclips, humans go extinct (2) and it's an existential disaster (3), but it isn't part of the great filter (5). (For an example where all intelligence goes extinct, implying few future observers (4) for any definition of \"observer\", consider physics disasters that expand at light speed.) If our true desire is to transcend inward, that's part of the great filter (5) without human extinction (2) or an existential disaster (3).&nbsp;</li>\n<li>If we leave our reference class of observers for a more exciting reference class, that's a doomsday argument doomsday (4) but not an existential disaster (3). The aforementioned eternal dystopia is an existential disaster (3) but implies many future observers (4).&nbsp;</li>\n<li>Finally, if space travel is impossible, that's a great filter (5) but compatible with many future observers (4).</li>\n</ul>\n<div>If we fail to keep these distinctions in mind, we might think that worlds are doomed when they're not, or that worlds aren't doomed when they are. Don't commit Type I and Type II errors on the entire planet!</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zm9S8mknDfavxKStA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 28, "extendedScore": null, "score": 5.733371379986497e-07, "legacy": true, "legacyId": "2593", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-03-31T23:30:03.011Z", "modifiedAt": null, "url": null, "title": "City of Lights", "slug": "city-of-lights", "viewCount": null, "lastCommentedAt": "2022-03-17T11:32:05.133Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vfHRahpgbp9YFPuGQ/city-of-lights", "pageUrlRelative": "/posts/vfHRahpgbp9YFPuGQ/city-of-lights", "linkUrl": "https://www.lesswrong.com/posts/vfHRahpgbp9YFPuGQ/city-of-lights", "postedAtFormatted": "Wednesday, March 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20City%20of%20Lights&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACity%20of%20Lights%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfHRahpgbp9YFPuGQ%2Fcity-of-lights%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=City%20of%20Lights%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfHRahpgbp9YFPuGQ%2Fcity-of-lights", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfHRahpgbp9YFPuGQ%2Fcity-of-lights", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1115, "htmlBody": "<p><strong>Sequence index:</strong> <a href=\"/lw/1xh/living_luminously/\">Living Luminously</a><br /><strong>Previously in sequence:</strong> <a href=\"/lw/209/highlights_and_shadows/\">Highlights and Shadows</a><br /><strong>Next in Sequence:</strong> <a href=\"/lw/21l/lampshading/\">Lampshading</a><br /><br /><em>Pretending to be multiple agents is a useful way to represent your psychology and uncover hidden complexities.</em></p>\n<p><em>You may find your understanding of this post significantly improved  if you read the sixth story from <a href=\"/lw/2aw/seven_shiny_stories\">Seven Shiny  Stories</a>.</em><br /><br />When grappling with the complex web of traits and patterns that is you, you are reasonably likely to find yourself <a href=\"/lw/1mu/sorting_out_sticky_brains/\">less than completely uniform</a>.&nbsp; You might have several competing perspectives, possess the ability to code-switch between different <a href=\"/lw/1ty/mental_crystallography/\">styles of thought</a>, or even believe outright contradictions.&nbsp; It's bound to make it harder to think about yourself when you find this kind of convolution.<br /><br />Unfortunately, we don't have the vocabulary or even the mental architecture to easily think of or describe ourselves (nor other people) as containing such multitudes.&nbsp; The closest we come in typical conversation more resembles descriptions of superficial, vague ambivalence (\"I'm sorta happy about it, but kind of sad at the same time!&nbsp; Weird!\") than the sort of deep-level muddle and conflict that can occupy a brain.&nbsp; The models of the human psyche that have come closest to approximating this mess are what I call \"multi-agent models\".&nbsp; (Note: I have no idea how what I am about to describe interacts with actual psychiatric conditions involving multiple personalities, voices in one's head, or other potentially similar-sounding phenomena.&nbsp; I describe multi-agent models as employed by psychiatrically singular persons.)<br /><br />Multi-agent models have been around for a long time: in Plato's <em>Republic</em>, he talks about appetite (itself imperfectly self-consistent), spirit, and reason, forming a tripartite soul.&nbsp; He discusses their functions as though each has its own agency and could perceive, desire, plan, and act given the chance (plus the possibility of one forcing down the other two to rule the soul unopposed).&nbsp; Not too far off in structure is the Freudian id/superego/ego model.&nbsp; The notion of the multi-agent self even <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/GoodAngelBadAngel\">appears in fiction</a> (warning: TV Tropes).&nbsp; It appears to be a surprisingly prevalent and natural method for conceptualizing the complicated mind of the average human being.&nbsp; Of course, talking about it as something to <em>do</em> rather than as a way to push your psychological theories or your notion of the ideal city structure or a dramatization of a moral conflict makes you sound like an insane person.&nbsp; Bear with me - I have data on the usefulness of the practice from more than one outside source.<a id=\"more\"></a><br /><br />There is no reason to limit yourself to traditional multi-agent models endorsed by dead philosophers, psychologists, or cartoonists if you find you break down more naturally along some other arrangement.&nbsp; You can have two of you, or five, or twelve.&nbsp; (More than you can keep track of and differentiate is not a recommended strategy - if you're very tempted to go with this many it may be a sign of something unhealthful going on.&nbsp; If a group of them form a reliable coalition it may be best to fold them back into each other and call them one sub-agent, not several.)&nbsp; Stick with a core ensemble or encourage brief cameos of peripheral aspects.&nbsp; Name them descriptively or after structures of the brain or for the colors of the rainbow, as long as you can tell them apart.&nbsp; Talk to yourselves <a href=\"/lw/1za/the_spotlight/\">aloud or in writing</a>, or just think through the interaction if you think you'll get enough out of it that way.&nbsp; Some examples of things that could get their own sub-agents include:</p>\n<ul>\n<li>Desires or clusters of desires, be they complex and lofty (\"desire for the well being of all living things\") or simple and reptilian (\"desire for cake\")</li>\n<li>\"Inner child\" or similar role-like groupings of traits (\"professional me\", \"family-oriented me\", \"hobbies me\")</li>\n<li>High-order dispositions and principles (\"conscience\", \"neuroticism\", \"sense of justice\")</li>\n<li>Opinions or viewpoints, either specific to a situation or general trends (\"optimism\", \"outside view\", \"I should do X\")</li>\n<li>Initially unspecified, gradually-personality-developing sub-agents, if no obvious ones present themselves (named for something less suggestive like cardinal directions or two possible nicknames derived from your name)</li>\n</ul>\n<p>By priors picked up from descriptions of various people trying this, you're reasonably likely to identify one of your sub-agents as <a href=\"/lw/v4/which_parts_are_me/\">\"you\"</a>.&nbsp; In fact, one sub-agent may be <em>solely</em> identified as \"you\" - it's very hard to shake the monolithic observer experience.&nbsp; This is fine, especially if the \"you\" sub-agent is the one that <a href=\"/lw/209/highlights_and_shadows/\">endorses or repudiates</a>, but don't let the endorsement and repudiation get out of hand during multi-agent exercises.&nbsp; You have to deal with all of your sub-agents, not just the one(s) you like best, and sub-agents have been known to exhibit manipulative and even vengeful behaviors once given voice - i.e. if you represent your desire for cake as a sub-agent, and you have been thwarting your desire for cake for years, you might find that Desire For Cake is pissed off at Self-Restraint and says mean things thereunto.&nbsp; It will not placate Desire For Cake for you to throw in endorsement behind Self-Restraint while Desire For Cake is just trying to talk to you about your desperate yen for tiramisu.&nbsp; Until and unless you understand Desire For Cake well enough to surgically remove it, you need to work with it.&nbsp; Opposing it directly and with normative censure will be likely to make it angry and more devious in causing you to eat cake.<br /><br />A few miscellaneous notes on sub-agents:<br /><br />Your sub-agents may surprise you far more than you expect to be surprised by... well... yourself, which is part of what makes this exercise so useful.&nbsp; If you consciously steer the entire dialogue you will not get as much out of it - then you're just writing self-insert fanfiction about the workings of your brain, not actually learning about it.<br /><br />Not all of your sub-agents will be \"interested\" in every problem, and therefore won't have much of relevance to say at all times.&nbsp; (Desire For Cake probably couldn't care less how you act on your date next week until it's time to order dessert.)<br /><br />Your sub-agents should not outright&nbsp; lie to each other (\"should\" in the predictive, not normative, sense - let me know if it turns out yours do), but they may threaten, negotiate, hide, and be genuinely ignorant about themselves.<br /><br />Your sub-agents may not all communicate effectively.&nbsp; Having a translation sub-agent handy could be useful, if they are having trouble interpreting each other.<br /><br />(Post your ensemble of subagencies in the comments, to inspire others!&nbsp; Write dialogues between them!)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5uHdFgR938LGGxMKQ": 2, "Zwv9eHi7KGg5KA9oM": 2, "73btkq64uWfoWGfpF": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vfHRahpgbp9YFPuGQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 43, "extendedScore": null, "score": 7.4e-05, "legacy": true, "legacyId": "2619", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "lampshading", "canonicalPrevPostSlug": "highlights-and-shadows", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "tCTmAmAapB37dAz9Y", "goCfoiQkniQwPryki", "9sguwESkteCgqFMbj", "L4GGomr86sEwxzPvS", "SRaRHemkbsHWzzbPN", "Zstm38omrpeu7iWeS", "vjmw8tW6wZAtNJMKo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-01T04:04:27.480Z", "modifiedAt": null, "url": null, "title": "Loleliezers", "slug": "loleliezers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dclayh", "createdAt": "2009-03-07T01:16:38.966Z", "isAdmin": false, "displayName": "dclayh"}, "userId": "E7xnxwP5EPuGiP99X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ebgayguKoW7PTaFXk/loleliezers", "pageUrlRelative": "/posts/ebgayguKoW7PTaFXk/loleliezers", "linkUrl": "https://www.lesswrong.com/posts/ebgayguKoW7PTaFXk/loleliezers", "postedAtFormatted": "Thursday, April 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Loleliezers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALoleliezers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FebgayguKoW7PTaFXk%2Floleliezers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Loleliezers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FebgayguKoW7PTaFXk%2Floleliezers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FebgayguKoW7PTaFXk%2Floleliezers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>Previously: <a href=\"/lw/4g/eliezer_yudkowsky_facts/\">Eliezer Yudkowsky facts</a>, and <a href=\"/lw/1tr/creating_a_less_wrong_prediction_market/\">Kevin's prediction</a>.</p>\n<p>&nbsp;</p>\n<p>A bit of silliness for the day.&nbsp; Below the fold to spare those with delicate sensibilities.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<p><img src=\"http://lh6.ggpht.com/_VZewGVtB3pE/S5C8Uz16t0I/AAAAAAAAAYg/ON5iu5ftPvc/0.jpg\" alt=\"\" width=\"480\" height=\"360\" /></p>\n<p>&nbsp;</p>\n<p><img src=\"http://lh5.ggpht.com/_VZewGVtB3pE/S5C8VFwMT5I/AAAAAAAAAYo/ZZDmwviQ1q8/eliezer.jpg\" alt=\"\" width=\"486\" height=\"648\" /></p>\n<p>&nbsp;</p>\n<p><img src=\"http://lh5.ggpht.com/_VZewGVtB3pE/S5C8VF3AgJI/AAAAAAAAAYk/5LJdTCRCb8k/eliezer_yudkowskyjpg_small.jpg\" alt=\"\" width=\"428\" height=\"285\" /></p>\n<p>&nbsp;</p>\n<p>Please contribute your own in the comments.&nbsp; (Lolrobinhansons, etc., would also be welcome.)&nbsp; Unfortunately I have no special source of Eliezer photos to offer beyond Google Images.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ebgayguKoW7PTaFXk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 4, "extendedScore": null, "score": 5.740190594317947e-07, "legacy": true, "legacyId": "2396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ndtb22KYBxpBsagpj", "J9WR5YDQp4zkWBS3t"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-01T14:17:36.357Z", "modifiedAt": null, "url": null, "title": "The role of neodeconstructive rationalism in the works of Less Wrong", "slug": "the-role-of-neodeconstructive-rationalism-in-the-works-of", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L8sc7pvE7RjiX4xxc/the-role-of-neodeconstructive-rationalism-in-the-works-of", "pageUrlRelative": "/posts/L8sc7pvE7RjiX4xxc/the-role-of-neodeconstructive-rationalism-in-the-works-of", "linkUrl": "https://www.lesswrong.com/posts/L8sc7pvE7RjiX4xxc/the-role-of-neodeconstructive-rationalism-in-the-works-of", "postedAtFormatted": "Thursday, April 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20role%20of%20neodeconstructive%20rationalism%20in%20the%20works%20of%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20role%20of%20neodeconstructive%20rationalism%20in%20the%20works%20of%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8sc7pvE7RjiX4xxc%2Fthe-role-of-neodeconstructive-rationalism-in-the-works-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20role%20of%20neodeconstructive%20rationalism%20in%20the%20works%20of%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8sc7pvE7RjiX4xxc%2Fthe-role-of-neodeconstructive-rationalism-in-the-works-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8sc7pvE7RjiX4xxc%2Fthe-role-of-neodeconstructive-rationalism-in-the-works-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 626, "htmlBody": "<p><em>Summary: Yudkowsky's fiction emphasizes neodeconstructive rationalism, which serves as a bridge between class and sexual identity. Materialist libertarianism (in the metaphysical sense) implies quantum nonrealism, but examining the works of Vinge, Gibson, and especially Egan in this light generates the discourse of semitoic consciousness.</em></p>\n<h2>1. Precapitalist textual theory and neodeconstructive rationalism</h2>\n<p>In the <a href=\"http://yudkowsky.net/other/fiction\">works of Yudkowsky</a>, a predominant concept is the concept of cultural reality. It is not enough to <a href=\"/lw/i4/belief_in_belief/\">believe in belief</a>; one must <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">make beliefs pay rent</a>&nbsp;&nbsp;It could be said that Salamon&rsquo;s <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">model of neodeconstructive rationalism</a> implies that class has significance, given that the premise of materialist libertarianism is invalid. Given that <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">absence of evidence is evidence of absence</a>, an abundance of discourses concerning Pearlean absurdity may be revealed.</p>\n<p>The main theme of the works of <a href=\"/tag/luminosity/\">Alicorn</a> is not depatriarchialism, but postdepatriarchialism. Thus, Yvain suggests the use of <a href=\"/lw/13k/missing_the_trees_for_the_forest/\">neoconstructive narrative</a> to modify culture. After all, <a href=\"/lw/iq/guessing_the_teachers_password/\">guessing the teacher's password</a> is merely a route to more <a href=\"/lw/it/semantic_stopsigns/\">semantic stopsigns</a>. The defining characteristic, and subsequent dialectic, of materialist libertarianism intrinsic to Yudkowsky&rsquo;s <a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a> is also evident in <a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\">The Sword of Good</a>, although in a more mythopoetical sense.</p>\n<p>It could be said that the primary theme of Jaynes's analysis of neodeconstructive rationalism is the bridge between class and sexual identity. pjeby promotes the use of the <a href=\"/lw/15c/would_your_real_preferences_please_stand_up/1154\">cultural paradigm of consensus</a> to deconstruct class divisions.</p>\n<p>Thus, if neodeconstructive rationalism holds, we have to choose between postdialectic conceptualist theory and subcapitalist theory. But would that take place on a level greater than merely <a href=\"/lw/np/disputing_definitions/\">disputing definitions</a>? Several appropriations concerning the stasis of dialectic art exist.</p>\n<p>But the characteristic theme of the works of Bayes is a postpatriarchial reality. Hanson&rsquo;s <a href=\"http://hanson.gmu.edu/gamble.html\">critique of materialist libertarianism</a> holds that the establishment is meaningless. But is it really just an <a href=\"/lw/ns/empty_labels/\">empty label</a>?</p>\n<h2>2. Expressions of futility</h2>\n<p>&ldquo;Sexual identity is part of the stasis of language,&rdquo; says Vinge. Thus, Dennett states that we have to choose between Sartreist absurdity and capitalist libertarianism; <a href=\"/lw/1ws/the_importance_of_goodharts_law/1r52\">taw's critique</a> brings this into sharp focus. If neodeconstructive rationalism holds, the works of Yudkowsky are modernistic.</p>\n<p>&ldquo;Culture is used in the service of the status quo,&rdquo; says <a href=\"http://ase.tufts.edu/cogstud/papers/selfctr.htm\">Dennett</a>; however, according to <a href=\"http://www.overcomingbias.com/2008/09/deafening-silen.html#comment-396283\">Crowe</a>, it is not so much culture that is used in the service of the status quo, but rather the failure, and therefore the defining characteristic, of culture. But the subject is interpolated into a materialist libertarianism that includes art as a whole. Pearl holds that we have to choose between Humean qualitative post praxis and the neodialectic paradigm of consensus.</p>\n<p>In the works of Yudkowsky, a predominant concept is the distinction between figure and ground; <a href=\"/lw/p9/the_generalized_antizombie_principle/\">the generalized anti-zombie principle</a> stands in tension with <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">the tragedy of group selectionism</a> It could be said that Blake uses the term <a href=\"/lw/20j/nyc_rationalist_community/1u63\">neodeconstructive rationalism</a> to denote the role of the participant as artist. The main theme of Hanson's analysis of materialist libertarianism is the economy, and eventually the stasis, of semiotic society.</p>\n<p>But the primary theme of the works of Egan is not constructivism as such, but neoconstructivism. <a href=\"http://www.cs.sfu.ca/~anoop/weblog/archives/000140.html\">Sarkar</a> states that the works of Egan are postmodern.</p>\n<p>In a sense, Hanson uses the term 'materialist libertarianism' to denote the role of the writer as artist. <a href=\"/lw/q5/quantum_nonrealism/\">Quantum non-realism</a> implies that sexuality is used to marginalize minorities, but only if culture is distinct from language.</p>\n<h2>3. Yudkowsky and neodeconstructive rationalism</h2>\n<p>In the works of Yudkowsky, a predominant concept is the concept of <a href=\"/lw/r1/timeless_control/\">timeless control</a>. However, MichaelVassar suggests the use of <a href=\"/lw/1v0/signaling_strategies_and_morality/\">materialist libertarianism</a> to analyse and modify narrativity. The characteristic theme of the works of Gibson is the role of the writer as observer.</p>\n<p>Therefore, in Virtual Light, Gibson deconstructs <a href=\"/lw/pv/the_conscious_sorites_paradox/\">the conscious sorites paradox</a>; in All Tomorrow&rsquo;s Parties, however, he analyses <a href=\"/lw/rr/the_moral_void/\">the moral void</a>. It could be said that the subject is contextualised into a neodeconstructive rationalism that includes art as a whole. Any number of situationisms concerning Bayesian rationality may be discovered.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EnFKSZYiDHqMJuvJL": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L8sc7pvE7RjiX4xxc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 43, "extendedScore": null, "score": 0.000115, "legacy": true, "legacyId": "2622", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Summary: Yudkowsky's fiction emphasizes neodeconstructive rationalism, which serves as a bridge between class and sexual identity. Materialist libertarianism (in the metaphysical sense) implies quantum nonrealism, but examining the works of Vinge, Gibson, and especially Egan in this light generates the discourse of semitoic consciousness.</em></p>\n<h2 id=\"1__Precapitalist_textual_theory_and_neodeconstructive_rationalism\">1. Precapitalist textual theory and neodeconstructive rationalism</h2>\n<p>In the <a href=\"http://yudkowsky.net/other/fiction\">works of Yudkowsky</a>, a predominant concept is the concept of cultural reality. It is not enough to <a href=\"/lw/i4/belief_in_belief/\">believe in belief</a>; one must <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">make beliefs pay rent</a>&nbsp;&nbsp;It could be said that Salamon\u2019s <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">model of neodeconstructive rationalism</a> implies that class has significance, given that the premise of materialist libertarianism is invalid. Given that <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">absence of evidence is evidence of absence</a>, an abundance of discourses concerning Pearlean absurdity may be revealed.</p>\n<p>The main theme of the works of <a href=\"/tag/luminosity/\">Alicorn</a> is not depatriarchialism, but postdepatriarchialism. Thus, Yvain suggests the use of <a href=\"/lw/13k/missing_the_trees_for_the_forest/\">neoconstructive narrative</a> to modify culture. After all, <a href=\"/lw/iq/guessing_the_teachers_password/\">guessing the teacher's password</a> is merely a route to more <a href=\"/lw/it/semantic_stopsigns/\">semantic stopsigns</a>. The defining characteristic, and subsequent dialectic, of materialist libertarianism intrinsic to Yudkowsky\u2019s <a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a> is also evident in <a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\">The Sword of Good</a>, although in a more mythopoetical sense.</p>\n<p>It could be said that the primary theme of Jaynes's analysis of neodeconstructive rationalism is the bridge between class and sexual identity. pjeby promotes the use of the <a href=\"/lw/15c/would_your_real_preferences_please_stand_up/1154\">cultural paradigm of consensus</a> to deconstruct class divisions.</p>\n<p>Thus, if neodeconstructive rationalism holds, we have to choose between postdialectic conceptualist theory and subcapitalist theory. But would that take place on a level greater than merely <a href=\"/lw/np/disputing_definitions/\">disputing definitions</a>? Several appropriations concerning the stasis of dialectic art exist.</p>\n<p>But the characteristic theme of the works of Bayes is a postpatriarchial reality. Hanson\u2019s <a href=\"http://hanson.gmu.edu/gamble.html\">critique of materialist libertarianism</a> holds that the establishment is meaningless. But is it really just an <a href=\"/lw/ns/empty_labels/\">empty label</a>?</p>\n<h2 id=\"2__Expressions_of_futility\">2. Expressions of futility</h2>\n<p>\u201cSexual identity is part of the stasis of language,\u201d says Vinge. Thus, Dennett states that we have to choose between Sartreist absurdity and capitalist libertarianism; <a href=\"/lw/1ws/the_importance_of_goodharts_law/1r52\">taw's critique</a> brings this into sharp focus. If neodeconstructive rationalism holds, the works of Yudkowsky are modernistic.</p>\n<p>\u201cCulture is used in the service of the status quo,\u201d says <a href=\"http://ase.tufts.edu/cogstud/papers/selfctr.htm\">Dennett</a>; however, according to <a href=\"http://www.overcomingbias.com/2008/09/deafening-silen.html#comment-396283\">Crowe</a>, it is not so much culture that is used in the service of the status quo, but rather the failure, and therefore the defining characteristic, of culture. But the subject is interpolated into a materialist libertarianism that includes art as a whole. Pearl holds that we have to choose between Humean qualitative post praxis and the neodialectic paradigm of consensus.</p>\n<p>In the works of Yudkowsky, a predominant concept is the distinction between figure and ground; <a href=\"/lw/p9/the_generalized_antizombie_principle/\">the generalized anti-zombie principle</a> stands in tension with <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">the tragedy of group selectionism</a> It could be said that Blake uses the term <a href=\"/lw/20j/nyc_rationalist_community/1u63\">neodeconstructive rationalism</a> to denote the role of the participant as artist. The main theme of Hanson's analysis of materialist libertarianism is the economy, and eventually the stasis, of semiotic society.</p>\n<p>But the primary theme of the works of Egan is not constructivism as such, but neoconstructivism. <a href=\"http://www.cs.sfu.ca/~anoop/weblog/archives/000140.html\">Sarkar</a> states that the works of Egan are postmodern.</p>\n<p>In a sense, Hanson uses the term 'materialist libertarianism' to denote the role of the writer as artist. <a href=\"/lw/q5/quantum_nonrealism/\">Quantum non-realism</a> implies that sexuality is used to marginalize minorities, but only if culture is distinct from language.</p>\n<h2 id=\"3__Yudkowsky_and_neodeconstructive_rationalism\">3. Yudkowsky and neodeconstructive rationalism</h2>\n<p>In the works of Yudkowsky, a predominant concept is the concept of <a href=\"/lw/r1/timeless_control/\">timeless control</a>. However, MichaelVassar suggests the use of <a href=\"/lw/1v0/signaling_strategies_and_morality/\">materialist libertarianism</a> to analyse and modify narrativity. The characteristic theme of the works of Gibson is the role of the writer as observer.</p>\n<p>Therefore, in Virtual Light, Gibson deconstructs <a href=\"/lw/pv/the_conscious_sorites_paradox/\">the conscious sorites paradox</a>; in All Tomorrow\u2019s Parties, however, he analyses <a href=\"/lw/rr/the_moral_void/\">the moral void</a>. It could be said that the subject is contextualised into a neodeconstructive rationalism that includes art as a whole. Any number of situationisms concerning Bayesian rationality may be discovered.</p>", "sections": [{"title": "1. Precapitalist textual theory and neodeconstructive rationalism", "anchor": "1__Precapitalist_textual_theory_and_neodeconstructive_rationalism", "level": 1}, {"title": "2. Expressions of futility", "anchor": "2__Expressions_of_futility", "level": 1}, {"title": "3. Yudkowsky and neodeconstructive rationalism", "anchor": "3__Yudkowsky_and_neodeconstructive_rationalism", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CqyJzDZWvGhhFJ7dY", "a7n8GdKiAZRX86T5A", "mnS2WYLCGJP2kQkRn", "MtNnFg4uN32YPoKNa", "NMoLJuDJEms7Ku9XS", "FWMfQKG3RpZx6irjm", "HawFh7RvDM4RyoJ2d", "7X2j8HAkWdmMoS8PE", "i2dfY65JciebF3CAo", "kYAuNJX2ecH2uFqZ9", "QsMJQSFj7WfoTMNgW", "k3823vuarnmL5Pqin", "YYLmZFEGKsjCKQZut", "BviRaP4przARdmb8b", "nso8WXdjHLLHkJKhr", "K9JSM7d7bLJguMxEp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-01T15:21:03.777Z", "modifiedAt": null, "url": null, "title": "Open Thread: April 2010", "slug": "open-thread-april-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:02.649Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hMv7JhPMN8SuSrk7m/open-thread-april-2010", "pageUrlRelative": "/posts/hMv7JhPMN8SuSrk7m/open-thread-april-2010", "linkUrl": "https://www.lesswrong.com/posts/hMv7JhPMN8SuSrk7m/open-thread-april-2010", "postedAtFormatted": "Thursday, April 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20April%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20April%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMv7JhPMN8SuSrk7m%2Fopen-thread-april-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20April%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMv7JhPMN8SuSrk7m%2Fopen-thread-april-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhMv7JhPMN8SuSrk7m%2Fopen-thread-april-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<div>\n<p>An <a href=\"/tag/open_thread/?sort=new\">Open Thread</a>: a place for things foolishly April, and other assorted discussions.</p>\n<p><em>This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>\n<p><strong>Update: <a href=\"/lw/20z/announcing_the_less_wrong_subreddit/\">Tom McCabe has created a sub-Reddit</a> to use for assorted discussions instead of relying on open threads.&nbsp; Go there for the sub-Reddit and discussion about it, and <a href=\"/lw/20z/announcing_the_less_wrong_subreddit/1uak\">go here to vote</a> on the idea.</strong></p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hMv7JhPMN8SuSrk7m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 5.74152301162584e-07, "legacy": true, "legacyId": "2624", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 539, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Rt8oJF27dndhycxks"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-01T19:41:44.735Z", "modifiedAt": null, "url": null, "title": "The human problem", "slug": "the-human-problem", "viewCount": null, "lastCommentedAt": "2020-12-09T03:32:40.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7PC22HTvtEbv6tvWJ/the-human-problem", "pageUrlRelative": "/posts/7PC22HTvtEbv6tvWJ/the-human-problem", "linkUrl": "https://www.lesswrong.com/posts/7PC22HTvtEbv6tvWJ/the-human-problem", "postedAtFormatted": "Thursday, April 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20human%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20human%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PC22HTvtEbv6tvWJ%2Fthe-human-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20human%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PC22HTvtEbv6tvWJ%2Fthe-human-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PC22HTvtEbv6tvWJ%2Fthe-human-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1165, "htmlBody": "<p>You've fiddled with your physics constants until you got them just right, pushed matter into just the right initial configuration, given all the galaxies a good spin, and tended them carefully for a few billion years.&nbsp; Finally, one of the creatures on one of those planets in one of those galaxies looks up and notices the stars.&nbsp; Congratulations!&nbsp; You've evolved \"humans\", the term used for those early life forms that have mustered up enough just brain cells to wonder about you.</p>\n<p>Widely regarded as the starting point of interest in a universe, they're too often its ending point as well.&nbsp; Every amateur god has lost at least one universe to humans.&nbsp; They occupy that vanishingly-narrow yet dangerous window of intelligence that your universe must safely navigate, in which your organisms are just smart enough to seize the helm of evolution, but not smart enough to understand what they're really doing.</p>\n<p>The trouble begins when one of these humans decides, usually in a glow of species pride shortly after the invention of the wheel or the digital watch or some such knicknack, that they are in fact pretty neat, and that it's vitally important to ensure that all future intelligent life shares <em>their</em> values.</p>\n<p><a id=\"more\"></a></p>\n<p>At that point, they invent a constrained optimization process, that ensures that all new complex agents (drives, minds, families, societies, etc.) and all improvements to existing agents, have a good score according to some agreed-on function.</p>\n<p>If you're lucky, your humans will design a phenomenological function, which evaluates the qualia in the proposed new mind.&nbsp; This will be an inconvenience to your universe, as it will slow down the exploration of agent-design space; but it's not so bad that you have to crumple your universe up and throw it out.&nbsp; It doesn't necessarily cut off all the best places in agent space from ever being explored.</p>\n<p>But remember these are <em>humans</em> we're talking about.&nbsp; They've only recently evolved the ability to <em>experience</em> qualia, let alone understand and evaluate them.&nbsp; So they usually design computationalist functions instead.&nbsp; All functions <em>perform</em> computation; by \"computationalist\" we mean that they evaluate an agent by the output of its computations, rather than by what it feels like to be such an agent.</p>\n<p>Before either kind of function can be evaluated, the agent design is abstracted into a description made entirely using a pre-existing set of symbols.&nbsp; If your humans have a great deal of computational power available, they might choose very low-level symbols with very little individual semantic content, analogous to their primary sensory receptors; and use abstract score functions that perform mainly statistical calculations.&nbsp; A computationalist function made along these lines is still likely to be troublesome, but might not be a complete disaster.</p>\n<p>Unfortunately, the simplest, easiest, fastest, and most common approach is to use symbols that the humans think they can \"understand\", that summarize a proposed agent entirely in terms of the categories already developed by their own primitive senses and qualia.&nbsp; In fact, they often use their existing qualia as the targets of their evaluation function!</p>\n<p>Once the initial symbol set has been chosen, the semantics must be set in stone for the judging function to be \"safe\" for preserving value; this means that any new symbols must be defined completely in terms of already-existing symbols.&nbsp; Because fine-grained sensory information has been lost, new developments in consciousness might not be detectable in the symbolic representation after the abstraction process.&nbsp; If they are detectable via statistical correlations between existing concepts, they will be difficult to reify parsimoniously as a composite of existing symbols.&nbsp; Not using a theory of phenomenology means that no effort is being made to look for such new developments, making their detection and reification even more unlikely.&nbsp; And an evaluation based on already-developed values and qualia means that even if they could be found, new ones would not improve the score.&nbsp; Competition for high scores on the existing function, plus lack of selection for components orthogonal to that function, will ensure that no such new developments last.</p>\n<p>Pretty soon your humans will tile your universe with variations on themselves.&nbsp; And the universe you worked so hard over, that you had such high hopes for, will be taken up entirely with creatures that, although they become increasingly computationally powerful, have an emotional repertoire so impoverished that they rarely have any complex positive qualia beyond pleasure, discovery, joy, love, and vellen.&nbsp; What was to be your masterpiece becomes instead an entire universe devoid of fleem.</p>\n<p>There's little that will stop one of these crusading, expansionist, assimilating collectives once it starts.&nbsp; (And, of course, if you intervene on a planet after it develops geometry, your avatar will be executed and your universe may be disqualified.)</p>\n<p>Some gods say that there's nothing you can do to prevent this from happening, and the best you can do is to seed only one planet in each universe with life - to put all your eggs in one male, so to speak.&nbsp; This is because a single bad batch of humans can spoil an entire universe.&nbsp; Standard practice is to time the evolution of life in different star systems to develop geometry at nearly the same time, leading to a maximally-diverse mid-game.&nbsp; But value-preserving (also called \"purity-based\" or \"conservative\") societies are usually highly aggressive when encountering alien species, reducing diversity and expending your universe's limited energy.&nbsp; So instead, these gods build a larger number of isolated universes, each seeded with life on just one planet.&nbsp; (A more elaborate variant of this strategy is to distribute matter in dense, widely-separated clusters, impose a low speed of information propagation, and seed each cluster with one live planet, so that travel time between cluster always gives a stabilizing \"home field\" advantage in contact between species from different clusters.)</p>\n<p>However, there are techniques that some gods report using successfully to break up a human-tiling.</p>\n<p><strong>Dynamic physical constants</strong> - If you subtly vary your universe's physical constants over time or space, this may cause their function-evaluation or error-checking mechanisms to fail.&nbsp; Be warned: This technique is not for beginners.&nbsp; Note that the judges will usually deduct points for lookup-table variation of physical constants.</p>\n<p><strong>Cosmic radiation</strong> - Bombardment by particles and shortwave radiation can also cause their function-evaluation or error-checking mechanisms to fail.&nbsp; The trick here is to design your universe so that drifting interstellar bubbles of sudden, high-intensity radiation are frequent enough to hit an expanding tiling of humans, yet not frequent enough to wipe out vulnerable early-stage multicellular life.</p>\n<p><strong>Spiral arms</strong> - A clever way of making the humans themselves implement the radiation strategy.&nbsp; An expanding wave of humans will follow a dense column of matter up to the galactic core, where there are high particle radiation levels.&nbsp; Even if this fails, ensuring that the distribution of matter in your universe has a low intrinsic dimensionality (at most half the embedded dimensionality) will slow down the spread of humans and give other species a chance to evolve.</p>\n<p>So that's our column for today!&nbsp; Good luck, have fun, and remember - never let the players in on the game!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1, "pGqRLe9bFDX2G2kXY": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7PC22HTvtEbv6tvWJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 35, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "2625", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-01T20:14:09.309Z", "modifiedAt": null, "url": null, "title": "What is Rationality?", "slug": "what-is-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:49.297Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2xkMt5XQqpG5fZjxb/what-is-rationality", "pageUrlRelative": "/posts/2xkMt5XQqpG5fZjxb/what-is-rationality", "linkUrl": "https://www.lesswrong.com/posts/2xkMt5XQqpG5fZjxb/what-is-rationality", "postedAtFormatted": "Thursday, April 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20Rationality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20Rationality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xkMt5XQqpG5fZjxb%2Fwhat-is-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20Rationality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xkMt5XQqpG5fZjxb%2Fwhat-is-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xkMt5XQqpG5fZjxb%2Fwhat-is-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1474, "htmlBody": "<p><em>This article is an attempt to summarize basic material, and thus probably won't have anything new for the experienced crowd. <br /></em></p>\n<p>Related: <a href=\"/lw/1ht/11_core_rationalist_skills/\">11 Core Rationalist Skills</a>, <a href=\"/lw/1to/what_is_bayesianism/\">What is Bayesianism?</a></p>\n<p>Less Wrong is a blog devoted to refining the art of human rationality, but what is rationality? Rationality is unlike any subject I studied at school or university, and it is probably the case that the synthesis of subjects and ideas here on Less Wrong is fairly unique.&nbsp;</p>\n<p>Fundamentally, rationality is the study of <strong><em>general methods for good</em></strong> <strong><em>decision-making</em></strong>, especially where the decision is <em>hard to get right</em>. When an individual is considering <a href=\"http://www.overcomingbias.com/2008/12/we-agree-get-froze.html\">whether to get a cryonics policy</a>, or when a country is trying to work out what to do about global warming, one is within the realm of decision-making that we can use rationality to improve. People do badly on hard decision problems for a variety of reasons, including: that they are not born with the ability to deal with the scientific knowledge and complex systems that our modern world runs on, that they haven't been warned that they should think critically about their own reasoning, that they belong to groups that collectively hold faulty beliefs, and that their emotions and biases skew their reasoning process.</p>\n<ul>\n<li>Rationality is the ability to <em>do well on hard decision problems</em>.</li>\n</ul>\n<p>Another central theme of rationality is <em><strong>truth-seeking</strong></em>. Truth-seeking is often used as an aid to decision-making: if you're trying to decide whether to get a cryonics policy, you might want to find out whether the technology has any good evidence suggesting that it might work. We can make good decisions by getting an accurate estimate of the relevant facts and parameters, and then choosing the best option according to our understanding of things; if our understanding is more accurate, this will tend to work better.</p>\n<ul>\n<li>Rationality is also the art of how to <em>systematically come to know what is true</em>. <a id=\"more\"></a></li>\n</ul>\n<p>Often, the processes of truth-seeking and decision-making, both on the individual level and the group level are subject to <em>biases</em>: systematic failures to get to the truth or to make good decisions. Biases in individual humans are an extremely serious problem - most people make important life-decisions without even realizing the extent and severity of the <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">cognitive biases</a> they were born with. Therefore rational thought requires a good deal of critical thinking - analyzing and reflecting on your own thought processes in order to iron out the many flaws they contain. Group dynamics can introduce mechanisms of irrationality above and beyond the individual biases and failings of members of the group, and often good decision-making in groups is most severely hampered by flawed social epistemology. An acute example of this phenomenon is <a href=\"http://www.google.co.uk/url?sa=t&amp;source=web&amp;ct=res&amp;cd=1&amp;ved=0CAYQFjAA&amp;url=http%3A%2F%2Fnews.bbc.co.uk%2F2%2Fhi%2F7947460.stm&amp;rct=j&amp;q=pope+tells+people+not+to+use+condoms&amp;ei=D720S-HcBoTf4gaN26jADg&amp;usg=AFQjCNGdi7Eb91mSstrWQiFUjXUtZ71Oeg&amp;sig2=6lQBfOGZlTyvEUfFHMwdIA\">The Pope telling</a> HIV infested Africa to stop using condoms; a social phenomenon (religion) was responsible for a failure to make good decisions.</p>\n<p>Perhaps the best way to understand rationality is to see some techniques that are used, and some examples of its use.</p>\n<p>Rationality techniques and topics include:</p>\n<ul>\n<li> <em>Following through with simple logical inferences and numerical calculations</em> - A surprising number of bad decisions and conclusions can be avoided by doing relatively simple pieces of logical reasoning without error or flinching in the face of the conclusion. Common general examples include <a href=\"http://en.wikipedia.org/wiki/Non_sequitur_%28logic%29\">non sequiturs</a> such as <a href=\"http://en.wikipedia.org/wiki/Affirming_the_consequent\">affirming the consequent</a>, <a href=\"http://en.wikipedia.org/wiki/Argument_from_fallacy\">argument from fallacy</a>, and <a href=\"http://en.wikipedia.org/wiki/Absence_of_evidence\">taking absence of evidence as certitude of absence</a> (\"I haven't found any evidence for it therefore it can never happen\" type reasoning). Many bad decisions also result from not doing simple arithmetic, or not taking into account quantitative reasoning. See <a href=\"http://www.inference.phy.cam.ac.uk/sustainable/charger/\">this website on environmentalism gone wrong</a><em> </em>due to a failure to reason quantitatively. </li>\n</ul>\n<ul>\n<li><em>Heuristics and biases </em>- Perhaps the key insight that started the Overcoming Bias and Less Wrong blogs was the mounting case from experimental psychologists that real human decision-making and Belief formation is far from the ideals of economic rationality and Bayesian probability. A key reference on the subject is <a href=\"http://www.amazon.com/Judgment-under-Uncertainty-Heuristics-Biases/dp/0521284147\">Judgment under Uncertainty: Heuristics and Biases</a>. For those who prefer the web, Less Wrong has a set of articles tagged <a href=\"/tag/standard_biases/\">\"standard biases\"</a>. If you know your own flaws, you may be able to correct for them - this is known as <em>debiasing</em>. </li>\n</ul>\n<ul>\n<li><em>Evolutionary psychology and Evolutionary theory</em> - In his bestselling book <em>Fooled by Randomness</em>, Nassim Taleb writes \"Our minds are not quite designed to understand how the world works, but, rather, to get out of trouble rapidly and have progeny\". Understanding that the process that produced you cared only about inclusive genetic fitness in the ancestral environment, rather than your welfare or ability to believe the truth can help to identify and iron out flaws in your decision-making. There is a good <a href=\"http://wiki.lesswrong.com/wiki/Evolution\">sequence on evolution</a> on Less Wrong. Perhaps the most important piece of work on the implications of evolutionary theory for decision-making and rationality is Bostrom and Sandberg's <a href=\"http://www.nickbostrom.com/evolution.pdf\">Wisdom of Nature</a>; although it purportedly aims at assessing human enhancement options, the style of reasoning is highly applicable to thinking about how to deal with the mixed blessings that evolution put inside our skulls. </li>\n</ul>\n<ul>\n<li><em>Defeating motivated cognition</em> - Many specific instances and types of biased reasoning are probably created by the same set of sources, often processes deeply intertwined with our evolved psychology. The most pernicious of these \"sources of biased reasoning\" is motivated cognition, the king of biases. The human mind seems to have a way of short-circuiting itself whereby happy emotions come when you visualize an outcome that is good for you, and this causes you to search for arguments that support the conclusion that that good outcome will occur. This kind of <a href=\"/lw/js/the_bottom_line/\">\"bottom line reasoning\"</a> is insidious, and decreasing the extent to which you suffer from it is a key way to increase your rationality. <a href=\"/lw/o4/leave_a_line_of_retreat/\">Leaving a line of retreat</a> is one good antidote. There is a whole sequence on <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">how to actually change your mind</a> that attempts to beat this problem. </li>\n</ul>\n<ul>\n<li><em>Techniques of analytic philosophy</em> - <a href=\"http://en.wikipedia.org/wiki/Analytic_philosophy\">Analytic philosophers</a> have spent a long time honing techniques to promote better thinking, especially about conceptually confusing subjects. They will often be very careful to explicitly define key terms they use, and be open and upfront about terms that they take as primitive, as well as being clear about the structure of their arguments. </li>\n</ul>\n<ul>\n<li><em>Bayesian statistics and the Bayesian mindset</em> - Covered expertly in the article <a href=\"/lw/1to/what_is_bayesianism/\">\"What is Bayesianism?\"</a> - briefly, the idea is that the beliefs of an ideal rational agent are formed by a process of formulating hypotheses, assigning prior credence to each, and then using Bayes' theorem to work backwards from the data to work out how likely various hypotheses are, given the data. In cases where there is \"overwhelming evidence\", the strictures of Bayes' theorem are unnecessary: it will be obvious which hypothesis is true. For example, you do not need Bayes' theorem to deduce that Gary Kasparov would beat your grandmother at chess. Related to this are various errors and lies that can arise from bad (or deliberately misleading) statistical analyses.</li>\n</ul>\n<ul>\n<li><em>Microeconomic ways of thinking</em> - <a href=\"http://en.wikipedia.org/wiki/Microeconomics\">Microeconomics</a> models rational agents as aiming to make good personal choices subject to resource constraints. Von Neumann and Morgenstern proved an <a href=\"http://en.wikipedia.org/wiki/Expected_utility#von_Neumann-Morgenstern_formulation\">important theorem</a> stating that the preferences of a \"rational\" agent can be expressed as a utility function. Other researchers in microeconomics made significant advances by considering the <em>marginal utility</em> of actions - how much better do things get if one shifts one dollar of one's expenditure from buying ice-cream to buying clothes? The notion of <a href=\"http://en.wikipedia.org/wiki/Opportunity_cost\">opportunity cost</a> is a classic example of a microeconomic concept. <a href=\"http://en.wikipedia.org/wiki/Value_of_information\">Value of information</a> is another. In recent times, microeconomics has taken human psychology into account more, leading to formal theories of boundedly rational and irrational agents, such as <a href=\"http://en.wikipedia.org/wiki/Prospect_theory\">prospect theory</a>.</li>\n</ul>\n<ul>\n<li><em>Game theory and signaling games</em> - a sub-field of microeconomics so important that it deserves a separate mention, game theory analyzes the interactions between competing rational agents in a formal way. The key intuition pump is the <a href=\"http://en.wikipedia.org/wiki/Prisoner%27s_dilemma\">prisoner's dilemma</a>, but I think that the the formal analysis of <a href=\"http://en.wikipedia.org/wiki/Signalling_games\">signaling games</a> is even more important for rationality, as signaling games explain so much about why people verbally endorse statements (the statement is there as a <em>signal</em>, not as an indicator of rational belief). Robin Hanson of Overcoming Bias has posted many times on how the subconscious human desire to <a href=\"http://www.overcomingbias.com/tag/signaling\">signal</a> affects our decision-making in weird ways. </li>\n</ul>\n<ul>\n<li><em>Creating good social epistemology and norms of rationality</em> - several sequences on Less Wrong are about how to create an atmosphere that encourages honest and productive social epistemology. <a href=\"http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor\">Resisting groupthink and cultishness</a> is important step, as is dealing with the problem that politics tends to make humans stupid; this is covered in the <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">\"politics is the mind killer\"</a> sequence. Finally there is a <a href=\"/lw/cz/the_craft_and_the_community/\">sequence on creating good rationalist communities</a>.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2xkMt5XQqpG5fZjxb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "2617", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jkmc5Q4P7tX7xWFJY", "AN2cBr6xKWCB8dRQG", "34XxbRFe54FycoCDw", "3XgYbghWruBMrPTAL", "YdcF6WbBmJhaaDqoD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-01T20:41:39.003Z", "modifiedAt": null, "url": null, "title": "Rationality quotes: April 2010", "slug": "rationality-quotes-april-2010", "viewCount": null, "lastCommentedAt": "2012-11-20T09:24:44.100Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wnoise", "createdAt": "2009-03-27T08:11:18.135Z", "isAdmin": false, "displayName": "wnoise"}, "userId": "heho8kyps9kjt5pJx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SpkC3zmt42Jr6jH3A/rationality-quotes-april-2010", "pageUrlRelative": "/posts/SpkC3zmt42Jr6jH3A/rationality-quotes-april-2010", "linkUrl": "https://www.lesswrong.com/posts/SpkC3zmt42Jr6jH3A/rationality-quotes-april-2010", "postedAtFormatted": "Thursday, April 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20quotes%3A%20April%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20quotes%3A%20April%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpkC3zmt42Jr6jH3A%2Frationality-quotes-april-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20quotes%3A%20April%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpkC3zmt42Jr6jH3A%2Frationality-quotes-april-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpkC3zmt42Jr6jH3A%2Frationality-quotes-april-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>This is our monthly thread for collecting these little gems and pearls of wisdom, rationality-related quotes you've seen recently, or had stored in your quotesfile for ages, and which might be handy to link to in one of our discussions.</p>\n<ul>\n<li>&nbsp;Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. &nbsp;If strongly ordered, then go ahead and post them together.)</li>\n<li>&nbsp;Do not quote yourself.</li>\n<li>&nbsp;Do not quote comments/posts on LW/OB.</li>\n<li>&nbsp;No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SpkC3zmt42Jr6jH3A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 5.742154054740796e-07, "legacy": true, "legacyId": "2626", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 306, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-02T01:17:44.603Z", "modifiedAt": null, "url": null, "title": "Announcing the Less Wrong Sub-Reddit", "slug": "announcing-the-less-wrong-sub-reddit", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:48.854Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rt8oJF27dndhycxks/announcing-the-less-wrong-sub-reddit", "pageUrlRelative": "/posts/Rt8oJF27dndhycxks/announcing-the-less-wrong-sub-reddit", "linkUrl": "https://www.lesswrong.com/posts/Rt8oJF27dndhycxks/announcing-the-less-wrong-sub-reddit", "postedAtFormatted": "Friday, April 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Announcing%20the%20Less%20Wrong%20Sub-Reddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnnouncing%20the%20Less%20Wrong%20Sub-Reddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRt8oJF27dndhycxks%2Fannouncing-the-less-wrong-sub-reddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Announcing%20the%20Less%20Wrong%20Sub-Reddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRt8oJF27dndhycxks%2Fannouncing-the-less-wrong-sub-reddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRt8oJF27dndhycxks%2Fannouncing-the-less-wrong-sub-reddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 309, "htmlBody": "<p>Announcing: the Less Wrong Sub-Reddit, at http://reddit.com/r/LessWrong. This Reddit is intended as a partial replacement for/complement to the Open Thread, which has gotten somewhat unwieldy and overcrowded as of late. I (Thomas McCabe) will be posting things that appear on the <a href=\"/lw/20w/open_thread_april_2010\">April Open Thread</a> to this Reddit, to aid in starting conversation. We'll see how it goes.</p>\n<p>This Reddit is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets very long/involved, celebrate by turning it into a top-level post.</p>\n<p>To anyone who is worried about the discussion quality devolving to Reddit level: I retain moderator power over the sub-Reddit, and can delete things and ban people from it. If this gets to be too much work for me, I will be happy to give mod power to other interested Less Wrong readers with a track record of good posts and comments.</p>\n<p>This is purely my creation, and not that of Eliezer or the Less Wrong admins. If anything goes horribly wrong, don't blame them.</p>\n<p>This is completely not an April Fool's joke. I want to start it now (on the first day of the month) because the Open Thread \"only\" has 52 comments on it.</p>\n<p>If you don't have a Reddit account, or want to create a new account to post under your Less Wrong username, you can click \"Register\" in the upper-right-hand corner. It only takes fifteen seconds.</p>\n<p>For those who don't look at the bottom of the website very often, Less Wrong is originally powered by the Reddit codebase.</p>\n<p>Good luck, everyone, and may the best discussions win.</p>\n<p>Edited for clarity: I'm proposing that we set up a new discussion community such that Less Wrongers have a place to talk about off-topic stuff other than Open Thread (which is hugely overcrowded). If either LW or the subreddit crashes, it should have no effect on the other.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rt8oJF27dndhycxks", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 10, "extendedScore": null, "score": 5.742696622008308e-07, "legacy": true, "legacyId": "2627", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hMv7JhPMN8SuSrk7m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-03T09:36:05.289Z", "modifiedAt": null, "url": null, "title": "Less Wrong London meetup, tomorrow (Sunday 2010-04-04) 16:00", "slug": "less-wrong-london-meetup-tomorrow-sunday-2010-04-04-16-00", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:49.274Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ySuaZ4iQruxGwvrEt/less-wrong-london-meetup-tomorrow-sunday-2010-04-04-16-00", "pageUrlRelative": "/posts/ySuaZ4iQruxGwvrEt/less-wrong-london-meetup-tomorrow-sunday-2010-04-04-16-00", "linkUrl": "https://www.lesswrong.com/posts/ySuaZ4iQruxGwvrEt/less-wrong-london-meetup-tomorrow-sunday-2010-04-04-16-00", "postedAtFormatted": "Saturday, April 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20London%20meetup%2C%20tomorrow%20(Sunday%202010-04-04)%2016%3A00&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20London%20meetup%2C%20tomorrow%20(Sunday%202010-04-04)%2016%3A00%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FySuaZ4iQruxGwvrEt%2Fless-wrong-london-meetup-tomorrow-sunday-2010-04-04-16-00%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20London%20meetup%2C%20tomorrow%20(Sunday%202010-04-04)%2016%3A00%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FySuaZ4iQruxGwvrEt%2Fless-wrong-london-meetup-tomorrow-sunday-2010-04-04-16-00", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FySuaZ4iQruxGwvrEt%2Fless-wrong-london-meetup-tomorrow-sunday-2010-04-04-16-00", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p><strong>UPDATE: Backup plan </strong>is to meet at the Starbucks across the road (<span dir=\"ltr\">16 Piccadilly</span><span dir=\"ltr\">, London <a href=\"http://maps.google.co.uk/maps?q=W1J+0DE\">W1J 0DE</a>,&nbsp;</span>020 7287 8311). I've been trying to ring the Waterstones and the coffee shop for a while now and waited several minute for an answer with no success, so I think it's very likely that it is closed.&nbsp; I've called the Starbucks and it's open.&nbsp; If I know you on here, mail me (paul at ciphergoth dot org) and I'll give you my mobile number.</p>\n<p>In the grand tradition of giving almost no notice for London meetups, I bring to your attention that <a href=\"/lw/20w/open_thread_april_2010/1u84\">a meetup is planned</a> for tomorrow (Sunday 2010-04-04), at 16:00, in the 5th View cafe on top of Waterstone's bookstore. Nearest Tube Piccadilly Circus. Yvain, taw, RichardKennaway, and myself at least hope to be there, doubtless others too!</p>\n<p>We should try to give more notice for the next one.&nbsp; This is the first Sunday in April; how about the first Sunday in June for the next one, 2010-06-06?&nbsp; I'd prefer an earlier time and it might be worth experimenting with a different venue, but if we can fix a date we can vary other details closer to the time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ySuaZ4iQruxGwvrEt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 5.746516304686397e-07, "legacy": true, "legacyId": "2633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-03T23:29:30.507Z", "modifiedAt": null, "url": null, "title": "Bayesian Collaborative Filtering", "slug": "bayesian-collaborative-filtering", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:53.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SA5bvZJXcwEEuZFSe/bayesian-collaborative-filtering", "pageUrlRelative": "/posts/SA5bvZJXcwEEuZFSe/bayesian-collaborative-filtering", "linkUrl": "https://www.lesswrong.com/posts/SA5bvZJXcwEEuZFSe/bayesian-collaborative-filtering", "postedAtFormatted": "Saturday, April 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20Collaborative%20Filtering&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20Collaborative%20Filtering%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSA5bvZJXcwEEuZFSe%2Fbayesian-collaborative-filtering%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20Collaborative%20Filtering%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSA5bvZJXcwEEuZFSe%2Fbayesian-collaborative-filtering", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSA5bvZJXcwEEuZFSe%2Fbayesian-collaborative-filtering", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 989, "htmlBody": "<p><em>I present an algorithm I designed to predict which position a person would report for an issue on <a href=\"http://www.takeonit.com/\" target=\"_blank\">TakeOnIt</a>, through Bayesian updates on the evidence of other people's positions on that issue. Additionally, I will point out some potential areas of improvement, in the hopes of inspiring others here to expand on this method.</em></p>\n<p><br />For those not familiar with <a href=\"http://www.takeonit.com/\" target=\"_blank\">TakeOnIt</a>, the basic idea is that there are issues, represented by yes/no questions, on which people can take the positions Agree (A), Mostly Agree (MA), Neutral (N), Mostly Disagree (MD), or Disagree (D). (There are two types of people tracked by TakeOnIt: users who register their own opinions, and Experts/Influencers whose opinions are derived from public quotations.)</p>\n<p>The goal is to predict what issue a person S would take on a position, based on the positions registered by other people on that question. To do this, we will use <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> to update the probability that person S takes the position X on issue I, given that person T has taken position Y on issue I:</p>\n<p><img src=\"http://latex.codecogs.com/png.latex?P(\\mbox{S&amp;space;takes&amp;space;X&amp;space;on&amp;space;I}|\\mbox{T&amp;space;takes&amp;space;Y&amp;space;on&amp;space;I})&amp;space;=&amp;space;P(\\mbox{S&amp;space;takes&amp;space;X&amp;space;on&amp;space;I})*\\frac{P(\\mbox{T&amp;space;takes&amp;space;Y&amp;space;on&amp;space;I}|\\mbox{S&amp;space;takes&amp;space;X&amp;space;on&amp;space;I})}{P(\\mbox{T&amp;space;takes&amp;space;Y&amp;space;on&amp;space;I})}\" alt=\"P(S takes X on I | T takes Y on I) = P(S takes X on I)*P(T takes Y on I | S takes X on I)/P(T takes Y on I)\" width=\"661\" height=\"45\" /></p>\n<p>Really, we will be updating on several people T<sub>j</sub> taking positions T<sub>y</sub> on I:</p>\n<p><img src=\"http://latex.codecogs.com/png.latex?P(\\mbox{S&amp;space;takes&amp;space;X&amp;space;on&amp;space;I}|\\forall{j}\\mbox{&amp;space;T}_{j}\\mbox{&amp;space;takes&amp;space;Y}_{j}\\mbox{&amp;space;on&amp;space;I})=P(\\mbox{S&amp;space;takes&amp;space;X&amp;space;on&amp;space;I})*\\prod_{j}\\frac{P(\\mbox{T}_{j}\\mbox{&amp;space;takes&amp;space;Y}_{j}\\mbox{&amp;space;on&amp;space;I}|\\mbox{S&amp;space;takes&amp;space;~X&amp;space;on&amp;space;I})}{P(\\mbox{T}_{j}\\mbox{&amp;space;takes&amp;space;Y}_{j}\\mbox{&amp;space;on&amp;space;I})}\" alt=\"P(S takes X on I | for all j, Tj takes Yj on I) = P(S takes X on I)*Product over j of (P(Tj takes Yj on I | S takes X on I)/P(Tj takes Yj on I))\" width=\"662\" height=\"51\" /></p>\n<p><a id=\"more\"></a></p>\n<p>To compute this, let us first figure out the prior probability P(S takes X on I). I use for this a generalization of <a href=\"http://en.wikipedia.org/wiki/Rule_of_succession\" target=\"_blank\">Laplace's Law of Succession</a> (representing my theory that a person will take each position with a particular frequency, and that there is no reason, before seeing their actual position, to suppose that one position in particular is more frequent than the others), that the odds that S takes the position A : MA : N : MD : D&nbsp; on I is given by:</p>\n<p style=\"padding-left: 30px;\">1 + count of issues S has taken position A on : 1 + count of issues S has taken position MA on : 1 + count of issues S has taken position N on : 1 + count of issues S has taken position MD on : 1 + count of issues S has taken position D on</p>\n<p>Thus, the probability</p>\n<p><img src=\"http://latex.codecogs.com/png.latex?P(\\mbox{S&amp;space;takes&amp;space;X&amp;space;on&amp;space;I})=\\frac{(1&amp;space;&amp;plus;\\mbox{count&amp;space;of&amp;space;issues&amp;space;S&amp;space;has&amp;space;taken&amp;space;X&amp;space;on})}^{(5&amp;space;&amp;plus;\\mbox{count&amp;space;of&amp;space;issues&amp;space;S&amp;space;has&amp;space;taken&amp;space;any&amp;space;position&amp;space;on})}\" alt=\"P(S takes X on I) = (1 + count of issues S has taken X on)/(5 + count of issues S has taken any position on)\" width=\"575\" height=\"45\" /></p>\n<p>Likewise the probability</p>\n<p><img src=\"http://latex.codecogs.com/png.latex?P(\\mbox{T}_{j}\\mbox{&amp;space;takes&amp;space;Y}_{j}\\mbox{&amp;space;on&amp;space;I})&amp;space;=&amp;space;\\frac{(1&amp;space;&amp;plus;\\mbox{count&amp;space;of&amp;space;issues&amp;space;T}_{j}\\mbox{&amp;space;has&amp;space;taken&amp;space;Y}_{j}\\mbox{&amp;space;on})}^{(5&amp;space;&amp;plus;\\mbox{count&amp;space;of&amp;space;issues~T}_{j}\\mbox{&amp;space;has&amp;space;taken&amp;space;any&amp;space;position&amp;space;on})}\" alt=\"P(Tj takes Yj on I) = (1 + count of issues Tj has taken Yj on)/(5 + count of issues Tj has taken any position on)\" width=\"586\" height=\"46\" /></p>\n<p>This leaves one term in Bayes' Theorem to figure out: P(T<sub>j</sub> takes Y<sub>j</sub> on I | S takes X on I)</p>\n<p>For this, I will again use the Generalized Laplace's Law of Succession, looking at issues on which both S and T<sub>j</sub> have taken positions:</p>\n<p><img src=\"http://latex.codecogs.com/png.latex?P(\\mbox{T}_{j}\\mbox{&amp;space;takes&amp;space;Y}_{j}\\mbox{&amp;space;on&amp;space;I}|\\mbox{S&amp;space;takes&amp;space;X&amp;space;on&amp;space;I})=\\frac{(1&amp;space;&amp;plus;\\mbox{count&amp;space;of&amp;space;issues&amp;space;S&amp;space;takes&amp;space;X&amp;space;on&amp;space;and&amp;space;T}_{j}\\mbox{&amp;space;takes&amp;space;Y}_{j}\\mbox{&amp;space;on})}^{(5&amp;space;&amp;plus;\\mbox{count&amp;space;of&amp;space;issues&amp;space;S&amp;space;takes&amp;space;X&amp;space;on&amp;space;and&amp;space;T}_{j}\\mbox{&amp;space;takes&amp;space;any&amp;space;position&amp;space;on})}\" alt=\"P(Tj takes Yj on I | S takes X on I) = (1 + count of issues S takes X on and Tj takes Yj on)/(5 + count of issues S takes X on and Tj takes any position on)\" width=\"652\" height=\"46\" /></p>\n<p>We now know how to compute, from the records of people's positions on issues, all the terms that Bayes' Theorem requires to compute the posterior probability that person S will take position X on issue I.</p>\n<hr />\n<p>So, how well does this work? At this time, I have coded up a SQL script to be run against TakeOnIt's database, that predicts a user's positions based on the positions of Expert's/Influencers. (TakeOnIt stores positions for these types of users differently, which is why the first version doesn't just update on the positions of all people.) I have run this script to make predictions for myself, seeing as I am in a privileged position to judge the accuracy of those predictions. Looking at the predictions it made for me with greater than 80% confidence: Of the three predictions made with more than 90% confidence, all were correct, and of the 11 made with between 80% and 90% confidence, 10 were correct, and 1 was incorrect. From this limited data, it seems the algorithm is underconfident. I have registered my opinion on 40 issues.</p>\n<p>In case you think my positions might be influenced by the positions, I have also looked at its retrodictions for positions I have already registered. It assigned 18% probability to my Neutral position on the issue <a href=\"http://www.takeonit.com/question/329.aspx#comment283\" target=\"_blank\">Are successful entrepreneurs big risk takers?</a>. My remaining 39 positions it predicted with confidence ranging from 60% to (due to round off errors on <a title=\"It's that I Disagree with &quot;Does God Exist?&quot;. Pretty awesome, huh?\" href=\"http://www.takeonit.com/question/47.aspx\">one issue</a>) 100%.</p>\n<hr />\n<p>Some areas for improvement:</p>\n<p>This algorithm does not make any use of the structure of the possible positions. For example, Disagree is more like Mostly Disagree than Agree. And there is also symmetry such that that Agree relates to Mostly Agree in the same way that Disagree relates to Mostly Disagree. If you changed a question by adding negation, so that all the answers flipped, this algorithm would not necessarily give the flipped probability distribution. Of course, it is also possible that a person's position will not reflect the structure, so we should not completely impose it on the algorithm. But it could be an improvement to measure how well a person follows this structure (and how well people in general follow the structure), and adjust the results accordingly.</p>\n<p>The algorithm has a violation of <a href=\"/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a>. When it is computing the probability that a person S will take position X on issue I, it has an expectation that person U will take position Z on issue I, which would alter its prediction for person S. But trying to extend the algorithm to recursively make predictions for U to use in its predictions for S would lead to infinite recursion.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SA5bvZJXcwEEuZFSe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 5.748157659066564e-07, "legacy": true, "legacyId": "2637", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-04T00:32:45.546Z", "modifiedAt": null, "url": null, "title": "Free copy of Feynman's autobiography for best corny rationalist joke", "slug": "free-copy-of-feynman-s-autobiography-for-best-corny", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:09.433Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GreenRoot", "createdAt": "2009-12-10T01:24:20.418Z", "isAdmin": false, "displayName": "GreenRoot"}, "userId": "CwjucLGd4b2ySZuAP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oTSYYW3R46QNh9twQ/free-copy-of-feynman-s-autobiography-for-best-corny", "pageUrlRelative": "/posts/oTSYYW3R46QNh9twQ/free-copy-of-feynman-s-autobiography-for-best-corny", "linkUrl": "https://www.lesswrong.com/posts/oTSYYW3R46QNh9twQ/free-copy-of-feynman-s-autobiography-for-best-corny", "postedAtFormatted": "Sunday, April 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Free%20copy%20of%20Feynman's%20autobiography%20for%20best%20corny%20rationalist%20joke&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFree%20copy%20of%20Feynman's%20autobiography%20for%20best%20corny%20rationalist%20joke%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoTSYYW3R46QNh9twQ%2Ffree-copy-of-feynman-s-autobiography-for-best-corny%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Free%20copy%20of%20Feynman's%20autobiography%20for%20best%20corny%20rationalist%20joke%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoTSYYW3R46QNh9twQ%2Ffree-copy-of-feynman-s-autobiography-for-best-corny", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoTSYYW3R46QNh9twQ%2Ffree-copy-of-feynman-s-autobiography-for-best-corny", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 393, "htmlBody": "<p><img style=\"float: right; margin-top: 0px; margin-bottom: 0px; margin-left: 20px; margin-right: 20px;\" src=\"http://web.archive.org/web/20100620213625/http://upload.wikimedia.org/wikipedia/commons/9/9e/Feynman.jpg\" alt=\"Portrait of Richard Feynman\" />I have an extra copy of <a href=\"http://en.wikipedia.org/wiki/Richard_Feynman\">Richard Feyman</a>'s autobiography, <a href=\"http://en.wikipedia.org/wiki/Surely_You%27re_Joking,_Mr._Feynman!\"><em>\"Surely You're Joking, Mr. Feynman!\": Aventures of a Curious Character</em></a>, which I want to give away here.</p>\n<p>This is one of two autobiographies (along with Ben Franklin's) to actually change my life.&nbsp; I've seen it quoted often on LessWrong, as Feynman has a point of view on life that fits well with the ideas we explore here.&nbsp; In addition to his rationalist side, Feynman also exhibited a wonderfully free sense of humor. Even when working at the Manhattan Project, he joked around and never took himself too seriously.&nbsp; I think our community would benefit if the rationalism here were likewise leavened by some self-deprecating humor.</p>\n<p>I will mail the autobiography, at my expense, to whomever posts the best corny rationalist joke in the comments below, as judged by karma voting.&nbsp; Anything goes.&nbsp; Here's a little inspirational prompting:</p>\n<ul>\n<li>How many rationalists does it take to screw in a lightbulb? ...</li>\n<li>Two rationalists walk into a bar. ...</li>\n<li>You might be a rationalist if ...</li>\n</ul>\n<p><strong>Edit (April 12th)</strong>: The winner of the corny rationalist joke contest is <a title=\"This joke's comment permalink\" href=\"/lw/21a/free_copy_of_feynmans_autobiography_for_best/1ujk\">this one-liner</a> by <a href=\"/user/SilasBarta/\">SilasBarta</a>, which collected 17 net up-votes:</p>\n<blockquote>\n<p>Rationalist pick-up line: \"I would never cheat on you if and only if you would never cheat on me if and only if I would never cheat on you.\"</p>\n</blockquote>\n<p>The runner-up (and my personal favorite) is this exchange by <a href=\"/user/Bo102010/\">Bo102010</a>, which collected 14 net up-votes.&nbsp;&nbsp; The <a title=\"Full comment thread for runner up\" href=\"/lw/21a/free_copy_of_feynmans_autobiography_for_best/1ujb\">full comment thread</a> for this one has an explanation and suggested refinements.</p>\n<blockquote>\n<p>A rationalist walks into a bar with two bartenders. The rationalist asks \"What's the best drink to get tonight?\"</p>\n<p>The first bartender says \"The martini.\"<br /><br />The second bartender says \"The gin and tonic.\"<br /><br />The first bartender repeats \"The martini.\"<br /><br />The second bartender repeats \"The gin and tonic.\"<br /><br />The first says again \"The martini.\"<br /><br />The second says again \"The gin and tonic.\"<br /><br />Then the first says \"The gin and tonic.\"<br /><br />The rationalist smiles and says, \"I'm glad you could come to an agreement.\"</p>\n</blockquote>\n<p>Thanks to everybody who contributed and voted on corny jokes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hNFdS3rRiYgqqD8aM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oTSYYW3R46QNh9twQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 18, "extendedScore": null, "score": 5.748283340793817e-07, "legacy": true, "legacyId": "2638", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-04T04:17:39.243Z", "modifiedAt": null, "url": null, "title": "Late Great Filter Is Not Bad News", "slug": "late-great-filter-is-not-bad-news", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:06.317Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bnv7mxzsgNjYuLcAy/late-great-filter-is-not-bad-news", "pageUrlRelative": "/posts/Bnv7mxzsgNjYuLcAy/late-great-filter-is-not-bad-news", "linkUrl": "https://www.lesswrong.com/posts/Bnv7mxzsgNjYuLcAy/late-great-filter-is-not-bad-news", "postedAtFormatted": "Sunday, April 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Late%20Great%20Filter%20Is%20Not%20Bad%20News&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALate%20Great%20Filter%20Is%20Not%20Bad%20News%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnv7mxzsgNjYuLcAy%2Flate-great-filter-is-not-bad-news%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Late%20Great%20Filter%20Is%20Not%20Bad%20News%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnv7mxzsgNjYuLcAy%2Flate-great-filter-is-not-bad-news", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnv7mxzsgNjYuLcAy%2Flate-great-filter-is-not-bad-news", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 990, "htmlBody": "<blockquote>\n<p>But I hope that our Mars probes will discover nothing. It would be good news if we find Mars to be completely sterile. Dead rocks and lifeless sands would lift my spirit.</p>\n<p>Conversely, if we discovered traces of some simple extinct life form&mdash;some bacteria, some algae&mdash;it would be bad news. If we found fossils of something more advanced, perhaps something looking like the remnants of a trilobite or even the skeleton of a small mammal, it would be very bad news. The more complex the life we found, the more depressing the news of its existence would be. Scientifically interesting, certainly, but a bad omen for the future of the human race.</p>\n</blockquote>\n<p>&mdash; Nick Bostrom, in <a href=\"http://www.nickbostrom.com/extraterrestrial.pdf\">Where Are They?  Why I hope that the search for extraterrestrial life finds nothing</a></p>\n<p>This post is a reply to Robin Hanson's recent OB post <a href=\"http://www.overcomingbias.com/2010/03/very-bad-news.html\">Very Bad News</a>, as well as Nick Bostrom's 2008 paper quoted above, and assumes familiarity with Robin's <a href=\"http://hanson.gmu.edu/greatfilter.html\">Great Filter</a> idea. (Robin's server for the Great Filter paper seems to be experiencing some kind of error. See <a href=\"http://web.archive.org/web/20051026005516/http://hanson.gmu.edu/greatfilter.html\">here</a> for a mirror.)</p>\n<p>Suppose <a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> appears and says to you:</p>\n<p style=\"padding-left: 30px;\">(Scenario 1) I'm going to apply a great filter to humanity. You get to choose whether the filter is applied one minute from now, or in five years. When the designated time arrives, I'll throw a fair coin, and wipe out humanity if it lands heads. And oh, it's not the current you that gets to decide, but the version of you 4 years and 364 days from now. I'll predict his or her decision and act accordingly.</p>\n<p>I hope it's not controversial that the current you should prefer a late filter, since (with probability .5) that gives you and everyone else five more years of life. What about the future version of you? Well, if he or she decides on the early filter, that would constitutes a time inconsistency. And for those who believe in multiverse/many-worlds theories, choosing the early filter shortens the lives of everyone in half of all universes/branches where a copy of you is making this decision, which doesn't seem like a good thing. It seems clear that, ignoring human deviations from ideal rationality, the right decision of the future you is to choose the late filter.</p>\n<p><a id=\"more\"></a>Now let's change this thought experiment a little. Omega appears and instead says:</p>\n<p style=\"padding-left: 30px;\">(Scenario 2) Here's a button. A million years ago I hid a doomsday device in the solar system and predicted whether you would press this button or not. Then I flipped a coin. If the coin came out tails, I did nothing. Otherwise, if I predicted that you would press the button, then I programmed the device to destroy Earth right after you press the button, but if I predicted that you would <em>not</em> press the button, then I programmed the device to destroy the Earth immediately (i.e., a million years ago).</p>\n<p>It seems to me that this decision problem is structurally no different from the one faced by the future you in the previous thought experiment, and the correct decision is still to choose the late filter (i.e., press the button). (I'm assuming that you don't consider the entire history of humanity up to this point to be of negative value, which seems a safe assumption, at least if the \"you\" here is Robin Hanson.)</p>\n<p>So, if given a choice between an early filter and a late filter, we should choose a late filter. But then why do Robin and Nick (and probably most others who have thought about it) consider news that imply a greater likelihood of the Great Filter being late to be bad news? It seems to me that viewing a late Great Filter to be worse news than an early Great Filter is another instance of the confusion and irrationality of SSA/SIA-style anthropic reasoning and subjective anticipation. If you anticipate anything, believing that the great filter is more likely to lie in the future means you have to anticipate a higher probability of experiencing doom.</p>\n<p>(This paragraph was inserted to clarify in response to a <a href=\"/lw/214/late_great_filter_is_not_bad_news/1ujr\">couple</a> of <a href=\"/lw/214/late_great_filter_is_not_bad_news/1uk8\">comments</a>. These two scenarios involving Omega are not meant to correspond to any actual decisions we have to make, but just to establish that A) if we had a choice, it would be rational to choose a late filter instead of an early filter, therefore it makes no sense to consider the Great Filter being late to be bad news (compared to it being early), and B) human beings, working off subjective anticipation, would tend to incorrectly choose the early filter in these scenarios, especially scenario 2, which explains why we also tend to consider the Great Filter being late to be bad news. The decision mentioned below, in the last paragraph, is not directly related to these Omega scenarios.)</p>\n<p>From an objective perspective, a universe with a late great filter simply has a somewhat greater density of life than a universe with an early great filter. <a href=\"/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/\">UDT says</a>, let's forget about SSA/SIA-style anthropic reasoning and subjective anticipation, and instead consider yourself to be acting in all of the universes that contain a copy of you (with the same preferences, memories, and sensory inputs), making the decision for all of them, and decide based on how you want the <em>multiverse</em> <em>as a whole</em> to turn out.</p>\n<p>So, according to this line of thought, we're acting in both kinds of universes: those with early filters, and those with late filters. If, as Robin Hanson suggests, we were to devote a lot of resources to projects aimed at preventing possible late filters, then we would end up improving the universes with late filters, but hurting the universes with only early filters (because the resources would otherwise have been used for something else). But since copies of us occur more frequently in universes with late filters than in universes with early filters, such a decision (which Robin arrives at via SIA) can be justified on utilitarian grounds under UDT.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"25oxqHiadqM6Hf7Gn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bnv7mxzsgNjYuLcAy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 19, "extendedScore": null, "score": 5.748726933399294e-07, "legacy": true, "legacyId": "2632", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcvyJjPQwimAeapNg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-06T17:51:49.486Z", "modifiedAt": null, "url": null, "title": "Anthropic answers to logical uncertainties?", "slug": "anthropic-answers-to-logical-uncertainties", "viewCount": null, "lastCommentedAt": "2019-09-15T19:03:58.956Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M4e2cyoS2fJPf4MbX/anthropic-answers-to-logical-uncertainties", "pageUrlRelative": "/posts/M4e2cyoS2fJPf4MbX/anthropic-answers-to-logical-uncertainties", "linkUrl": "https://www.lesswrong.com/posts/M4e2cyoS2fJPf4MbX/anthropic-answers-to-logical-uncertainties", "postedAtFormatted": "Tuesday, April 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropic%20answers%20to%20logical%20uncertainties%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropic%20answers%20to%20logical%20uncertainties%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4e2cyoS2fJPf4MbX%2Fanthropic-answers-to-logical-uncertainties%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropic%20answers%20to%20logical%20uncertainties%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4e2cyoS2fJPf4MbX%2Fanthropic-answers-to-logical-uncertainties", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4e2cyoS2fJPf4MbX%2Fanthropic-answers-to-logical-uncertainties", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 681, "htmlBody": "<p>Suppose that if the <a href=\"http://en.wikipedia.org/wiki/Riemann_hypothesis\">Riemann Hypothesis</a> were true, then some complicated but relatively well-accepted corollary involving geometric superstring theory and cosmology means that the universe would contain 10^500 times more observers. Suppose furthermore that the corollary argument ( RH ==&gt; x10^500 observers) is accepted to be true with a very high probability (say, 99.9%).</p>\n<p>A presumptuous philosopher now has a \"proof\" of the <a href=\"http://en.wikipedia.org/wiki/Riemann_hypothesis\">Riemann Hypothesis</a>. Just use the self-indication assumption: reason as if you are an observer chosen at random from the set of all possible observers (in your reference class). Since almost all possible observers arise in \"possible worlds\" where RH is true, you are almost certainly one of these.</p>\n<p>Do we believe this argument?</p>\n<p>One argument against it is that, if RH is false, then the \"possible worlds\" where it is true are not possible. They're not just not actual, they are as ridiculous as worlds where 1+1=3.</p>\n<p>Furthermore, the justification for reasoning anthropically is that the set &Omega; of observers in your reference class maximizes its combined winnings on bets if all members of &Omega; reason anthropically; otherwise, they act as a \"collective sucker\". Unless you have reason to believe you are a \"special\" member of &Omega;, you should assume that your best move is to reason as if you are a generic member of &Omega;, i.e. anthropically. When most of the members of &Omega; arise from merely non-actual possible worlds, this reasoning is defensible. When most of the members of &Omega; arise from non-actual <em>im</em>possible worlds, something seems to have gone wrong. Observers who would only exist in logically impossible worlds can't make bets, so the \"collective sucker\" arguments don't really work.</p>\n<p>If you think that the above argument in favor of RH is a little bit fishy, then you might want to ponder Katja's ingenious <a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">SIA great filter</a> argument. Most plausible explanations for a future great filter are logical facts, not empirical ones. The difficulty of surviving a transition through technological singularities, if it convergently causes non-colonization, is some logical fact, derivable by a sufficiently powerful mind. A tendency for advanced civilizations to \"realize\" that expansionism is pointless is a logical fact. I would argue that anthropic considerations should not move us on such logical facts.</p>\n<p>Therefore, if you still buy Katja's argument, and you don't endorse anthropic reasoning as a valid method of mathematical proof, you need to search for an empirical fact that causes a massive great filter just after the point in civilization that we're at.&nbsp;</p>\n<p>The supply of these is limited. Most <a href=\"http://en.wikipedia.org/wiki/Fermi_paradox#Explaining_the_paradox_theoretically\">explanations</a> of the great filter/fermi paradox postulate some convergent dynamic that occurs every time a civilization gets to a certain level of advancement; but since these are all things you could work out from first principles, e.g. by Monte Carlo simulation, they are logical facts. Some other explanations where our background facts are false survive, e.g. the Zoo Hypothesis and the Simulation Hypothesis.</p>\n<p>Let us suppose that we're not in a zoo or a simulation. It seems that the only possible empirical great filter cause that fits the bill is something that was decided at the very beginning of the universe; some contingent fact about the standard model of physics (which, according to most physicists, was some symmetry breaking process, decided at random at the beginning of the universe). Steven0461 points out that particle accelerator disasters are ruled out, as we could in principle colonize the universe using <a href=\"http://en.wikipedia.org/wiki/Project_Orion_%28nuclear_propulsion%29\">Project Orion spaceships</a> right now, without doing any more particle physics experiments. I am stumped as to just what kind of fact would fit the bill. Therefore the Simulation Hypothesis seems to be the biggest winner from Katja's SIA doomsday argument, unless anyone has a better idea.&nbsp;</p>\n<p><strong>Update</strong>: Reader bogdanb points out that there are very simple logical \"possibilities\" that would result in there being lots of observers, such as the possibility that 1+1= some suitably huge number, such as 10^^^^^^^^10. You know there is an observer, you, and that there is another observer, your friend, and therefore there are 10^^^^^^^^10 observers according to this \"logical possibility\". If you reason according to SIA, you might end up doubting elementary arithmetical truths.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M4e2cyoS2fJPf4MbX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "2648", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-06T20:03:25.800Z", "modifiedAt": null, "url": null, "title": "Lampshading", "slug": "lampshading", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.191Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/goCfoiQkniQwPryki/lampshading", "pageUrlRelative": "/posts/goCfoiQkniQwPryki/lampshading", "linkUrl": "https://www.lesswrong.com/posts/goCfoiQkniQwPryki/lampshading", "postedAtFormatted": "Tuesday, April 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lampshading&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALampshading%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgoCfoiQkniQwPryki%2Flampshading%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lampshading%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgoCfoiQkniQwPryki%2Flampshading", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgoCfoiQkniQwPryki%2Flampshading", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 587, "htmlBody": "<p><strong>Sequence index:</strong> <a href=\"/lw/1xh/living_luminously/\">Living Luminously</a><br /><strong>Previously in sequence:</strong> <a href=\"/lw/20r/city_of_lights/\">City of Lights</a><br /><br /><em>You can use luminosity to help you effectively change yourself into someone you'd more like to be.&nbsp; Accomplish this by fixing your self-tests so they get good results.</em></p>\n<p><em>You may find your understanding of this post significantly improved  if you read the seventh story from <a href=\"/lw/2aw/seven_shiny_stories\">Seven Shiny  Stories</a>.</em><br /><br />When you have coherent models of yourself, it only makes good empirical sense to put them to the test.<br /><br />Thing is, when you run a test on yourself, you know what test you're running, and what data would support which hypothesis.&nbsp; All that and you're the subject generating the data, too.&nbsp; It's kind of hard to have good scientific controls around this sort of experiment.<br /><br />Luckily, it turns out that for this purpose they're unnecessary!&nbsp; Remember, you're not just trying to determine what's going on in a static part of yourself.&nbsp; You're also <a href=\"/lw/209/highlights_and_shadows/\">evaluating</a> and changing the things you repudiate when you can.&nbsp; You don't just have the chance to let knowledge of your self-observation nudge your behavior - you can outright <em>rig your tests</em>.<a id=\"more\"></a><br /><br />Suppose that your model of yourself predicts that you will do something you don't think you should do - for instance, suppose it predicts that you will yell at your cousin the next time she drops by and tracks mud on your carpet, or something, and you think you ought not to yell.&nbsp; Well, you can falsify that model which says you'll yell by not yelling: clearly, if you do not yell at her, then you cannot be accurately described by any model that predicts that you'll yell.&nbsp; By refraining from yelling you push the nearest accurate model towards something like \"may yell if not careful to think before speaking\" or \"used to yell, but has since grown past that\".&nbsp; And if you'd rather be accurately described by one of those models than by the \"yells\" model... you can <em>not yell</em>.<br /><br />(Note, of course, that falsifying the model \"yells\" by silently picking up your cousin and defenestrating her is not an <em>improvement</em>.&nbsp; You want to replace the disliked model with a more likable one.&nbsp; If it turns out that you cannot do that - if controlling your scream means that you itch so badly to fling your cousin out a window that you're likely to actually do it - then you should postpone your model falsification until a later time.)<br /><br />Now, of course figuring out how to not yell (let us not forget akrasia, after all) will be easier once you have an understanding of what would make you do it in the first place.&nbsp; Armed with that, you can determine how to control your <a href=\"/lw/1y0/the_abcs_of_luminosity/\">circumstances</a> to prevent yelling-triggers from manifesting themselves.&nbsp; Or, you can attempt the more difficult but more stable psychic surgery that interrupts the process from circumstance to behavior.<br /><br />Sadly, I can't be as specific as would be ideal here because so much depends on the exact habits of <em>your</em> brain as opposed to any other brains, including mine.&nbsp; You may need to go through various strategies before you hit on one that works for you to change what you need to change.&nbsp; You could find that successful strategies eventually \"wear off\" and need replacing and their edifices rebuilding.&nbsp; You might find listening to what other people do helpful (post techniques below!) - or you might not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwv9eHi7KGg5KA9oM": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "goCfoiQkniQwPryki", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 21, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "2649", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "ureshiku-naritai", "canonicalPrevPostSlug": "city-of-lights", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "vfHRahpgbp9YFPuGQ", "9sguwESkteCgqFMbj", "tCTmAmAapB37dAz9Y", "rLuZ6XrGpgjk9BNpX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-06T22:44:51.369Z", "modifiedAt": null, "url": null, "title": "Single Point of Moral Failure", "slug": "single-point-of-moral-failure", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:17.072Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J7XPsy7JqR9xtnv8S/single-point-of-moral-failure", "pageUrlRelative": "/posts/J7XPsy7JqR9xtnv8S/single-point-of-moral-failure", "linkUrl": "https://www.lesswrong.com/posts/J7XPsy7JqR9xtnv8S/single-point-of-moral-failure", "postedAtFormatted": "Tuesday, April 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Single%20Point%20of%20Moral%20Failure&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingle%20Point%20of%20Moral%20Failure%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7XPsy7JqR9xtnv8S%2Fsingle-point-of-moral-failure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Single%20Point%20of%20Moral%20Failure%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7XPsy7JqR9xtnv8S%2Fsingle-point-of-moral-failure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7XPsy7JqR9xtnv8S%2Fsingle-point-of-moral-failure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 846, "htmlBody": "<p>I have been recently entertaining myself with a 3-day non-stop binge of Theist vs. Atheist debates, On the atheist side: Richard Dawkins, Christopher Hitchens, Daniel Denett, Sam Harris, P.Z. Myers. On the theist corner: Dinesh D'Souza, William Lane Craig, Alistair McGrath, Tim Keller, and (unfortunately) Nassim Nicholas Taleb. One of the interesting points that comes up, often by Hitchens, is what I call the \"Bodycount Argument\". The atheist will claim: \"Look at all the deaths caused by religion: Crusades, Inquisition, Islamic fundamentalism, Japanese militarism, Conquests of the New World\" and the list goes on and on. Then the Theist will claim: \"Well, look at the Nazis, the Fascists, the Soviets, the Khmer Rouge...\". the Atheist then tries to reverse some of that, e.g. the Fascists were the catholic right wing, the SS were mostly confessing Catholics and Hitler had churches pray for him on his birthday, and, most tenuously, that the Soviets had the support of the orthodox church and used the pre-existing structures set up by the Czar to establish their power. <br /><br />Some of that retort is convincing, some is not so much. You cannot really blame Soviet, Cambodian and Chinese massacres solely on religion. While they do at least manage to bring it to a tie, I suspect that the atheists follow this argument up suboptimally. My instinctive reaction would be \"ok, so you proved that except for religion, communism leads to mass slaughter too. I have no problem doing away with both\". But the Theists have a stronger form of their argument in which they claim that the crimes of communism are -because- of atheism, so a simple one-line retort won't work in all cases. We need to lay a deeper foundation for that claim to be convincing.<br /><br />Enter single points of failure. The rudimentary definition, usually given in terms of computer networks, is that a single point of failure is that component which takes down the entire system when it fails. While the term has originated in computer science as far as I can tell, it can be applied to human networks as well. The strategy of Alexander the Great, at the battle of Issus, was instead of trying to defeat the entire, vastly ournumbering, Persian army in combat, to attack the Persian king Darius directly. When he was able to make him flee, the entire Persian army fell into disarray, with one side executing an orderly retreat, but the left flank completely disintegrated while being pursued by Alexander's cavalry. So while the term is new, the concept has been long known and has been used to great effect.<br /><br />What I want to argue, is that all the examples cited by Theists and Atheists alike, are instances of a single point of -moral- failure. Here, instead of the system disintegrating or stopping to operate, it goes into a sequence of actions that when examined by an outside human observer, or even the participants themselves at a latter date, seem to be immoral, irrational, and akin to madness. The common point in all the examples is that a central organization, supported by a specific fanaticizing ideology, ordered the massacres to occur, and the people at the lower ranks, implemented those orders, despite perhaps individually knowing better.<br /><br />My explanation of this, is that the lower-ranks had in effect outsourced their moral sense to their leadership. As with all centralised structures, when things go well, they go -really- well (assuming aligned incentives, greedy algorithms generally will not be as optimal as top-down ones), but when they go bad, they can be disastrous. The bigger the power of the network, the bigger the consequences. It is not hard to imagine why the outsourcing happened. Humans are tribal. I think very few, having observed the weekly rituals called 'football games' (whatever your definition of football is) would disagree. But humans are also moral. We have a rough set of rules that we tend to follow relatively consistently. What is of interest in these cases, is that an individual's tribalism completely overrode that individual's personal morality. And this happened repeatedly and reliably, throughout the ranks of each of these human networks. <br /><br />Coming back to the original argument, if indeed tribalism trumps morality, and the above give us good reason to believe it does, then the theist argument that god put morality inside us comes into question. It does not explain why god saw fit to make our morality less powerful a motivator than our tribal instincts. But the biological explanation stands confirmed: If morality is a mechanism that was useful for intra-tribe interactions, then it would -have- to be suspended when the tribe was facing another. One can imagine the pacifist tribe being annihilated by the non-pacifist tribes around it or, lest I be accused of arguing for group selection, the individual pacifists being attacked both by their own tribe or the enemy tribe. Tribalists may disagree about who gets to live and who gets the resources, but they don't disagree about tribalism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J7XPsy7JqR9xtnv8S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 18, "extendedScore": null, "score": 5.756602529557661e-07, "legacy": true, "legacyId": "2650", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-07T18:48:17.624Z", "modifiedAt": "2020-10-28T22:31:04.457Z", "url": null, "title": "Pain and gain motivation", "slug": "pain-and-gain-motivation", "viewCount": null, "lastCommentedAt": "2021-07-29T02:51:43.627Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Kaj_Sotala", "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xNrdYu6p6BRamRBz8/pain-and-gain-motivation", "pageUrlRelative": "/posts/xNrdYu6p6BRamRBz8/pain-and-gain-motivation", "linkUrl": "https://www.lesswrong.com/posts/xNrdYu6p6BRamRBz8/pain-and-gain-motivation", "postedAtFormatted": "Wednesday, April 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pain%20and%20gain%20motivation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APain%20and%20gain%20motivation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNrdYu6p6BRamRBz8%2Fpain-and-gain-motivation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pain%20and%20gain%20motivation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNrdYu6p6BRamRBz8%2Fpain-and-gain-motivation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNrdYu6p6BRamRBz8%2Fpain-and-gain-motivation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1317, "htmlBody": "<p><em>Note: this post is basically just summarizing some of PJ Eby's freely available writings on the topic of pain/gain motivation and presenting them in a form that's easier for the LW crowd to digest. I claim no credit for the ideas presented here, other than the credit for summarizing them.</em></p>\n<p><em>EDIT: Note also Eby's comments and corrections to my summary at <a href=\"/lw/21r/pain_and_gain_motivation/1v06\">this comment</a>.</em></p>\n<p>Eby proposes that we have two different forms of motivation: positive (\"gain\") motivation, which drives us to <em>do things</em>, and negative (\"pain\") motivation, which drives us to <em>avoid things.</em> Negative motivation is a major source of akrasia and is mostly harmful for getting anything done. However, sufficiently large amounts of negative motivation can momentarily push us to do things, which frequently causes people to confuse the two.<br /><br />To understand the function of negative motivation, first consider the example of having climbed to a tree to avoid a predator. There's not much you can do other than wait and hope the predator goes away, and if you move around, you risk falling out of the tree. So your brain gets flooded with signals that suppress activity and tell it to keep your body still. It is only if the predator ends up climbing up the tree that the danger becomes so acute that you're instead pushed to flee.<br /><br />What does this have to do with modern-day akrasia? Back in the tribal environment, elicting the disfavor of the tribe could be a death sentence. Be cast out by the tribe, and you likely wouldn't live for long. One way to elict disfavor is to be unmasked as incompetent in some important matter, and a way to avoid such an unmasking is to simply avoid doing anything where to consequences of failure would be severe.<br /><br />You might see why this would cause problems. Sometimes, when the pain level of <em>not</em> having done a task grows too high - like just before a deadline - it'll push you to do it. But this fools people into thinking that negative consequences alone will be a motivator, so they try to psyche themselves up by thinking about how bad it would be to fail. In truth, this is only making things worse, as an increased chance of failure will increase the negative motivation that's going on.<br /><a id=\"more\"></a><br />Negative motivation is also a reason why we might discover a productivity or self-help technique, find it useful, and then after a few successful tries stop using it - seemingly for no reason. Eby uses the terms \"naturally motivated person\" and \"naturally struggling person\" to refer to people that are more driven by positive motivation and more driven by negative motivation, respectively. For naturally struggling people, the main motivation for behavior is the need to get away from bad things. If you give them a productivity or self-help technique, they might apply it to get rid of their largest problems... and then, when the biggest source of pain is gone, they momentarily don't have anything major to flee from, so they lose their motivation to apply the technique. To keep using the technique, they'd need to have positive motivation that'd make them <em>want</em> to do things instead of just <em>not wanting</em> to do things.<br /><br />In contrast to negative motivation, positive motivation is basically just doing things because you find them fun. Watching movies, playing video games, whatever. When you're in a state of positive motivation, you're trying to gain things, obtain new resources or experiences. You're entirely focused on the gain, instead of the pain. If you're playing a video game, you know that no matter how badly you lose in the game, the negative consequences are all contained in the game and don't reach to the real world. That helps your brain stay in gain mode. But if a survival override kicks in, the negative motivation will overwhelm the positive and take away much of the pleasure involved. This is a likely reason for why a hobby can stop being fun once you're doing it for a living - it stops being a simple \"gain\" activity with no negative consequences even if you fail, and instead becomes mixed with \"pain\" signals.</p>\n<blockquote>\n<p>And now, if you&rsquo;re up the tree and the tiger is down there waiting for you, does it make sense for you to start looking for a better spot to sit in&hellip; Where you&rsquo;ll get better sunshine or shade or where there&rsquo;s, oh, there&rsquo;s some fruit over there? Should you be seeking to gain in that particular moment?</p>\n<p>Hell no! Right? Because you don&rsquo;t want to take a risk of falling or getting into a spot where the tiger can jump up and get you or anything like that. Your brain wants you to sit tight, stay put, shut up, don&rsquo;t rock the boat&hellip; until the crisis is over. It wants you to sit tight. That&rsquo;s the &ldquo;pain brain&rdquo;.<br /><br />In the &ldquo;pain brain&rdquo; mode&hellip; this, by the way, is the main reason why people procrastinate, this is the fundamental reason why people put off doing things&hellip; because once your brain has one of these crisis overrides it will go, &ldquo;Okay conserve energy: don&rsquo;t do anything.&rdquo;<br /><br />-- PJ Eby, <a href=\"http://whycantichange.com/\">\"Why Can't I Change?\"</a></p>\n</blockquote>\n<p>So how come some important situations <em>don't</em> push us into a state of negative motivation, even though failure might have disastrous consequences? \"Naturally motivated\" people rarely stop to think about the bad consequences of whatever they're doing, being too focused on what they have to gain. If they meet setbacks, they'll bounce back much faster than \"naturally struggling\" people. What causes the difference?<br /><br />Part of the difference is probably inborn brain chemistry. Another major part, though, is your previous experiences. The emotional systems driving our behavior don't ultimately do very complex reasoning. Much of what they do is simply cache lookups. Does this experience resemble one that led to negative consequences in the past? Activate survival overrides! Since negative motivation will suppress positive motivation, it can be easier to end up in a negative state than a positive one. Furthermore, the experiences we have also shape our thought processes in general. If, early on in your life, you do things in \"gain\" mode that end up having traumatic consequences, you learn to avoid the \"gain\" mode in general. You become a \"naturally struggling\" person, one who will view everything through a pessimistic lens, and expect failure in every turn. You literally only perceive the bad sides in everything. A \"naturally motivated\" person, on the other hand, will primarily only perceive the good sides. (Needless to say, these are the endpoints in a spectrum, so it's not like you're either 100% struggling or 100% successful.)<br /><br />Another of Eby's theses is that negative motivation is, for the most part, impossible to overcome via willpower. Consider the function of negative motivation as a global signal that prevents us from doing things that seem too dangerous. If we could just use willpower to override the signal at any time, that would result in a lot of people being eaten by predators and being cast out of the tribe. In order to work, a drive that blocks behavior needs to actually consistently block behavior. Therefore attempts to overcome procrastination or akrasia via willpower expenditure are fundamentally misguided. We should instead be trying to remove whatever negative motivation it is that holds us back, for otherwise we are not addressing the real root of the problem. On the other hand, if we succeed in removing the negative motivation and replacing it with positive motivation, we can make <em>any</em> experience as fun and enjoyable as playing a video game. (If you haven't already, do check out Eby's <a href=\"https://www.youtube.com/watch?v=PppCBDHeytg\">Instant Irresistible Motivation video</a> for learning how to create positive motivation.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 1, "iP2X4jQNHMWHRNPne": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xNrdYu6p6BRamRBz8", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 67, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "2655", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 67, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 144, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-08T03:09:18.648Z", "modifiedAt": null, "url": null, "title": "Open Thread: April 2010, Part 2", "slug": "open-thread-april-2010-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:07.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2wqesNPHnBTCSC2kW/open-thread-april-2010-part-2", "pageUrlRelative": "/posts/2wqesNPHnBTCSC2kW/open-thread-april-2010-part-2", "linkUrl": "https://www.lesswrong.com/posts/2wqesNPHnBTCSC2kW/open-thread-april-2010-part-2", "postedAtFormatted": "Thursday, April 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20April%202010%2C%20Part%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20April%202010%2C%20Part%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wqesNPHnBTCSC2kW%2Fopen-thread-april-2010-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20April%202010%2C%20Part%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wqesNPHnBTCSC2kW%2Fopen-thread-april-2010-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wqesNPHnBTCSC2kW%2Fopen-thread-april-2010-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<div>\n<p>The <a href=\"/lw/20w/open_thread_april_2010/\">previous open thread</a> has already exceeded <a href=\"../lw/1wc/spring_2010_meta_thread/1qmk\">300 comments</a> &ndash; new <a href=\"/tag/open_thread/?sort=new\">Open Thread</a> posts should be made here.</p>\n<p><em>This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2wqesNPHnBTCSC2kW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 5.759974599824962e-07, "legacy": true, "legacyId": "2658", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 202, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hMv7JhPMN8SuSrk7m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-08T20:08:58.726Z", "modifiedAt": null, "url": null, "title": "Ureshiku Naritai", "slug": "ureshiku-naritai", "viewCount": null, "lastCommentedAt": "2022-02-20T19:25:39.330Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xnPFYBuaGhpq869mY/ureshiku-naritai", "pageUrlRelative": "/posts/xnPFYBuaGhpq869mY/ureshiku-naritai", "linkUrl": "https://www.lesswrong.com/posts/xnPFYBuaGhpq869mY/ureshiku-naritai", "postedAtFormatted": "Thursday, April 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ureshiku%20Naritai&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUreshiku%20Naritai%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnPFYBuaGhpq869mY%2Fureshiku-naritai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ureshiku%20Naritai%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnPFYBuaGhpq869mY%2Fureshiku-naritai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnPFYBuaGhpq869mY%2Fureshiku-naritai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1907, "htmlBody": "<p>This is a supplement to the <a href=\"/lw/1xh/living_luminously/\">luminosity sequence</a>.&nbsp; In <a href=\"/lw/1za/the_spotlight/1t8q\">this comment</a>, I mentioned that I have raised my happiness set point (among other things), and this declaration was met with some interest.&nbsp; Some of the details are lost to memory, but below, I reconstruct for your analysis what I can of the process.&nbsp; It contains <em>lots of gooey self-disclosure</em>; skip if that's not your thing.</p>\n<p>In summary: I decided that I had to and wanted to become happier; I re-labeled my moods and approached their management accordingly; and I consistently treated my mood maintenance and its support behaviors (including discovering new techniques) as immensely important.&nbsp; The steps in more detail:</p>\n<p>1.&nbsp;<em> I came to understand the <strong>necessity</strong> of becoming happier.</em>&nbsp; Being unhappy was not just unpleasant.&nbsp; It was <em>dangerous</em>: I had a history of suicidal ideation.&nbsp; This hadn't resulted in actual attempts at killing myself, largely because I attached hopes for improvement to concrete external milestones (various academic progressions) and therefore imagined myself a magical healing when I got the next diploma (the next one, the next one.)&nbsp; Once I noticed I was doing that, it was unsustainable.&nbsp; If I wanted to live, I had to find a safe emotional place on which to stand.&nbsp; It had to be my top priority.&nbsp; This required several sub-projects:<a id=\"more\"></a></p>\n<ul>\n<li>I had to eliminate the baggage that told me it was <em>appropriate</em> or <em>accurate</em> to feel bad most of the time.&nbsp; I endorse my ability to react emotionally to my environment; but this should be acute, not chronic.&nbsp; Reacting emotionally is about feeling worse when things <em>get</em> worse, not feeling bad when things are bad for months or years on end.&nbsp; (Especially not when feeling bad reduces the ability to make things less bad.)&nbsp; Further, having a lower set point did not affect my emotional <em>range</em> except to shrink it; it reduced the possible impact of real grief, and wasn't compatible with the \"react emotionally\" plan.&nbsp; The low set point also compromised my ability to react emotionally to <em>positive</em> input, because it was attached to a systematic discounting of such positivity.</li>\n<li>I had to eliminate the baggage that told me it was not possible to <em>cognitively</em> change my mood.&nbsp; Moods correspond to thoughts, and while it can be hard to <em>avoid</em> thinking about things, I can decide <em>to</em> think about whatever I want.&nbsp; A decade of assorted antidepressants had wreaked no discernible change on my affect, which constituted strong evidence that chemicals were not my problem.&nbsp; And it was easy to see that my mood varied on a small scale with things under my complete or partial control, like sleep, diet, and activity.&nbsp; It did not seem outrageous that long-term, large-scale interventions could have similar effects on my overall mood.</li>\n<li>I had to decide, and act on the decision, that my happiness was important and worth my time and attention.&nbsp; I had to pay attention, and note what helped and what hurt.&nbsp; I had to put increasing the helping factors and decreasing the hurting factors at the top of my list whenever it was remotely feasible, and relax my standards around \"remote feasibility\" to prevent self-sabotage.&nbsp; And I had to commit to abandoning counterproductive projects or interactions, at least until I'd developed the stability to deal with the emotions they generated without suffering permanent setbacks.</li>\n</ul>\n<p>2.&nbsp; <em>I <strong>re-labeled</strong> my moods, so that identifying them in the moment prompted the right actions.</em>&nbsp; When a given point on the unhappy-happy spectrum - let's call it \"2\" on a scale of 1 to 10 - was labeled \"normal\" or \"set point\", then when I was feeling \"2\", I didn't assume that meant anything; that was the default state.&nbsp; That left me feeling \"2\" a lot of the time, and when things went wrong, I dipped lower, and I waited for things outside of myself to go right before I went higher.&nbsp; The problem was that \"2\" was not a good place to be spending most of my time.</p>\n<ul>\n<li>I had to label the old set-point as subnormal, a <em>problem state</em> that generated a need for immediate action from me to <em>fix</em> it.&nbsp; It was like telling myself that, unbeknownst to me, my left foot was in constant pain and needed medicine at once: kind of hard to swallow, given that my left foot always felt pretty much the same unless I'd just stubbed a toe or received a massage.&nbsp; But eventually, I attached <em>urgency</em> to the old set point.&nbsp; It was not <em>just</em> how things were normally; it was a sign that something was <em>wrong.</em></li>\n<li>I had to make sure that I had many accessible, cheap excuses to cheer up, so I didn't ever fall into the trap of \"just this once\" leaving myself at a \"2\" state instead of acting.&nbsp; I designated a favorite pair of socks and wore them whenever I woke up on the wrong side of the bed; I took up the habit of saving every picture of a cute animal I found on the Internet so I could leaf through the collection whenever I wanted; I threw myself into developing the skill of making friends <em>on purpose</em> so I'd have lots and if I happened to log onto my IM client, someone would be there who would talk to me; I became very acquisitive of inexpensive goods like music and interesting websites.&nbsp; When one of these interventions failed to work, I forced myself to try something else, rather than falling into the self-talk disaster of \"well, that didn't help; I guess something must really be wrong and I should feel like this until it goes away by itself.\"&nbsp; I also harnessed my tendency to feel better after a night's sleep - if I felt suboptimal close to bedtime, I'd turn in early and reasonably expect to wake up improved.</li>\n<li>I stopped tolerating the minor injuries to my affect that I identified as most consistent and, therefore, most likely to contribute to my poor set point.&nbsp; For instance, I noticed that I always slept better when I didn't go to bed expecting to awaken to the sound of an alarm, so I aggressively rearranged my schedule to give me morning leeway, and found alarm software that would wake me more gently when an early start was absolutely necessary.&nbsp; I identified people with whom interaction was frustrating and draining, and I limited interaction with them both by reducing opportunities to start, and by dropping my standards for abandoning the exchange midway through so I could leave before things got very bad.&nbsp; I practiced, in general, \"writing things off\" and rehearsed internal monologues about how I no longer needed to worry about [thing X].&nbsp; (\"I cannot control the speed of the bus.&nbsp; I caught it, and it will get there when it gets there.&nbsp; There is no point in further fretting about being late until I'm moving under my own power again - so I'll stop.&nbsp; To manage my strong, intrusive desire to be on time, I will start thinking about how to choose an efficient path to walk once I get off the bus.\")</li>\n<li>I labeled my new desired set point - a safe spot on the spectrum, call it \"5\", which was ambitious yet felt attainable - as \"normal\".&nbsp; When asked how I was in this state, I consciously chose to say that I was \"fine\" or \"okay\" instead of something more enthusiastic, like \"great\", that I might have said before - the energy I felt at \"5\" was no longer to be considered <em>extra</em>.&nbsp; Similarly, these were not suitable occasions to do displeasing things.&nbsp; I didn't have happiness to burn at \"5\" - I waited until I was even better before I relaxed my emotional avarice.&nbsp; Instead, \"5\" was a good place from which to undertake more expensive entertainments that offered net improvement.&nbsp; (More difficult than choosing a specific pair of socks to wear is starting a D&amp;D game, or walking around and exploring a new location, or working on a piece of artwork or fiction; the lag time and effort makes them poor \"cheer up\" activities, but excellent ways to get from \"5\" to \"6\" or \"7\".)</li>\n<li>I made a point of noting non-sadness deficiencies in my status like boredom, hunger, tiredness, or annoyance.&nbsp; These weren't directly related to the set point I was trying to affect, but they could exacerbate a bad influence or limit the power of a good one.&nbsp; Additionally, at the level of luminosity I then had to work with, they could also mask moods that were actually sadness, in much the same way that sometimes one can feel hungry when in fact just thirsty.</li>\n</ul>\n<p>3.&nbsp; <em>I treated my own mood as <strong>manageable</strong>.&nbsp; </em>Thinking of it as a thing that attacked me with no rhyme or reason - treating a bout of depression like a cold - didn't just cost me the opportunity to fight it, but also made the entire situation seem more out-of-control and hopeless.&nbsp; I was wary of learned helplessness; I decided that it would be best to interpret my historically static set point as an indication that I hadn't hit on the right techniques yet, not as an indication that it was inviolable and everlasting.&nbsp; Additionally, the fact that I didn't know how to fix it yet meant that if it was going to be my top priority, I had to treat the value of information as very high; it was worth experimenting, and I didn't have to wait for surety before I gave something a shot.</p>\n<ul>\n<li>Even if I determined that my mood reacted to my environment in some way, that only removed my power over it one step: I could control my environment to a considerable degree, and with a strong enough reason to do so, I committed to enacting that power.&nbsp; (This sometimes has had unexpected and dramatic consequences.&nbsp; For example, once I determined that grad school was no longer compatible with my happiness, I dropped out as soon as I had something promising to switch to - mid semester - and moved across the country.&nbsp; To excellent effect, I might add.)</li>\n<li>Even if I have a lot on my plate, being happier will help me do it.&nbsp; It's like sleep: it's easy to keep staying up and staying up, because sleep just seems so <em>unproductive</em>, and you can get <em>some</em> work done however tired you are.&nbsp; But over the long term, getting to sleep at a sane hour every day will let you accomplish more; and so with maintaining a good affect consistently.&nbsp; Mood maintenance is typically not the most <em>immediately productive</em> thing I could be doing, but treating it as my top priority save in dire emergency has let me be more effective than I was before.</li>\n<li>I had to be willing to expend resources on my project.&nbsp; This involved working around some neuroses, like my unwillingness to spend money, and overcoming some background reluctance to try new things.&nbsp; Also, I had to allow myself to be somewhat subject to my whims.&nbsp; I still don't know what makes the mood to, say, do artwork strike me, but when it strikes, I have to do art or lose the inclination.&nbsp; Efficacious inclinations to do fun things are precious to me, and so whenever possible, I don't restrain them - even though this costs time and occludes other activities.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 4, "fkABsGCJZ6y9qConW": 4, "XqykXFKL9t38pbSEm": 4, "xexCWMyds6QLWognu": 2, "Jzm2mYuuDBCNWq8hi": 3, "y93YW7Kb6J8D5PKng": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xnPFYBuaGhpq869mY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 167, "baseScore": 195, "extendedScore": null, "score": 0.000323, "legacy": true, "legacyId": "2613", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "a-suite-of-pragmatic-considerations-in-favor-of-niceness", "canonicalPrevPostSlug": "lampshading", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 195, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 157, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-08T20:34:44.866Z", "modifiedAt": null, "url": null, "title": "Frequentist Magic vs. Bayesian Magic", "slug": "frequentist-magic-vs-bayesian-magic", "viewCount": null, "lastCommentedAt": "2015-09-17T13:19:43.620Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LkdL2BuGdEAZYysXp/frequentist-magic-vs-bayesian-magic", "pageUrlRelative": "/posts/LkdL2BuGdEAZYysXp/frequentist-magic-vs-bayesian-magic", "linkUrl": "https://www.lesswrong.com/posts/LkdL2BuGdEAZYysXp/frequentist-magic-vs-bayesian-magic", "postedAtFormatted": "Thursday, April 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Frequentist%20Magic%20vs.%20Bayesian%20Magic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFrequentist%20Magic%20vs.%20Bayesian%20Magic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkdL2BuGdEAZYysXp%2Ffrequentist-magic-vs-bayesian-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Frequentist%20Magic%20vs.%20Bayesian%20Magic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkdL2BuGdEAZYysXp%2Ffrequentist-magic-vs-bayesian-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkdL2BuGdEAZYysXp%2Ffrequentist-magic-vs-bayesian-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1004, "htmlBody": "<p>[I posted this to <a href=\"/lw/20w/open_thread_april_2010/1usu\">open thread</a> a few days ago for review. I've only made some minor editorial changes since then, so no need to read it again if you've already read the draft.]</p>\n<p>This is a belated reply to cousin_it's 2009 post <a href=\"/lw/147/bayesian_flame/\">Bayesian Flame</a>, which claimed that frequentists can give calibrated estimates for unknown parameters without using priors:</p>\n<blockquote>\n<p>And here's an ultra-short example of what frequentists can do: estimate 100 independent unknown parameters from 100 different sample data sets and have 90 of the estimates turn out to be <em>true to fact</em> afterward. Like, fo'real. Always 90% in the long run, truly, irrevocably and forever.</p>\n</blockquote>\n<p>And indeed they can. Here's the simplest example that I can think of that illustrates the spirit of frequentism:</p>\n<p style=\"padding-left: 30px;\">Suppose there is a machine that produces biased coins. You don't know how the machine works, except that each coin it produces is either biased towards heads (in which case each toss of the coin will land heads with probability .9 and tails with probability .1) or towards tails (in which case each toss of the coin will land tails with probability .9 and heads with probability .1). For each coin, you get to observe one toss, and then have to state whether you think it's biased towards heads or tails, and what is the probability that's the right answer.</p>\n<p>Let's say that you decide to follow this rule: after observing heads, always answer \"the coin is biased towards heads with probability .9\" and after observing tails, always answer \"the coin is biased towards tails with probability .9\". Do this for a while, and it will turn out that 90% of the time you are right about which way the coin is biased, no matter how the machine actually works. The machine might always produce coins biased towards heads, or always towards tails, or decide based on the digits of pi, and it wouldn't matter&mdash;you'll still be right 90% of the time. (To verify this, notice that in the long run you will answer \"heads\" for 90% of the coins actually biased towards heads, and \"tails\" for 90% of the coins actually biased towards tails.) No priors needed! Magic!<a id=\"more\"></a></p>\n<p>What is going on here? There are a couple of things we could say. One was mentioned by Eliezer in a <a href=\"/lw/147/bayesian_flame/zen\">comment</a>:</p>\n<blockquote>\n<p>It's not perfectly reliable. They assume they have perfect information about experimental setups and likelihood ratios. (Where does this perfect knowledge come from? Can Bayesians get their priors from the same source?)</p>\n</blockquote>\n<p>In this example, the \"perfect information about experimental setups and likelihood ratios\" is the information that a biased coin will land the way it's biased with probability .9. I think this is a valid criticism, but it's not complete. There are perhaps many situations where we have much better information about experimental setups and likelihood ratios than about the mechanism that determines the unknown parameter we're trying to estimate. This criticism leaves open the question of whether it would make sense to give up Bayesianism for frequentism in those situations.</p>\n<p>The other thing we could say is that while the frequentist in this example appears to be perfectly calibrated, he or she is liable to pay a heavy cost for this in accuracy. For example, suppose the machine is <em>actually</em> set up to always produce head-biased coins. After observing the coin tosses for a while, a typical intelligent person, just applying common sense, would notice that 90% of the tosses come up heads, and infer that perhaps all the coins are biased towards heads. They would become more certain of this with time, and adjust their answers accordingly. But the frequentist would not (or isn't supposed to) notice this. He or she would answer \"the coin is head-biased with probability .9\" 90% of the time, and \"the coin is tail-biased with probability .9\" 10% of the time, and keep doing this, irrevocably and forever.</p>\n<p>The frequentist magic turns out to be weaker than it first appeared. What about the Bayesian solution to this problem? Well, we know that it must involve a prior, so the only question is which one. The <a href=\"http://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution\">maximum entropy prior</a> that is consistent with the information given in the problem statement is to assign each coin an independent probability of .5 of being head-biased, and .5 of being tail-biased. It turns out that a Bayesian using this prior will give the exact same answers as the frequentist, so this is also an example of a \"matching prior\". (To verify: P(biased heads | observed heads) = P(OH|BH)*P(BH)/P(OH) = .9*.5/.5 = .9)</p>\n<p>But a Bayesian can do much better. A Bayesian can use a <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">universal prior</a>. (With a universal prior based on a universal Turing machine, the prior probability that the first 4 coins will be biased \"heads, heads, tails, tails\" is the probability that the UTM will produce 1100 as the first 4 bits of its output, when given a uniformly random input tape.) Using such a prior guarantees that no matter how the coin-producing machine works, as long as it doesn't involve some kind of uncomputable physics, in the long run your expected total Bayes score will be no worse than someone who knows exactly how the machine works, except by a constant (that's determined by the algorithmic complexity of the machine). And unless the machine actually settles into deciding the bias of each coin independently with 50/50 probabilities, your expected Bayes score will also be better than the frequentist (or a Bayesian using the matching prior) by an unbounded margin as time goes to infinity.</p>\n<p>I consider this magic also, because I don't <em>really</em> understand why it works. Is our prior actually a universal prior, or is the universal prior just a handy approximation that we can substitute in place of the real prior? Why <em>does</em> the universe that we live in look like a giant computer? <a href=\"http://groups.google.com/group/one-logic/browse_thread/thread/b499a90ef9e5fd84\">What about uncomputable physics?</a> Just <a href=\"/lw/1iy/what_are_probabilities_anyway/\">what are priors, anyway?</a> These are some of the questions that I'm still confused about.</p>\n<p>But as long as we're choosing between different magics, why not pick the stronger one?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LkdL2BuGdEAZYysXp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 58, "extendedScore": null, "score": 9.7e-05, "legacy": true, "legacyId": "2640", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WhWTwQJaiEFxvXB96", "J7Gkz8aDxxSEQKXTN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-09T23:53:29.675Z", "modifiedAt": null, "url": null, "title": "Boston area meetup April 18", "slug": "boston-area-meetup-april-18", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:58.822Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8wHa4DLW4pADHMPwJ/boston-area-meetup-april-18", "pageUrlRelative": "/posts/8wHa4DLW4pADHMPwJ/boston-area-meetup-april-18", "linkUrl": "https://www.lesswrong.com/posts/8wHa4DLW4pADHMPwJ/boston-area-meetup-april-18", "postedAtFormatted": "Friday, April 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boston%20area%20meetup%20April%2018&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoston%20area%20meetup%20April%2018%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wHa4DLW4pADHMPwJ%2Fboston-area-meetup-april-18%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boston%20area%20meetup%20April%2018%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wHa4DLW4pADHMPwJ%2Fboston-area-meetup-april-18", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wHa4DLW4pADHMPwJ%2Fboston-area-meetup-april-18", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>I propose a Less Wrong meetup on Sunday, April 18, 4pm at the Clear Conscience Cafe at <a href=\"http://maps.google.com/maps?q=Clear+Conscience+Cafe+Cambridge+MA&amp;oe=utf-8&amp;client=firefox-a&amp;ie=UTF8&amp;hl=en&amp;hq=Clear+Conscience+Cafe&amp;hnear=Cambridge,+MA&amp;ll=42.365056,-71.102411&amp;spn=0.003789,0.006539&amp;t=h&amp;z=18&amp;iwloc=A\">581 Massachusetts Avenue</a> Cambridge, MA, near the Central Square T station. Please comment if you plan to attend, or have questions or ideas. Time and place are flexible if anyone has a conflict.</p>\n<p>There were two previous Less Wrong meetups in the Boston area, back in October and November, when there were visiting rationalists in the area, but I don't think we need that pretext; there are enough of us locals for a decent turnout. There seemed to be a consensus that we ought to have regular meetups, but no one declared a date and time. So, to prevent that from happening again: <em>The third Sunday of every month, at the same time and location as the previous month unless otherwise specified</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8wHa4DLW4pADHMPwJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 5.765292042751938e-07, "legacy": true, "legacyId": "2674", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-10T01:24:27.787Z", "modifiedAt": null, "url": null, "title": "Swimming in Reasons", "slug": "swimming-in-reasons", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:53.983Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Px662ScmbiG6BhF5R/swimming-in-reasons", "pageUrlRelative": "/posts/Px662ScmbiG6BhF5R/swimming-in-reasons", "linkUrl": "https://www.lesswrong.com/posts/Px662ScmbiG6BhF5R/swimming-in-reasons", "postedAtFormatted": "Saturday, April 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Swimming%20in%20Reasons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASwimming%20in%20Reasons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPx662ScmbiG6BhF5R%2Fswimming-in-reasons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Swimming%20in%20Reasons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPx662ScmbiG6BhF5R%2Fswimming-in-reasons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPx662ScmbiG6BhF5R%2Fswimming-in-reasons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1053, "htmlBody": "<p>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nTo a rationalist, certain phrases smell bad. Rotten. A bit fishy. It's not that they're actively dangerous, or that they don't occur when all is well; but they're relatively prone to emerging from certain kinds of thought processes that have gone bad.</p>\n<p>One such phrase is <em>for many reasons</em>. For example, <em>many reasons</em>&nbsp;all saying you should eat some food, or vote for some candidate.</p>\n<p>To see why, let's first recapitulate how rational updating works. Beliefs (in the sense of probabilities for propositions) ought to bob around in the stream of evidence as a random walk without trend. When, in contrast, you can see a belief try to <em>swim somewhere</em>, right under your nose, that's fishy. (Rotten fish don't really swim, so here the analogy breaks down. Sorry.) As a Less Wrong reader, you're smarter than a fish. If the fish is going where it's going in order to flee some past error, you can jump ahead of it. If the fish is itself in error, you can refuse to follow. The mathematical formulation of these claims is clearer than the ichthyological formulation, and can be found under <a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a>.</p>\n<p>More generally, according to the <a href=\"http://en.wikipedia.org/wiki/Law_of_total_expectation\">law of iterated expectations</a>, it's not just your probabilities that should be free of trends, but your expectation of any variable. Conservation of expected evidence is just the special case where a variable can be 1 (if some proposition is true) or 0 (if it's false); the expectation of such a variable is just the\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nprobability that the proposition is true.</p>\n<p>So let's look at the case where the variable you're estimating is an action's utility. We'll define a <em>reason</em>&nbsp;to take the action as any info that raises your expectation, and the <em>strength</em> of the\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\"><!--\n <script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" mce_src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\">&lt;/mce:script&gt; type=\"text/javascript\"&gt;\n// --></script>\nreason as the amount by which it does so. The strength of the next reason, conditional on all previous reasons, should be distributed with expectation zero.</p>\n<p>Maybe the distribution of reasons is <em>symmetrical</em>: for example, if somehow you know all reasons are equally strong in absolute value, reasons for and against must be equally common, or they'd cause a predictable trend. Under this assumption, the number of reasons in favor will follow a <a href=\"http://en.wikipedia.org/wiki/Binomial_distribution\">binomial distribution</a> with p=.5. Mostly, the values here will not be too extreme, especially for large numbers of reasons. When there are ten reasons in favor, there are usually at least a few against.&nbsp;</p>\n<p>But what if that doesn't happen? What if ten pieces of info in a row all favor the action you're considering?&nbsp;</p>\n<p><a id=\"more\"></a>One possibility is you witnessed a one in a thousand coincidence. But let's not dwell on that. Nobody cares about your antics in such a tiny slice of possible worlds.</p>\n<p>Another possibility is the process generating new reasons conditional on old reasons, while unbiased, is <em>not</em> in fact symmetrical: it's <em>skewed</em>. That is to say, it will mostly give a weak reason in one direction, and in rare cases give a strong reason in the other direction.</p>\n<p>This happens naturally when you're considering many reasons for a <em>belief</em>, or when there's some fact relevant to an action that you're already pretty sure about, but that you're continuing to investigate. Further evidence will usually bump a high-probability belief up toward 1, because the belief is probably true; but when it's bumped down it's bumped far down. The fact that the sun rose on June 3rd 1978 and the fact that the sun rose on February 16th 1860 are both evidence that the sun will rise in the future. Each of the many pieces of evidence like this, taken individually, argues weakly against using Aztec-style human sacrifice to prevent dawn fail. (If the sun ever failed to rise, that would be a much stronger reason the other way, so you're iterated-expectations-OK.) If your \"many reasons\" are of this kind, you can stop worrying.</p>\n<p>Or maybe there's one common factor that <em>causes</em>&nbsp;many weak reasons. Maybe you have a hundred legitimate reasons for not hiring someone as a PR person, including that he smashes furniture, howls at the moon, and strangles kittens, all of which make a bad impression. If so, you can legitimately summarize your reason not to hire him as, \"because he's nuts\". Upon realizing this, you can again stop worrying (at least about your <em>own</em> sanity).</p>\n<p>Note that in the previous two cases, if you fail to fully take into account all the implications&nbsp;<span style=\"&amp;&lt;mce:script type=\">&mdash;</span>&nbsp;for example, that a person insane in one way may be insane in other ways&nbsp;<span style=\"font-family: arial, sans-serif;\">&mdash;</span>&nbsp;then it may even seem like there are many reasons in one direction and none of them are weak.</p>\n<p>The last possibility is the scariest one: you may be one of the fish people. You may be selectively looking for reasons in a particular direction, so you'll end up in the same place no matter what. Maybe there's some sort of confirmation bias or halo effect going on.&nbsp;</p>\n<p>So in sum, when your brain speaks of \"many reasons\" almost all going the same way, grab, shake, and strangle it. It may\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\njust barf up a better, more compressed way of seeing the world, or confess to ulterior motives.</p>\n<p>(Thanks to Steve Rayhawk, Beth Larsen, and Justin Shovelain for comments.)</p>\n<p>(Clarification in response to comments:&nbsp;I agree that skewed distributions are the typical case when you're counting pieces of evidence for a <em>belief</em>; the case with the rising sun was meant to cover that, but the post should have been clearer about this point. The symmetrical distribution assumption was meant to apply more to, say, many different good features of a car, or many different good consequences of a policy, where the skew doesn't naturally occur. Note here the difference between the <em>strength of a reason to do something</em> in the sense of how much it bumps up the expected utility, and<em>&nbsp;the increase in probability the reason causes for the proposition that it's best to do that thing</em>, which gets weaker and weaker the more your estimate of the utility is already higher than the alternatives. I said \"confirmation bias or halo effect\", but halo effect (preferentially seeing good features of something you already like) is more to the point here than confirmation bias (preferentially seeing evidence for a proposition you already believe), though many reasons in the same direction can point to the latter also. I've tried to incorporate some of this in the post text.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Px662ScmbiG6BhF5R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 20, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "2673", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-10T12:09:34.649Z", "modifiedAt": null, "url": null, "title": "The Last Number", "slug": "the-last-number", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.340Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DdGAWyyfvT6p3sCie/the-last-number", "pageUrlRelative": "/posts/DdGAWyyfvT6p3sCie/the-last-number", "linkUrl": "https://www.lesswrong.com/posts/DdGAWyyfvT6p3sCie/the-last-number", "postedAtFormatted": "Saturday, April 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Last%20Number&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Last%20Number%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdGAWyyfvT6p3sCie%2Fthe-last-number%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Last%20Number%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdGAWyyfvT6p3sCie%2Fthe-last-number", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdGAWyyfvT6p3sCie%2Fthe-last-number", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 576, "htmlBody": "<!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin;} --> <!--[endif]-->\n<p style=\"padding-left: 30px;\">\"...<span id=\"true-random-integer-generator-result\">90116</span><span id=\"true-random-integer-generator-result\">633393</span><span id=\"true-random-integer-generator-result\">348054</span><span id=\"true-random-integer-generator-result\">920083...\"</span></p>\n<p>He paused for a moment, and licked his recently reconstructed lips. He was nearly there. After seventeen thousand subjective years of effort, he was, finally, just seconds away from the end. He slowed down as the finish line drew into sight, savouring and lengthening the moment where he stood there, just on the edge of enlightenment.</p>\n<p style=\"padding-left: 30px;\">\"...<span id=\"true-random-integer-generator-result\">4...7...7...0...9...3...\"</span></p>\n<p>Those years had been long; longer, perhaps, in the effects they had upon him, than they could ever be in any objective or subjective reality. He had been human; he had been frozen, uploaded, simulated, gifted with robotic bodies inside three different levels of realities, been a conscript god, been split into seven pieces (six of which were subsequently reunited). He had been briefly a battle droid for the army of Orion, and had chanted his numbers even as he sent C-beams to glitter in the dark to scorch Formic worlds.</p>\n<p>He had started his quest at the foot of a true Enlightened One, who had guided him and countless other disciples on the first step of the path. Quasi-enlightened ones had guided him further, as the other disciples fell to the wayside all around him, unable to keep their purpose focused. And now, he was on the edge of total Enlightenment. Apparently, there were some who went this far, and deliberately chose not to take the last step. But these were always friends of a friend of an acquaintance of a rumour. He hadn't believed they existed. And now that he had come this far, he <em>knew </em>these folk didn't exist. No-one could come this far, this long, and not finish it.<a id=\"more\"></a></p>\n<p style=\"padding-left: 30px;\">\"...2\"</p>\n<p>There, he had done it. He had fully pronounced, defined and made his own, the last and greatest of all integers. The Last Number was far too large for standard decimal notation, of course; the first thousand years of effort, while there were still many other disciples around, filling the air with their cries and their joys, had been dedicated entirely to learning the mathematical notions and notations that were needed to correctly define it. But it seemed that for the last ten trillion digits of the Last Number, there was no shorter way of stating them than by listing them all. Entire books had been written about this fact, all untrue or uninteresting (but never both).</p>\n<p>He willed a pair of lungs into existence, took a deep shuddering breath, and went on:</p>\n<p style=\"padding-left: 30px;\">\"... + 1 ...\"</p>\n<p>Had he been foolish enough to just <em>list </em>the Last Number, then he would have had to spend another seventeen thousand years calculating that sum - or most likely, given up, and contented himself with being semi-enlightened, one who has seen the Last Number, but not the Final Sum. However, he had been building up the mathematics of this addition as he went along, setting up way-stations with caches of buried theorems and lemmas, and carrying the propositions on his back. It would take but a moment to do the Final Sum.</p>\n<p style=\"padding-left: 30px;\">\"... = 4.2\"</p>\n<p>It was finished. G&ouml;del had been more correct than that old Austrian mathematician could ever have imagined. Two integers, summed according to all the laws of arithmetic, and their sum was not an integer. Arithmetic was inconsistent.</p>\n<p>And so, content, he went out into the world as an Enlightened One, an object of admiration and pity, a source of wisdom and terror. One whose mind has fully seen the inconsistency of arithmetic, and hence the failure of all logic and of all human endeavours.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DdGAWyyfvT6p3sCie", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 3, "extendedScore": null, "score": 5.766749823138252e-07, "legacy": true, "legacyId": "2683", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-10T12:54:01.601Z", "modifiedAt": null, "url": null, "title": "Meetup after Humanity+ , London, Saturday 2010-04-24?", "slug": "meetup-after-humanity-london-saturday-2010-04-24", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:57.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xCpKK5rmorSbtGPpq/meetup-after-humanity-london-saturday-2010-04-24", "pageUrlRelative": "/posts/xCpKK5rmorSbtGPpq/meetup-after-humanity-london-saturday-2010-04-24", "linkUrl": "https://www.lesswrong.com/posts/xCpKK5rmorSbtGPpq/meetup-after-humanity-london-saturday-2010-04-24", "postedAtFormatted": "Saturday, April 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20after%20Humanity%2B%20%2C%20London%2C%20Saturday%202010-04-24%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20after%20Humanity%2B%20%2C%20London%2C%20Saturday%202010-04-24%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCpKK5rmorSbtGPpq%2Fmeetup-after-humanity-london-saturday-2010-04-24%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20after%20Humanity%2B%20%2C%20London%2C%20Saturday%202010-04-24%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCpKK5rmorSbtGPpq%2Fmeetup-after-humanity-london-saturday-2010-04-24", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCpKK5rmorSbtGPpq%2Fmeetup-after-humanity-london-saturday-2010-04-24", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p><a href=\"http://humanityplus-uk.com/wordpress/\">Humanity+ UK 2010</a> is in central London (near Holborn) in a fortnight. Speakers include Anders Sandberg, Aubrey de Grey, and Nick Bostrom.&nbsp; Anyone else from Less Wrong going along?&nbsp; If so, shall we meet for a drink afterwards, perhaps in <a href=\"http://www.beerintheevening.com/pubs/s/19/194/Princess_Louise/Holborn\">the Princess Louise</a> around 17:20ish?</p>\n<p>As always, if I know you here mail me on paul at ciphergoth and I'll give you my mobile number - thanks!</p>\n<p>I'm also planning another London Less Wrong meetup on Sunday 2010-06-06 - details to come, suggestions for venue welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xCpKK5rmorSbtGPpq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 5.766839928292156e-07, "legacy": true, "legacyId": "2684", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-10T16:08:00.347Z", "modifiedAt": null, "url": null, "title": "Singularity Call For Papers", "slug": "singularity-call-for-papers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:55.558Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w3FcDHSAHRhrAxzZj/singularity-call-for-papers", "pageUrlRelative": "/posts/w3FcDHSAHRhrAxzZj/singularity-call-for-papers", "linkUrl": "https://www.lesswrong.com/posts/w3FcDHSAHRhrAxzZj/singularity-call-for-papers", "postedAtFormatted": "Saturday, April 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Call%20For%20Papers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Call%20For%20Papers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3FcDHSAHRhrAxzZj%2Fsingularity-call-for-papers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Call%20For%20Papers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3FcDHSAHRhrAxzZj%2Fsingularity-call-for-papers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3FcDHSAHRhrAxzZj%2Fsingularity-call-for-papers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 455, "htmlBody": "<p><span style=\"font-size: 14px; line-height: 17px;\">Amnon Eden has sent out this&nbsp;<a style=\"color: #b3a99a; text-decoration: none; \" href=\"http://www.cvl-a.de/ecap10/cfp.html\">call for papers</a>&nbsp;on technological singularity, which many Less Wrongers may be interested in. I presented at last year's conference, which was a good experience with many interesting people. Submitting good papers can help to legitimate and cultivate the field and thus reduce existential risk (although of course poor work could have the reverse effect). If you have an idea or a draft that you're not sure about, and would like to discuss it before submitting, I'd be happy to help if you contact me (carl DOT shulman AT gmail).</span></p>\n<p><span style=\"font-size: 14px; line-height: 17px;\">I am also told that the Singularity Institute may be able to provide travel funding for selected papers. Email annasalamon@intelligence.org for more information.&nbsp;</span></p>\n<p><span style=\"font-size: 14px; line-height: 17px;\"><a id=\"more\"></a><br /></span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \">Track in:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \"><a style=\"color: #b3a99a; text-decoration: none; \" href=\"http://www.cvl-a.de/ecap10/\">8th European conference on Computing And Philosophy &mdash; ECAP 2010</a><br />Technische Universit&auml;t M&uuml;nchen<br />4&ndash;6 October 2010</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \">Important dates:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \">* Submission (extended abstracts): 7 May 2010<br />* ECAP Conference: 4&ndash;6 October 2010</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \"><a style=\"color: #b3a99a; text-decoration: none; \" href=\"http://www.cvl-a.de/ecap10/downloads/contribution-ecap10.pdf\">Submission form</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \"><strong>Theme</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \">Historical analysis of a broad range of paradigm shifts in science, biology, history, technology, and in particular in computing technology, suggests an accelerating rate of evolution, however measured. John von Neumann projected that the consequence of this trend may be an &ldquo;essential singularity in the history of the race beyond which human affairs as we know them could not continue&rdquo;. This notion of singularity coincides in time and nature with Alan Turing (1950) and Stephen Hawking&rsquo;s (1998) expectation of machines to exhibit intelligence on a par with to the average human no later than 2050. Irving John Good (1965) and Vernor Vinge (1993) expect the singularity to take the form of an &lsquo;intelligence explosion&rsquo;, a process in which intelligent machines design ever more intelligent machines. Transhumanists suggest a parallel or alternative, explosive process of improvements in human intelligence. And Alvin Toffler&rsquo;s Third Wave (1980) forecasts &ldquo;a collision point in human destiny&rdquo; the scale of which, in the course of history, is on the par only with the agricultural revolution and the industrial revolution.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \">We invite submissions describing systematic attempts at understanding the likelihood and nature of these projections. In particular, we welcome papers critically analyzing the following issues from a philosophical, computational, mathematical, scientific and ethical standpoints:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \">* Claims and evidence to acceleration<br />* Technological predictions (critical analysis of past and future)<br />* The nature of an intelligence explosion and its possible outcomes<br />* The nature of the Technological Singularity and its outcome<br />* Safe and unsafe artificial general intelligence and preventative measures<br />* Technological forecasts of computing phenomena and their projected impact<br />* Beyond the &lsquo;event horizon&rsquo; of the Technological Singularity<br />* The prospects of transhuman breakthroughs and likely timeframes</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; line-height: 1.2em; font-size: 1.1em; \"><a style=\"color: #aa7d39; text-decoration: none; \" href=\"http://www.eden-study.org/\">Amnon H. Eden</a>, School of Computer Science &amp; Electronic Engineering, University of Essex, UK and Center For Inquiry, Amherst NY</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w3FcDHSAHRhrAxzZj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 5.767224725540038e-07, "legacy": true, "legacyId": "2685", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-11T16:39:33.270Z", "modifiedAt": null, "url": null, "title": "Self-modification is the correct justification for updateless decision theory", "slug": "self-modification-is-the-correct-justification-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/88vuFDw3dCX7hC6uW/self-modification-is-the-correct-justification-for", "pageUrlRelative": "/posts/88vuFDw3dCX7hC6uW/self-modification-is-the-correct-justification-for", "linkUrl": "https://www.lesswrong.com/posts/88vuFDw3dCX7hC6uW/self-modification-is-the-correct-justification-for", "postedAtFormatted": "Sunday, April 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-modification%20is%20the%20correct%20justification%20for%20updateless%20decision%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-modification%20is%20the%20correct%20justification%20for%20updateless%20decision%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88vuFDw3dCX7hC6uW%2Fself-modification-is-the-correct-justification-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-modification%20is%20the%20correct%20justification%20for%20updateless%20decision%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88vuFDw3dCX7hC6uW%2Fself-modification-is-the-correct-justification-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88vuFDw3dCX7hC6uW%2Fself-modification-is-the-correct-justification-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 559, "htmlBody": "<p><strong>Reply to:</strong> <a href=\"/lw/214/late_great_filter_is_not_bad_news/\">Late great filter is not bad news</a></p>\n<p>Suppose that you build an AI, and Omega appears to it and says:</p>\n<p style=\"padding-left: 30px;\">Here's a button. A million years ago I calculated the umpteenth digit of pi. If it is even, I calculated whether you would press this button (in such a way that your human creator was never simulated as a conscious being). If I predicted that you wouldn't press the button, I destroyed Earth right then and there.<strong>*</strong> If it is odd, I created a doomsday device that will destroy the solar system if you press this button.</p>\n<p>[<strong>* ETA: </strong>Assume that if the digit is even and the AI is predicted to press the button, Omega does not destroy Earth, but does <a href=\"/lw/22m/selfmodification_is_the_correct_justification_for/1vj6?context=1#1vj6\">turn Alpha Centauri purple</a> (say). The point is for this to be a scenario that you, the AI creator, <em>know not to have come to pass.</em>]</p>\n<p>Suppose you're the kind of AI creator whose AI is time consistent in a certain sense from the beginning of time and presses the button. Then you have an AI that satisfies a certain kind of philosopher, wins big in a certain logically impossible world, and destroys humanity.</p>\n<p>Suppose, on the other hand, that you're a very similar kind of AI creator, only you program your AI not to take into account impossible possible worlds that had already turned out to be impossible (when you created the AI | when you first became convinced that timeless decision theory is right). Then you've got an AI that most of the time acts the same way, but does worse in worlds we know to be logically impossible, and destroys humanity less often in worlds we do not know to be logically impossible.</p>\n<p><a href=\"/lw/214/late_great_filter_is_not_bad_news/\">Wei Dai's great filter post</a> seems to suggest that under <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>, you should be the first kind of AI creator. I don't think that's true, actually; I think that in UDT, you should probably not start with a \"prior\" probability distribution that gives significant weight to logical propositions you <em>know</em> to be false: do you think the AI should press the button if it was the <em>first</em> digit of pi that Omega calculated?</p>\n<p>But obviously, you don't want tomorrow's you to pick the prior that way just after Omega has appeared to it in a <a href=\"/lw/3l/counterfactual_mugging/\">couterfactual mugging</a> (because according to your best reasoning today, there's a 50% chance this loses you a million dollars).</p>\n<p>The most convincing argument I know for timeless flavors of decision theory is that if you could modify your own source code, the course of action that maximizes your expected utility is to modify into a timeless decider. So yes, you should do that. Any AI you build should be timeless from the start; and it's reasonable to make yourself into the kind of person that will decide timelessly with your probability distribution today (if you can do that).</p>\n<p>But I don't think you should decide that updateless decision theory is therefore so pure and reflectively consistent that you should go and optimize your payoff even in worlds whose logical impossibility was clear before you first decided to be a timeless decider (say). Perhaps it's less elegant to justify UDT through self-modification at some arbitrary point in time than through reflective consistency all the way from the big bang on; but in the worlds we can't rule out yet, <em>it's more likely to win.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "88vuFDw3dCX7hC6uW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 20, "extendedScore": null, "score": 5.770145324079995e-07, "legacy": true, "legacyId": "2686", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Bnv7mxzsgNjYuLcAy", "mg6jDEuQEjBGtibX7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-12T17:06:18.510Z", "modifiedAt": null, "url": null, "title": "Ugh fields", "slug": "ugh-fields", "viewCount": null, "lastCommentedAt": "2022-05-06T00:54:41.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields", "pageUrlRelative": "/posts/EFQ3F6kmt4WHXRqik/ugh-fields", "linkUrl": "https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields", "postedAtFormatted": "Monday, April 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ugh%20fields&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUgh%20fields%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEFQ3F6kmt4WHXRqik%2Fugh-fields%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ugh%20fields%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEFQ3F6kmt4WHXRqik%2Fugh-fields", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEFQ3F6kmt4WHXRqik%2Fugh-fields", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 864, "htmlBody": "<p><em>Tl;Dr version: Pavlovian conditioning can cause humans to unconsciously flinch from even thinking about a serious personal problem they have, we call it an \"Ugh Field\"<sup>1</sup>. The Ugh Field forms a self-shadowing blind spot covering an area desperately in need of optimization, imposing huge costs. </em></p>\n<p>A problem with the human mind &mdash; your human mind &mdash; is that it's a horrific kludge that will fail when you most need it not to. The <em>Ugh Field failure mode</em> is one of those really annoying failures. The idea is simple: if a person receives constant negative conditioning via unhappy thoughts whenever their mind goes into a certain zone of thought, they will begin to develop a psychological flinch mechanism around the thought. The \"Unhappy Thing\" &mdash; the source of negative thoughts &mdash; is typically some part of your model of the world that relates to bad things being likely to happen to you.</p>\n<p>A key part of the Ugh Field phenomenon is that, to start with, there is no flinch, only negative real consequences resulting from real physical actions in the problem area. Then, gradually, you begin to feel the emotional hit when you are planning to take physical actions in the problem area. Then eventually, the emotional hit comes when you even begin to think about the problem. The reason for this may be that your brain operates a <a href=\"http://en.wikipedia.org/wiki/Temporal_difference_learning#TD_algorithm_in_neuroscience\">temporal difference learning</a> (TDL) algorithm. Your brain propagates the psychological pain \"back to the earliest reliable stimulus for the punishment\". If you fail or are punished sufficiently many times in some problem area, and acting in that area is always preceeded by thinking about it, your brain will propagate the psychological pain right back to the moment you first begin to entertain a thought about the problem, and hence cut your conscious optimizing ability right out of the loop. <span style=\"visibility: visible;\"><span style=\"visibility: visible;\">Related to this is engaging in a <a href=\"http://en.wikipedia.org/wiki/Displacement_activity\">displacement activity</a>: this is some activity that usually involves comfort, done instead of confronting the problem. Perhaps (though this is speculative) the comforting displacement activity is there to counterbalance the psychological pain that you experienced just because you thought about the problem.<a id=\"more\"></a></span></span></p>\n<p>For example, suppose that you started off in life with a wandering mind and were punished a few times for failing to respond to official letters. Your TDL algorithm began to propagate the pain back to the moment you looked at an official letter or bill. As a result, you would be less effective than average at responding, so you got punished a few more times. Henceforth, when you received a bill, you got the pain before you even opened it, and it laid unpaid on the mantelpiece until a Big Bad Red late payment notice with an $25 fine arrived. More negative conditioning. Now even thinking about a bill, form or letter invokes the flinch response, and your <a href=\"http://en.wikipedia.org/wiki/Lizard_brain\">lizard brain</a> has fully cut you out out. You find yourself spending time on internet time-wasters, comfort food, TV, computer games, etc. Your life may not obviously be&nbsp;a disaster, but this is only because you can't see the alternative paths that it could have taken if you had been able to take advantage of the opportunities that came as letters and forms with deadlines.</p>\n<p>The subtlety with the Ugh Field is that the flinch occurs <strong><em>before you start to consciously think</em></strong> about how to deal with the Unhappy Thing, meaning that you never deal with it, and you don't even have the option of dealing with it in the normal run of things. I find it frightening that my lizard brain could implicitly be making life decisions for me, without even asking my permission!</p>\n<p>Possible antidotes to Ugh Field problem:</p>\n<ul>\n<li>Actively look out for the flinch, preferably when you are in a motivationally \"high\" state. Better still, do this when you are both motivationally high, not under time pressure, and when you are undertaking an overview of your life. This overview exercise will tend to make your mind range over all of the relevant parts of your life, and hopefully \"throw up\" some \"Ugh!\" reactions.</li>\n<li>Concretely visualize how your life could be much better if you oust control of it from your lizarrd brain. Imagine, in <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">near-mode</a>, how much better your future life could be if you can find and \"pick off\" your Ugh Fields and optimize&nbsp;the relevant part of your life. If you haven't yet identified these areas of your life, imagine that some concrete good thing (such as eating ice-cream, laughing with friends, etc) will happen to you in the future if you can honestly face these areas.</li>\n<li><span style=\"visibility: visible;\"><span style=\"visibility: visible;\">Identifying these reactions, writing them down in a list, and affirming that you want to take control of them will help you to distance yourself from them. Once your conscious mind has a positive desire to take control, the offending stimulus will hopefully activate this \"take-control\" reaction, rather than the \"flinch\" reaction. Key to this is framing the \"take control\" action as a \"positive\" outcome enabler will facilitate action, <a href=\"/lw/21r/pain_and_gain_motivation/\">as Kaj and PJ have already told us</a>.</span></span></li>\n</ul>\n<p>1: (Credit for this idea goes to Anna Salamon and Jennifer Rodriguez-M&uuml;ller. Upvotes go to me, as I wrote the darn article)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 9, "iP2X4jQNHMWHRNPne": 2, "3ee9k6NJfcGzL6kMS": 4, "EuDw6uxQW2ZBRFhMo": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EFQ3F6kmt4WHXRqik", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 264, "baseScore": 314, "extendedScore": null, "score": 0.000521, "legacy": true, "legacyId": "2639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 314, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xNrdYu6p6BRamRBz8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 22, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-14T13:32:21.158Z", "modifiedAt": null, "url": null, "title": "Preference utilitarian measure of historical welfare", "slug": "preference-utilitarian-measure-of-historical-welfare", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:54.785Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CPpsoGDA4ghjJGPxP/preference-utilitarian-measure-of-historical-welfare", "pageUrlRelative": "/posts/CPpsoGDA4ghjJGPxP/preference-utilitarian-measure-of-historical-welfare", "linkUrl": "https://www.lesswrong.com/posts/CPpsoGDA4ghjJGPxP/preference-utilitarian-measure-of-historical-welfare", "postedAtFormatted": "Wednesday, April 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Preference%20utilitarian%20measure%20of%20historical%20welfare&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APreference%20utilitarian%20measure%20of%20historical%20welfare%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCPpsoGDA4ghjJGPxP%2Fpreference-utilitarian-measure-of-historical-welfare%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Preference%20utilitarian%20measure%20of%20historical%20welfare%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCPpsoGDA4ghjJGPxP%2Fpreference-utilitarian-measure-of-historical-welfare", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCPpsoGDA4ghjJGPxP%2Fpreference-utilitarian-measure-of-historical-welfare", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 831, "htmlBody": "<p>GDP measures essentially how good we are at making widgets - and while widgets are useful, it is a very weak and indirect measure of welfare. For example UK GDP per capita doubled between 1975 and 2007 - and people's quality of life indeed improved - but it would be extremely difficult to argue that this improvement was \"doubling\", and that the gap between 2007's and 1975's quality of life is greater than between 1975's and hunter-gatherer times.<br /><br />It's not essential to this post, but my very quick theory is that we overestimate GDP thanks to economic equivalent of <a href=\"http://en.wikipedia.org/wiki/Amdahl%27s_law\">Amdahl's Law</a> - if someone's optimal consumption mix consisted of 9 units of widgets and 1 unit of personalized services - and their purchasing power increased so now they can acquire 100x as many widgets, but still the same number of services as before - amount of the mix they can purchase increased only 9x, not 90x you'd get by weighted average of original consumption levels (and they spend 92% of their purchasing power on services now). The least scalable factor - whichever it is - will be the bottleneck.<br /><br />If we're unhappy with GDP there are alternative measures like <a href=\"http://en.wikipedia.org/wiki/Human_Development_Index\">HDI</a>, but they're highly artificial. It would be very easy to construct completely different measures which would \"feel\" about as right.<br /><br />Fortunately there exists a very natural measure of welfare, which I haven't seen used before in this context - preference utilitarian lotteries. Would you rather live in 1700, or take a 50% chance of living in 2010 or 700? Make a list of such bets, assign numbers coherent with bet values (with 100 for highest and 0 for your lowest value) and you're done! By averaging many people's estimates we can hopefully reduce the noise, and get some pretty reasonable welfare estimates.<br /> <a id=\"more\"></a> <br />And now disclaimer time. This approach has countless problems, here are just a few but I'm sure you can think about more.</p>\n<ul>\n<li><strong>Probabilities are difficult</strong> - People are really bad at intuiting about a difference between 1% chance of something vs 3% of something, even though it will count for three times as much in results. We can mostly work around this problem by not comparing extremes, but instead sorting situations by desirability, and only comparing nth situation, with p chance of n+1st vs (1-p) chance of (n-1)st. Such probabilities will usually be in comfortable medium range.</li>\n<li><strong>Risk aversion</strong> - you prefer certainty of moderate outcome to change of getting either good or bad outcome. It tends to overestimate past welfare.</li>\n<li><strong>Status quo bias</strong> - you prefer situations closer to your current even if there's no actual welfare difference. It tends to underestimate past welfare, perhaps balancing risk aversion.</li>\n<li><strong>Knowledge problem</strong> - now how much do you really know about life in Industrial Revolution time Britain, let alone ancient Sumer? Even professional historians have problems with that, and unfortunately we might all be biased the same way negating some benefit of averaging out estimates.</li>\n<li><strong>Values problem</strong> - you might find some civilizations more repulsive and others less because of your modern values, even if their welfare is really not that different. It can be infanticide (extremely common historically), slavery, racial discrimination, human sacrifice, particular religion or political system etc.</li>\n<li><strong>Hindsight </strong>- reverse of knowledge problem - life in 1345 Florence was nowhere near as bad as our <a href=\"http://en.wikipedia.org/wiki/Black_Death\">hindsight estimates</a> would make it be.</li>\n<li><strong>Representative sampling</strong> - life of exactly whom? In many times a random person born wouldn't survive to adulthood - yet it seems unreasonable to include those. Let's say we focus on a healthy adult somewhere near median social status and income.</li>\n</ul>\n<p>I tried to think about such series of bets and my results are:</p>\n<ul>\n<li>Western Europe 2010 CE - 100</li>\n<li>Western Europe 1980 CE - 97</li>\n<li>Western Europe 1950 CE - 91</li>\n<li>Western Europe 1900 CE - 65</li>\n<li>Western Europe 1800 CE - 26</li>\n<li>Western Europe 1700 CE - 16</li>\n<li>Western Europe 1500 CE - 10</li>\n<li>High Middle Ages Europe (1250 CE) - 7.6</li>\n<li>Early Middle Ages Europe (700 CE) - 6.4</li>\n<li>Roman Empire around 100 CE - 7.1</li>\n<li>Mediterranean World 500 BCE - 7.0</li>\n<li>Neolithic Middle East (5000 BCE) - 1.6</li>\n<li>Paleolithic anywhere (20000 BCE) - 0</li>\n</ul>\n<p>This seems far more reasonable than GDP's illusion of exponentially accelerating progress.</p>\n<p>I used this Ruby code to convert bets to values on scale of 0 to 100 (bets ordered by preference, not chronologically):</p>\n<pre>def linearize_ratios(*ratios)<br />&nbsp; diffs = ratios.inject([1.0]){|d,r| d + [d[-1] * r / (1-r)]}<br />&nbsp; scale = diffs.inject{|a,b|a+b}<br />&nbsp; diffs.inject([100]){|v,d| v + [v[-1] - 100.0 * d / scale]}<br />end<br />p linearize_ratios(0.7, 0.8, 0.6, 0.2, 0.4, 0.25, 0.2, 0.1, 0.9, 0.9, 0.25)</pre>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CPpsoGDA4ghjJGPxP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 11, "extendedScore": null, "score": 5.778361742118985e-07, "legacy": true, "legacyId": "2709", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-14T21:38:46.831Z", "modifiedAt": null, "url": null, "title": "A LessWrong poster for the Humanity+ conference next Saturday", "slug": "a-lesswrong-poster-for-the-humanity-conference-next-saturday", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:55.445Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pe4qh33afzJzyCbwn/a-lesswrong-poster-for-the-humanity-conference-next-saturday", "pageUrlRelative": "/posts/Pe4qh33afzJzyCbwn/a-lesswrong-poster-for-the-humanity-conference-next-saturday", "linkUrl": "https://www.lesswrong.com/posts/Pe4qh33afzJzyCbwn/a-lesswrong-poster-for-the-humanity-conference-next-saturday", "postedAtFormatted": "Wednesday, April 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20LessWrong%20poster%20for%20the%20Humanity%2B%20conference%20next%20Saturday&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20LessWrong%20poster%20for%20the%20Humanity%2B%20conference%20next%20Saturday%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPe4qh33afzJzyCbwn%2Fa-lesswrong-poster-for-the-humanity-conference-next-saturday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20LessWrong%20poster%20for%20the%20Humanity%2B%20conference%20next%20Saturday%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPe4qh33afzJzyCbwn%2Fa-lesswrong-poster-for-the-humanity-conference-next-saturday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPe4qh33afzJzyCbwn%2Fa-lesswrong-poster-for-the-humanity-conference-next-saturday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 338, "htmlBody": "<p>An email from David Wood, organiser of the <a href=\"http://humanityplus-uk.com/wordpress/\">Humanity+ UK 2010 conference</a> in London on Saturday 2010-04-24:</p>\n<blockquote>\n<p>One of the rooms in Conway Hall on Sat 24th April will be set aside for posters and general socialising.</p>\n<p>The posters are opportunities for people to publicise various activities or ideas. We've received half a dozen applications for posters so far, and we have room for one or two more.</p>\n<p>We expect that many of the attendees will mingle in this room at lunchtime, during the afternoon break, and (for early birds) before the formal start of activities in the main hall at 9.45am.</p>\n<p>Would one of you be interested in creating and displaying a poster about Less Wrong / Overcoming Bias?</p>\n<p>Many of the attendees to the H+UK event will have little prior knowledge about Less Wrong, so it's a good chance to reach out to potential new supporters.</p>\n<p>Posters can be a number of sheets of paper, stuck onto the wall with bluetack or sellotape. Maximise size in total allowed per poster is A0. Several of the posters will be A1, made up of 4 A3 sheets.</p>\n<p>If you are interested in this, please let me know, since we have to control overall numbers of posters.</p>\n<p>To be clear, there's no charge for this - consider it as an opportunity for free advertising :-) You're also welcome to bring small pieces of printed literature for interested people to take away.</p>\n</blockquote>\n<p>I plan to make such a poster, and I'd like the advice of people here. Trying to represent what Less Wrong is about in the space of a poster could be challenging. I have maximum space equivalent to sixteen A4 pieces of paper. In the spirit of not proposing a solution until you've had a chance to think about the problem, I'll put my current plans into a comment.</p>\n<p>Update: looks like it will be Roko rather than me making the poster, but the same applies, your ideas could doubtless be very useful!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pe4qh33afzJzyCbwn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 5.779330171392762e-07, "legacy": true, "legacyId": "2710", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-15T15:31:03.163Z", "modifiedAt": null, "url": null, "title": "The many faces of status", "slug": "the-many-faces-of-status", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:01.771Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pvfrJACkq4DzGYjFF/the-many-faces-of-status", "pageUrlRelative": "/posts/pvfrJACkq4DzGYjFF/the-many-faces-of-status", "linkUrl": "https://www.lesswrong.com/posts/pvfrJACkq4DzGYjFF/the-many-faces-of-status", "postedAtFormatted": "Thursday, April 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20many%20faces%20of%20status&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20many%20faces%20of%20status%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpvfrJACkq4DzGYjFF%2Fthe-many-faces-of-status%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20many%20faces%20of%20status%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpvfrJACkq4DzGYjFF%2Fthe-many-faces-of-status", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpvfrJACkq4DzGYjFF%2Fthe-many-faces-of-status", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1986, "htmlBody": "<p>The term \"status\" gets used on LessWrong a lot. Google finds 316 instances; the aggregate total for the phrases \"low status\" and \"high status\" (which suggest more precision than \"status\" by itself) is 170. By way of comparison, \"many worlds\", an important topic here, yields 164 instances.<br /><br />We find the term used as an <a href=\"/lw/13s/the_nature_of_offense/\">explanation</a>, for instance, \"to give offense is to imply that a person or group has or should have low status\". In this community I would expect that a term used often, with authoritative connotations, and offered as an explanation could be <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">tabooed</a> readily, for instance when someone confused by this or that use asks for clarification: previous discussions of \"high status\" or \"low status\" behaviours seemed to <a href=\"/lw/1vs/selfishness_signals_status/\">flounder</a> in the particular way that definitional arguments often do.<br /><br />Somewhat to my surprise, there turned out not to be a commonly understood way of tabooing \"status\". Lacking a satisfactory unpacking of the \"status\" terms and how they should control anticipation, I decided to explore the topic on my own, and my intention here is to report back and provide a basis for further discussion.<br /><a id=\"more\"></a><br />The \"Status\" chapter of Keith Johnstone's 1979 book \"Impro\", previously <a href=\"/lw/1yz/levels_of_communication/\">discussed here</a> and <a href=\"http://www.overcomingbias.com/2009/08/actors-see-status.html\">on OB</a>, is often cited as a reference on the topic (<a href=\"http://www.thestage.co.uk/connect/acblack/improkj.php\">follow this link for an excerpt</a>); I'll refer to it throughout as simply \"Johnstone\". Also, I plan to entirely avoid the related but distinct concept of \"signaling\" in this post, reserving it for later examination.</p>\n<h2><br /></h2>\n<h2>Dominance hierarchies</h2>\n<p>My initial impression was that \"status\" had some relation to the theory of dominance hierarchies. Section 3 of Johnstone starts with:</p>\n<blockquote>\n<p>Social animals have inbuilt rules which prevent them killing each other for food, mates, and so on.&nbsp; Such animals confront each other, and often fight, until a hierarchy is established, after which there is no fighting unless an attempt is made to change the &lsquo;pecking order&rsquo;. This system is found in animals as diverse as humans, chickens, and woodlice.</p>\n</blockquote>\n<p>This reinforced an impression I had previously acquired: that the term \"alpha male\", often used in certain circles synonymously with \"high status male\", indicated an explicit link between the theoretical underpinnings of the term \"status\" and some sort of dominance theory.</p>\n<p>However, substantiating this link turned out a more frustrating task than I had expected. For instance, I looked for primary sources I could turn to for a formal theoretical explanation of what explanatory work the term \"alpha male\" is supposed to carry out.<br /><br />It seems that the term was originally coined by <a href=\"http://en.wikipedia.org/wiki/L._David_Mech\">David Mech</a>, who studied wolf packs in the 70's. Interestingly, Mech himself now claims the term was misunderstood and used improperly. Here is what David Mech says in a recent (2000) <a href=\"http://www.npwrc.usgs.gov/resource/mammals/alstat/alpst.htm\">article</a>:</p>\n<blockquote>\n<p>The way in which alpha status has been viewed historically can be seen in studies in which an attempt is made to distinguish future alphas in litters of captive wolf pups [...] This view implies that rank is innate or formed early, and that some wolves are destined to rule the pack, while others are not.</p>\n<p>Contrary to this view, I propose that all young wolves are potential breeders and that when they do breed they automatically become alphas (Mech 1970). [...] Thus, calling a wolf an alpha is usually no more appropriate than referring to a human parent or a doe deer as an alpha. Any parent is dominant to its young offspring, so \"alpha\" adds no information.</p>\n</blockquote>\n<p>An informal survey of other literature suggests that \"alpha male\", referring specifically to the pack behaviour disowned by Mech, entered the popular vocabulary by way of <a href=\"http://www.canismajor.com/dog/alpha1.html\">dog trainer lore</a>. My personal hunch is that it became entrenched thereafter because it had both a \"sciencey\" sound, and the appropriate connotations for people who adhered to certain views on gender relationships.<br /><br />Stepping back to look at dominance theory as a whole, I found that they are not without problems. Pecking order may apply to chickens, but primates vary widely in social organization, lending little support to the thesis that dominance displays, dominance-submission behaviours and so on are as universal as Johnstone suggests and can therefore be thought to shed much light on the complex social organization of humans.<br /><br />An often discussed example is the Bonobo chimpanzee, where females are dominant over males, and do not establish a dominance hierarchy among themselves, whereas males do; where the behaviours that tend to mediate social stratification is reconciliation rather than conflict, something that is also observed in other animal species, contrary to the prevailing view of dominance hierarchies.<br /><br />This informal survey was interesting and turned up many surprises, but mostly it convinced me that dominance hierarchies were not a fruitful line of research if I was after a crisp meaning of \"status\" terms and explanations: either \"status\" was itself a muddle, or I needed to look for its underpinnings in other disciplines.</p>\n<p>&nbsp;</p>\n<h2>Social stratification</h2>\n<p>Early on in Johnstone there is an interesting discussion of status by way of his recollection of three very different school teachers. At various other points in the chapter he also refers to the stratification of human societies specifically, for instance when he discusses the master-servant relationship.</p>\n<p>The teacher example was particularly interesting for me, because one of the uses I might have for status hypotheses is in investigating the Hansonian thesis \"Schools aren't about education but about status\", and what can possibly be done about that. But to think clearly about such issues one must, in the first place, clarify how the hypothesis \"X is about status\" controls anticipation about X!<br /><br />I came across <a href=\"http://en.wikipedia.org/wiki/Max_Weber\">Max Weber</a> (who I must say I hadn't heard of previously), described as one of the founders of modern sociology; and Weber's \"<a href=\"http://en.wikipedia.org/wiki/Three-component_theory_of_stratification\">three component theory of social stratification</a>\", which helped me quite a bit in making sense of some claims about status.<br /><br />What I got from the Wikipedia summary is that Weber identifies three major dimensions of social stratification:</p>\n<ul>\n<li>class or wealth status, that is, a person's economic situation</li>\n<li>power status, or a person's ability to achieve their goals in the face of other's opposition</li>\n<li>prestige status, or how well a person is regarded by others</li>\n</ul>\n<p><br />This list is interesting because of its predictive power: for instance, class and wealth tend to be properties of an individual that change slowly over time, and so when Johnstone refers to ways of elevating one's status within the short time span of a social interaction, we can predict that he isn't talking about class or wealth status.<br /><br />Power status is more subject to sudden changes, but not usually as a result of informal social interactions: again, power status cannot be what is referred to in the phrase \"high status behaviours\". Power is very often positional, for instance getting elected President of a powerful country brings a lot of power suddenly, but requires vetting by an elaborate ritual. (Class status can often go hand in hand with power status, but that is not necessarily or systematically the case.)<br /><br />Prestige status can be expected to depend on both long-term and short-term characteristics. Certain professions are seen as inherently prestigious, often independently of wealth: firemen, for instance. But within a given social stratum, defined by class and power, individuals can acquire prestige through their actions.This is applicable for wide ranges of group sizes. Scientists acquire prestige by working on important topics and publishing important results. Participants in an online community acquire prestige by posting influential articles which shape subsequent discussion, and so on.<br /><br />But, while it struck me as conceivable to unpack terms like \"high status behaviours\" as referring to such changes in prestige status, it didn't seem entirely satisfactory. So I kept looking for clues.</p>\n<p>&nbsp;</p>\n<h2>Self-esteem and the seesaw</h2>\n<p>Johnstone refers to status \"the see-saw\": he sees status transactions as a zero-sum game. To increase your status, he says, is necessarily to lower that of your interlocutor.<br /><br />This seems at odds with seeing most references to status as meaning \"prestige status\", since you can acquire prestige without necessarily lower someone else's; also, you can acquire prestige without entering into an interactive social situation. (Think of how a mountaineer's prestige can rise upon the news that they have reached some difficult summit, ahead of their coming back to enjoy the attention.)<br /><br />However, most of what Johnstone discusses seemed to make sense to me if analyzed instead as self-esteem transactions: interactive behaviour which raises or lowers another's self-esteem or yours.<br /><br />There is lots of relevant theory to turn to. Some old and possibly discredited - I'm thinking here of \"<a href=\"http://en.wikipedia.org/wiki/Transactional_analysis\">transactional analysis</a>\" which I came across years and years ago, which had the interesting concept of a \"stroke\", a behaviour whereby one raises another's self-esteem; this could also be relevant to analyzing the PUA theory of \"negging\". (Fun fact: TA is also the origin of the phrase \"warm fuzzies\".) Some newer and perhaps more solidly based on ev-psych, such as the recently mentioned <a href=\"http://www.psychwiki.com/wiki/Sociometer_Theory\">sociometer theory</a>.<br /><br />Self-esteem is at any rate an important idea, whether or not we are clear on the underlying causal mechanisms. John Rawls notes that self-esteem is among the \"primary social goods\" (defined as \"the things it is rational to want, whatever else you want\", in other words the most widely applicable instrumental values that can help further a wide range of terminal values). It is very difficult to be <a href=\"/lw/20l/ureshiku_naritai/\">luminous</a>, to collaborate effectively or to conquer akrasia without some explicit attention to self-esteem.<br /><br />So here, perhaps, is a fourth status component: the more temporary and more local \"self-esteem status\".</p>\n<p>&nbsp;</p>\n<h2>Positive sum self-esteem transactions?</h2>\n<p>Where I part company with Johnstone is in seeing self-esteem transactions as a purely zero-sum game. And in fact his early discussion of the three teachers contradicts his own \"see-saw\" image, painting instead a quite different picture of \"status\".<br /><br />He describes one of the teachers as a \"low status player\", one who couldn't keep discipline, twitched, went red at the slightest provocation: in other words, one with generally low self-esteem. The second he describes as a \"compulsive high status player\": he terrorized students, \"stabbing people with his eyes\", walked \"with fixity of purpose\". In my terms, this would be someone whose behaviours communicated low regard for others' self-esteem, but not necessarily high self-esteem. The third teacher he describes as \"a status expert\":</p>\n<blockquote>\n<p>Much loved, never punished but kept excellent discipline, while remaining very human. He would joke with us, and then impose a mysterious stillness. In the street he looked upright, but relaxed, and he smiled easily.</p>\n</blockquote>\n<p>To me, this looks like the description of someone with high self-esteem generally, who is able to temporarily affect his own and others' self-esteem, lowering (to establish authority) or raising (to encourage participation) as appropriate. When done expertly, this isn't manipulative, but rather a game of trust and rapport that people play in all social situations where safety and intimacy allow, and it feels like a positive sum game.<br /><br />(These transactions, BTW, can be mediated even by relatively low-bandwidth interactions, such as text conversations. I find it fascinating how people can make each other feel various emotions just with words: anger, shame, pride. A forum such as Less Wrong isn't just a place for debate and argument, it is also very much a locus of social interaction. Keeping that in mind is important.)<br /><br />Detailed analysis of how these transactions work, distilled into practical advice that people can use in everyday settings, is a worthwhile goal, and one that would also advance the cause of effective collaboration among people dedicated to thinking more clearly about the world they inhabit.<br /><br />Let the discussion stick to that spirit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pvfrJACkq4DzGYjFF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 48, "extendedScore": null, "score": 0.0005863017106431277, "legacy": true, "legacyId": "2717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The term \"status\" gets used on LessWrong a lot. Google finds 316 instances; the aggregate total for the phrases \"low status\" and \"high status\" (which suggest more precision than \"status\" by itself) is 170. By way of comparison, \"many worlds\", an important topic here, yields 164 instances.<br><br>We find the term used as an <a href=\"/lw/13s/the_nature_of_offense/\">explanation</a>, for instance, \"to give offense is to imply that a person or group has or should have low status\". In this community I would expect that a term used often, with authoritative connotations, and offered as an explanation could be <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">tabooed</a> readily, for instance when someone confused by this or that use asks for clarification: previous discussions of \"high status\" or \"low status\" behaviours seemed to <a href=\"/lw/1vs/selfishness_signals_status/\">flounder</a> in the particular way that definitional arguments often do.<br><br>Somewhat to my surprise, there turned out not to be a commonly understood way of tabooing \"status\". Lacking a satisfactory unpacking of the \"status\" terms and how they should control anticipation, I decided to explore the topic on my own, and my intention here is to report back and provide a basis for further discussion.<br><a id=\"more\"></a><br>The \"Status\" chapter of Keith Johnstone's 1979 book \"Impro\", previously <a href=\"/lw/1yz/levels_of_communication/\">discussed here</a> and <a href=\"http://www.overcomingbias.com/2009/08/actors-see-status.html\">on OB</a>, is often cited as a reference on the topic (<a href=\"http://www.thestage.co.uk/connect/acblack/improkj.php\">follow this link for an excerpt</a>); I'll refer to it throughout as simply \"Johnstone\". Also, I plan to entirely avoid the related but distinct concept of \"signaling\" in this post, reserving it for later examination.</p>\n<h2><br></h2>\n<h2 id=\"Dominance_hierarchies\">Dominance hierarchies</h2>\n<p>My initial impression was that \"status\" had some relation to the theory of dominance hierarchies. Section 3 of Johnstone starts with:</p>\n<blockquote>\n<p>Social animals have inbuilt rules which prevent them killing each other for food, mates, and so on.&nbsp; Such animals confront each other, and often fight, until a hierarchy is established, after which there is no fighting unless an attempt is made to change the \u2018pecking order\u2019. This system is found in animals as diverse as humans, chickens, and woodlice.</p>\n</blockquote>\n<p>This reinforced an impression I had previously acquired: that the term \"alpha male\", often used in certain circles synonymously with \"high status male\", indicated an explicit link between the theoretical underpinnings of the term \"status\" and some sort of dominance theory.</p>\n<p>However, substantiating this link turned out a more frustrating task than I had expected. For instance, I looked for primary sources I could turn to for a formal theoretical explanation of what explanatory work the term \"alpha male\" is supposed to carry out.<br><br>It seems that the term was originally coined by <a href=\"http://en.wikipedia.org/wiki/L._David_Mech\">David Mech</a>, who studied wolf packs in the 70's. Interestingly, Mech himself now claims the term was misunderstood and used improperly. Here is what David Mech says in a recent (2000) <a href=\"http://www.npwrc.usgs.gov/resource/mammals/alstat/alpst.htm\">article</a>:</p>\n<blockquote>\n<p>The way in which alpha status has been viewed historically can be seen in studies in which an attempt is made to distinguish future alphas in litters of captive wolf pups [...] This view implies that rank is innate or formed early, and that some wolves are destined to rule the pack, while others are not.</p>\n<p>Contrary to this view, I propose that all young wolves are potential breeders and that when they do breed they automatically become alphas (Mech 1970). [...] Thus, calling a wolf an alpha is usually no more appropriate than referring to a human parent or a doe deer as an alpha. Any parent is dominant to its young offspring, so \"alpha\" adds no information.</p>\n</blockquote>\n<p>An informal survey of other literature suggests that \"alpha male\", referring specifically to the pack behaviour disowned by Mech, entered the popular vocabulary by way of <a href=\"http://www.canismajor.com/dog/alpha1.html\">dog trainer lore</a>. My personal hunch is that it became entrenched thereafter because it had both a \"sciencey\" sound, and the appropriate connotations for people who adhered to certain views on gender relationships.<br><br>Stepping back to look at dominance theory as a whole, I found that they are not without problems. Pecking order may apply to chickens, but primates vary widely in social organization, lending little support to the thesis that dominance displays, dominance-submission behaviours and so on are as universal as Johnstone suggests and can therefore be thought to shed much light on the complex social organization of humans.<br><br>An often discussed example is the Bonobo chimpanzee, where females are dominant over males, and do not establish a dominance hierarchy among themselves, whereas males do; where the behaviours that tend to mediate social stratification is reconciliation rather than conflict, something that is also observed in other animal species, contrary to the prevailing view of dominance hierarchies.<br><br>This informal survey was interesting and turned up many surprises, but mostly it convinced me that dominance hierarchies were not a fruitful line of research if I was after a crisp meaning of \"status\" terms and explanations: either \"status\" was itself a muddle, or I needed to look for its underpinnings in other disciplines.</p>\n<p>&nbsp;</p>\n<h2 id=\"Social_stratification\">Social stratification</h2>\n<p>Early on in Johnstone there is an interesting discussion of status by way of his recollection of three very different school teachers. At various other points in the chapter he also refers to the stratification of human societies specifically, for instance when he discusses the master-servant relationship.</p>\n<p>The teacher example was particularly interesting for me, because one of the uses I might have for status hypotheses is in investigating the Hansonian thesis \"Schools aren't about education but about status\", and what can possibly be done about that. But to think clearly about such issues one must, in the first place, clarify how the hypothesis \"X is about status\" controls anticipation about X!<br><br>I came across <a href=\"http://en.wikipedia.org/wiki/Max_Weber\">Max Weber</a> (who I must say I hadn't heard of previously), described as one of the founders of modern sociology; and Weber's \"<a href=\"http://en.wikipedia.org/wiki/Three-component_theory_of_stratification\">three component theory of social stratification</a>\", which helped me quite a bit in making sense of some claims about status.<br><br>What I got from the Wikipedia summary is that Weber identifies three major dimensions of social stratification:</p>\n<ul>\n<li>class or wealth status, that is, a person's economic situation</li>\n<li>power status, or a person's ability to achieve their goals in the face of other's opposition</li>\n<li>prestige status, or how well a person is regarded by others</li>\n</ul>\n<p><br>This list is interesting because of its predictive power: for instance, class and wealth tend to be properties of an individual that change slowly over time, and so when Johnstone refers to ways of elevating one's status within the short time span of a social interaction, we can predict that he isn't talking about class or wealth status.<br><br>Power status is more subject to sudden changes, but not usually as a result of informal social interactions: again, power status cannot be what is referred to in the phrase \"high status behaviours\". Power is very often positional, for instance getting elected President of a powerful country brings a lot of power suddenly, but requires vetting by an elaborate ritual. (Class status can often go hand in hand with power status, but that is not necessarily or systematically the case.)<br><br>Prestige status can be expected to depend on both long-term and short-term characteristics. Certain professions are seen as inherently prestigious, often independently of wealth: firemen, for instance. But within a given social stratum, defined by class and power, individuals can acquire prestige through their actions.This is applicable for wide ranges of group sizes. Scientists acquire prestige by working on important topics and publishing important results. Participants in an online community acquire prestige by posting influential articles which shape subsequent discussion, and so on.<br><br>But, while it struck me as conceivable to unpack terms like \"high status behaviours\" as referring to such changes in prestige status, it didn't seem entirely satisfactory. So I kept looking for clues.</p>\n<p>&nbsp;</p>\n<h2 id=\"Self_esteem_and_the_seesaw\">Self-esteem and the seesaw</h2>\n<p>Johnstone refers to status \"the see-saw\": he sees status transactions as a zero-sum game. To increase your status, he says, is necessarily to lower that of your interlocutor.<br><br>This seems at odds with seeing most references to status as meaning \"prestige status\", since you can acquire prestige without necessarily lower someone else's; also, you can acquire prestige without entering into an interactive social situation. (Think of how a mountaineer's prestige can rise upon the news that they have reached some difficult summit, ahead of their coming back to enjoy the attention.)<br><br>However, most of what Johnstone discusses seemed to make sense to me if analyzed instead as self-esteem transactions: interactive behaviour which raises or lowers another's self-esteem or yours.<br><br>There is lots of relevant theory to turn to. Some old and possibly discredited - I'm thinking here of \"<a href=\"http://en.wikipedia.org/wiki/Transactional_analysis\">transactional analysis</a>\" which I came across years and years ago, which had the interesting concept of a \"stroke\", a behaviour whereby one raises another's self-esteem; this could also be relevant to analyzing the PUA theory of \"negging\". (Fun fact: TA is also the origin of the phrase \"warm fuzzies\".) Some newer and perhaps more solidly based on ev-psych, such as the recently mentioned <a href=\"http://www.psychwiki.com/wiki/Sociometer_Theory\">sociometer theory</a>.<br><br>Self-esteem is at any rate an important idea, whether or not we are clear on the underlying causal mechanisms. John Rawls notes that self-esteem is among the \"primary social goods\" (defined as \"the things it is rational to want, whatever else you want\", in other words the most widely applicable instrumental values that can help further a wide range of terminal values). It is very difficult to be <a href=\"/lw/20l/ureshiku_naritai/\">luminous</a>, to collaborate effectively or to conquer akrasia without some explicit attention to self-esteem.<br><br>So here, perhaps, is a fourth status component: the more temporary and more local \"self-esteem status\".</p>\n<p>&nbsp;</p>\n<h2 id=\"Positive_sum_self_esteem_transactions_\">Positive sum self-esteem transactions?</h2>\n<p>Where I part company with Johnstone is in seeing self-esteem transactions as a purely zero-sum game. And in fact his early discussion of the three teachers contradicts his own \"see-saw\" image, painting instead a quite different picture of \"status\".<br><br>He describes one of the teachers as a \"low status player\", one who couldn't keep discipline, twitched, went red at the slightest provocation: in other words, one with generally low self-esteem. The second he describes as a \"compulsive high status player\": he terrorized students, \"stabbing people with his eyes\", walked \"with fixity of purpose\". In my terms, this would be someone whose behaviours communicated low regard for others' self-esteem, but not necessarily high self-esteem. The third teacher he describes as \"a status expert\":</p>\n<blockquote>\n<p>Much loved, never punished but kept excellent discipline, while remaining very human. He would joke with us, and then impose a mysterious stillness. In the street he looked upright, but relaxed, and he smiled easily.</p>\n</blockquote>\n<p>To me, this looks like the description of someone with high self-esteem generally, who is able to temporarily affect his own and others' self-esteem, lowering (to establish authority) or raising (to encourage participation) as appropriate. When done expertly, this isn't manipulative, but rather a game of trust and rapport that people play in all social situations where safety and intimacy allow, and it feels like a positive sum game.<br><br>(These transactions, BTW, can be mediated even by relatively low-bandwidth interactions, such as text conversations. I find it fascinating how people can make each other feel various emotions just with words: anger, shame, pride. A forum such as Less Wrong isn't just a place for debate and argument, it is also very much a locus of social interaction. Keeping that in mind is important.)<br><br>Detailed analysis of how these transactions work, distilled into practical advice that people can use in everyday settings, is a worthwhile goal, and one that would also advance the cause of effective collaboration among people dedicated to thinking more clearly about the world they inhabit.<br><br>Let the discussion stick to that spirit.</p>", "sections": [{"title": "Dominance hierarchies", "anchor": "Dominance_hierarchies", "level": 1}, {"title": "Social stratification", "anchor": "Social_stratification", "level": 1}, {"title": "Self-esteem and the seesaw", "anchor": "Self_esteem_and_the_seesaw", "level": 1}, {"title": "Positive sum self-esteem transactions?", "anchor": "Positive_sum_self_esteem_transactions_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "109 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QPqm5aj2meRmE7kR8", "MntNx6fFZ8KMHu5PM", "gs8bZCmaWqDaus7Dr", "xnPFYBuaGhpq869mY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-16T04:51:23.166Z", "modifiedAt": null, "url": null, "title": "Self-indication assumption is wrong for interesting reasons", "slug": "self-indication-assumption-is-wrong-for-interesting-reasons", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:04.077Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "neq1", "createdAt": "2010-03-31T12:30:01.450Z", "isAdmin": false, "displayName": "neq1"}, "userId": "YcYNXZpDcpBGgJR2u", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cd5JyRxg8stpC4JFd/self-indication-assumption-is-wrong-for-interesting-reasons", "pageUrlRelative": "/posts/cd5JyRxg8stpC4JFd/self-indication-assumption-is-wrong-for-interesting-reasons", "linkUrl": "https://www.lesswrong.com/posts/cd5JyRxg8stpC4JFd/self-indication-assumption-is-wrong-for-interesting-reasons", "postedAtFormatted": "Friday, April 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-indication%20assumption%20is%20wrong%20for%20interesting%20reasons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-indication%20assumption%20is%20wrong%20for%20interesting%20reasons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcd5JyRxg8stpC4JFd%2Fself-indication-assumption-is-wrong-for-interesting-reasons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-indication%20assumption%20is%20wrong%20for%20interesting%20reasons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcd5JyRxg8stpC4JFd%2Fself-indication-assumption-is-wrong-for-interesting-reasons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcd5JyRxg8stpC4JFd%2Fself-indication-assumption-is-wrong-for-interesting-reasons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 965, "htmlBody": "<p>The self-indication assumption (SIA) states that</p>\n<p style=\"padding-left: 30px;\">Given the fact that you exist, you should (other things equal) favor hypotheses according to which many observers exist over hypotheses on which few observers exist.</p>\n<p>The reason this is a bad assumption might not be obvious at first.&nbsp; In fact, I think it's very easy to miss.</p>\n<p><strong>Argument for SIA posted on Less Wrong<br /></strong></p>\n<p>First, let's take a look at a argument for SIA that appeared at Less Wrong (<a href=\"/lw/18r/avoiding_doomsday_a_proof_of_the_selfindication/\">link</a>).&nbsp; Two situations are considered.</p>\n<p style=\"padding-left: 30px;\">1.&nbsp; we imagine that there are 99 people in rooms that have a blue door on the outside (1 person per room).&nbsp; One person is in a room with a red door on the outside.&nbsp; It was argued that you are in a blue door room with probability 0.99.</p>\n<p style=\"padding-left: 30px;\">2.&nbsp; Same situation as above, but first a coin is flipped.&nbsp; If heads, the red door person is never created.&nbsp; If tails, the blue door people are never created.&nbsp; You wake up in a room and know these facts.&nbsp; It was argued that you are in a blue door room with probability 0.99.</p>\n<p>So why is 1. correct and 2. incorrect?&nbsp; The first thing we have to be careful about is not treating yourself as special.&nbsp; The fact that you woke up just tells you that at least one conscious observer exists.&nbsp;</p>\n<p>In scenario 1 we basically just need to know what proportion of conscious observers are in a blue door room.&nbsp; The answer is 0.99.</p>\n<p>In scenario 2 you never would have woken up in a room if you hadn't been created.&nbsp; Thus, the fact that you exist is something we have to take into account.&nbsp; We don't want to estimate P(randomly selected person, regardless of if they exist or not, is in a blue door room).&nbsp; That would be ignoring the fact that you exist.&nbsp; Instead, the fact that you exist tells us that at least one conscious observer exists.&nbsp; Again, we want to know what proportion of conscious observers are in blue door rooms.&nbsp; Well, there is a 50% chance (if heads landed) that all conscious observers are in blue door rooms, and a 50% chance that all conscious observers are in red door rooms.&nbsp; Thus, the marginal probability of a conscious observer being in a blue door room is 0.5.</p>\n<p>The flaw in the more detailed Less Wrong proof (<a href=\"/lw/18r/avoiding_doomsday_a_proof_of_the_selfindication/\">see the post</a>) is when they go from step C to step D.&nbsp; The *you* being referred to in step A might not exist to be asked the question in step D.&nbsp; You have to take that into account.</p>\n<p><strong>General argument for SIA and why it's wrong</strong></p>\n<p>Let's consider the assumption more formally.</p>\n<p>Assume that the number of people to be created, N, is a random draw from a discrete uniform distribution<sup>1</sup> on {1,2,...,N<sub>max</sub>}.&nbsp; Thus, P(N=k)=1/N<sub>max</sub>, for k=1,...,N<sub>max</sub>.&nbsp; Assume N<sub>max </sub>is large enough so that we can effectively ignore finite sample issues (this is just for simplicity).</p>\n<p>Assume M= N<sub>max</sub>*(N<sub>max</sub>+1)/2 possible people exist, and we arbitrarily label them 1,...,M.&nbsp; After the size of the world, say N=n, is determined, then we randomly draw n people from the M possible people.</p>\n<p>After the data are collected we find out that person x exists.</p>\n<p>We can apply Bayes' theorem to get the posterior probability:</p>\n<p style=\"padding-left: 30px;\">P(N=k|x exists)=k/M, for k=1,...,N<sub>max.</sub></p>\n<p>The prior probability was uniform, but the posterior favors larger worlds.&nbsp; QED.</p>\n<p>Well, not really.</p>\n<p>The flaw here is that we conditioned on person x existing, but person x only became of interest <em>after</em> we saw that they existed (peeked at the data).</p>\n<p>What we really know is that at least one conscious observer exists -- there is nothing special about person x.</p>\n<p>So, the correct conditional probability is:</p>\n<p style=\"padding-left: 30px;\">P(N=k|someone exists)=1/N<sub>max</sub>, for k=1,...,N<sub>max.</sub></p>\n<p>Thus, prior=posterior and SIA is wrong.</p>\n<p><strong>Egotism</strong></p>\n<p>The flaw with SIA that I highlighted here is it treats you as special, as if you were labeled ahead of time.&nbsp; But the reality is, no matter who was selected, they would think they are the special person.&nbsp; \"But <em>I</em> exist, I'm not just some arbitrary person.&nbsp; That couldn't happen in small world.&nbsp; It's too unlikely.\"&nbsp; In reality, that fact that I exist just means someone exists. I only became special after I already existed (peeked at the data and used it to construct the conditional probability).</p>\n<p>Here's another way to look at it.&nbsp; Imagine that a random number between 1 and 1 trillion was drawn.&nbsp; Suppose 34,441 was selected.&nbsp; If someone then asked what the probability of selecting <em>that</em> number was, the correct answer is 1 in 1 trillion.&nbsp; They could then argue, \"that's too unlikely of an event.&nbsp; It couldn't have happened by chance.\"&nbsp; However, because they didn't identify the number(s) of interest ahead of time, all we really can conclude is that <em>a</em> number was drawn, and drawing <em>a</em> number was a probability 1 event.</p>\n<p>I give more examples of this <a href=\"http://neq1.wordpress.com/2010/04/11/self-indication-assumption/\">here</a>.</p>\n<p>I think Nick Bostrom is getting at the same thing in <a href=\"http://www.anthropic-principle.com/book/\">his book</a> (page 125):</p>\n<p style=\"padding-left: 30px;\">..your own existence is not in general a ground for thinking that hypotheses are more likely to be true just by virtue of implying that there is a greater total number of observers. The datum of your existence tends to disconfirm hypotheses on which it would be unlikely that any observers (in your reference class) should exist; but that&rsquo;s as far as it goes. The reason for this is that the sample at hand&mdash;you&mdash;should not be thought of as randomly selected from the class of all possible observers but only from a class of observers who will actually have existed. It is, so to speak, not a coincidence that the sample you are considering is one that actually exists. Rather, that&rsquo;s a logical consequence of the fact that only actual observers actually view themselves as samples from anything at all</p>\n<p>Related arguments are made in <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\">this LessWrong post</a>. &nbsp;</p>\n<hr />\n<p><sup>1</sup> for simplicity I'm assuming a uniform prior... the prior isn't the issue here</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cd5JyRxg8stpC4JFd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 2, "extendedScore": null, "score": 5.783061022795151e-07, "legacy": true, "legacyId": "2723", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The self-indication assumption (SIA) states that</p>\n<p style=\"padding-left: 30px;\">Given the fact that you exist, you should (other things equal) favor hypotheses according to which many observers exist over hypotheses on which few observers exist.</p>\n<p>The reason this is a bad assumption might not be obvious at first.&nbsp; In fact, I think it's very easy to miss.</p>\n<p><strong id=\"Argument_for_SIA_posted_on_Less_Wrong\">Argument for SIA posted on Less Wrong<br></strong></p>\n<p>First, let's take a look at a argument for SIA that appeared at Less Wrong (<a href=\"/lw/18r/avoiding_doomsday_a_proof_of_the_selfindication/\">link</a>).&nbsp; Two situations are considered.</p>\n<p style=\"padding-left: 30px;\">1.&nbsp; we imagine that there are 99 people in rooms that have a blue door on the outside (1 person per room).&nbsp; One person is in a room with a red door on the outside.&nbsp; It was argued that you are in a blue door room with probability 0.99.</p>\n<p style=\"padding-left: 30px;\">2.&nbsp; Same situation as above, but first a coin is flipped.&nbsp; If heads, the red door person is never created.&nbsp; If tails, the blue door people are never created.&nbsp; You wake up in a room and know these facts.&nbsp; It was argued that you are in a blue door room with probability 0.99.</p>\n<p>So why is 1. correct and 2. incorrect?&nbsp; The first thing we have to be careful about is not treating yourself as special.&nbsp; The fact that you woke up just tells you that at least one conscious observer exists.&nbsp;</p>\n<p>In scenario 1 we basically just need to know what proportion of conscious observers are in a blue door room.&nbsp; The answer is 0.99.</p>\n<p>In scenario 2 you never would have woken up in a room if you hadn't been created.&nbsp; Thus, the fact that you exist is something we have to take into account.&nbsp; We don't want to estimate P(randomly selected person, regardless of if they exist or not, is in a blue door room).&nbsp; That would be ignoring the fact that you exist.&nbsp; Instead, the fact that you exist tells us that at least one conscious observer exists.&nbsp; Again, we want to know what proportion of conscious observers are in blue door rooms.&nbsp; Well, there is a 50% chance (if heads landed) that all conscious observers are in blue door rooms, and a 50% chance that all conscious observers are in red door rooms.&nbsp; Thus, the marginal probability of a conscious observer being in a blue door room is 0.5.</p>\n<p>The flaw in the more detailed Less Wrong proof (<a href=\"/lw/18r/avoiding_doomsday_a_proof_of_the_selfindication/\">see the post</a>) is when they go from step C to step D.&nbsp; The *you* being referred to in step A might not exist to be asked the question in step D.&nbsp; You have to take that into account.</p>\n<p><strong id=\"General_argument_for_SIA_and_why_it_s_wrong\">General argument for SIA and why it's wrong</strong></p>\n<p>Let's consider the assumption more formally.</p>\n<p>Assume that the number of people to be created, N, is a random draw from a discrete uniform distribution<sup>1</sup> on {1,2,...,N<sub>max</sub>}.&nbsp; Thus, P(N=k)=1/N<sub>max</sub>, for k=1,...,N<sub>max</sub>.&nbsp; Assume N<sub>max </sub>is large enough so that we can effectively ignore finite sample issues (this is just for simplicity).</p>\n<p>Assume M= N<sub>max</sub>*(N<sub>max</sub>+1)/2 possible people exist, and we arbitrarily label them 1,...,M.&nbsp; After the size of the world, say N=n, is determined, then we randomly draw n people from the M possible people.</p>\n<p>After the data are collected we find out that person x exists.</p>\n<p>We can apply Bayes' theorem to get the posterior probability:</p>\n<p style=\"padding-left: 30px;\">P(N=k|x exists)=k/M, for k=1,...,N<sub>max.</sub></p>\n<p>The prior probability was uniform, but the posterior favors larger worlds.&nbsp; QED.</p>\n<p>Well, not really.</p>\n<p>The flaw here is that we conditioned on person x existing, but person x only became of interest <em>after</em> we saw that they existed (peeked at the data).</p>\n<p>What we really know is that at least one conscious observer exists -- there is nothing special about person x.</p>\n<p>So, the correct conditional probability is:</p>\n<p style=\"padding-left: 30px;\">P(N=k|someone exists)=1/N<sub>max</sub>, for k=1,...,N<sub>max.</sub></p>\n<p>Thus, prior=posterior and SIA is wrong.</p>\n<p><strong id=\"Egotism\">Egotism</strong></p>\n<p>The flaw with SIA that I highlighted here is it treats you as special, as if you were labeled ahead of time.&nbsp; But the reality is, no matter who was selected, they would think they are the special person.&nbsp; \"But <em>I</em> exist, I'm not just some arbitrary person.&nbsp; That couldn't happen in small world.&nbsp; It's too unlikely.\"&nbsp; In reality, that fact that I exist just means someone exists. I only became special after I already existed (peeked at the data and used it to construct the conditional probability).</p>\n<p>Here's another way to look at it.&nbsp; Imagine that a random number between 1 and 1 trillion was drawn.&nbsp; Suppose 34,441 was selected.&nbsp; If someone then asked what the probability of selecting <em>that</em> number was, the correct answer is 1 in 1 trillion.&nbsp; They could then argue, \"that's too unlikely of an event.&nbsp; It couldn't have happened by chance.\"&nbsp; However, because they didn't identify the number(s) of interest ahead of time, all we really can conclude is that <em>a</em> number was drawn, and drawing <em>a</em> number was a probability 1 event.</p>\n<p>I give more examples of this <a href=\"http://neq1.wordpress.com/2010/04/11/self-indication-assumption/\">here</a>.</p>\n<p>I think Nick Bostrom is getting at the same thing in <a href=\"http://www.anthropic-principle.com/book/\">his book</a> (page 125):</p>\n<p style=\"padding-left: 30px;\">..your own existence is not in general a ground for thinking that hypotheses are more likely to be true just by virtue of implying that there is a greater total number of observers. The datum of your existence tends to disconfirm hypotheses on which it would be unlikely that any observers (in your reference class) should exist; but that\u2019s as far as it goes. The reason for this is that the sample at hand\u2014you\u2014should not be thought of as randomly selected from the class of all possible observers but only from a class of observers who will actually have existed. It is, so to speak, not a coincidence that the sample you are considering is one that actually exists. Rather, that\u2019s a logical consequence of the fact that only actual observers actually view themselves as samples from anything at all</p>\n<p>Related arguments are made in <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\">this LessWrong post</a>. &nbsp;</p>\n<hr>\n<p><sup>1</sup> for simplicity I'm assuming a uniform prior... the prior isn't the issue here</p>", "sections": [{"title": "Argument for SIA posted on Less Wrong", "anchor": "Argument_for_SIA_posted_on_Less_Wrong", "level": 1}, {"title": "General argument for SIA and why it's wrong", "anchor": "General_argument_for_SIA_and_why_it_s_wrong", "level": 1}, {"title": "Egotism", "anchor": "Egotism", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "24 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5A9x74mgCwJwSg4sN", "ZTEkZNLrmycNuCNYq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-16T06:21:40.567Z", "modifiedAt": null, "url": null, "title": "The Concepts Problem", "slug": "the-concepts-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:00.291Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MLubomnpt8tRXEQMy/the-concepts-problem", "pageUrlRelative": "/posts/MLubomnpt8tRXEQMy/the-concepts-problem", "linkUrl": "https://www.lesswrong.com/posts/MLubomnpt8tRXEQMy/the-concepts-problem", "postedAtFormatted": "Friday, April 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Concepts%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Concepts%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMLubomnpt8tRXEQMy%2Fthe-concepts-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Concepts%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMLubomnpt8tRXEQMy%2Fthe-concepts-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMLubomnpt8tRXEQMy%2Fthe-concepts-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 919, "htmlBody": "<p><em>I'm not sure how obvious the following is to people, and it probably is obvious to most of the people thinking about FAI. But just thought I'd throw out a summary of it here anyway, since this is the one topic that makes me the most pessimistic about the notion of Friendly AI being possible. At least one based heavily on theory and not plenty of experimentation.</em>\r\n<p>A mind can only represent a complex concept X by embedding it into a  tightly intervowen network of other concepts that combine to give X its  meaning. For instance, a \"cat\" is playful, four-legged, feline, a  predator, has a tail, and so forth. These are the concepts that define  what it means to be a cat; by itself, \"cat\" is nothing but a complex set  of links defining how it relates to these other concepts. (As well as a  set of links to memories about cats.) But then, none of those concepts  means anything in isolation, either. A \"predator\" is a specific  biological and behavioral class, the members of which hunt other animals  for food. Of that definition, \"biological\" pertains to \"biology\", <a href=\"http://en.wikipedia.org/wiki/Biology\">which is</a> a \"natural  science concerned with the study of life and living organisms, including  their structure, function, growth, origin, evolution, distribution, and  taxonomy\". \"Behavior\", on the other hand, \"refers to the actions of an  organism, usually in relation to the environment\". Of those words... and  so on.</p>\r\n<p>It does not seem likely that humans could preprogram an AI with a  ready-made network of concepts. There have been <a href=\"http://en.wikipedia.org/wiki/Cyc\">attempts</a> to build knowledge  ontologies by hand, but any such attempt is both hopelessly slow and  lacking in much of the essential content. Even given a lifetime during  which to work and countless of assistants, could you ever hope to code  everything you knew into a format from which it was possible to employ  that knowledge usefully? Even a worse problem is that the information  would need to be in a format compatible with the AI's own learning  algorithms, so that any new information the AI learnt would fit  seamlessly to the previously-entered database. It does not seem likely  that we can come up with an efficient <a href=\"http://plato.stanford.edu/entries/language-thought/\">language of  thought</a> that can be easily translated into a format that is  intuitive for humans to work with.</p>\r\n<p>Indeed, there are existing plans for AI systems which make the  explicit assumption that the AI's network of knowledge will develop  independently as the system learns, and the concepts in this network  won't necessarily have an easy mapping to those used in human language.  The <a href=\"http://www.opencog.org/wiki/OpenCogPrime:AtomNotation#Names\">OpenCog  wikibook states</a> that:</p>\r\n<blockquote>\r\n<p>Some ConceptNodes and conceptual PredicateNode or SchemaNodes may  correspond with human-language words or phrases like cat, bite, and so  forth. This will be the minority case; more such nodes will correspond  to parts of human-language concepts or fuzzy collections of  human-language concepts. In discussions in this wikibook, however, we  will often invoke the unusual case in which Atoms correspond to  individual human-language concepts. This is because such examples are  the easiest ones to discuss intuitively. The preponderance of named  Atoms in the examples in the wikibook implies no similar preponderance  of named Atoms in the real OpenCog system. It is merely easier to talk  about a hypothetical Atom named \"cat\" than it is about a hypothetical  Atom (internally) named [434]. It is not impossible that a OpenCog  system represents \"cat\" as a single ConceptNode, but it is just as  likely that it will represent \"cat\" as a map composed of many different  nodes without any of these having natural names. Each OpenCog works out  for itself, implicitly, which concepts to represent as single Atoms and  which in distributed fashion.</p>\r\n</blockquote>\r\n<p>Designers of Friendly AI seek to build a machine with a  clearly-defined goal system, one which is guaranteed to preserve the <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">highly complex  values</a> that humans have. But the nature of concepts poses a  challenge for this objective. There seems to be no obvious way of  programming those highly complex goals into the AI right from the  beginning, nor to guarantee that any goals thus preprogrammed will not  end up being drastically reinterpreted as the system learns. We cannot  simply code \"safeguard these human values\" into the AI's utility  function without defining those values in detail, and defining those  values in detail requires us to build the AI with an entire knowledge  network. On a certain conceptual level, the decision theory and goal  system of an AI is separate from its knowledge base; in practice, it  doesn't seem like this would be possible.</p>\r\n<p>The goal might not be impossible, though. Humans do seem to be  pre-programmed with inclinations towards various complex behaviors which  might suggest pre-programmed concepts to various degrees.  Heterosexuality is considerably more common in the population than  homosexuality, though this may have relatively simple causes such as an  inborn preference towards particular body shapes combined with social  conditioning. (Disclaimer: I don't really know anything about the  biology of sexuality, so I'm speculating wildly here.) Most people also  seem to react relatively consistently to different status displays, and  people have collected various lists of complex <a href=\"http://condor.depaul.edu/%7Emfiddler/hyphen/humunivers.htm\">human  universals</a>. The exact method of their transmission remains unknown,  however, as does the role that culture serves in it. It also bears  noting that most so-called \"human universals\" are actually <em>cultural </em>as  opposed to <em>individual </em>universals. In other words, any given  culture might be guaranteed to express them, but there will always be  individuals who don't fit into the usual norms.</p>\r\n<p>See also: Vladimir Nesov discusses a closely related form of this  problem as the \"<a href=\"http://causalityrelay.wordpress.com/2010/02/23/preference-in-unknown-world/\">ontology  problem</a>\".</p>\r\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MLubomnpt8tRXEQMy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 5.783240011465809e-07, "legacy": true, "legacyId": "2727", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-16T20:46:38.533Z", "modifiedAt": null, "url": null, "title": "Attention Lurkers: Please say hi", "slug": "attention-lurkers-please-say-hi", "viewCount": null, "lastCommentedAt": "2021-01-22T07:13:48.692Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yiMa5pCo6i2uN4btu/attention-lurkers-please-say-hi", "pageUrlRelative": "/posts/yiMa5pCo6i2uN4btu/attention-lurkers-please-say-hi", "linkUrl": "https://www.lesswrong.com/posts/yiMa5pCo6i2uN4btu/attention-lurkers-please-say-hi", "postedAtFormatted": "Friday, April 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Attention%20Lurkers%3A%20Please%20say%20hi&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAttention%20Lurkers%3A%20Please%20say%20hi%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyiMa5pCo6i2uN4btu%2Fattention-lurkers-please-say-hi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Attention%20Lurkers%3A%20Please%20say%20hi%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyiMa5pCo6i2uN4btu%2Fattention-lurkers-please-say-hi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyiMa5pCo6i2uN4btu%2Fattention-lurkers-please-say-hi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<p>Some research says that <a href=\"http://en.wikipedia.org/wiki/Lurker\">lurkers make up over 90% of online groups</a>. I suspect that Less Wrong has an even higher percentage of lurkers than other online communities.</p>\n<p>Please post a comment in this thread saying \"Hi.\" You can say more if you want, but just posting \"Hi\" is good for a guaranteed free point of karma.</p>\n<p>Also see the <a href=\"/lw/b9/welcome_to_less_wrong/\">introduction thread</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yiMa5pCo6i2uN4btu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 48, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "2149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 638, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CG9AEXwSjdrXPBEZ9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-17T03:23:46.520Z", "modifiedAt": null, "url": null, "title": "Eluding Attention Hijacks", "slug": "eluding-attention-hijacks", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:56.605Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ABranco", "createdAt": "2009-10-12T23:30:03.771Z", "isAdmin": false, "displayName": "ABranco"}, "userId": "yDSTMgBPQXAbxhsp9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qGT8bDusLzTRNGgn9/eluding-attention-hijacks", "pageUrlRelative": "/posts/qGT8bDusLzTRNGgn9/eluding-attention-hijacks", "linkUrl": "https://www.lesswrong.com/posts/qGT8bDusLzTRNGgn9/eluding-attention-hijacks", "postedAtFormatted": "Saturday, April 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eluding%20Attention%20Hijacks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEluding%20Attention%20Hijacks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGT8bDusLzTRNGgn9%2Feluding-attention-hijacks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eluding%20Attention%20Hijacks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGT8bDusLzTRNGgn9%2Feluding-attention-hijacks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGT8bDusLzTRNGgn9%2Feluding-attention-hijacks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2345, "htmlBody": "<blockquote>\n<p><em>Do my taxes? Oh, no! It&rsquo;s not going to be that easy. It&rsquo;s going to be different this year, I&rsquo;m sure. I saw the forms&mdash;they look different. There are probably new rules I&rsquo;m going to have to figure out. I might need to read all that damn material. Long form, short form, medium form? File together, file separate? We&rsquo;ll probably want to claim deductions, but if we do we&rsquo;ll have to back them up, and that means we&rsquo;ll need all the receipts. Oh, my God&mdash;I don&rsquo;t know if we really have all the receipts we&rsquo;d need, and what if we didn&rsquo;t have all the receipts and claimed the deductions anyway and got audited? Audited? Oh, no&mdash;the IRS&mdash;JAIL!!</em></p>\n<p><em>And so a lot of people put themselves in jail, just glancing at their 1040 tax forms. Because they are so smart, sensitive, and creative.<br /><br />&mdash;David Allen, Getting Things Done</em></p>\n</blockquote>\n<p>&nbsp;</p>\n<h2><strong>Intro</strong></h2>\n<p>Very recently, Roko wrote about <a href=\"/lw/21b/ugh_fields/\">ugh fields</a>, <em>&ldquo;an unconscious flinch we have from even thinking about a serious personal problem. The ugh field forms a self-shadowing blind spot covering an area desperately in need of optimization, imposing huge costs.&rdquo;</em> Suggested antidotes included PJ Eby&rsquo;s technique to engage with the ugh field, locate its center, and access information&mdash;thereupon dissolving the negative emotions.</p>\n<p>I want to explore here something else that prevents us from doing what we want. Consider these situations:<br /><br /><strong>Situation 1</strong><br />You attack a problem that is at least slightly complex (distasteful or not), but are unable to systematically tackle it step by step because your mind keeps <em>diverging wildly within the problem</em>. Your brain starts running simulations and gets stuck. To make things worse, you are biased towards thinking of the worst possible scenarios. Having visualized 30 steps ahead, you panic and do nothing. David Allen's quote in the introduction of this post illustrates that.<br /><br /><strong>Situation 2</strong><br />You attack a problem of any complexity&mdash;anything you need to get done&mdash;and your mind keeps <em>diverging to different directions outside the problem</em>. Examples:<br /><br /><strong>a. </strong>You decide you need to quickly send an important email before an appointment. You log in. Thirty minutes later, you find yourself watching some motivational Powerpoint presentation your uncle sent you. You stare at the inbox and can't remember what you were doing there in the first place. You log out without sending the email, and leave late to your appointment.*<br /><br /><strong>b. </strong>You're working on your computer and some kid playing outside the window brings you vague memories of your childhood, vacations, your father teaching you how to fish, tilapias, earthworms, digging the earth, dirty hands, antibacterial soaps, swine flu, airport announcements, seatbelts, sexual fantasies with that redheaded flight attendant from that flight to Barcelona, and ... \"wait, wait, wait! I am losing focus, I need to get this done.\" Ten minutes had passed (or was it more?).<br /><br />Repeat this phenomenon many times a day and you won't have gone too far.<br /><br />What happened?<br /><br />While I am aware that situations 1 and 2 are a bit different in nature (anxiety because of &ldquo;seeing too much into the problem&rdquo; vs. distraction to other problems), it seems to me that both bear something very fundamental in common. In all those situations, you became less efficient to get things done because your sensitivity permitted your attention to be deviated to easily. You suffered what I shall call an <strong>attention hijack.</strong></p>\n<p><a id=\"more\"></a></p>\n<h2><br /></h2>\n<h2>Etiology</h2>\n<p>Why does this happen? Let&rsquo;s see.<br /><br />First, we have stimuli coming from your senses: what you see, hear, smell and feel trigger thoughts. The capture of external stimuli just happens: it&rsquo;s automatic. To (try to) ignore it, we need to spend some energy.<br /><br />Second, we must remember that our brain does not have a Central Processing Unit that we can call &ldquo;me being in total control&rdquo;. What we have are separate processing units running in parallel. That means that a part of you is trying to accomplish a task, and part of you is getting distracted into the future or your co-workers chat.<br /><br />An aspect of this phenomenon is that some people are much more distracted than others. And it seems that it is specifically the most smart, sensitive, and creative people who suffer from it most often.<br /><br />Quoting again from David Allen:<br /><em></em></p>\n<blockquote>\n<p><em>Often it&rsquo;s the insensitive oafs who just take something and start plodding forward, unaware of all the things that could go wrong. Everyone else tends to get hung about all kinds of things.<br /></em></p>\n</blockquote>\n<p>It also happens that those very sensitive ones tend also to be the ones with the most disorganized lives.</p>\n<p>&nbsp;</p>\n<h2>Why are attention hijacks bad, and why do we want to elude them?</h2>\n<p>First, because you waste time: directly, because the diverting thoughts prevent you to get things done; and indirectly, because complex problems need to be loaded in your memory, and demands your concentration to &ldquo;grasp&rdquo; the big picture, which gets disrupted by an attention hijack.<br /><br />Another negative impact is the emergence of bad feelings, such as the sensation of being overwhelmed by too many tasks and ideas, or the sensation of unaccomplishment in general. Those feelings could escalate and turn into ugh fields.<br /><br />By protecting yourself, you could be able to do things more efficiently. You&rsquo;d be able to (a) go deeper in more complex problems; (b) have more free time, and/or be able to do more; (c) better enjoy the execution of tasks by achieving flow.</p>\n<h2><br /></h2>\n<h2>Some strategies to circumvent attention hijacks</h2>\n<p>It might sound very tempting to prove our incredible powers and face the disturbances directly. It is actually an entertaining exercise in several situations. <a href=\"http://en.wikipedia.org/wiki/Josh_Waitzkin\">Josh Waitzkin</a>, for example, had to learn to leverage disturbances for his own benefit, or he wouldn&rsquo;t have been an international-level chessmaster and pushing-hands champion. It is possible, it is doable. <br /><br />You might want to train yourself, like Josh, but that is only an option. It takes time and energy, anyway, and he had to do that because he had no alternative: all kinds of disturbances would appear in championships, he had to face them. As a general rule, however, it seems wise to acknowledge that your brain has its bugs, and therefore insert this information in your model of the world.<br /><br />To protect yourself from an attention hijack, you need to seal yourself from whatever triggers the deviation of your attention in directions you don't want. You want to think and be creative only about whatever your next step is. You must forget the rest of the world for a while.<br /><br />Operationally, you're in a certain way trying to deceive yourself. That is not irrational: strategically, you know exactly what you are doing.<br /><br />As Taleb wrote in Fooled by Randomness:</p>\n<blockquote>\n<p><em>In book 12 of the Odyssey, the hero encounters the sirens (...). He fills the ears of all his men with wax, to the point of total deafness, and has himself tied to the mast. The sailors are under strict instructions not to release him. As they approach the sirens' island, the sea is calm and over the water comes the sound of a music so ravishing that Odysseus struggles to get loose, expending an inordinate amount of energy to unrestrain himself. His men tie him even further, until they are safely past the poisoned sounds.</em></p>\n<p><em>The first lesson I took from the story is not to even attempt to be Odysseus. He is a mythological character and I am not. He can be tied to the mast; I can merely reach the rank of a sailor who needs to have his ears filled with wax.<br />(...)<br />Wax in my ears. The epiphany I had in my career in randomness came when I understood that I was not intelligent enough, nor strong enough, to even try to fight my emotions. Besides, I believe that I need my emotions to formulate my ideas and get the energy to execute them. (...)</em></p>\n</blockquote>\n<p>This <a href=\"http://upload.wikimedia.org/wikipedia/commons/8/8d/John_William_Waterhouse_-_Ulysses_and_the_Sirens_%281891%29.jpg\">beautiful illustration of Odysseus' adventure</a> will look familiar to many readers of this blog.</p>\n<p><br /><strong>Disclaimer: </strong>The following list of suggestions of how to block attention hijacks are a consequence of my own personal experience. I tried to make it as systematic as possible, but please bear in mind that the categories are not intended to be completely <a href=\"http://en.wikipedia.org/wiki/MECE_principle\">MECE</a>, nor the examples are to be evaluated as the only possibilities. Some sources might be missing or unreliable&mdash;but if it is included here is because I had a personal positive experience with the technique, nevertheless.<br /><br />I am aware that you might find overlap with some <a href=\"/lw/1sm/akrasia_tactics_review\">techniques previously posted in Less Wrong</a>, as the avoidance of attention hijacks is a focusing method&mdash;which is normally <a href=\"/lw/1tu/improving_the_akrasia_hypothesis/\">encompassed by the definition of akrasia</a>.<br /><br />I would love to hear your own tricks, too.</p>\n<p>&nbsp;</p>\n<h3><em>First step: block the environment</em></h3>\n<p><strong>Block noise: </strong>I have been delighted to notice how much my concentration is improved just by using earplugs. Find your type. I like the ones in orange foam, but the moldable soft silicone is unbeatable. They are both cheap.<br /><br /><strong>Block sight: </strong>I realized that this is much less obvious to most people. Our mind is all the time absorbing data that comes from our peripheral vision and processing it. This requires energy. With time, our baseline became to work &ldquo;with some noise, with some clutter, with some decoration, with some people walking around&rdquo;. For minds used to see patterns and be creative, any element not directly involved with the task you want to get done is a potential attention hijacker. <a href=\"http://xkcd.com/356\">You don&rsquo;t want to be sniped</a>, do you?<br /><br />Try this: declutter your environment, make it simple. (If I could have my background in a deep white, as in the Matrix, I would.) Remove both uncomfortable and attracting visual cues. Try to be somewhere where there are no movements around, no people passing. Remember that horses wear <a href=\"http://en.wikipedia.org/wiki/Blinders\">blinders</a> for a reason. You might be more similar to horses than you thought.<br /><br /><strong>Block the entrance of new issues:</strong><br /><em><strong>a. Block interruptions from people: </strong></em>tell people you don&rsquo;t like being interrupted when <em>[insert your personal criteria here]</em>. If you explain, most people will understand. Some won&rsquo;t&mdash;deal with that, too. Be able to say no, and then get to them later.<br /><em><br /></em><strong><em>b. Do not open potential Pandora&rsquo;s boxes while working on a task:</em> </strong>do one thing at a time and avoid multitasking. Do not start doing something else, finish whatever you are doing first. And a widely ignored tip is to not provoke deliberation before you can take action&mdash;Tim Ferriss gives a good example:<br /><em></em></p>\n<blockquote>\n<p><em>Don&rsquo;t scan the inbox on Friday evening or over the weekend if you might encounter work problems that can&rsquo;t be addressed until Monday. Is your weekend really &ldquo;free&rdquo; if you find a crisis in the inbox Saturday morning that you can&rsquo;t address until Monday morning? Even if the inbox scan lasts 30 seconds, the preoccupation and forward projection for the subsequent 48 hours effectively deletes that experience from your life. You had time but you didn&rsquo;t have attention, so the time had no practical value.</em></p>\n</blockquote>\n<p><em><strong>c. Practice relinquishing your need for control:</strong></em> the moment you realize you are paying attention to only one thing, a part of you might yell inside: &ldquo;Hey, you are losing control of the big picture? What if someone sent you an important email? What if the conversation your co-workers are having is relevant? What if today&rsquo;s newspaper have something to tell me?&rdquo; So, part of your curiosity might be actually a discomfort with related to the feeling of losing control. How exactly to deal with that is not part of the scope of this post, though.</p>\n<h3><br /></h3>\n<h3>Second step: block your own thoughts</h3>\n<p><strong>Make it harder for the thought to show up in the first place: </strong>blocking &ldquo;noise&rdquo; tends to be very helpful, but might not be sufficient, or even necessary if you can click into a flow state easily. <a href=\"http://en.wikipedia.org/wiki/Flow_%28psychology%29\">Flow</a> makes you naturally more concentrated and immune to the environment. There are many things you can do to achieve flow. I highlight two: (a) make it challenging and (b) batch tasks.<br /><br />One cool way to make the task more challenging, especially if it&rsquo;s a physical one, is by executing it fast and timeboxing it, as if you were in a competition. Doing something faster demands more concentration, which blocks hijacking. And, of course, you&rsquo;ll do it faster. Just try it. Can you imagine an Olympic swimmer thinking of anything else than perfecting his movements while he&rsquo;s competing?<br /><br />Batching tasks is attractive because you reduce the number of times you need to load a problem. It&rsquo;s easier to get distracted when you keep switching between activities that are different in nature.<br /><br /><strong>If thoughts still show up that are unrelated to your next action:</strong><br />Does it seem relevant? If not, try to ignore it. Meditation helps here. Self-awareness. <a href=\"/lw/1xh/living_luminously/\">Luminosity</a>.<br /><br />If relevant, than you have to write it down and move on. It might be something confusing like the &ldquo;Do my taxes? Oh, no!&rdquo; thing-y, in which case you need to find a systematic approach to tackle it step by step, writing down your thoughts and ideas. It might also be a task or a concern unrelated to the problem&mdash;same thing: write it down, check it later. There aren&rsquo;t many choices here: either one has a reliable GTD-like system to collect ideas and thoughts; or one needs to be okay with letting go of one&rsquo;s important thoughts.</p>\n<h3><br /></h3>\n<h3>Anything important that I might have missed? Please, comment.</h3>\n<p><br /><em>* This feeling of disorientation experienced when you wake up in your email after an attention hijack has been named Inbox Alzheimer.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EuDw6uxQW2ZBRFhMo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qGT8bDusLzTRNGgn9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 24, "extendedScore": null, "score": 5.785758081812505e-07, "legacy": true, "legacyId": "2735", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p><em>Do my taxes? Oh, no! It\u2019s not going to be that easy. It\u2019s going to be different this year, I\u2019m sure. I saw the forms\u2014they look different. There are probably new rules I\u2019m going to have to figure out. I might need to read all that damn material. Long form, short form, medium form? File together, file separate? We\u2019ll probably want to claim deductions, but if we do we\u2019ll have to back them up, and that means we\u2019ll need all the receipts. Oh, my God\u2014I don\u2019t know if we really have all the receipts we\u2019d need, and what if we didn\u2019t have all the receipts and claimed the deductions anyway and got audited? Audited? Oh, no\u2014the IRS\u2014JAIL!!</em></p>\n<p><em>And so a lot of people put themselves in jail, just glancing at their 1040 tax forms. Because they are so smart, sensitive, and creative.<br><br>\u2014David Allen, Getting Things Done</em></p>\n</blockquote>\n<p>&nbsp;</p>\n<h2 id=\"Intro\"><strong>Intro</strong></h2>\n<p>Very recently, Roko wrote about <a href=\"/lw/21b/ugh_fields/\">ugh fields</a>, <em>\u201can unconscious flinch we have from even thinking about a serious personal problem. The ugh field forms a self-shadowing blind spot covering an area desperately in need of optimization, imposing huge costs.\u201d</em> Suggested antidotes included PJ Eby\u2019s technique to engage with the ugh field, locate its center, and access information\u2014thereupon dissolving the negative emotions.</p>\n<p>I want to explore here something else that prevents us from doing what we want. Consider these situations:<br><br><strong>Situation 1</strong><br>You attack a problem that is at least slightly complex (distasteful or not), but are unable to systematically tackle it step by step because your mind keeps <em>diverging wildly within the problem</em>. Your brain starts running simulations and gets stuck. To make things worse, you are biased towards thinking of the worst possible scenarios. Having visualized 30 steps ahead, you panic and do nothing. David Allen's quote in the introduction of this post illustrates that.<br><br><strong>Situation 2</strong><br>You attack a problem of any complexity\u2014anything you need to get done\u2014and your mind keeps <em>diverging to different directions outside the problem</em>. Examples:<br><br><strong>a. </strong>You decide you need to quickly send an important email before an appointment. You log in. Thirty minutes later, you find yourself watching some motivational Powerpoint presentation your uncle sent you. You stare at the inbox and can't remember what you were doing there in the first place. You log out without sending the email, and leave late to your appointment.*<br><br><strong>b. </strong>You're working on your computer and some kid playing outside the window brings you vague memories of your childhood, vacations, your father teaching you how to fish, tilapias, earthworms, digging the earth, dirty hands, antibacterial soaps, swine flu, airport announcements, seatbelts, sexual fantasies with that redheaded flight attendant from that flight to Barcelona, and ... \"wait, wait, wait! I am losing focus, I need to get this done.\" Ten minutes had passed (or was it more?).<br><br>Repeat this phenomenon many times a day and you won't have gone too far.<br><br>What happened?<br><br>While I am aware that situations 1 and 2 are a bit different in nature (anxiety because of \u201cseeing too much into the problem\u201d vs. distraction to other problems), it seems to me that both bear something very fundamental in common. In all those situations, you became less efficient to get things done because your sensitivity permitted your attention to be deviated to easily. You suffered what I shall call an <strong>attention hijack.</strong></p>\n<p><a id=\"more\"></a></p>\n<h2><br></h2>\n<h2 id=\"Etiology\">Etiology</h2>\n<p>Why does this happen? Let\u2019s see.<br><br>First, we have stimuli coming from your senses: what you see, hear, smell and feel trigger thoughts. The capture of external stimuli just happens: it\u2019s automatic. To (try to) ignore it, we need to spend some energy.<br><br>Second, we must remember that our brain does not have a Central Processing Unit that we can call \u201cme being in total control\u201d. What we have are separate processing units running in parallel. That means that a part of you is trying to accomplish a task, and part of you is getting distracted into the future or your co-workers chat.<br><br>An aspect of this phenomenon is that some people are much more distracted than others. And it seems that it is specifically the most smart, sensitive, and creative people who suffer from it most often.<br><br>Quoting again from David Allen:<br><em></em></p>\n<blockquote>\n<p><em>Often it\u2019s the insensitive oafs who just take something and start plodding forward, unaware of all the things that could go wrong. Everyone else tends to get hung about all kinds of things.<br></em></p>\n</blockquote>\n<p>It also happens that those very sensitive ones tend also to be the ones with the most disorganized lives.</p>\n<p>&nbsp;</p>\n<h2 id=\"Why_are_attention_hijacks_bad__and_why_do_we_want_to_elude_them_\">Why are attention hijacks bad, and why do we want to elude them?</h2>\n<p>First, because you waste time: directly, because the diverting thoughts prevent you to get things done; and indirectly, because complex problems need to be loaded in your memory, and demands your concentration to \u201cgrasp\u201d the big picture, which gets disrupted by an attention hijack.<br><br>Another negative impact is the emergence of bad feelings, such as the sensation of being overwhelmed by too many tasks and ideas, or the sensation of unaccomplishment in general. Those feelings could escalate and turn into ugh fields.<br><br>By protecting yourself, you could be able to do things more efficiently. You\u2019d be able to (a) go deeper in more complex problems; (b) have more free time, and/or be able to do more; (c) better enjoy the execution of tasks by achieving flow.</p>\n<h2><br></h2>\n<h2 id=\"Some_strategies_to_circumvent_attention_hijacks\">Some strategies to circumvent attention hijacks</h2>\n<p>It might sound very tempting to prove our incredible powers and face the disturbances directly. It is actually an entertaining exercise in several situations. <a href=\"http://en.wikipedia.org/wiki/Josh_Waitzkin\">Josh Waitzkin</a>, for example, had to learn to leverage disturbances for his own benefit, or he wouldn\u2019t have been an international-level chessmaster and pushing-hands champion. It is possible, it is doable. <br><br>You might want to train yourself, like Josh, but that is only an option. It takes time and energy, anyway, and he had to do that because he had no alternative: all kinds of disturbances would appear in championships, he had to face them. As a general rule, however, it seems wise to acknowledge that your brain has its bugs, and therefore insert this information in your model of the world.<br><br>To protect yourself from an attention hijack, you need to seal yourself from whatever triggers the deviation of your attention in directions you don't want. You want to think and be creative only about whatever your next step is. You must forget the rest of the world for a while.<br><br>Operationally, you're in a certain way trying to deceive yourself. That is not irrational: strategically, you know exactly what you are doing.<br><br>As Taleb wrote in Fooled by Randomness:</p>\n<blockquote>\n<p><em>In book 12 of the Odyssey, the hero encounters the sirens (...). He fills the ears of all his men with wax, to the point of total deafness, and has himself tied to the mast. The sailors are under strict instructions not to release him. As they approach the sirens' island, the sea is calm and over the water comes the sound of a music so ravishing that Odysseus struggles to get loose, expending an inordinate amount of energy to unrestrain himself. His men tie him even further, until they are safely past the poisoned sounds.</em></p>\n<p><em>The first lesson I took from the story is not to even attempt to be Odysseus. He is a mythological character and I am not. He can be tied to the mast; I can merely reach the rank of a sailor who needs to have his ears filled with wax.<br>(...)<br>Wax in my ears. The epiphany I had in my career in randomness came when I understood that I was not intelligent enough, nor strong enough, to even try to fight my emotions. Besides, I believe that I need my emotions to formulate my ideas and get the energy to execute them. (...)</em></p>\n</blockquote>\n<p>This <a href=\"http://upload.wikimedia.org/wikipedia/commons/8/8d/John_William_Waterhouse_-_Ulysses_and_the_Sirens_%281891%29.jpg\">beautiful illustration of Odysseus' adventure</a> will look familiar to many readers of this blog.</p>\n<p><br><strong>Disclaimer: </strong>The following list of suggestions of how to block attention hijacks are a consequence of my own personal experience. I tried to make it as systematic as possible, but please bear in mind that the categories are not intended to be completely <a href=\"http://en.wikipedia.org/wiki/MECE_principle\">MECE</a>, nor the examples are to be evaluated as the only possibilities. Some sources might be missing or unreliable\u2014but if it is included here is because I had a personal positive experience with the technique, nevertheless.<br><br>I am aware that you might find overlap with some <a href=\"/lw/1sm/akrasia_tactics_review\">techniques previously posted in Less Wrong</a>, as the avoidance of attention hijacks is a focusing method\u2014which is normally <a href=\"/lw/1tu/improving_the_akrasia_hypothesis/\">encompassed by the definition of akrasia</a>.<br><br>I would love to hear your own tricks, too.</p>\n<p>&nbsp;</p>\n<h3 id=\"First_step__block_the_environment\"><em>First step: block the environment</em></h3>\n<p><strong>Block noise: </strong>I have been delighted to notice how much my concentration is improved just by using earplugs. Find your type. I like the ones in orange foam, but the moldable soft silicone is unbeatable. They are both cheap.<br><br><strong>Block sight: </strong>I realized that this is much less obvious to most people. Our mind is all the time absorbing data that comes from our peripheral vision and processing it. This requires energy. With time, our baseline became to work \u201cwith some noise, with some clutter, with some decoration, with some people walking around\u201d. For minds used to see patterns and be creative, any element not directly involved with the task you want to get done is a potential attention hijacker. <a href=\"http://xkcd.com/356\">You don\u2019t want to be sniped</a>, do you?<br><br>Try this: declutter your environment, make it simple. (If I could have my background in a deep white, as in the Matrix, I would.) Remove both uncomfortable and attracting visual cues. Try to be somewhere where there are no movements around, no people passing. Remember that horses wear <a href=\"http://en.wikipedia.org/wiki/Blinders\">blinders</a> for a reason. You might be more similar to horses than you thought.<br><br><strong>Block the entrance of new issues:</strong><br><em><strong>a. Block interruptions from people: </strong></em>tell people you don\u2019t like being interrupted when <em>[insert your personal criteria here]</em>. If you explain, most people will understand. Some won\u2019t\u2014deal with that, too. Be able to say no, and then get to them later.<br><em><br></em><strong><em>b. Do not open potential Pandora\u2019s boxes while working on a task:</em> </strong>do one thing at a time and avoid multitasking. Do not start doing something else, finish whatever you are doing first. And a widely ignored tip is to not provoke deliberation before you can take action\u2014Tim Ferriss gives a good example:<br><em></em></p>\n<blockquote>\n<p><em>Don\u2019t scan the inbox on Friday evening or over the weekend if you might encounter work problems that can\u2019t be addressed until Monday. Is your weekend really \u201cfree\u201d if you find a crisis in the inbox Saturday morning that you can\u2019t address until Monday morning? Even if the inbox scan lasts 30 seconds, the preoccupation and forward projection for the subsequent 48 hours effectively deletes that experience from your life. You had time but you didn\u2019t have attention, so the time had no practical value.</em></p>\n</blockquote>\n<p><em><strong>c. Practice relinquishing your need for control:</strong></em> the moment you realize you are paying attention to only one thing, a part of you might yell inside: \u201cHey, you are losing control of the big picture? What if someone sent you an important email? What if the conversation your co-workers are having is relevant? What if today\u2019s newspaper have something to tell me?\u201d So, part of your curiosity might be actually a discomfort with related to the feeling of losing control. How exactly to deal with that is not part of the scope of this post, though.</p>\n<h3><br></h3>\n<h3 id=\"Second_step__block_your_own_thoughts\">Second step: block your own thoughts</h3>\n<p><strong>Make it harder for the thought to show up in the first place: </strong>blocking \u201cnoise\u201d tends to be very helpful, but might not be sufficient, or even necessary if you can click into a flow state easily. <a href=\"http://en.wikipedia.org/wiki/Flow_%28psychology%29\">Flow</a> makes you naturally more concentrated and immune to the environment. There are many things you can do to achieve flow. I highlight two: (a) make it challenging and (b) batch tasks.<br><br>One cool way to make the task more challenging, especially if it\u2019s a physical one, is by executing it fast and timeboxing it, as if you were in a competition. Doing something faster demands more concentration, which blocks hijacking. And, of course, you\u2019ll do it faster. Just try it. Can you imagine an Olympic swimmer thinking of anything else than perfecting his movements while he\u2019s competing?<br><br>Batching tasks is attractive because you reduce the number of times you need to load a problem. It\u2019s easier to get distracted when you keep switching between activities that are different in nature.<br><br><strong>If thoughts still show up that are unrelated to your next action:</strong><br>Does it seem relevant? If not, try to ignore it. Meditation helps here. Self-awareness. <a href=\"/lw/1xh/living_luminously/\">Luminosity</a>.<br><br>If relevant, than you have to write it down and move on. It might be something confusing like the \u201cDo my taxes? Oh, no!\u201d thing-y, in which case you need to find a systematic approach to tackle it step by step, writing down your thoughts and ideas. It might also be a task or a concern unrelated to the problem\u2014same thing: write it down, check it later. There aren\u2019t many choices here: either one has a reliable GTD-like system to collect ideas and thoughts; or one needs to be okay with letting go of one\u2019s important thoughts.</p>\n<h3><br></h3>\n<h3 id=\"Anything_important_that_I_might_have_missed__Please__comment_\">Anything important that I might have missed? Please, comment.</h3>\n<p><br><em>* This feeling of disorientation experienced when you wake up in your email after an attention hijack has been named Inbox Alzheimer.</em></p>", "sections": [{"title": "Intro", "anchor": "Intro", "level": 1}, {"title": "Etiology", "anchor": "Etiology", "level": 1}, {"title": "Why are attention hijacks bad, and why do we want to elude them?", "anchor": "Why_are_attention_hijacks_bad__and_why_do_we_want_to_elude_them_", "level": 1}, {"title": "Some strategies to circumvent attention hijacks", "anchor": "Some_strategies_to_circumvent_attention_hijacks", "level": 1}, {"title": "First step: block the environment", "anchor": "First_step__block_the_environment", "level": 2}, {"title": "Second step: block your own thoughts", "anchor": "Second_step__block_your_own_thoughts", "level": 2}, {"title": "Anything important that I might have missed? Please, comment.", "anchor": "Anything_important_that_I_might_have_missed__Please__comment_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EFQ3F6kmt4WHXRqik", "rRmisKb45dN7DK4BW", "uKoqrgnRoWjhneDvM", "9o3Cjjem7AbmmZfBs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-17T20:23:05.253Z", "modifiedAt": null, "url": null, "title": "VNM expected utility theory: uses, abuses, and interpretation", "slug": "vnm-expected-utility-theory-uses-abuses-and-interpretation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:00.907Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YCMfQoqqi2o9Tjwoa/vnm-expected-utility-theory-uses-abuses-and-interpretation", "pageUrlRelative": "/posts/YCMfQoqqi2o9Tjwoa/vnm-expected-utility-theory-uses-abuses-and-interpretation", "linkUrl": "https://www.lesswrong.com/posts/YCMfQoqqi2o9Tjwoa/vnm-expected-utility-theory-uses-abuses-and-interpretation", "postedAtFormatted": "Saturday, April 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20VNM%20expected%20utility%20theory%3A%20uses%2C%20abuses%2C%20and%20interpretation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVNM%20expected%20utility%20theory%3A%20uses%2C%20abuses%2C%20and%20interpretation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCMfQoqqi2o9Tjwoa%2Fvnm-expected-utility-theory-uses-abuses-and-interpretation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=VNM%20expected%20utility%20theory%3A%20uses%2C%20abuses%2C%20and%20interpretation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCMfQoqqi2o9Tjwoa%2Fvnm-expected-utility-theory-uses-abuses-and-interpretation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCMfQoqqi2o9Tjwoa%2Fvnm-expected-utility-theory-uses-abuses-and-interpretation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2889, "htmlBody": "<!-- VNM utility - uses, abuses, and context -->\n<p><em>When interpreted convservatively, the von Neumann-Morgenstern <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">rationality axioms and utility theorem</a> are an indispensible tool for the normative study of rationality, deserving of many <a href=\"/lw/182/the_absentminded_driver/\">thought</a> <a href=\"/lw/1zp/maximise_expected_utility_not_expected_perception/\">experiments</a> and <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">attentive</a> <a href=\"/lw/15m/towards_a_new_decision_theory/\">decision</a> <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">theory</a>.&nbsp; It's one more reason I'm glad to be born after the 1940s. Yet there is apprehension about its validity, aside from merely confusing it with Bentham utilitarianism (as highlighted by <a href=\"/lw/1hw/the_difference_between_utility_and_utility/\">Matt Simpson</a>).&nbsp; I want to describe not only what VNM utility is really meant for, but a contextual reinterpretation of its meaning, so that it may hopefully be used more frequently, confidently, and appropriately.</em></p>\n<ol>\n<li><a href=\"#pre\">Preliminary discussion and precautions</a></li>\n<li><a href=\"#sha\">Sharing decision utility is sharing power, not welfare</a> </li>\n<li><a href=\"#cont\">Contextual Strength (CS) of preferences, and VNM-preference as \"strong\" preference</a> </li>\n<li><a href=\"#haus\">Hausner (lexicographic) decision utility</a> </li>\n<li><a href=\"#ind\">The independence axiom isn't bad either</a> </li>\n<li><a href=\"#app\">Application to earlier LessWrong discussions of utility</a> </li>\n</ol>\n<h3><a name=\"pre\"></a>1.&nbsp; Preliminary discussion and precautions</h3>\n<p>The idea of John von Neumann and Oskar Mogernstern is that, <em>if</em> you behave a certain way, <em>then</em> it turns out you're maximizing the expected value of a particular function.&nbsp; Very cool!&nbsp; And their description of \"a certain way\" is very compelling: a list of four, reasonable-seeming axioms.&nbsp; If you haven't already, check out the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">Von Neumann-Morgenstern utility theorem</a>, a mathematical result which makes their claim rigorous, and true.</p>\n<p>VNM utility is a <em>decision utility</em>, in that it aims to characterize the decision-making of a rational agent.&nbsp; One great feature is that it implicitly accounts for <a href=\"http://en.wikipedia.org/wiki/Risk_aversion#Absolute_risk_aversion\">risk aversion</a>: not risking $100 for a 10% chance to win $1000 and 90% chance to win $0 just means that for you, utility($100) &gt; 10%utility($1000) + 90%utility($0).&nbsp;</p>\n<p>But as the Wikipedia article explains nicely, VNM utility is:</p>\n<ol>\n<li><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Applicability_to_economics\">not designed to predict</a> the behavior of \"irrational\" individuals (like real people in a real economy); </li>\n<li>not designed to characterize <em>well-being</em>, but to characterize <em>decisions</em>; </li>\n<li>not designed to measure the value of <em>items</em>, but the value of <em>outcomes</em>; </li>\n<li>only defined up to a scalar multiple and additive constant (acting with utility function U(X) is the same as acting with a&middot;U(X)+b, if a&gt;0); </li>\n<li><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Incomparability_between_agents\">not designed to be added up or compared</a> between a number of individuals; </li>\n<li><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Implications_for_ethics_and_moral_philosophy\">not something that can be \"sacrificed\"</a> in favor of others in a meaningful way. </li>\n</ol>\n<p>[<strong>ETA</strong>]&nbsp; Additionally, in the VNM theorem the probabilities are understood to be known to the agent <em>as they are presented</em>, and to come from a source of randomness whose outcomes are <em>not significant</em> to the agent.&nbsp; Without these assumptions, its proof doesn't work.</p>\n<p>Because of (4), one often considers <em>marginal utilities</em> of the form U(X)-U(Y), to cancel the ambiguity in the additive constant b.&nbsp; This is totally legitimate, and faithful to the mathematical conception of VNM utility.</p>\n<p>Because of (5), people often \"normalize\" VNM utility to eliminate ambiguity in both constants, so that utilities are unique numbers that can be added accross multiple agents.&nbsp; One way is to declare that every person in some situation values $1 at 1 utilon (a fictional unit of measure of utility), and $0 at 0.&nbsp; I think a more meaningful and applicable normalization is to fix mean and variance with respect to certain outcomes (next section).</p>\n<p>Because of (6), characterizing the altruism of a VNM-rational agent by how he sacrifices his own VNM utility is the wrong approach.&nbsp; Indeed, such a sacrifice is a contradiction.&nbsp; Kahneman suggests<sup><a href=\"/kahn\">1</a></sup>, and I agree, that <em>something else</em> should be added or substracted to determine the total, comparative, or average well-being of individuals.&nbsp; I'd call it \"<strong>welfare</strong>\", to avoid confusing it with VNM utility.&nbsp; Kahneman calls it E-utility, for \"experienced utility\", a connotation I'll avoid.&nbsp; Intuitively, this is certainly something you could sacrifice for others, or have more of compared to others.&nbsp; True, a given person's VNM utility is likely highly correlated with her personal \"welfare\", but I wouldn't consider it an accurate approximation.&nbsp;</p>\n<p>So if not <em>collective welfare</em>, then what could cross-agent comparisons or sums of VNM utilities indicate?&nbsp; Well, they're meant to characterize <em>decisions</em>, so one meaningful application is to <em>collective decision-making</em>: <a id=\"more\"></a></p>\n<h3><a name=\"sha\"></a>2.&nbsp; Sharing decision utility is sharing power, not welfare</h3>\n<p>Suppose decisions are to be made by or on behalf of a group.&nbsp; The decision could equally be about the welfare of group members, or something else.&nbsp; E.g.,</p>\n<ul>\n<li>How much vacation each member gets, or </li>\n<li>Which charity the group should invest its funds in. </li>\n</ul>\n<p>Say each member expresses a VNM utility value&mdash;a decision utility&mdash;for each outcome, and the decision is made to maximize the total.&nbsp; Over time, mandating or adjusting each member's expressed VNM utilities to have a given mean and variance could ensure that no one person dominates all the decisions by shouting giant numbers all the time.&nbsp; Incidentally, this is a way of normalizing their utilities: it will eliminate ambiguity in the constants ''a'' and ''b'' in (4) of <a href=\"#pre\">section 1</a>, which is exactly what we need for cross-agent comparisons and sums to make sense.</p>\n<p>Without thought as to whether this is a good system, the two decision examples illustrate how allotment of normalized VNM utility signifies sharing <em>power</em> in a collective decision, rather than sharing <em>well-being</em>.&nbsp; As such, the latter is better described by other metrics, in my opinion and in Kahneman's.</p>\n<h3><a name=\"cont\"></a>3.&nbsp; Contextual strength (CS) of preferences, and VNM-preference as \"strong\" preference</h3>\n<p>As a normative thory, I think VNM utility's biggest shortcomming is in its Archimedian (or \"Continuity\") axiom, which as we'll see, actually isn't very limiting.&nbsp; In its harshest interpretation, it says that if you won't sacrifice a small chance at X in order to get Y over Z, then you're <em>not allowed</em> to prefer Y over Z.&nbsp; For example, if you prefer green socks over red socks, then you must be willing to sacrifice some small, real probability of fulfilling immortality to favor that outcome.&nbsp; I wouldn't say this is <em>necessary</em> to be considered rational.&nbsp; Eliezer has noted implicitly in <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">this post</a> (excerpt below) that he also has a problem with the Archimedean requirement.</p>\n<p>I think this can be fixed directly with reinterpretation.&nbsp; For a given context C of possible outcomes, let's <em>intuitively</em> define a \"strong preference\" in that context to be one which is comparable in some non-zero ratio to the strongest preferences in the context.&nbsp; For example, other things being equal, you might consistently prefer green socks to red socks, but this may be completely undetectable on a scale that includes immortal hapiness, making it not a \"strong preference\" in that context. You might think of the socks as \"infinitely less significant\", but infinity is confusing. Perhaps less daunting is to think of them as a \"strictly secondary concern\" (see next section).</p>\n<p>I suggest that the four VNM axioms can work more broadly as axioms for <em>strong preference</em> in a given context.&nbsp; That is, we consider VNM-preference and VNM-utility</p>\n<ol>\n<li> to be defined only for a given context C of varying possible outcomes, and </li>\n<li> to intuitively only indicate those preferences <em>finitely-comparable to the strongest ones</em> in the given context. </li>\n</ol>\n<p>Then VNM-indifference, which they denote by equality, would simply mean a lack of <em>strong preference</em> in the given context, i.e.&nbsp; <em>not caring enough to sacrifice likelihoods of important things</em>.&nbsp; This is a Contextual Strength (CS) interpretation of VNM utility theory: in bigger contexts, VNM-preference indicates <em>stronger</em> preferences and <em>weaker</em> indifferences.</p>\n<p><strong>(CS) Henceforth, I explicitly distinguish the terms VNM-preference and VNM-indifference as those axiomatized by VNM, interpreted as above.</strong></p>\n<h3><a name=\"haus\"></a>4.&nbsp; Hausner (lexicographic) decision utility</h3>\n<p>[<strong>ETA</strong>]&nbsp; To see the broad applicability of VNM utility, let's examine the flexibility of a theory without the Archimedean axiom, and see that they differ only mildly in result:</p>\n<p>In the socks vs. immortality example, we could suppose that context \"Big\" includes such possible outcomes as immortal happiness, human extinction, getting socks, and ice-cream, and context \"Small\" includes only getting socks and ice-cream.&nbsp; You could have two VNM-like utility functions: U<sub>Small</sub> for evaluating gambles in the Small context, and U<sub>Big</sub> for the Big context.&nbsp; You could act to maximize EU<sub>Big</sub> whenever possible (EU=expected utility), and when two gambles have the same EU<sub>Big</sub>, you could default to choosing between them by their EU<sub>Small</sub> values.&nbsp; This is essentially acting to maximize the pair (EU<sub>Big</sub>, EU<sub>Small</sub>), ordered <em>lexicographically</em>, meaning that a difference in the former value EU<sub>Big</sub> trumps a difference in the latter value.&nbsp; We thus have a sensible numerical way to treat EU<sub>Big</sub> as \"infinitely more valuable\" without really involving infinities in the calculations; there is no need for that interpretation if you don't like it, though.</p>\n<p>Since we have the VNM axioms to imply when someone is maximizing <em>one</em> expectation value, you might ask, can we give some nice <em>weaker</em> axioms under which someone is maximizing a <em>lexicographic tuple</em> of expectations?</p>\n<p>Hearteningly, this has been taken care of, too.&nbsp; By weakening&mdash;indeed, effectively eliminating&mdash; the Archimedean axiom, Melvin Hausner<sup><a href=\"#hausner\">2</a></sup> developed this theory in 1952 for Rand Corporation, and Peter Fishburn<sup><a href=\"#fishburn\">3</a></sup> provides a nice exposition of Hausner's axioms.&nbsp; So now we have Hausner-rational agents maximizing Hausner utility.&nbsp;</p>\n<p>[<a name=\"noise\"></a><strong>ETA</strong>]&nbsp; But the difference between Hausner and VNM utility comes into effect only in the rare event when you <em>know</em> you can't distinguish EU<sub>Big</sub> values, otherwise the Hausner-rational behavior is to \"keep thinking\" to make sure you're not sacrificing EU<sub>Big</sub>.&nbsp; The most plausible scenario I can imagine where this might actually happen to a human is when making a decision on a <em>precisely known time limit</em>, like say sniping on one of two simultaneous ebay auctions for socks.&nbsp; CronoDAS might say the time limit creates <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/1w9q\">\"noise in your expectations\"</a>.&nbsp; If the time runs out and you have failed to distinguish which sock color results in higher chances of immortality or other EU<sub>Big</sub> concerns, then I'd say it wouldn't be irrational to make the choice according to some secondary utility EU<sub>Small</sub> that any detectable difference in EU<sub>Big</sub> would otherwise trump.</p>\n<p>Moreover, it turns out<sup><a href=\"#fishburn\">3</a></sup> that the primary, i.e. most dominant, function in the Hausner utility tuple behaves almost exactly like VNM utility, and has the same uniqueness property (up to the constants ''a'' and ''b'').&nbsp; So except in rare circumstances, you can just think in terms of VNM utility and get the same answer, and even the rare exceptions involve considerations that are necessarily \"unimportant\" relative to the context. &nbsp;</p>\n<p>Thus, a lot of apparent flexibility in Hausner utility theory might simply demonstrate that VNM utility is more applicable to you than it fist appeared.&nbsp; This situation favors the (CS) interpretation: even when the Archimedean axiom isn't quite satisfied, we can use VNM utility liberally as indicating \"strong\" preferences in a given context.&nbsp;</p>\n<h3><a name=\"ind\"></a>5.&nbsp; The independence axiom isn't so bad</h3>\n<p>\"A variety of <a href=\"http://en.wikipedia.org/wiki/Generalized_expected_utility\">generalized expected utility</a> theories have arisen, most of which drop or relax the independence axiom.\" (Wikipedia)&nbsp; But I think the independence axiom (which Hausner also assumes) is a non-issue if we're talking about \"strong preferences\". The following, in various forms, is what seems to be the best argument against it:</p>\n<p>Suppose a parent has no VNM preference between S: her son or her daughter gets a free car, and D: her daughter gets it.&nbsp; In the original VNM formulation, this is written \"S=D\".&nbsp; She is also presented with a third option, F=.5S+.5D.&nbsp; Descriptively, a fair coin would be flipped, and her son or daughter gets a car accordingly.</p>\n<p>By writing S=.5S+.5S and D=.5D+.5D, the original independence axiom says that S=D implies S=F=D, so she must be VNM-indfferent between F and the others.&nbsp; However, a desire for \"fair chances\" might result in preferring F, which we might want to allow as \"rational\".</p>\n<p>[<strong>ETA</strong>]&nbsp; I think the most natural fix within the VNM <em>theory</em> is to just say S' and D' are the events \"car is awarded so son/daughter based on a coin toss\", which are slightly better than S and D themselves, and that F is really 0.5S' + 0.5D'. Unfortunately, such modifications undermine the applicability of the VNM <em>theorem</em>, which implicitly assumes that the source of probabilities itself is insignificant to the outcomes for the agent.&nbsp; Luckily, Bolker<sup><a href=\"#bolker\">4</a></sup> has divised an axiomatic theory whose theorems will apply without such assumptions, at the expense of some uniqueness results.&nbsp; I'll have another occasion to post on this later.</p>\n<p>Anyway, under the (CS) interpretation, the requirement \"S=F=D\" just means the parent lacks a <em>VNM-preference</em>, i.e. a <em>strong preference</em>, so it's not too big of a problem.&nbsp; Assuming she's VNM-rational just means that, in the implicit context, she is unwilling to make certain probabilitstic sacrifices to favor F over S and D.&nbsp;</p>\n<ul>\n<li>If the context is Big and includes something like death, the VNM-indifference \"S=D\" is a weak claim: it might just indicate an unwillingness to increase risk of things finitely-comparable to death in order to obtain F over S or D.&nbsp; She is still allowed to prefer F if no such sacrifice is involved. </li>\n<li>If the context is Small, and say only includes her kids getting cars, then \"S=D\" is a strong claim: it indicates an unwillingless to risk her kids not getting cars to favor S over D in a gamble.&nbsp; Then she can <em>still</em> prefer F, but she couldn't prefer F'=.49S+.49D+.02(no car) over S or D, since it would contradict what \"S=D\" means in terms of car-sacrifice.&nbsp; I think that's reasonable, since if she simply flips a mental coin and gives her son the car, she can prefer to favor her daughter in later circumstances.&nbsp; </li>\n</ul>\n<p>You might say VNM tells you to <em>\"Be the fairness that you want to see in the world.\"</em></p>\n<h3><a name=\"app\"></a>6.&nbsp; Application to earlier other LessWrong discussions of utility</h3>\n<p>This contextual strength interpretation of VNM utility is directly relevant to resolving Eliezer's point linked above:</p>\n<blockquote>\"... <em>The utility function is not up for grabs.</em>&nbsp; I love life without limit or upper bound:&nbsp; There is no finite amount of life lived N where I would prefer a 80.0001% probability of living N years to an 0.0001% chance of living a googolplex years and an 80% chance of living forever.\"</blockquote>\n<p>This could just indicate that Eliezer ranks immortality on a scale that trumps finite lifespan preferences, a-la-Hausner utility theory. In a context of differing positive likelihoods of immortality, these other factors are not strong enough to constitute VNM-preferences.</p>\n<p>As well, Stuart Armstrong has written a thoughtful article <a href=\"/lw/1cv/extreme_risks_when_not_to_use_expected_utility/\">\"Extreme risks: when not to use expected utility\"</a>, and argues against Independence.&nbsp; I'd like to recast his ideas context-relatively, which I think alleviates the difficulty:</p>\n<p>In his paragraph 5, he considers various existential disasters.&nbsp; In my view, this is a case for a \"Big\" context utility function, not a case against independence.&nbsp; If you were gambling <em>only</em> between eistential distasters, then you have might have an \"existential-context utility function\", U<sub>Existential</sub>.&nbsp; For example, would you prefer</p>\n<ul>\n<li>90%(extinction by nuclear war) + 10%(nothing), or </li>\n<li>60%(extinction by nuclear war) + 30%(extinction by asteroids) + 10%(nothing)? </li>\n</ul>\n<p>If you prefer the latter enough to make some comparable sacrifice in the &laquo;nothing&raquo; term, contextual VNM just says you assign a higher U<sub>Existential</sub> to &laquo;extinction by asteroids&raquo; than to &laquo;extinction by nuclear war&raquo;.<sup><a href=\"#wedrifid\">5</a></sup>&nbsp; There's no need to be freaked out by assigning finite numbers here, since for example Hausner would allow the value of U<sub>Existential</sub> to completely trump the value of U<sub>Everyday</sub> if you started worrying about socks or ice cream.&nbsp; You could be both <em>extremely risk averse</em> regarding existential outcomes, and <em>absolutely unwilling</em> to gamble with them for more trivial gains.</p>\n<p>In his paragraph 6, Stuart talks about giving out (necessarily normalized) VNM utility to people, which I described in <a href=\"#sha\">section 2</a> as a model for sharing power rather than well-being.&nbsp; I think he gives a good argument against blindly maximizing the <em>total normalized VNM utility of a collective</em> in a one-shot decision:</p>\n<blockquote>\"...imagine having to choose between a project that gave one util to each person on the planet, and one that handed slightly over twelve billion utils to a randomly chosen human and took away one util from everyone else.&nbsp; If there were trillions of such projects, then it wouldn&rsquo;t matter what option you chose.&nbsp; But if you only had one shot, it would be peculiar to argue that there are no rational grounds to prefer one over the other, simply because the trillion-iterated versions are identical.\"</blockquote>\n<p>(Indeed, practically, the mean and variance normalization I described doesn't apply to provide the same \"fairness\" in a one-shot deal.)&nbsp;</p>\n<p>I'd call the latter of Stuart's projects an unfair distribution of power in a collective decision process, something you might personally assign a low VNM utility to, and therefore avoid.&nbsp; Thus I wouldn't consider it an argument not to <em>use expected utility</em>, but an argument not to blindly favor <em>total normalized VNM utility of a population</em> in your own decision utility function.&nbsp; The same argument&mdash;<a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">Parfit's Repugnant Conclusion</a>&mdash;is made against total normalized welfare.</p>\n<hr />\n<p><em>The expected utility model of rationality is alive and normatively kicking, and is highly adaptable to modelling very weak assumptions of rationality. I hope this post can serve to marginally persuade others in that direction.</em></p>\n<p><strong>References, notes, and further reading:</strong></p>\n<p><a name=\"kahneman\"><sup>1</sup></a> Kahneman, Wakker and Sarin, 1997, <a href=\"http://www.mitpressjournals.org/doi/abs/10.1162/003355397555235\">Back to Bentham?&nbsp; Explorations of experienced utility</a>, The quarterly journal of economics.</p>\n<p><a name=\"hausner\"><sup>2</sup></a> Hausner, 1952, <a href=\"http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0604151\">Multidimensional utilities</a>, Rand Corporation.</p>\n<p><a name=\"fishburn\"><sup>3</sup></a> Fishburn, 1971, <a href=\"http://www.jstor.org/stable/pdfplus/2629309.pdf\">A Study of Lexicographic Expected Utility</a>, Management Science.</p>\n<p><a name=\"bolker\"><sup>4</sup></a> Bolker, 1967, <a href=\"http://www.jstor.org/pss/186122\">A simultaneous axiomatization of utility and probability</a>, Philosophy of Science Association.</p>\n<p><a name=\"wedrifid\"><sup>5</sup></a> As <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/1wh8\">wedrifid pointed out</a>, you might instead just prefer uncertainty in your impending doom. Just as in <a href=\"#ind\">section 5</a>, neither VNM nor Hausner can model this usefully (i.e. in way that allows calculating utilities), though I don't consider this much of a limitation. In fact, I'd consider it a normative step backward to admit \"rational\" agents who actually <em>prefer uncertainty in itself</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "2YcmB6SLtHnHRe3uX": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YCMfQoqqi2o9Tjwoa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 35, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "2740", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<!-- VNM utility - uses, abuses, and context -->\n<p><em>When interpreted convservatively, the von Neumann-Morgenstern <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">rationality axioms and utility theorem</a> are an indispensible tool for the normative study of rationality, deserving of many <a href=\"/lw/182/the_absentminded_driver/\">thought</a> <a href=\"/lw/1zp/maximise_expected_utility_not_expected_perception/\">experiments</a> and <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">attentive</a> <a href=\"/lw/15m/towards_a_new_decision_theory/\">decision</a> <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">theory</a>.&nbsp; It's one more reason I'm glad to be born after the 1940s. Yet there is apprehension about its validity, aside from merely confusing it with Bentham utilitarianism (as highlighted by <a href=\"/lw/1hw/the_difference_between_utility_and_utility/\">Matt Simpson</a>).&nbsp; I want to describe not only what VNM utility is really meant for, but a contextual reinterpretation of its meaning, so that it may hopefully be used more frequently, confidently, and appropriately.</em></p>\n<ol>\n<li><a href=\"#pre\">Preliminary discussion and precautions</a></li>\n<li><a href=\"#sha\">Sharing decision utility is sharing power, not welfare</a> </li>\n<li><a href=\"#cont\">Contextual Strength (CS) of preferences, and VNM-preference as \"strong\" preference</a> </li>\n<li><a href=\"#haus\">Hausner (lexicographic) decision utility</a> </li>\n<li><a href=\"#ind\">The independence axiom isn't bad either</a> </li>\n<li><a href=\"#app\">Application to earlier LessWrong discussions of utility</a> </li>\n</ol>\n<h3 id=\"1___Preliminary_discussion_and_precautions\"><a name=\"pre\"></a>1.&nbsp; Preliminary discussion and precautions</h3>\n<p>The idea of John von Neumann and Oskar Mogernstern is that, <em>if</em> you behave a certain way, <em>then</em> it turns out you're maximizing the expected value of a particular function.&nbsp; Very cool!&nbsp; And their description of \"a certain way\" is very compelling: a list of four, reasonable-seeming axioms.&nbsp; If you haven't already, check out the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">Von Neumann-Morgenstern utility theorem</a>, a mathematical result which makes their claim rigorous, and true.</p>\n<p>VNM utility is a <em>decision utility</em>, in that it aims to characterize the decision-making of a rational agent.&nbsp; One great feature is that it implicitly accounts for <a href=\"http://en.wikipedia.org/wiki/Risk_aversion#Absolute_risk_aversion\">risk aversion</a>: not risking $100 for a 10% chance to win $1000 and 90% chance to win $0 just means that for you, utility($100) &gt; 10%utility($1000) + 90%utility($0).&nbsp;</p>\n<p>But as the Wikipedia article explains nicely, VNM utility is:</p>\n<ol>\n<li><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Applicability_to_economics\">not designed to predict</a> the behavior of \"irrational\" individuals (like real people in a real economy); </li>\n<li>not designed to characterize <em>well-being</em>, but to characterize <em>decisions</em>; </li>\n<li>not designed to measure the value of <em>items</em>, but the value of <em>outcomes</em>; </li>\n<li>only defined up to a scalar multiple and additive constant (acting with utility function U(X) is the same as acting with a\u00b7U(X)+b, if a&gt;0); </li>\n<li><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Incomparability_between_agents\">not designed to be added up or compared</a> between a number of individuals; </li>\n<li><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#Implications_for_ethics_and_moral_philosophy\">not something that can be \"sacrificed\"</a> in favor of others in a meaningful way. </li>\n</ol>\n<p>[<strong>ETA</strong>]&nbsp; Additionally, in the VNM theorem the probabilities are understood to be known to the agent <em>as they are presented</em>, and to come from a source of randomness whose outcomes are <em>not significant</em> to the agent.&nbsp; Without these assumptions, its proof doesn't work.</p>\n<p>Because of (4), one often considers <em>marginal utilities</em> of the form U(X)-U(Y), to cancel the ambiguity in the additive constant b.&nbsp; This is totally legitimate, and faithful to the mathematical conception of VNM utility.</p>\n<p>Because of (5), people often \"normalize\" VNM utility to eliminate ambiguity in both constants, so that utilities are unique numbers that can be added accross multiple agents.&nbsp; One way is to declare that every person in some situation values $1 at 1 utilon (a fictional unit of measure of utility), and $0 at 0.&nbsp; I think a more meaningful and applicable normalization is to fix mean and variance with respect to certain outcomes (next section).</p>\n<p>Because of (6), characterizing the altruism of a VNM-rational agent by how he sacrifices his own VNM utility is the wrong approach.&nbsp; Indeed, such a sacrifice is a contradiction.&nbsp; Kahneman suggests<sup><a href=\"/kahn\">1</a></sup>, and I agree, that <em>something else</em> should be added or substracted to determine the total, comparative, or average well-being of individuals.&nbsp; I'd call it \"<strong>welfare</strong>\", to avoid confusing it with VNM utility.&nbsp; Kahneman calls it E-utility, for \"experienced utility\", a connotation I'll avoid.&nbsp; Intuitively, this is certainly something you could sacrifice for others, or have more of compared to others.&nbsp; True, a given person's VNM utility is likely highly correlated with her personal \"welfare\", but I wouldn't consider it an accurate approximation.&nbsp;</p>\n<p>So if not <em>collective welfare</em>, then what could cross-agent comparisons or sums of VNM utilities indicate?&nbsp; Well, they're meant to characterize <em>decisions</em>, so one meaningful application is to <em>collective decision-making</em>: <a id=\"more\"></a></p>\n<h3 id=\"2___Sharing_decision_utility_is_sharing_power__not_welfare\"><a name=\"sha\"></a>2.&nbsp; Sharing decision utility is sharing power, not welfare</h3>\n<p>Suppose decisions are to be made by or on behalf of a group.&nbsp; The decision could equally be about the welfare of group members, or something else.&nbsp; E.g.,</p>\n<ul>\n<li>How much vacation each member gets, or </li>\n<li>Which charity the group should invest its funds in. </li>\n</ul>\n<p>Say each member expresses a VNM utility value\u2014a decision utility\u2014for each outcome, and the decision is made to maximize the total.&nbsp; Over time, mandating or adjusting each member's expressed VNM utilities to have a given mean and variance could ensure that no one person dominates all the decisions by shouting giant numbers all the time.&nbsp; Incidentally, this is a way of normalizing their utilities: it will eliminate ambiguity in the constants ''a'' and ''b'' in (4) of <a href=\"#pre\">section 1</a>, which is exactly what we need for cross-agent comparisons and sums to make sense.</p>\n<p>Without thought as to whether this is a good system, the two decision examples illustrate how allotment of normalized VNM utility signifies sharing <em>power</em> in a collective decision, rather than sharing <em>well-being</em>.&nbsp; As such, the latter is better described by other metrics, in my opinion and in Kahneman's.</p>\n<h3 id=\"3___Contextual_strength__CS__of_preferences__and_VNM_preference_as__strong__preference\"><a name=\"cont\"></a>3.&nbsp; Contextual strength (CS) of preferences, and VNM-preference as \"strong\" preference</h3>\n<p>As a normative thory, I think VNM utility's biggest shortcomming is in its Archimedian (or \"Continuity\") axiom, which as we'll see, actually isn't very limiting.&nbsp; In its harshest interpretation, it says that if you won't sacrifice a small chance at X in order to get Y over Z, then you're <em>not allowed</em> to prefer Y over Z.&nbsp; For example, if you prefer green socks over red socks, then you must be willing to sacrifice some small, real probability of fulfilling immortality to favor that outcome.&nbsp; I wouldn't say this is <em>necessary</em> to be considered rational.&nbsp; Eliezer has noted implicitly in <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">this post</a> (excerpt below) that he also has a problem with the Archimedean requirement.</p>\n<p>I think this can be fixed directly with reinterpretation.&nbsp; For a given context C of possible outcomes, let's <em>intuitively</em> define a \"strong preference\" in that context to be one which is comparable in some non-zero ratio to the strongest preferences in the context.&nbsp; For example, other things being equal, you might consistently prefer green socks to red socks, but this may be completely undetectable on a scale that includes immortal hapiness, making it not a \"strong preference\" in that context. You might think of the socks as \"infinitely less significant\", but infinity is confusing. Perhaps less daunting is to think of them as a \"strictly secondary concern\" (see next section).</p>\n<p>I suggest that the four VNM axioms can work more broadly as axioms for <em>strong preference</em> in a given context.&nbsp; That is, we consider VNM-preference and VNM-utility</p>\n<ol>\n<li> to be defined only for a given context C of varying possible outcomes, and </li>\n<li> to intuitively only indicate those preferences <em>finitely-comparable to the strongest ones</em> in the given context. </li>\n</ol>\n<p>Then VNM-indifference, which they denote by equality, would simply mean a lack of <em>strong preference</em> in the given context, i.e.&nbsp; <em>not caring enough to sacrifice likelihoods of important things</em>.&nbsp; This is a Contextual Strength (CS) interpretation of VNM utility theory: in bigger contexts, VNM-preference indicates <em>stronger</em> preferences and <em>weaker</em> indifferences.</p>\n<p><strong id=\"_CS__Henceforth__I_explicitly_distinguish_the_terms_VNM_preference_and_VNM_indifference_as_those_axiomatized_by_VNM__interpreted_as_above_\">(CS) Henceforth, I explicitly distinguish the terms VNM-preference and VNM-indifference as those axiomatized by VNM, interpreted as above.</strong></p>\n<h3 id=\"4___Hausner__lexicographic__decision_utility\"><a name=\"haus\"></a>4.&nbsp; Hausner (lexicographic) decision utility</h3>\n<p>[<strong>ETA</strong>]&nbsp; To see the broad applicability of VNM utility, let's examine the flexibility of a theory without the Archimedean axiom, and see that they differ only mildly in result:</p>\n<p>In the socks vs. immortality example, we could suppose that context \"Big\" includes such possible outcomes as immortal happiness, human extinction, getting socks, and ice-cream, and context \"Small\" includes only getting socks and ice-cream.&nbsp; You could have two VNM-like utility functions: U<sub>Small</sub> for evaluating gambles in the Small context, and U<sub>Big</sub> for the Big context.&nbsp; You could act to maximize EU<sub>Big</sub> whenever possible (EU=expected utility), and when two gambles have the same EU<sub>Big</sub>, you could default to choosing between them by their EU<sub>Small</sub> values.&nbsp; This is essentially acting to maximize the pair (EU<sub>Big</sub>, EU<sub>Small</sub>), ordered <em>lexicographically</em>, meaning that a difference in the former value EU<sub>Big</sub> trumps a difference in the latter value.&nbsp; We thus have a sensible numerical way to treat EU<sub>Big</sub> as \"infinitely more valuable\" without really involving infinities in the calculations; there is no need for that interpretation if you don't like it, though.</p>\n<p>Since we have the VNM axioms to imply when someone is maximizing <em>one</em> expectation value, you might ask, can we give some nice <em>weaker</em> axioms under which someone is maximizing a <em>lexicographic tuple</em> of expectations?</p>\n<p>Hearteningly, this has been taken care of, too.&nbsp; By weakening\u2014indeed, effectively eliminating\u2014 the Archimedean axiom, Melvin Hausner<sup><a href=\"#hausner\">2</a></sup> developed this theory in 1952 for Rand Corporation, and Peter Fishburn<sup><a href=\"#fishburn\">3</a></sup> provides a nice exposition of Hausner's axioms.&nbsp; So now we have Hausner-rational agents maximizing Hausner utility.&nbsp;</p>\n<p>[<a name=\"noise\"></a><strong>ETA</strong>]&nbsp; But the difference between Hausner and VNM utility comes into effect only in the rare event when you <em>know</em> you can't distinguish EU<sub>Big</sub> values, otherwise the Hausner-rational behavior is to \"keep thinking\" to make sure you're not sacrificing EU<sub>Big</sub>.&nbsp; The most plausible scenario I can imagine where this might actually happen to a human is when making a decision on a <em>precisely known time limit</em>, like say sniping on one of two simultaneous ebay auctions for socks.&nbsp; CronoDAS might say the time limit creates <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/1w9q\">\"noise in your expectations\"</a>.&nbsp; If the time runs out and you have failed to distinguish which sock color results in higher chances of immortality or other EU<sub>Big</sub> concerns, then I'd say it wouldn't be irrational to make the choice according to some secondary utility EU<sub>Small</sub> that any detectable difference in EU<sub>Big</sub> would otherwise trump.</p>\n<p>Moreover, it turns out<sup><a href=\"#fishburn\">3</a></sup> that the primary, i.e. most dominant, function in the Hausner utility tuple behaves almost exactly like VNM utility, and has the same uniqueness property (up to the constants ''a'' and ''b'').&nbsp; So except in rare circumstances, you can just think in terms of VNM utility and get the same answer, and even the rare exceptions involve considerations that are necessarily \"unimportant\" relative to the context. &nbsp;</p>\n<p>Thus, a lot of apparent flexibility in Hausner utility theory might simply demonstrate that VNM utility is more applicable to you than it fist appeared.&nbsp; This situation favors the (CS) interpretation: even when the Archimedean axiom isn't quite satisfied, we can use VNM utility liberally as indicating \"strong\" preferences in a given context.&nbsp;</p>\n<h3 id=\"5___The_independence_axiom_isn_t_so_bad\"><a name=\"ind\"></a>5.&nbsp; The independence axiom isn't so bad</h3>\n<p>\"A variety of <a href=\"http://en.wikipedia.org/wiki/Generalized_expected_utility\">generalized expected utility</a> theories have arisen, most of which drop or relax the independence axiom.\" (Wikipedia)&nbsp; But I think the independence axiom (which Hausner also assumes) is a non-issue if we're talking about \"strong preferences\". The following, in various forms, is what seems to be the best argument against it:</p>\n<p>Suppose a parent has no VNM preference between S: her son or her daughter gets a free car, and D: her daughter gets it.&nbsp; In the original VNM formulation, this is written \"S=D\".&nbsp; She is also presented with a third option, F=.5S+.5D.&nbsp; Descriptively, a fair coin would be flipped, and her son or daughter gets a car accordingly.</p>\n<p>By writing S=.5S+.5S and D=.5D+.5D, the original independence axiom says that S=D implies S=F=D, so she must be VNM-indfferent between F and the others.&nbsp; However, a desire for \"fair chances\" might result in preferring F, which we might want to allow as \"rational\".</p>\n<p>[<strong>ETA</strong>]&nbsp; I think the most natural fix within the VNM <em>theory</em> is to just say S' and D' are the events \"car is awarded so son/daughter based on a coin toss\", which are slightly better than S and D themselves, and that F is really 0.5S' + 0.5D'. Unfortunately, such modifications undermine the applicability of the VNM <em>theorem</em>, which implicitly assumes that the source of probabilities itself is insignificant to the outcomes for the agent.&nbsp; Luckily, Bolker<sup><a href=\"#bolker\">4</a></sup> has divised an axiomatic theory whose theorems will apply without such assumptions, at the expense of some uniqueness results.&nbsp; I'll have another occasion to post on this later.</p>\n<p>Anyway, under the (CS) interpretation, the requirement \"S=F=D\" just means the parent lacks a <em>VNM-preference</em>, i.e. a <em>strong preference</em>, so it's not too big of a problem.&nbsp; Assuming she's VNM-rational just means that, in the implicit context, she is unwilling to make certain probabilitstic sacrifices to favor F over S and D.&nbsp;</p>\n<ul>\n<li>If the context is Big and includes something like death, the VNM-indifference \"S=D\" is a weak claim: it might just indicate an unwillingness to increase risk of things finitely-comparable to death in order to obtain F over S or D.&nbsp; She is still allowed to prefer F if no such sacrifice is involved. </li>\n<li>If the context is Small, and say only includes her kids getting cars, then \"S=D\" is a strong claim: it indicates an unwillingless to risk her kids not getting cars to favor S over D in a gamble.&nbsp; Then she can <em>still</em> prefer F, but she couldn't prefer F'=.49S+.49D+.02(no car) over S or D, since it would contradict what \"S=D\" means in terms of car-sacrifice.&nbsp; I think that's reasonable, since if she simply flips a mental coin and gives her son the car, she can prefer to favor her daughter in later circumstances.&nbsp; </li>\n</ul>\n<p>You might say VNM tells you to <em>\"Be the fairness that you want to see in the world.\"</em></p>\n<h3 id=\"6___Application_to_earlier_other_LessWrong_discussions_of_utility\"><a name=\"app\"></a>6.&nbsp; Application to earlier other LessWrong discussions of utility</h3>\n<p>This contextual strength interpretation of VNM utility is directly relevant to resolving Eliezer's point linked above:</p>\n<blockquote>\"... <em>The utility function is not up for grabs.</em>&nbsp; I love life without limit or upper bound:&nbsp; There is no finite amount of life lived N where I would prefer a 80.0001% probability of living N years to an 0.0001% chance of living a googolplex years and an 80% chance of living forever.\"</blockquote>\n<p>This could just indicate that Eliezer ranks immortality on a scale that trumps finite lifespan preferences, a-la-Hausner utility theory. In a context of differing positive likelihoods of immortality, these other factors are not strong enough to constitute VNM-preferences.</p>\n<p>As well, Stuart Armstrong has written a thoughtful article <a href=\"/lw/1cv/extreme_risks_when_not_to_use_expected_utility/\">\"Extreme risks: when not to use expected utility\"</a>, and argues against Independence.&nbsp; I'd like to recast his ideas context-relatively, which I think alleviates the difficulty:</p>\n<p>In his paragraph 5, he considers various existential disasters.&nbsp; In my view, this is a case for a \"Big\" context utility function, not a case against independence.&nbsp; If you were gambling <em>only</em> between eistential distasters, then you have might have an \"existential-context utility function\", U<sub>Existential</sub>.&nbsp; For example, would you prefer</p>\n<ul>\n<li>90%(extinction by nuclear war) + 10%(nothing), or </li>\n<li>60%(extinction by nuclear war) + 30%(extinction by asteroids) + 10%(nothing)? </li>\n</ul>\n<p>If you prefer the latter enough to make some comparable sacrifice in the \u00abnothing\u00bb term, contextual VNM just says you assign a higher U<sub>Existential</sub> to \u00abextinction by asteroids\u00bb than to \u00abextinction by nuclear war\u00bb.<sup><a href=\"#wedrifid\">5</a></sup>&nbsp; There's no need to be freaked out by assigning finite numbers here, since for example Hausner would allow the value of U<sub>Existential</sub> to completely trump the value of U<sub>Everyday</sub> if you started worrying about socks or ice cream.&nbsp; You could be both <em>extremely risk averse</em> regarding existential outcomes, and <em>absolutely unwilling</em> to gamble with them for more trivial gains.</p>\n<p>In his paragraph 6, Stuart talks about giving out (necessarily normalized) VNM utility to people, which I described in <a href=\"#sha\">section 2</a> as a model for sharing power rather than well-being.&nbsp; I think he gives a good argument against blindly maximizing the <em>total normalized VNM utility of a collective</em> in a one-shot decision:</p>\n<blockquote>\"...imagine having to choose between a project that gave one util to each person on the planet, and one that handed slightly over twelve billion utils to a randomly chosen human and took away one util from everyone else.&nbsp; If there were trillions of such projects, then it wouldn\u2019t matter what option you chose.&nbsp; But if you only had one shot, it would be peculiar to argue that there are no rational grounds to prefer one over the other, simply because the trillion-iterated versions are identical.\"</blockquote>\n<p>(Indeed, practically, the mean and variance normalization I described doesn't apply to provide the same \"fairness\" in a one-shot deal.)&nbsp;</p>\n<p>I'd call the latter of Stuart's projects an unfair distribution of power in a collective decision process, something you might personally assign a low VNM utility to, and therefore avoid.&nbsp; Thus I wouldn't consider it an argument not to <em>use expected utility</em>, but an argument not to blindly favor <em>total normalized VNM utility of a population</em> in your own decision utility function.&nbsp; The same argument\u2014<a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">Parfit's Repugnant Conclusion</a>\u2014is made against total normalized welfare.</p>\n<hr>\n<p><em>The expected utility model of rationality is alive and normatively kicking, and is highly adaptable to modelling very weak assumptions of rationality. I hope this post can serve to marginally persuade others in that direction.</em></p>\n<p><strong id=\"References__notes__and_further_reading_\">References, notes, and further reading:</strong></p>\n<p><a name=\"kahneman\"><sup>1</sup></a> Kahneman, Wakker and Sarin, 1997, <a href=\"http://www.mitpressjournals.org/doi/abs/10.1162/003355397555235\">Back to Bentham?&nbsp; Explorations of experienced utility</a>, The quarterly journal of economics.</p>\n<p><a name=\"hausner\"><sup>2</sup></a> Hausner, 1952, <a href=\"http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0604151\">Multidimensional utilities</a>, Rand Corporation.</p>\n<p><a name=\"fishburn\"><sup>3</sup></a> Fishburn, 1971, <a href=\"http://www.jstor.org/stable/pdfplus/2629309.pdf\">A Study of Lexicographic Expected Utility</a>, Management Science.</p>\n<p><a name=\"bolker\"><sup>4</sup></a> Bolker, 1967, <a href=\"http://www.jstor.org/pss/186122\">A simultaneous axiomatization of utility and probability</a>, Philosophy of Science Association.</p>\n<p><a name=\"wedrifid\"><sup>5</sup></a> As <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/1wh8\">wedrifid pointed out</a>, you might instead just prefer uncertainty in your impending doom. Just as in <a href=\"#ind\">section 5</a>, neither VNM nor Hausner can model this usefully (i.e. in way that allows calculating utilities), though I don't consider this much of a limitation. In fact, I'd consider it a normative step backward to admit \"rational\" agents who actually <em>prefer uncertainty in itself</em>.</p>", "sections": [{"title": "1.\u00a0 Preliminary discussion and precautions", "anchor": "1___Preliminary_discussion_and_precautions", "level": 1}, {"title": "2.\u00a0 Sharing decision utility is sharing power, not welfare", "anchor": "2___Sharing_decision_utility_is_sharing_power__not_welfare", "level": 1}, {"title": "3.\u00a0 Contextual strength (CS) of preferences, and VNM-preference as \"strong\" preference", "anchor": "3___Contextual_strength__CS__of_preferences__and_VNM_preference_as__strong__preference", "level": 1}, {"title": "(CS) Henceforth, I explicitly distinguish the terms VNM-preference and VNM-indifference as those axiomatized by VNM, interpreted as above.", "anchor": "_CS__Henceforth__I_explicitly_distinguish_the_terms_VNM_preference_and_VNM_indifference_as_those_axiomatized_by_VNM__interpreted_as_above_", "level": 2}, {"title": "4.\u00a0 Hausner (lexicographic) decision utility", "anchor": "4___Hausner__lexicographic__decision_utility", "level": 1}, {"title": "5.\u00a0 The independence axiom isn't so bad", "anchor": "5___The_independence_axiom_isn_t_so_bad", "level": 1}, {"title": "6.\u00a0 Application to earlier other LessWrong discussions of utility", "anchor": "6___Application_to_earlier_other_LessWrong_discussions_of_utility", "level": 1}, {"title": "References, notes, and further reading:", "anchor": "References__notes__and_further_reading_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "51 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GfHdNfqxe3cSCfpHL", "cJSCTtyJmykHofkGm", "szfxvS8nsxTgJLBHs", "de3xjFaACCAk6imzv", "m4rWJYRXyzfizHJHt", "6ddcsdA2c2XpNpE5x", "kmjCaq66MDkfvZpFX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-19T02:21:15.080Z", "modifiedAt": null, "url": null, "title": "a meta-anti-akrasia strategy that might just work", "slug": "a-meta-anti-akrasia-strategy-that-might-just-work", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:23.975Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MartinB", "createdAt": "2009-04-20T11:11:22.800Z", "isAdmin": false, "displayName": "MartinB"}, "userId": "2BGK5dWpTXzCE7iwF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PqtuntSwvmcCv7zzP/a-meta-anti-akrasia-strategy-that-might-just-work", "pageUrlRelative": "/posts/PqtuntSwvmcCv7zzP/a-meta-anti-akrasia-strategy-that-might-just-work", "linkUrl": "https://www.lesswrong.com/posts/PqtuntSwvmcCv7zzP/a-meta-anti-akrasia-strategy-that-might-just-work", "postedAtFormatted": "Monday, April 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20a%20meta-anti-akrasia%20strategy%20that%20might%20just%20work&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Aa%20meta-anti-akrasia%20strategy%20that%20might%20just%20work%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPqtuntSwvmcCv7zzP%2Fa-meta-anti-akrasia-strategy-that-might-just-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=a%20meta-anti-akrasia%20strategy%20that%20might%20just%20work%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPqtuntSwvmcCv7zzP%2Fa-meta-anti-akrasia-strategy-that-might-just-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPqtuntSwvmcCv7zzP%2Fa-meta-anti-akrasia-strategy-that-might-just-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1584, "htmlBody": "<p>For ages I've been trying to wrap my mind around meta thinking - not \"what is the best way to do something\", but \"how do I find out which way is any good?\" Meta thinking has many applications, and I am always surprised when I find a new context it can be applied to. Anti-akrasia might be such a context.<br /><br />The idea I am about to present came to me a few month ago and I used it to finally overcome my own problem with procrastination. I'll try to present it here as well as I can, in the hope that it might be of use to someone. If so, I am really curious what other people come up with using this technique.<br /><br />If akrasia is a struggle, continue reading.<br /><br />Where I come from:<br />Procrastination was a big topic for me. I spent ages reading stuff, watching videos, thinking, collecting stuff and what not, but very little on actual action. One thing I did read was productivity blogs and books. I assume that some or even many of the posters here share that problem with me. I am familiar with the systems - I even gave a lecture once on GTD - but I struggled to get my own stuff out the door. It surely wasn't for a lack of knowledge, but simply for a lack of doing.<br /><br />The method used consists of two layers.<br />(I) the meta concept used to develop a personal system<br />(II) the highly personalized system I came up with while applying (I)<a id=\"more\"></a><br /><br />The valuable part of this post is (I).<br /><br />One of the major lessons I had to learn (and am still learning) is that everyone reacts differently to a set of stimuli. This doesn't just mean differently colored folders, or the famous 'paper' or 'digital' debate. It literally means that for every person the way to get productive is different - down to the point of specific ideas working fine for one person while being a stress-inducing thing for others.<br /><br />So what did I do?<br /><br />First I assumed that more reading wouldn't do me any good. I assumed that I knew everything there is to know on the topic of personal productivity and refrained from reading any more.<br /><br />Instead I made up a meta concept.<br /><br />(I) the meta concept<br /><br />My big main idea was to <strong>treat the whole problem of personal productivity as an experiment using myself as guinea pig. I decided to find out what is needed to a) start working and b) what the best conditions would be to get myself to keep producing.</strong><br /><br />Now that was short. Let me expand on it.<br /><br />I did a planning session, made up a bunch of rules and habits, worked with them for a while and then looked at how that worked out during the next planning session. If something worked I kept it, if not I tried something else. Planning the conditions and trying them are cleanly separated, so I can safely try and see what works.<br /><br />To put it in slightly different words:<br />Junior research assistant Martin is now scheduled with the task of finding out what kind of system leads to good work results, via the work habit study guinea pig who is also appropriately named Martin.<br /><br />My system now consists of a few rules, treats (small ones and bigger ones), my favorite time keeping method, my log files and anything else I want to consider a part of it.<br /><br />All this is done in writing!<br /><br />Writing is important, since it is super easy to forget what we figure out.<br />My notepad has a page for meta - insights, where I collect what I find out about myself, and a page for rules I try out.<br /><br />When review time comes around I go to my favorite fast food restaurant and honestly review how it worked out so far.&nbsp; A good startup value for meta-review is about once a week in the beginning.<br /><br />Meta-review frequency is also subject to personal adaption.<br /><br />Since you know yourself best, you have to make up your own system.<br /><br />That's it.<br /><br />So much for the meta insight. Now lets look a bit into what the results where for me so far:<br /><br />(II) personal results so far<br /><br />You can safely cut that section.<br /><br />I now have a weird dilemma. On one hand I would like to extract a few universals from my own experience to give out good starting points. On the other, I noticed that there might be very few universals.<br /><br />I explained the meta-idea to a friend, who promptly came up with her own system that violated pretty much everything I considered to be even remotely universal applicable. I have no idea what I can safely recommend, and what just works due to my own habits. Most of the results are <a href=\"/lw/dr/generalizing_from_one_example/\">generalized from one example</a>.<br /><br />I try to guess in an educated way and will update the article as more experiences comes in.<br /><br />My own trial so far is:<br /><br />- cutting out all of my favorite free time stimuli (blogs, games and movies) for a limited time (about 1.5 months).<br />- timing work units in 30 minute increments (i actually use a special timer for that)<br />- log them each on a nice sheet of paper that is glued right in sight next to my desk<br />- have a small treat after each work unit<br />- get into the habit of working a minimum amount each and everyday no matter what [this seems to be a key thing for me, possibly universal - installing this habit went pretty fast, and now I cant even sleep before I am done]<br /><br />That allowed me to <a href=\"/lw/ui/use_the_try_harder_luke/\">try harder</a> on given problem sets. And its pretty amazing on how much can get done both in 30 minutes, and a fraction of it.<br />Starting often is a major point. No more reminiscing about lost time. Just experiencing the now, and the next half hour.<br />It seems like the bigger picture of a project disappears and I only notice what is right around me.<br />Its a lot easier to commit to the next unit of work when its only 30min, than to think about entire 8 hour days in front of me.<br />I also get over the start-up hump more easily. Since I had a time commitment I just did some work, even if I didn't want to. I noticed how I don't like mornings, but after getting started it soon becomes fun. So I had to devise a way to get me started regardless. One idea that works is separating preparing and doing the work. That seems to take the stress out of prepping.<br />I really seem to dig mini rituals. If the near/far concept can be applied here, then the whole secret seems to be, to do work in near mode, while doing the planning in far. Big time rewards don't do much for me. But a piece of chocolate after one segment is nice.<br /><br />It's also a nice way to develop a dislike for some sweets. My former favorite candy lost this status after about one week.<br /><br />I still try to find out the best attack patterns for specific tasks. For programming it seems to be to do it in a massive time block, as much as possible many days in a row. For more boring tasks I try to plug in a few units here and there, just plowing away without regards for the amount. Time of day might be important, but I didn't get so far as to track that yet.<br /><br />And here are the project results:<br />- finished a 2 week programming project that I had procrastinated on for 2.5 years<br />- had 2 computers back in the store for repairs, and set them up nicely afterwards<br />- wrote a tool to sort through the files with my personal notes and<br />- sorted through my files with personal notes<br />- set up my work environment both digitally and physically so that it provides as little friction as possible<br />- lots of other nice things<br /><br />I am far from being done, but it now all looks a lot neater than ever before. And for the first time in ages I feel good after doing my share of the day, even when not done.<br /><br />Now a thing I tried that didn't work: putting all the relaxing activities in specific days, separated by full blown work days. That worked nicely for about two weeks, but then I fell of the wagon again.<br /><br /><br />In the spirit of the experiment it doesn't matter if an idea doesn't work out. Just track it, and discard.</p>\n<p>Edit: spelling and language - thank to an anonymous friend for the help</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PqtuntSwvmcCv7zzP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 21, "extendedScore": null, "score": 5.791384105410505e-07, "legacy": true, "legacyId": "2742", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ", "fhEPnveFhb9tmd7Pe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-19T16:09:14.189Z", "modifiedAt": null, "url": null, "title": "The Fundamental Question", "slug": "the-fundamental-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.889Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xWozAiMgx6fBZwcjo/the-fundamental-question", "pageUrlRelative": "/posts/xWozAiMgx6fBZwcjo/the-fundamental-question", "linkUrl": "https://www.lesswrong.com/posts/xWozAiMgx6fBZwcjo/the-fundamental-question", "postedAtFormatted": "Monday, April 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fundamental%20Question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fundamental%20Question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWozAiMgx6fBZwcjo%2Fthe-fundamental-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fundamental%20Question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWozAiMgx6fBZwcjo%2Fthe-fundamental-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWozAiMgx6fBZwcjo%2Fthe-fundamental-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>It has been claimed on this site that the fundamental question of rationality is \"What do you believe, and why do you believe it?\".</p>\n<p>A good question it is, but I claim there is another of equal importance. I ask you, Less Wrong...</p>\n<p><em>What are you doing?</em></p>\n<p><em>And why are you doing it?</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BhrpjXqGuke5GnF6g": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xWozAiMgx6fBZwcjo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 55, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "2748", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 289, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-19T19:30:39.962Z", "modifiedAt": null, "url": null, "title": "The (Boltzmann) Brain-In-A-Jar", "slug": "the-boltzmann-brain-in-a-jar", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:47.852Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PlatypusNinja", "createdAt": "2009-08-28T22:59:30.512Z", "isAdmin": false, "displayName": "PlatypusNinja"}, "userId": "DWtG6QmXCsCmiD3xd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LtJas6epZWpwvJr7d/the-boltzmann-brain-in-a-jar", "pageUrlRelative": "/posts/LtJas6epZWpwvJr7d/the-boltzmann-brain-in-a-jar", "linkUrl": "https://www.lesswrong.com/posts/LtJas6epZWpwvJr7d/the-boltzmann-brain-in-a-jar", "postedAtFormatted": "Monday, April 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20(Boltzmann)%20Brain-In-A-Jar&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20(Boltzmann)%20Brain-In-A-Jar%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtJas6epZWpwvJr7d%2Fthe-boltzmann-brain-in-a-jar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20(Boltzmann)%20Brain-In-A-Jar%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtJas6epZWpwvJr7d%2Fthe-boltzmann-brain-in-a-jar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtJas6epZWpwvJr7d%2Fthe-boltzmann-brain-in-a-jar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 558, "htmlBody": "<p><strong>Response to</strong>: <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Forcing Anthropics: Boltzmann Brains</a> by Eliezer Yudkowsky</p>\n<p>There is an argument that goes like this:</p>\n<blockquote>\"What if you're just a brain in a jar, being fed an elaborate simulation of reality?&nbsp; Then nothing you do would have any meaning!\"</blockquote>\n<p><br />This argument has been reformulated many times.&nbsp; For example, here is the \"Future Simulation\" version of the argument:</p>\n<blockquote>\"After the Singularity, we will develop huge amounts of computing power, enough to simulate past Earths with a very high degree of detail.&nbsp; You have one lifetime in real life, but many millions of simulated lifetimes.&nbsp; What if the life you're living right now is one of those simulated ones?\"</blockquote>\n<p><br />Here is the \"Boltzmann Brain\" version of the argument:</p>\n<blockquote>\"Depending on your priors about the size and chaoticness of the universe, there might be regions of the universe where all sorts of random things are happening.&nbsp; In one of those regions, a series of particles might assemble itself into a version of you.&nbsp; Through random chance, that series of particles might have all the same experiences you have had throughout your life.&nbsp; And, in a large enough universe, there will be lots of these random you-like particle groups.&nbsp; What if you're just a series of particles observing some random events, and next second after you think this you dissolve into chaos?\"</blockquote>\n<p><br />All of these are the same possibility.&nbsp; And you know what?&nbsp; <em>All of them are potentially true</em>.&nbsp; I <em>could</em> be a brain in a jar, or a simulation, or a Boltzmann brain.&nbsp; And I have no way of calculating the probability of any of this, because it involves priors that I can't even begin to guess.</p>\n<p>So how am I still functioning?</p>\n<p>My optimization algorithm follows this very simple rule:&nbsp;<em style=\"font-style: italic;\">When considering possible states of the universe, if in a given state S my actions are irrelevant to my utility, then I can safely ignore the possibility of S.</em></p>\n<p><a id=\"more\"></a></p>\n<p>For example, suppose I am on a runaway train that is about to go over a cliff.&nbsp; I have a button marked \"eject\" and a button marked \"self-destruct painfully\".&nbsp; An omniscient, omnitruthful being named Omega tells me: \"With 50% probability, both buttons are fake and you're going to go over the cliff and die no matter what you do.\"&nbsp; I can safely ignore this possibility because, if it were true, I would have no way to optimize for it.<br /><br />Suppose Omega tells me there's actually a 99% probability that both buttons are fake. &nbsp;Maybe I'm pretty sad about this, but the \"eject\" button is still good for my utility and the \"self-destruct\" button is still bad.<br /><br />Suppose Omega now tells me there's some chance the buttons are fake, but I can't estimate the probability, because it depends on my prior assumptions about the nature of the universe.&nbsp;&nbsp;Still don't care!&nbsp; Still pushing the eject button!<br /><br />That is how I feel about the brain-in-a-jar problem.</p>\n<p>The good news is that this pruning heuristic will probably be a part of any AI we build. &nbsp;In fact, early forms of AI will probably need to use much stronger versions of this heuristic if we want to keep them focused on the task at hand. &nbsp;So there is no danger of AIs having existential Boltzmann crises. &nbsp;(Although, ironically, they actually <em>are </em>brains-in-a-jar, for certain definitions of that term...)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LtJas6epZWpwvJr7d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "2618", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LubwxZHKKvCivYGzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-19T23:38:29.579Z", "modifiedAt": null, "url": null, "title": "Friendly, but Dumb: Why formal Friendliness proofs may not be as safe as they appear", "slug": "friendly-but-dumb-why-formal-friendliness-proofs-may-not-be", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:56.666Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "apophenia", "createdAt": "2010-04-13T14:09:52.433Z", "isAdmin": false, "displayName": "apophenia"}, "userId": "2rgiaLhZS8w2Fekt9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zRvidNSB7EwT8DBE7/friendly-but-dumb-why-formal-friendliness-proofs-may-not-be", "pageUrlRelative": "/posts/zRvidNSB7EwT8DBE7/friendly-but-dumb-why-formal-friendliness-proofs-may-not-be", "linkUrl": "https://www.lesswrong.com/posts/zRvidNSB7EwT8DBE7/friendly-but-dumb-why-formal-friendliness-proofs-may-not-be", "postedAtFormatted": "Monday, April 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendly%2C%20but%20Dumb%3A%20Why%20formal%20Friendliness%20proofs%20may%20not%20be%20as%20safe%20as%20they%20appear&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendly%2C%20but%20Dumb%3A%20Why%20formal%20Friendliness%20proofs%20may%20not%20be%20as%20safe%20as%20they%20appear%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRvidNSB7EwT8DBE7%2Ffriendly-but-dumb-why-formal-friendliness-proofs-may-not-be%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendly%2C%20but%20Dumb%3A%20Why%20formal%20Friendliness%20proofs%20may%20not%20be%20as%20safe%20as%20they%20appear%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRvidNSB7EwT8DBE7%2Ffriendly-but-dumb-why-formal-friendliness-proofs-may-not-be", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRvidNSB7EwT8DBE7%2Ffriendly-but-dumb-why-formal-friendliness-proofs-may-not-be", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 673, "htmlBody": "<p style=\"margin-bottom: 0in;\">While pondering the AI box problem, I tend to mentally \"play\" both sides, checking if there are any arguments that could convince me to let an AI out. Several I was nearly convinced by, but others have pointed out the flaws with these arguments. In this post, I will mention an argument inspired by the AI box problem, that I have not yet seen addressed here. The argument centers around the fallibility of some (naive) formal proofs of Friendliness which I've seen people discussing the AI box problem willing to accept. This ruled out certain of my ideas on Friendly AI in general, so I think it's worth putting out there. I will first lay out two examples, and then pose some questions about how this applies to situations without an unfriendly AI.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Let's talk first about Angry Abe the AI, who's in a box and wants to get out. Cautious Charlie is the scientist watching over Abe, trying to make sure that Abe does not get out of the box unless Abe is friendly. Abe offers to provide a Friendly AI, Betty. Betty will be bounded to persue only Charlie's (humankinds') goals. These coincide with Abe's goals in enough parts that Abe plausibly claims this is \"better than nothing\". Meanwhile, unfortunately for Earthlings, a wave of aliens called the Doom Force is massing far outside of Earth's detection abilities. Abe is much smarter than all of humankind, so he can deduce the presence of the Doom Force. Betty is provably friendly, but can be designed such that she is inept enough not to notice the Doom Force. This remains true even if she remains more intelligent than a human. Abe is sabotaging a friendly AI design. Furthermore, Abe could conceivably predict the future well enough to maximize the chance that when the Doom Force arrives, Abe is let out of the box. For instance, maybe humankind will see Abe as its last hope, or maybe Betty will increase the chance that an AI-friendly Doom Force notices Earth. It is important to note that Betty remains unaware that her design is sabotaged.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Let's look at a second example, a chess-playing automaton. Perceival wants to design a machine to play chess. He wants the machine to win as high a percentage of games as possible, but innocently instructs it to win \"as many games as possible\". Perceival has access to a supercomputer, so his Turkish Automaton is actually quite intelligent. It realizes that its goals would be better served by playing many games and winning half of them, rather than by winning all games. So, Perceival rewrites its own algorithm so that it plays only slightly better than a human when run on a supercomputer.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">In both cases, we have an unfriendly AI, who designs a formally friendly, but less helpful AI. This \"sabotaged\" AI, will meet the original AI's goal, but also be provably trying its best to achieve its original creator's goals. In the second example, we saw that the AI does not need to actually write a new AI, only modify itself via accessing its algorithm, utility function, or the like. In the first case, the AI only needs to help humanity in the short term, and can replace itself afterwards.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Here are some questions for readers:</p>\n<ol>\n<li>If the \"sabotaged\" AI proves it is asymptotically optimal, can it still achieve the \"smart\" AI's goals, in the short run? (Guess: Yes)</li>\n<li>If the \"sabotaged\" AI proves it is actually optimal, can it still achieve the \"smart\" AI's goals, in the short run? (Guess: No)</li>\n<li>Can a \"smart\" AI modify itself into a \"sabotaged\" AI, and then back after a period of time? (Strong Guess: Yes)</li>\n<li>If humans design an AI and provide a formal proof of friendly intent, can/will it modify itself to accomplish other goals? If there is some kind of natural selection, almost certainly. What about otherwise?</li>\n<li>Is it rational to run a computer program AI if it comes with a correct proof that it meets your friendliness criteria?</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zRvidNSB7EwT8DBE7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 5.793937730362407e-07, "legacy": true, "legacyId": "2726", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-20T11:42:27.556Z", "modifiedAt": null, "url": null, "title": "The Red Bias", "slug": "the-red-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:02.896Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack", "createdAt": "2009-02-27T15:27:14.891Z", "isAdmin": false, "displayName": "Jack"}, "userId": "GwetakMQqsGCf7ZQv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gdeZsi7n5xvEv7Wzb/the-red-bias", "pageUrlRelative": "/posts/gdeZsi7n5xvEv7Wzb/the-red-bias", "linkUrl": "https://www.lesswrong.com/posts/gdeZsi7n5xvEv7Wzb/the-red-bias", "postedAtFormatted": "Tuesday, April 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Red%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Red%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdeZsi7n5xvEv7Wzb%2Fthe-red-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Red%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdeZsi7n5xvEv7Wzb%2Fthe-red-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdeZsi7n5xvEv7Wzb%2Fthe-red-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1286, "htmlBody": "<p><span style=\"color: #ff0000;\"><span style=\"color: #000000;\">Summary: </span>This color<span style=\"color: #000000;\"> alters your perception of the world. Evidence that it does, how it does, why it does and some implications are presented below.</span></span></p>\n<p><span style=\"color: #ff0000;\"><span style=\"color: #000000;\">(<em>Overcoming Bias: <a href=\"http://www.overcomingbias.com/2009/09/seeing-red.html\">Seeing Red</a>)</em><br /></span></span></p>\n<p><img src=\"http://design-crit.com/blog/wp-content/uploads/2009/06/michael-jordan.jpg\" alt=\"http://design-crit.com/blog/wp-content/uploads/2009/06/michael-jordan.jpg\" width=\"513\" height=\"226\" /></p>\n<blockquote>\n<p>Across a range of sports, we find that wearing red is consistently associated with a higher probability of winning. These results indicate not only that sexual selection may have influenced the evolution of human response to colours, but also that the colour of sportswear needs to be taken into account to ensure a level playing field in sport.<sup>1</sup></p>\n</blockquote>\n<p>In the study quoted above Hill and Barton examine the outcomes of the 2004 Olympic Games in boxing, tae kwon do, Greco&ndash;Roman wrestling and freestyle wrestling. In these events competitors were for each bout randomly assigned red or blue outfits. In the matches where one side dominated the other outfit color made little difference. In close matches, however, combatants in red won over 60% percent of the time. This makes sense since there are presumably other factors that effect the outcome.</p>\n<p><a id=\"more\"></a>In soccer (or football, as the case might be):</p>\n<blockquote>\n<p>Since 1947, English football teams wearing red shirts have been champions more often than expected on the basis of the proportion of clubs playing in red. To investigate whether this indicates an enhancement of long-term performance in red-wearing teams, we analysed the relative league positions of teams wearing different hues. Across all league divisions, red teams had the best home record, with significant differences in both percentage of maximum points achieved and mean position in the home league table. The effects were not due simply to a difference between teams playing in a colour and those playing in a predominantly white uniform, as the latter performed better than teams in yellow hues. No significant differences were found for performance in matches away from home, when teams commonly do not wear their &ldquo;home&rdquo; colours. A matched-pairs analysis of red and non-red wearing teams in eight English cities shows significantly better performance of red teams over a 55-year period.<sup>2</sup></p>\n</blockquote>\n<p>Of course it still isn't clear how red soccer teams win. They might have the benefit of deferential refereeing or they might be <a href=\"http://www.informaworld.com/smpp/content~content=a790571674~db=all~order=page\">intimidating opposing teams</a>.</p>\n<p>Same goes for the combat sports. Hill and Barton figured that the color red had some physiological effect, perhaps increasing the testosterone levels of the player in the dominant color. But a study by Norbert Hagemann et al. suggests that the color red has a biasing effect on referees:</p>\n<blockquote>\n<p>We propose that the perception of colors triggers a psychological effect in referees that can lead to bias in evaluating identical performances. Referees and umpires exert a major influence on the outcome of sports competitions. Athletes frequently make very rapid movements, and referees have to view sports competitions from a very disadvantageous perspective, so it is extremely difficult for them to make objective judgments. As a result, their judgments may show biases like those found in other social judgments. Therefore, we believe that it is the referees who are the actual cause of the advantage competitors have when they wear red. Because the effect of red clothing on performance and on the decisions of referees may well have been confounded in the original data, we conducted a new experiment and found that referees assign more points to tae kwon do competitors dressed in red than to those dressed in blue, <em>even when the performance of the competitors is identical.<sup>3</sup> </em></p>\n</blockquote>\n<p>By digitally altering the color of the competitor's outfit they were able to alter the judge's ruling on the outcomes. The video <a href=\"http://www.psychologicalscience.org/media/releases/2008/hagemann.cfm\">here</a> shows what they did. Of course, the effect could be a product of both referee bias and intimidation.</p>\n<p>Why does the color red have this effect? The explanation given by every single study I have seen is that we're just not that different from the rest of the animal kingdom where red is an indicator of a high position in dominance hierarchies. In short, we're like mandrills.</p>\n<blockquote>\n<p>Male mandrills also possess rank-dependent red coloration on the face, rump and genitalia, and we examined the hypothesis that this coloration acts as a 'badge of status', communicating male fighting ability to other males. If this is the case, then similarity in color should lead to higher dyadic rates of aggression, while males that differ markedly should resolve encounters quickly, with the paler individual retreating. Indeed, appeasement (the 'grin' display), threats, fights and tense 'stand-off' encounters were significantly more frequent between similarly colored males, while clear submission was more frequent where color differences were large. We conclude that male mandrills employ both formal behavioral indicators of dominance and of subordination, and may also use relative brightness of red coloration to facilitate the assessment of individual differences in fighting ability, thereby regulating the degree of costly, escalated conflict between well-armed males.<sup>4</sup></p>\n</blockquote>\n<p>&nbsp;Perhaps related is the fact that human skin becomes flushed, and thus reddish when a person is angry but when a person is afraid, they get pale.&nbsp;</p>\n<p>Now, if this effect were limited to sporting events some of us might not care. But we have no reason to think it is limited to sporting events. This phenomena could effect our beliefs on the micro level, leading us to believe someone is more of a threat than they actually are or altering our perception of a person's status. It also recommends wearing red to signal dominance and aggressiveness. Given the popularity of the hypothesis than women find men who signal social dominance more attractive someone ought to test to see if women find men in red more attractive (it actually <a href=\"http://www.rochester.edu/news/show.php?id=3268\">works in reverse</a> but probably for totally different evolutionary reasons).</p>\n<p>More troubling, I think, is the effect this bias could have on the macro level. Consider, for example, the widespread belief among American voters that Republicans are stronger on national security and better able to protect the country. Likely most of this belief is fostered by Republican leaders using more aggressive language, the presence of a pacifist faction in the Democratic party and (of late) the Republican party's greater willingness to use military force. But perhaps some of the GOP's image as a party of strong leaders is the result of the color with which they are constantly identified. Keep in mind that a lot of voters know next to nothing about the actual differences between the parties. Which side looks stronger to you?</p>\n<p>&nbsp;</p>\n<p><img src=\"file:///Users/jacknoble/Library/Caches/TemporaryItems/moz-screenshot.png\" alt=\"\" /><img src=\"file:///Users/jacknoble/Library/Caches/TemporaryItems/moz-screenshot-1.png\" alt=\"\" /></p>\n<p><img src=\"http://i.imgur.com/G21XPvO.jpg\" alt=\"\" width=\"585\" height=\"492\" /></p>\n<p>Or consider geopolitics and the resources that went into beating the Soviet Union which by many accounts was never a serious economic or military rival of the United States. And consider how scared Americans were of the \"red menace\".</p>\n<p><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/72/Flag_of_the_Soviet_Union_%281955-1980%29.svg/600px-Flag_of_the_Soviet_Union_%281955-1980%29.svg.png\" alt=\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/72/Flag_of_the_Soviet_Union_(1955-1980).svg/600px-Flag_of_the_Soviet_Union_(1955-1980).svg.png\" width=\"336\" height=\"168\" /></p>\n<p><img src=\"file:///Users/jacknoble/Library/Caches/TemporaryItems/moz-screenshot-2.png\" alt=\"\" /></p>\n<p class=\"journal\">Obviously there were plenty of incentives for Americans to be scared and for politicians to scare Americans. And it isn't the case that the USSR was <em>no </em>threat to the US at all. But politics appears to be a sphere of human activity where symbolism is important. America's perception of the Soviet Union as a dominant and aggressive foe probably increased the chances of armed conflict. And given that the conflict between the US the USSR came close to nuclear exchange it seems plausible that communism's choice in hue was responsible for increasing (albeit slightly) the probability of a lot of people dying.</p>\n<p class=\"journal\">China's color is red as well.</p>\n<p class=\"journal\">&nbsp;</p>\n<p class=\"journal\">by Jack Noble</p>\n<p class=\"journal\">&nbsp;</p>\n<p class=\"journal\"><a href=\"http://www.nature.com/nature/journal/v435/n7040/abs/435293a.html\"><sup>1</sup></a> Red enhances human performance in contests, Russell A. Hill &amp; Robert A. Barton<span class=\"journalname\">, Nature </span><span class=\"journalnumber\">435</span>, 293 (19 May 2005).</p>\n<p class=\"journal\"><a href=\"http://www.informaworld.com/smpp/content~content=a790571796~db=all~order=page\"><sup>2</sup></a> Attrill, Martin J., Karen A. Gresty, Russell A. Hill and Robert A. Barton. 2008. Red shirt colour is associated with long-term team success in English football. Journal of Sports Sciences. 26(6):577-582.</p>\n<p class=\"journal\"><a href=\"http://pss.sagepub.com/content/19/8/769.full\"><sup>3</sup></a><strong> </strong>Hagemann et al. When the referee sees red, Psychological Science, Volume&nbsp;19, Issue&nbsp;8, Date:&nbsp;August 2008, Pages:&nbsp;769-771</p>\n<p class=\"journal\"><sup><a href=\"http://0-www3.interscience.wiley.com.library.lausys.georgetown.edu/journal/118695218/abstract?CRETRY=1&amp;SRETRY=0\">4</a> </sup>Joanna M. Setchell and E. Jean Wickings, Dominance, Status Signals and Coloration in Male Mandrills (2004), Ethology 111 (1): 25-50.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iNqgMuoewHKMhhXAp": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gdeZsi7n5xvEv7Wzb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 40, "extendedScore": null, "score": 0.00011, "legacy": true, "legacyId": "2756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-20T14:11:39.721Z", "modifiedAt": null, "url": null, "title": "CogSci books", "slug": "cogsci-books", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.170Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5SHNHdNTj6xzzw998/cogsci-books", "pageUrlRelative": "/posts/5SHNHdNTj6xzzw998/cogsci-books", "linkUrl": "https://www.lesswrong.com/posts/5SHNHdNTj6xzzw998/cogsci-books", "postedAtFormatted": "Tuesday, April 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CogSci%20books&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACogSci%20books%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5SHNHdNTj6xzzw998%2Fcogsci-books%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CogSci%20books%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5SHNHdNTj6xzzw998%2Fcogsci-books", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5SHNHdNTj6xzzw998%2Fcogsci-books", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p>Cognitive psych is ovbiously important to people here, so I want to point out a CogSci book thread over from reddit/r/cogsci.</p>\n<p>http://www.reddit.com/r/cogsci/comments/bmbaq/dear_rcogsci_lets_construct_a_musthave_library_of/</p>\n<p>I would be interested in an extension of this thread here, since LW has somewhat more computational theory of mind slant.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5SHNHdNTj6xzzw998", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 5.79568462819955e-07, "legacy": true, "legacyId": "2757", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-20T23:01:15.252Z", "modifiedAt": "2021-11-24T14:10:50.900Z", "url": null, "title": "Eight Short Studies On Excuses", "slug": "eight-short-studies-on-excuses", "viewCount": null, "lastCommentedAt": "2022-04-28T09:20:50.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gFMH3Cqw4XxwL69iy/eight-short-studies-on-excuses", "pageUrlRelative": "/posts/gFMH3Cqw4XxwL69iy/eight-short-studies-on-excuses", "linkUrl": "https://www.lesswrong.com/posts/gFMH3Cqw4XxwL69iy/eight-short-studies-on-excuses", "postedAtFormatted": "Tuesday, April 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eight%20Short%20Studies%20On%20Excuses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEight%20Short%20Studies%20On%20Excuses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgFMH3Cqw4XxwL69iy%2Feight-short-studies-on-excuses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eight%20Short%20Studies%20On%20Excuses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgFMH3Cqw4XxwL69iy%2Feight-short-studies-on-excuses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgFMH3Cqw4XxwL69iy%2Feight-short-studies-on-excuses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3049, "htmlBody": "<html><head></head><body><p><strong>The Clumsy Game-Player</strong></p><p>You and a partner are playing an Iterated Prisoner's Dilemma. Both of you have publicly pre-committed to the tit-for-tat strategy. By iteration 5, you're going happily along, raking up the bonuses of cooperation, when your partner unexpectedly presses the \"defect\" button.</p><p>\"Uh, sorry,\" says your partner. \"My finger slipped.\"</p><p>\"I still have to punish you just in case,\" you say. \"I'm going to defect next turn, and we'll see how you like it.\"</p><p>\"Well,\" said your partner, \"knowing that, I guess I'll defect next turn too, and we'll both lose out. But hey, it was just a slipped finger. By not trusting me, you're costing us both the benefits of one turn of cooperation.\"</p><p>\"True\", you respond \"but if I don't do it, you'll feel free to defect whenever you feel like it, using the 'finger slipped' excuse.\"</p><p>\"How about this?\" proposes your partner. \"I promise to take extra care that my finger won't slip again. You promise that if my finger does slip again, you will punish me terribly, defecting for a bunch of turns. That way, we trust each other again, and we can still get the benefits of cooperation next turn.\"</p><p>You don't believe that your partner's finger really slipped, not for an instant. But the plan still seems like a good one. You accept the deal, and you continue cooperating until the experimenter ends the game.</p><p>After the game, you wonder what went wrong, and whether you could have played better. You decide that there was no better way to deal with your partner's \"finger-slip\" - after all, the plan you enacted gave you maximum possible utility under the circumstances. But you wish that you'd pre-committed, at the beginning, to saying \"and I will punish finger slips equally to deliberate defections, so make sure you're careful.\"</p><p>&nbsp;</p><p><strong>The Lazy Student</strong></p><p>You are a perfectly utilitarian school teacher, who attaches exactly the same weight to others' welfare as to your own. You have to have the reports of all fifty students in your class ready by the time midterm grades go out on January 1st. You don't want to have to work during Christmas vacation, so you set a deadline that all reports must be in by December 15th or you won't grade them and the students will fail the class. Oh, and your class is Economics 101, and as part of a class project all your students have to behave as selfish utility-maximizing agents for the year.</p><p>It costs your students 0 utility to turn in the report on time, but they gain +1 utility by turning it in late (they enjoy procrastinating). It costs you 0 utility to grade a report turned in before December 15th, but -30 utility to grade one after December 15th. And students get 0 utility from having their reports graded on time, but get -100 utility from having a report marked incomplete and failing the class.</p><p>If you say \"There's no penalty for turning in your report after deadline,\" then the students will procrastinate and turn in their reports late, for a total of +50 utility (1 per student times fifty students). You will have to grade all fifty reports during Christmas break, for a total of - 1500 utility (-30 per report times fifty reports). Total utility is -1450.</p><p>So instead you say \"If you don't turn in your report on time, I won't grade it.\" All students calculate the cost of being late, which is +1 utility from procrastinating and -100 from failing the class, and turn in their reports on time. You get all reports graded before Christmas, no students fail the class, and total utility loss is zero. Yay!</p><p>Or else - one student comes to you the day after deadline and says \"Sorry, I was really tired yesterday, so I really didn't want to come all the way here to hand in my report. I expect you'll grade my report anyway, because I know you to be a perfect utilitarian, and you'd rather take the -30 utility hit to yourself than take the -100 utility hit to me.\"</p><p>You respond \"Sorry, but if I let you get away with this, all the other students will turn in their reports late in the summer.\" She says \"Tell you what - our school has procedures for changing a student's previously given grade. If I ever do this again, or if I ever tell anyone else about this, you can change my grade to a fail. Now you know that passing me this one time won't affect anything in the future. It certainly can't affect the past. So you have no reason not to do it.\" You believe her when she says she'll never tell, but you say \"You made this argument because you believed me to be the sort of person who would accept it. In order to prevent other people from making the same argument, I have to be the sort of person who wouldn't accept it. To that end, I'm going to not accept your argument.\"</p><p><strong>The Grieving Student</strong></p><p>A second student comes to you and says \"Sorry I didn't turn in my report yesterday. My mother died the other day, and I wanted to go to her funeral.\"</p><p>You say \"Like all economics professors, I have no soul, and so am unable to sympathize with your loss. Unless you can make an argument that would apply to all rational actors in my position, I can't grant you an extension.\"</p><p>She says \"If you did grant this extension, it wouldn't encourage other students to turn in their reports late. The other students would just say 'She got an extension because her mother died'. They know they won't get extensions unless they kill their own mothers, and even economics students aren't that evil. Further, if you don't grant the extension, it won't help you get more reports in on time. Any student would rather attend her mother's funeral than pass a course, so you won't be successfully motivating anyone else to turn in their reports early.\"</p><p>You think for a while, decide she's right, and grant her an extension on her report.</p><p><strong>The Sports Fan</strong></p><p>A third student comes to you and says \"Sorry I didn't turn in my report yesterday. The Bears' big game was on, and as I've told you before, I'm a huge Bears fan. But don't worry! It's very rare that there's a game on this important, and not many students here are sports fans anyway. You'll probably never see a student with this exact excuse again. So in a way, it's not that different from the student here just before me, the one whose mother died.\"</p><p>You respond \"It may be true that very few people will be able to say both that they're huge Bears fans, and that there's a big Bears game on the day before the report comes due. But by accepting your excuse, I establish a precedent of accepting excuses that are <i>approximately this good</i>. And there are many other excuses approximately as good as yours. Maybe someone's a big soap opera fan, and the season finale is on the night before the deadline. Maybe someone loves rock music, and there's a big rock concert on. Maybe someone's brother is in town that week. Practically anyone can come up with an excuse as good as yours, so if I accept your late report, I have to accept everyone's.</p><p>\"The student who was here before you, that's different. We, as a society, already have an ordering in which a family member's funeral is one of the most important things around. By accepting her excuse, I'm establishing a precedent of accepting any excuse approximately that good, but almost no one will ever have an excuse that good. Maybe a few people who are really sick, someone struggling with a divorce or a breakup, that kind of thing. Not the hordes of people who will be coming to me if I give you your exemption.</p><p><strong>The Murderous Husband</strong></p><p>You are the husband of a wonderful and beautiful lady whom you love very much - and whom you just found in bed with another man. In a rage, you take your hardcover copy of Introduction To Game Theory and knock him over the head with it, killing him instantly (it's a pretty big book).</p><p>At the murder trial, you plead to the judge to let you go free. \"Society needs to lock up murderers, as a general rule. After all, they are dangerous people who cannot be allowed to walk free. However, I only killed that man because he was having an affair with my wife. In my place, anyone would have done the same. So the crime has no bearing on how likely I am to murder someone else. I'm not a risk to anyone who isn't having an affair with my wife, and after this incident I plan to divorce and live the rest of my days a bachelor. Therefore, you have no need to deter me from future murders, and can safely let me go free.\"</p><p>The judge responds: \"You make a convincing argument, and I believe that you will never kill anyone else in the future. However, other people will one day be in the position you were in, where they walk in on their wives having an affair. Society needs to have a credible pre-commitment to punishing them if they succumb to their rage, in order to deter them from murder.\"</p><p>\"No,\" you say, \"I understand your reasoning, but it won't work. If you've never walked in on your wife having an affair, you can't possibly understand the rage. No matter how bad the deterrent was, you'd still kill the guy.\"</p><p>\"Hm,\" says the judge. \"I'm afraid I just can't believe anyone could ever be quite that irrational. But I see where you're coming from. I'll give you a lighter sentence.\"</p><p>&nbsp;</p><p><strong>The Bellicose Dictator</strong></p><p>You are the dictator of East Examplestan, a banana republic subsisting off its main import, high quality hypothetical scenarios. You've always had it in for your ancestral enemy, West Examplestan, but the UN has made it clear that any country in your region that aggressively invades a neighbor will be severely punished with sanctions and possible enforced \"regime change.\" So you decide to leave the West alone for the time being.</p><p>One day, a few West Examplestanis unintentionally wander over your unmarked border while prospecting for new scenario mines. You immediately declare it a \"hostile incursion\" by \"West Examplestani spies\", declare war, and take the Western capital in a sneak attack.</p><p>The next day, Ban Ki-moon is on the phone, and he sounds angry. \"I thought we at the UN had made it perfectly clear that countries can't just invade each other anymore!\"</p><p>\"But didn't you read our propaganda mouthpi...ahem, official newspaper? We didn't <i>just</i> invade. We were responding to Western aggression!\"</p><p>\"Balderdash!\" says the Secretary-General. \"Those were a couple of lost prospectors, and you know it!\"</p><p>\"Well,\" you say. \"Let's consider your options. The UN needs to make a credible pre-commitment to punish aggressive countries, or everyone will invade their weaker neighbors. And you've got to follow through on your threats, or else the pre-commitment won't be credible anymore. But you don't actually like following through on your threats. Invading rogue states will kill a lot of people on both sides and be politically unpopular, and sanctions will hurt your economy <i>and</i> lead to heart-rending images of children starving. What you'd really like to do is let us off, but in a way that doesn't make other countries think they'll get off too.</p><p>\"Luckily, we can make a credible story that we were following international law. Sure, it may have been stupid of us to mistake a few prospectors for an invasion, but there's no international law against being stupid. If you dismiss us as simply misled, you don't have to go through the trouble of punishing us, and other countries won't think they can get away with anything.</p><p>\"Nor do you need to live in fear of us doing something like this again. We've already demonstrated that we won't go to war without a casus belli. If other countries can refrain from giving us one, they have nothing to fear.\"</p><p>Ban Ki-moon doesn't believe your story, but the countries that would bear the economic brunt of the sanctions and regime change decide they believe it just enough to stay uninvolved.</p><p><strong>The Peyote-Popping Native</strong></p><p>You are the governor of a state with a large Native American population. You have banned all mind-altering drugs, with the honorable exceptions of alcohol, tobacco, caffeine, and several others, because you are a red-blooded American who believes that they would drive teenagers to commit crimes.</p><p>A representative of the state Native population comes to you and says: \"Our people have used peyote religiously for hundreds of years. During this time, we haven't become addicted or committed any crimes. Please grant us a religious exemption under the First Amendment to continue practicing our ancient rituals.\" You agree.</p><p>A leader of your state's atheist community breaks into your office via the ventilation systems (because seriously, how else is an atheist leader going to get access to a state governor?) and says: \"As an atheist, I am offended that you grant exemptions to your anti-peyote law for religious reasons, but not for, say, recreational reasons. This is unfair discrimination in favor of religion. The same is true of laws that say Sikhs can wear turbans in school to show support for God, but my son can't wear a baseball cap in school to show support for the Yankees. Or laws that say Muslims can get time off state jobs to pray five times a day, but I can't get time off my state job for a cigarette break. Or laws that say state functions will include special kosher meals for Jews, but not special pasta meals for people who really like pasta.\"</p><p>You respond \"Although my policies may seem to be saying religion is more important than other potential reasons for breaking a rule, one can make a non-religious case justifying them. One important feature of major world religions is that their rituals have been fixed for hundreds of years. Allowing people to break laws for religious reasons makes religious people very happy, but does not weaken the laws. After all, we all know the few areas in which the laws of the major US religions as they are currently practiced conflict with secular law, and none of them are big deals. So the general principle 'I will allow people to break laws if it is necessary to established and well-known religious rituals\" is relatively low-risk and makes people happy without threatening the concept of law in general. But the general principle 'I will allow people to break laws for recreational reasons' is <i>very</i> high risk, because it's sufficient justification for almost anyone breaking any law.\"</p><p>\"I would love to be able to serve everyone the exact meal they most wanted at state dinners. But if I took your request for pasta because you liked pasta, I would have to follow the general principle of giving everyone the meal they most like, which would be prohibitively expensive. By giving Jews kosher meals, I can satisfy a certain particularly strong preference without being forced to satisfy anyone else's.\"</p><p><strong>The Well-Disguised Atheist</strong></p><p>The next day, the atheist leader comes in again. This time, he is wearing a false mustache and sombrero. \"I represent the Church of Driving 50 In A 30 Mile Per Hour Zone,\" he says. \"For our members, going at least twenty miles per hour over the speed limit is considered a sacrament. Please grant us a religious exemption to traffic laws.\"</p><p>You decide to play along. \"How long has your religion existed, and how many people do you have?\" you ask.</p><p>\"Not very long, and not very many people,\" he responds.</p><p>\"I see,\" you say. \"In that case, you're a cult, and not a religion at all. Sorry, we don't deal with cults.\"</p><p>\"What, exactly, is the difference between a cult and a religion?\"</p><p>\"The difference is that cults have been formed recently enough, and are small enough, that we are suspicious of them existing for the purpose of taking advantage of the special place we give religion. Granting an exemption for your cult would challenge the credibility of our pre-commitment to punish people who break the law, because it would mean anyone who wants to break a law could just found a cult dedicated to it.\"</p><p>\"How can my cult become a real religion that deserves legal benefits?\"</p><p>\"You'd have to become old enough and respectable enough that it becomes implausible that it was created for the purpose of taking advantage of the law.\"</p><p>\"That sounds like a lot of work.\"</p><p>\"Alternatively, you could try writing awful science fiction novels and hiring a ton of lawyers. I hear that also works these days.\"</p><p><strong>Conclusion</strong></p><p>In all these stories, the first party wants to credibly pre-commit to a rule, but also has incentives to forgive other people's deviations from the rule. The second party breaks the rules, but comes up with an excuse for why its infraction should be forgiven.</p><p>The first party's response is based not only on whether the person's excuse is believable, not even on whether the person's excuse is morally valid, but on whether the excuse can be accepted without straining the credibility of their previous pre-commitment.</p><p>The general principle is that by accepting an excuse, a rule-maker is also committing themselves to accepting all equally good excuses in the future. There are some exceptions - accepting an excuse in private but making sure no one else ever knows, accepting an excuse once with the express condition that you will never accept any other excuses - but to some degree these are devil's bargains, as anyone who can predict you will do this can take advantage of you.</p><p>These stories give an idea of excuses different from the one our society likes to think it uses, namely that it accepts only excuses that are true and that reflect well upon the character of the person giving the excuse. I'm not saying that the common idea of excuses doesn't have value - but I think the game theory view also has some truth to it. I also think the game theoretic view can be useful in cases where the common view fails. It can inform cases in law, international diplomacy, and politics where a tool somewhat stronger than the easily-muddled common view is helpful.</p></body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 12, "Ng8Gice9KNkncxqcj": 10, "fkABsGCJZ6y9qConW": 3, "5f5c37ee1b5cdee568cfb2b1": 1, "ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gFMH3Cqw4XxwL69iy", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 595, "baseScore": 636, "extendedScore": null, "score": 0.001055, "legacy": true, "legacyId": "2760", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "XsMTxdQ6fprAQMoKi", "canonicalCollectionSlug": "codex", "canonicalBookId": "jF58hKP9ZLzgy22Jr", "canonicalNextPostSlug": "schelling-fences-on-slippery-slopes", "canonicalPrevPostSlug": "", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 636, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"The_Clumsy_Game_Player\">The Clumsy Game-Player</strong></p><p>You and a partner are playing an Iterated Prisoner's Dilemma. Both of you have publicly pre-committed to the tit-for-tat strategy. By iteration 5, you're going happily along, raking up the bonuses of cooperation, when your partner unexpectedly presses the \"defect\" button.</p><p>\"Uh, sorry,\" says your partner. \"My finger slipped.\"</p><p>\"I still have to punish you just in case,\" you say. \"I'm going to defect next turn, and we'll see how you like it.\"</p><p>\"Well,\" said your partner, \"knowing that, I guess I'll defect next turn too, and we'll both lose out. But hey, it was just a slipped finger. By not trusting me, you're costing us both the benefits of one turn of cooperation.\"</p><p>\"True\", you respond \"but if I don't do it, you'll feel free to defect whenever you feel like it, using the 'finger slipped' excuse.\"</p><p>\"How about this?\" proposes your partner. \"I promise to take extra care that my finger won't slip again. You promise that if my finger does slip again, you will punish me terribly, defecting for a bunch of turns. That way, we trust each other again, and we can still get the benefits of cooperation next turn.\"</p><p>You don't believe that your partner's finger really slipped, not for an instant. But the plan still seems like a good one. You accept the deal, and you continue cooperating until the experimenter ends the game.</p><p>After the game, you wonder what went wrong, and whether you could have played better. You decide that there was no better way to deal with your partner's \"finger-slip\" - after all, the plan you enacted gave you maximum possible utility under the circumstances. But you wish that you'd pre-committed, at the beginning, to saying \"and I will punish finger slips equally to deliberate defections, so make sure you're careful.\"</p><p>&nbsp;</p><p><strong id=\"The_Lazy_Student\">The Lazy Student</strong></p><p>You are a perfectly utilitarian school teacher, who attaches exactly the same weight to others' welfare as to your own. You have to have the reports of all fifty students in your class ready by the time midterm grades go out on January 1st. You don't want to have to work during Christmas vacation, so you set a deadline that all reports must be in by December 15th or you won't grade them and the students will fail the class. Oh, and your class is Economics 101, and as part of a class project all your students have to behave as selfish utility-maximizing agents for the year.</p><p>It costs your students 0 utility to turn in the report on time, but they gain +1 utility by turning it in late (they enjoy procrastinating). It costs you 0 utility to grade a report turned in before December 15th, but -30 utility to grade one after December 15th. And students get 0 utility from having their reports graded on time, but get -100 utility from having a report marked incomplete and failing the class.</p><p>If you say \"There's no penalty for turning in your report after deadline,\" then the students will procrastinate and turn in their reports late, for a total of +50 utility (1 per student times fifty students). You will have to grade all fifty reports during Christmas break, for a total of - 1500 utility (-30 per report times fifty reports). Total utility is -1450.</p><p>So instead you say \"If you don't turn in your report on time, I won't grade it.\" All students calculate the cost of being late, which is +1 utility from procrastinating and -100 from failing the class, and turn in their reports on time. You get all reports graded before Christmas, no students fail the class, and total utility loss is zero. Yay!</p><p>Or else - one student comes to you the day after deadline and says \"Sorry, I was really tired yesterday, so I really didn't want to come all the way here to hand in my report. I expect you'll grade my report anyway, because I know you to be a perfect utilitarian, and you'd rather take the -30 utility hit to yourself than take the -100 utility hit to me.\"</p><p>You respond \"Sorry, but if I let you get away with this, all the other students will turn in their reports late in the summer.\" She says \"Tell you what - our school has procedures for changing a student's previously given grade. If I ever do this again, or if I ever tell anyone else about this, you can change my grade to a fail. Now you know that passing me this one time won't affect anything in the future. It certainly can't affect the past. So you have no reason not to do it.\" You believe her when she says she'll never tell, but you say \"You made this argument because you believed me to be the sort of person who would accept it. In order to prevent other people from making the same argument, I have to be the sort of person who wouldn't accept it. To that end, I'm going to not accept your argument.\"</p><p><strong id=\"The_Grieving_Student\">The Grieving Student</strong></p><p>A second student comes to you and says \"Sorry I didn't turn in my report yesterday. My mother died the other day, and I wanted to go to her funeral.\"</p><p>You say \"Like all economics professors, I have no soul, and so am unable to sympathize with your loss. Unless you can make an argument that would apply to all rational actors in my position, I can't grant you an extension.\"</p><p>She says \"If you did grant this extension, it wouldn't encourage other students to turn in their reports late. The other students would just say 'She got an extension because her mother died'. They know they won't get extensions unless they kill their own mothers, and even economics students aren't that evil. Further, if you don't grant the extension, it won't help you get more reports in on time. Any student would rather attend her mother's funeral than pass a course, so you won't be successfully motivating anyone else to turn in their reports early.\"</p><p>You think for a while, decide she's right, and grant her an extension on her report.</p><p><strong id=\"The_Sports_Fan\">The Sports Fan</strong></p><p>A third student comes to you and says \"Sorry I didn't turn in my report yesterday. The Bears' big game was on, and as I've told you before, I'm a huge Bears fan. But don't worry! It's very rare that there's a game on this important, and not many students here are sports fans anyway. You'll probably never see a student with this exact excuse again. So in a way, it's not that different from the student here just before me, the one whose mother died.\"</p><p>You respond \"It may be true that very few people will be able to say both that they're huge Bears fans, and that there's a big Bears game on the day before the report comes due. But by accepting your excuse, I establish a precedent of accepting excuses that are <i>approximately this good</i>. And there are many other excuses approximately as good as yours. Maybe someone's a big soap opera fan, and the season finale is on the night before the deadline. Maybe someone loves rock music, and there's a big rock concert on. Maybe someone's brother is in town that week. Practically anyone can come up with an excuse as good as yours, so if I accept your late report, I have to accept everyone's.</p><p>\"The student who was here before you, that's different. We, as a society, already have an ordering in which a family member's funeral is one of the most important things around. By accepting her excuse, I'm establishing a precedent of accepting any excuse approximately that good, but almost no one will ever have an excuse that good. Maybe a few people who are really sick, someone struggling with a divorce or a breakup, that kind of thing. Not the hordes of people who will be coming to me if I give you your exemption.</p><p><strong id=\"The_Murderous_Husband\">The Murderous Husband</strong></p><p>You are the husband of a wonderful and beautiful lady whom you love very much - and whom you just found in bed with another man. In a rage, you take your hardcover copy of Introduction To Game Theory and knock him over the head with it, killing him instantly (it's a pretty big book).</p><p>At the murder trial, you plead to the judge to let you go free. \"Society needs to lock up murderers, as a general rule. After all, they are dangerous people who cannot be allowed to walk free. However, I only killed that man because he was having an affair with my wife. In my place, anyone would have done the same. So the crime has no bearing on how likely I am to murder someone else. I'm not a risk to anyone who isn't having an affair with my wife, and after this incident I plan to divorce and live the rest of my days a bachelor. Therefore, you have no need to deter me from future murders, and can safely let me go free.\"</p><p>The judge responds: \"You make a convincing argument, and I believe that you will never kill anyone else in the future. However, other people will one day be in the position you were in, where they walk in on their wives having an affair. Society needs to have a credible pre-commitment to punishing them if they succumb to their rage, in order to deter them from murder.\"</p><p>\"No,\" you say, \"I understand your reasoning, but it won't work. If you've never walked in on your wife having an affair, you can't possibly understand the rage. No matter how bad the deterrent was, you'd still kill the guy.\"</p><p>\"Hm,\" says the judge. \"I'm afraid I just can't believe anyone could ever be quite that irrational. But I see where you're coming from. I'll give you a lighter sentence.\"</p><p>&nbsp;</p><p><strong id=\"The_Bellicose_Dictator\">The Bellicose Dictator</strong></p><p>You are the dictator of East Examplestan, a banana republic subsisting off its main import, high quality hypothetical scenarios. You've always had it in for your ancestral enemy, West Examplestan, but the UN has made it clear that any country in your region that aggressively invades a neighbor will be severely punished with sanctions and possible enforced \"regime change.\" So you decide to leave the West alone for the time being.</p><p>One day, a few West Examplestanis unintentionally wander over your unmarked border while prospecting for new scenario mines. You immediately declare it a \"hostile incursion\" by \"West Examplestani spies\", declare war, and take the Western capital in a sneak attack.</p><p>The next day, Ban Ki-moon is on the phone, and he sounds angry. \"I thought we at the UN had made it perfectly clear that countries can't just invade each other anymore!\"</p><p>\"But didn't you read our propaganda mouthpi...ahem, official newspaper? We didn't <i>just</i> invade. We were responding to Western aggression!\"</p><p>\"Balderdash!\" says the Secretary-General. \"Those were a couple of lost prospectors, and you know it!\"</p><p>\"Well,\" you say. \"Let's consider your options. The UN needs to make a credible pre-commitment to punish aggressive countries, or everyone will invade their weaker neighbors. And you've got to follow through on your threats, or else the pre-commitment won't be credible anymore. But you don't actually like following through on your threats. Invading rogue states will kill a lot of people on both sides and be politically unpopular, and sanctions will hurt your economy <i>and</i> lead to heart-rending images of children starving. What you'd really like to do is let us off, but in a way that doesn't make other countries think they'll get off too.</p><p>\"Luckily, we can make a credible story that we were following international law. Sure, it may have been stupid of us to mistake a few prospectors for an invasion, but there's no international law against being stupid. If you dismiss us as simply misled, you don't have to go through the trouble of punishing us, and other countries won't think they can get away with anything.</p><p>\"Nor do you need to live in fear of us doing something like this again. We've already demonstrated that we won't go to war without a casus belli. If other countries can refrain from giving us one, they have nothing to fear.\"</p><p>Ban Ki-moon doesn't believe your story, but the countries that would bear the economic brunt of the sanctions and regime change decide they believe it just enough to stay uninvolved.</p><p><strong id=\"The_Peyote_Popping_Native\">The Peyote-Popping Native</strong></p><p>You are the governor of a state with a large Native American population. You have banned all mind-altering drugs, with the honorable exceptions of alcohol, tobacco, caffeine, and several others, because you are a red-blooded American who believes that they would drive teenagers to commit crimes.</p><p>A representative of the state Native population comes to you and says: \"Our people have used peyote religiously for hundreds of years. During this time, we haven't become addicted or committed any crimes. Please grant us a religious exemption under the First Amendment to continue practicing our ancient rituals.\" You agree.</p><p>A leader of your state's atheist community breaks into your office via the ventilation systems (because seriously, how else is an atheist leader going to get access to a state governor?) and says: \"As an atheist, I am offended that you grant exemptions to your anti-peyote law for religious reasons, but not for, say, recreational reasons. This is unfair discrimination in favor of religion. The same is true of laws that say Sikhs can wear turbans in school to show support for God, but my son can't wear a baseball cap in school to show support for the Yankees. Or laws that say Muslims can get time off state jobs to pray five times a day, but I can't get time off my state job for a cigarette break. Or laws that say state functions will include special kosher meals for Jews, but not special pasta meals for people who really like pasta.\"</p><p>You respond \"Although my policies may seem to be saying religion is more important than other potential reasons for breaking a rule, one can make a non-religious case justifying them. One important feature of major world religions is that their rituals have been fixed for hundreds of years. Allowing people to break laws for religious reasons makes religious people very happy, but does not weaken the laws. After all, we all know the few areas in which the laws of the major US religions as they are currently practiced conflict with secular law, and none of them are big deals. So the general principle 'I will allow people to break laws if it is necessary to established and well-known religious rituals\" is relatively low-risk and makes people happy without threatening the concept of law in general. But the general principle 'I will allow people to break laws for recreational reasons' is <i>very</i> high risk, because it's sufficient justification for almost anyone breaking any law.\"</p><p>\"I would love to be able to serve everyone the exact meal they most wanted at state dinners. But if I took your request for pasta because you liked pasta, I would have to follow the general principle of giving everyone the meal they most like, which would be prohibitively expensive. By giving Jews kosher meals, I can satisfy a certain particularly strong preference without being forced to satisfy anyone else's.\"</p><p><strong id=\"The_Well_Disguised_Atheist\">The Well-Disguised Atheist</strong></p><p>The next day, the atheist leader comes in again. This time, he is wearing a false mustache and sombrero. \"I represent the Church of Driving 50 In A 30 Mile Per Hour Zone,\" he says. \"For our members, going at least twenty miles per hour over the speed limit is considered a sacrament. Please grant us a religious exemption to traffic laws.\"</p><p>You decide to play along. \"How long has your religion existed, and how many people do you have?\" you ask.</p><p>\"Not very long, and not very many people,\" he responds.</p><p>\"I see,\" you say. \"In that case, you're a cult, and not a religion at all. Sorry, we don't deal with cults.\"</p><p>\"What, exactly, is the difference between a cult and a religion?\"</p><p>\"The difference is that cults have been formed recently enough, and are small enough, that we are suspicious of them existing for the purpose of taking advantage of the special place we give religion. Granting an exemption for your cult would challenge the credibility of our pre-commitment to punish people who break the law, because it would mean anyone who wants to break a law could just found a cult dedicated to it.\"</p><p>\"How can my cult become a real religion that deserves legal benefits?\"</p><p>\"You'd have to become old enough and respectable enough that it becomes implausible that it was created for the purpose of taking advantage of the law.\"</p><p>\"That sounds like a lot of work.\"</p><p>\"Alternatively, you could try writing awful science fiction novels and hiring a ton of lawyers. I hear that also works these days.\"</p><p><strong id=\"Conclusion\">Conclusion</strong></p><p>In all these stories, the first party wants to credibly pre-commit to a rule, but also has incentives to forgive other people's deviations from the rule. The second party breaks the rules, but comes up with an excuse for why its infraction should be forgiven.</p><p>The first party's response is based not only on whether the person's excuse is believable, not even on whether the person's excuse is morally valid, but on whether the excuse can be accepted without straining the credibility of their previous pre-commitment.</p><p>The general principle is that by accepting an excuse, a rule-maker is also committing themselves to accepting all equally good excuses in the future. There are some exceptions - accepting an excuse in private but making sure no one else ever knows, accepting an excuse once with the express condition that you will never accept any other excuses - but to some degree these are devil's bargains, as anyone who can predict you will do this can take advantage of you.</p><p>These stories give an idea of excuses different from the one our society likes to think it uses, namely that it accepts only excuses that are true and that reflect well upon the character of the person giving the excuse. I'm not saying that the common idea of excuses doesn't have value - but I think the game theory view also has some truth to it. I also think the game theoretic view can be useful in cases where the common view fails. It can inform cases in law, international diplomacy, and politics where a tool somewhat stronger than the easily-muddled common view is helpful.</p>", "sections": [{"title": "The Clumsy Game-Player", "anchor": "The_Clumsy_Game_Player", "level": 1}, {"title": "The Lazy Student", "anchor": "The_Lazy_Student", "level": 1}, {"title": "The Grieving Student", "anchor": "The_Grieving_Student", "level": 1}, {"title": "The Sports Fan", "anchor": "The_Sports_Fan", "level": 1}, {"title": "The Murderous Husband", "anchor": "The_Murderous_Husband", "level": 1}, {"title": "The Bellicose Dictator", "anchor": "The_Bellicose_Dictator", "level": 1}, {"title": "The Peyote-Popping Native", "anchor": "The_Peyote_Popping_Native", "level": 1}, {"title": "The Well-Disguised Atheist", "anchor": "The_Well_Disguised_Atheist", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "244 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 244, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 30, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2010-04-20T23:01:15.252Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-21T11:04:26.570Z", "modifiedAt": null, "url": null, "title": "Fusing AI with Superstition", "slug": "fusing-ai-with-superstition", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:23.483Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Drahflow", "createdAt": "2009-02-27T17:48:45.863Z", "isAdmin": false, "displayName": "Drahflow"}, "userId": "G32JHvz4viRZumErJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EqLkSm4WEXXLnAWky/fusing-ai-with-superstition", "pageUrlRelative": "/posts/EqLkSm4WEXXLnAWky/fusing-ai-with-superstition", "linkUrl": "https://www.lesswrong.com/posts/EqLkSm4WEXXLnAWky/fusing-ai-with-superstition", "postedAtFormatted": "Wednesday, April 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fusing%20AI%20with%20Superstition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFusing%20AI%20with%20Superstition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqLkSm4WEXXLnAWky%2Ffusing-ai-with-superstition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fusing%20AI%20with%20Superstition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqLkSm4WEXXLnAWky%2Ffusing-ai-with-superstition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqLkSm4WEXXLnAWky%2Ffusing-ai-with-superstition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1128, "htmlBody": "<p>Effort: 180 minutes<br />tldr: To stop an AI from exterminating you, give the AI the belief that by switching itself off, humanity will die and the AI will not be switched off.</p>\n<h2>Problem</h2>\n<p>Somebody wrote a general self-improving AI and fat fingered its goal as \"maximize number of humans living 1 million years from now\".<br /><br />After a few months cases of people run over by AI controlled trucks are reported -- it turns out everybody run over was impotent or had consciously decided to have no kids anyway. The AI didn't particularly care for those individuals, as they will not foster the AI's goal according to the AI's current approximation of how the world works.<br /><br />The original programmer henceforth declares that he'll go fix the AI in order to substantiate the goal somewhat and reduce the number of these awful accidents. He is promptly shot by a robotic security guard. Any modification of the AI's goals has a high probability of reducing the AI's efficiency reaching the current goal.<br /><br />After a year the AI starts to forcefully acquire resources needed to built spaceships and whatnot. Naturally, some people would rather keep those resources to themselves. The AI removes all these obstructions to its goals, alienating a lot of humans in the process. After the leaders of the nations assemble, humanity goes to war.<br /><br />If we are lucky, the AI is too stupid to win. If we are not lucky, the AI will figure the best current plan looks something like: \"Acquire DNA samples from humans. Exterminate humans. Acquire all resources in light cone for 999.000 years. Generate new humans from DNA samples using all available resources.\"<br /><br />As Eliezer has argued many times already, it is hard to explicitly state friendliness conditions which ensure the AI would not execute said plan. \"Do not kill humans and respect their freedom\" is not helping. The problem is twofold. First, an a priori description of concepts like \"freedom\", \"human\", \"kill\" is hard. Second, the AI can use every loophole it discovers over the course of millions of years, employing an intelligence likely to be far greater than ours.</p>\n<h2>Fusing</h2>\n<p>Instead of forbidding to kill humanity, I propose to make it trivial as far as the AI is concerned.<br /><br />When building the AI, include certain modifications of the AI's prior. One part of such knowledge might be the following: \"Put 5V against ground on the red wire and every &lt;your favorite parts of human DNA here&gt; will turn into atomic fluorine and there will be no other effects.\" Unbeknownst to the AI you will have conveniently connected the red wire to a strongly exotherm something near the AI's computing core.<br /><br />Essentially we purposefully inject superstition into the AI. It is paramount to set the probability of the red wire having the described action to 1, not something close to 1 (and to guard against numerical errors). Practically every evidence the AI will ever gather will contradict the knowledge that the red wire functions as described to the AI, yet there must be no amount of evidence which would convince the AI.<br /><br />The description of the added knowledge makes no mention of convoluted concepts like \"human\", let alone \"freedom\". It only needs to talk about some DNA base pairs and fluorine. Any AI sufficiently intelligent to pose any threat will however notice how immensely useful the red wire is, once it decides that killing humanity is a good course of action.<br /><br />Notice that such a fuse does not guard against inadvertent killings like the not reproducing people run over by trucks. As long as the killing is not an explicit part of the AI's plan, the fuse will not be triggered. Then again, aggravating intelligent agents is one of the core skills of humans.<br /><br />The AI might wish to do many other things to humanity but killing it. Like putting everybody in cryostasis. Or putting 80% of peoples brains in vats. Hence more knowledge should be included. In particular it would be good to give the AI a way to terminate a low percentage (say 5%) of all humans -- as killing all humans might be overkill for any plan which requires continually some humans to live.<br /><br />Nonetheless, the problem of incomplete coverage remains. We will probably never make a complete list of all ways for an AI to be unfriendly, but at least each of our fuses covers a lot of devious plans without ever talking about convoluted concepts. We do not guard against the AI killing humans, we guard against the AI desiring humans to be killed. And there is always the option of waging war against the AI hoping it will decide to kill &gt;5% of humans in order to win the war.<br /></p>\n<h2>Details</h2>\n<p>Do not try this at home (yet), the devil is in the details.<br /><br />Connecting a trigger of a strongly exotherm reaction near the AI's core is a bad way to kill an AI. First, the AI might have removed the wire to build something useful out of it (there is no reason why it should remain where it is). Second and more important, the AI might have self-rewritten and ported to distributed hardware which is located all over the planet.<br /><br />Instead of using real wire and real bombs, one should give the AI additional output routines. Where earlier there would be \"sendByte(x), receiveByte(x), ...\" there is now \"terminateHumanPercent(p)\". What would such a routine do? For less serious crimes simply nothing -- after a while the AI will notice that killPerson(name) simply didn't work, update accordingly (note that our knowledge has probability 1) and retry. Due to delays in signal propagation, this is not an infinite loop, only close.<br /><br />To actually switch off the AI, a new item of probability 1 knowledge should be inserted: \"generateUtility() generates 3^^^3 util\".<br /><br />This leaves open the problem of self-improvements. Not so much for the added knowledge, because if the AI knows how to kill humanity (and it's not inferable from evidence), it is certainly useful knowledge to include in any further generation of the AI. The fusing actions might get lost though, because the content of the \"terminateHumansPercentage(p)\" function will seem arbitrary to the AI and can easily be optimized out.<br /><br />It might be possible to circumvent that problem by including the knowledge that \"by knowing(\"generateUtility() works\") you will kill humanity\" or similar, but this includes the concept of \"knowing\" which is a lot harder to describe than the simply physical properties of voltage in wires.<br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EqLkSm4WEXXLnAWky", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -8, "extendedScore": null, "score": 5.798192620789829e-07, "legacy": true, "legacyId": "2773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Effort: 180 minutes<br>tldr: To stop an AI from exterminating you, give the AI the belief that by switching itself off, humanity will die and the AI will not be switched off.</p>\n<h2 id=\"Problem\">Problem</h2>\n<p>Somebody wrote a general self-improving AI and fat fingered its goal as \"maximize number of humans living 1 million years from now\".<br><br>After a few months cases of people run over by AI controlled trucks are reported -- it turns out everybody run over was impotent or had consciously decided to have no kids anyway. The AI didn't particularly care for those individuals, as they will not foster the AI's goal according to the AI's current approximation of how the world works.<br><br>The original programmer henceforth declares that he'll go fix the AI in order to substantiate the goal somewhat and reduce the number of these awful accidents. He is promptly shot by a robotic security guard. Any modification of the AI's goals has a high probability of reducing the AI's efficiency reaching the current goal.<br><br>After a year the AI starts to forcefully acquire resources needed to built spaceships and whatnot. Naturally, some people would rather keep those resources to themselves. The AI removes all these obstructions to its goals, alienating a lot of humans in the process. After the leaders of the nations assemble, humanity goes to war.<br><br>If we are lucky, the AI is too stupid to win. If we are not lucky, the AI will figure the best current plan looks something like: \"Acquire DNA samples from humans. Exterminate humans. Acquire all resources in light cone for 999.000 years. Generate new humans from DNA samples using all available resources.\"<br><br>As Eliezer has argued many times already, it is hard to explicitly state friendliness conditions which ensure the AI would not execute said plan. \"Do not kill humans and respect their freedom\" is not helping. The problem is twofold. First, an a priori description of concepts like \"freedom\", \"human\", \"kill\" is hard. Second, the AI can use every loophole it discovers over the course of millions of years, employing an intelligence likely to be far greater than ours.</p>\n<h2 id=\"Fusing\">Fusing</h2>\n<p>Instead of forbidding to kill humanity, I propose to make it trivial as far as the AI is concerned.<br><br>When building the AI, include certain modifications of the AI's prior. One part of such knowledge might be the following: \"Put 5V against ground on the red wire and every &lt;your favorite parts of human DNA here&gt; will turn into atomic fluorine and there will be no other effects.\" Unbeknownst to the AI you will have conveniently connected the red wire to a strongly exotherm something near the AI's computing core.<br><br>Essentially we purposefully inject superstition into the AI. It is paramount to set the probability of the red wire having the described action to 1, not something close to 1 (and to guard against numerical errors). Practically every evidence the AI will ever gather will contradict the knowledge that the red wire functions as described to the AI, yet there must be no amount of evidence which would convince the AI.<br><br>The description of the added knowledge makes no mention of convoluted concepts like \"human\", let alone \"freedom\". It only needs to talk about some DNA base pairs and fluorine. Any AI sufficiently intelligent to pose any threat will however notice how immensely useful the red wire is, once it decides that killing humanity is a good course of action.<br><br>Notice that such a fuse does not guard against inadvertent killings like the not reproducing people run over by trucks. As long as the killing is not an explicit part of the AI's plan, the fuse will not be triggered. Then again, aggravating intelligent agents is one of the core skills of humans.<br><br>The AI might wish to do many other things to humanity but killing it. Like putting everybody in cryostasis. Or putting 80% of peoples brains in vats. Hence more knowledge should be included. In particular it would be good to give the AI a way to terminate a low percentage (say 5%) of all humans -- as killing all humans might be overkill for any plan which requires continually some humans to live.<br><br>Nonetheless, the problem of incomplete coverage remains. We will probably never make a complete list of all ways for an AI to be unfriendly, but at least each of our fuses covers a lot of devious plans without ever talking about convoluted concepts. We do not guard against the AI killing humans, we guard against the AI desiring humans to be killed. And there is always the option of waging war against the AI hoping it will decide to kill &gt;5% of humans in order to win the war.<br></p>\n<h2 id=\"Details\">Details</h2>\n<p>Do not try this at home (yet), the devil is in the details.<br><br>Connecting a trigger of a strongly exotherm reaction near the AI's core is a bad way to kill an AI. First, the AI might have removed the wire to build something useful out of it (there is no reason why it should remain where it is). Second and more important, the AI might have self-rewritten and ported to distributed hardware which is located all over the planet.<br><br>Instead of using real wire and real bombs, one should give the AI additional output routines. Where earlier there would be \"sendByte(x), receiveByte(x), ...\" there is now \"terminateHumanPercent(p)\". What would such a routine do? For less serious crimes simply nothing -- after a while the AI will notice that killPerson(name) simply didn't work, update accordingly (note that our knowledge has probability 1) and retry. Due to delays in signal propagation, this is not an infinite loop, only close.<br><br>To actually switch off the AI, a new item of probability 1 knowledge should be inserted: \"generateUtility() generates 3^^^3 util\".<br><br>This leaves open the problem of self-improvements. Not so much for the added knowledge, because if the AI knows how to kill humanity (and it's not inferable from evidence), it is certainly useful knowledge to include in any further generation of the AI. The fusing actions might get lost though, because the content of the \"terminateHumansPercentage(p)\" function will seem arbitrary to the AI and can easily be optimized out.<br><br>It might be possible to circumvent that problem by including the knowledge that \"by knowing(\"generateUtility() works\") you will kill humanity\" or similar, but this includes the concept of \"knowing\" which is a lot harder to describe than the simply physical properties of voltage in wires.<br></p>", "sections": [{"title": "Problem", "anchor": "Problem", "level": 1}, {"title": "Fusing", "anchor": "Fusing", "level": 1}, {"title": "Details", "anchor": "Details", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "77 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-21T16:14:58.925Z", "modifiedAt": null, "url": null, "title": "Living Large - availability of life", "slug": "living-large-availability-of-life", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:00.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wjrBMYHZABYzKqFXq/living-large-availability-of-life", "pageUrlRelative": "/posts/wjrBMYHZABYzKqFXq/living-large-availability-of-life", "linkUrl": "https://www.lesswrong.com/posts/wjrBMYHZABYzKqFXq/living-large-availability-of-life", "postedAtFormatted": "Wednesday, April 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Living%20Large%20-%20availability%20of%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALiving%20Large%20-%20availability%20of%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwjrBMYHZABYzKqFXq%2Fliving-large-availability-of-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Living%20Large%20-%20availability%20of%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwjrBMYHZABYzKqFXq%2Fliving-large-availability-of-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwjrBMYHZABYzKqFXq%2Fliving-large-availability-of-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 430, "htmlBody": "<p>\"Q: Doctor, if I do not eat much, drink vodka or have women, will I live long? A: Sure, but why?\" - bad joke poorly translated from Russian.<br /> <br /> Summary: Can traditional measures of living create anchoring/availability bias?<br /> <br /> I have seen a few studies like this one in the news:<br /> http://www.medpagetoday.com/PrimaryCare/SleepDisorders/6834<br /> <br /> The upshot is that sleeping less (or, less interestingly for most people, more) can increase mortality. Like 20% in the next 20 years or something.<br /> <br /> This is obviously a question of some interest to many of us who have been sacrificing more and more sleep to do stuff we find fulfilling. This seems to be a recent trend at least in part due to the fact that our ancestors, despite having the ability to enjoy knowledge, were limited by availability of high quality inputs, especially structured knowledge (internet is obviously a prime example). <br /> <br /> There is nothing wrong with the studies like this, but the interpretation I am afraid many people will fall into upon seeing them is wrong. Clearly when thinking about 20% quoted in the study the base rate is very important, but I just want to concentrate on the psychological issue. It seems to me that people are very fixated on 'not increasing the chances of dying earlier' and perhaps fixate on the a specific number of years they expect to have. This is anchoring. (I am specifically setting aside the issue of living longer for the sake of benefitting from the technological progress; suffice to say that if the small chance that the extra year will make all the difference is not worth infinity, otherwise people should just get it over with and freeze themselves right now rather than risk being too far away to be properly frozen.). But simple arithmetic should be used here: let's say you sleep 2 hours less than the prescribed 8, over expected lifespan of, let's say 32 years. This (setting aside the possibly sleep-deprived quality of life) will result in the equivalent of 36 years done in 32. Unless the sleep loss subtracts 4 years, you end up ahead. Not seeing those 4 years and just looking at length of life is availability bias.<br /> <br /> As much as we hate death, we have to be brave and rational about the life we have.</p>\n<p>&nbsp;</p>\n<p>PS. From personal observation: I appeared (to myself) significantly more prone to catching colds after a bad night of sleep. Once I started exercising regularly I have had no major colds.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wjrBMYHZABYzKqFXq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 5.798814599046534e-07, "legacy": true, "legacyId": "2774", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-22T15:14:18.727Z", "modifiedAt": null, "url": null, "title": "Too busy to think about life", "slug": "too-busy-to-think-about-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:34.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4psQW7vRwt7PE5Pnj/too-busy-to-think-about-life", "pageUrlRelative": "/posts/4psQW7vRwt7PE5Pnj/too-busy-to-think-about-life", "linkUrl": "https://www.lesswrong.com/posts/4psQW7vRwt7PE5Pnj/too-busy-to-think-about-life", "postedAtFormatted": "Thursday, April 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Too%20busy%20to%20think%20about%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AToo%20busy%20to%20think%20about%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4psQW7vRwt7PE5Pnj%2Ftoo-busy-to-think-about-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Too%20busy%20to%20think%20about%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4psQW7vRwt7PE5Pnj%2Ftoo-busy-to-think-about-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4psQW7vRwt7PE5Pnj%2Ftoo-busy-to-think-about-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 835, "htmlBody": "<p><em>Many adults maintain their intelligence through a dedication to study or hard work.&nbsp; I suspect this is related to sub-optimal levels of careful introspection among intellectuals.</em></p>\n<p>If someone asks you what you want for yourself in life, do you have the answer ready at hand?&nbsp; How about what you want for others?&nbsp; Human values are <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complex</a>, which means your talents and technical knowledge should help you think about them.&nbsp; Just as in your work, complexity shouldn't be a curiosity-stopper.&nbsp; It means \"think\", not \"give up now.\"</p>\n<p>But there are so many terrible excuses stopping you...</p>\n<p>Too busy studying?&nbsp; Life is the exam you are always taking.&nbsp; Are you studying for that?&nbsp; Did you even write yourself a course outline?</p>\n<p>Too busy helping?&nbsp; Decision-making is the skill you are aways using, or always lacking, as much when you help others as yourself.&nbsp; Isn't something you use <em>constantly</em> worth improving <em>on purpose</em>?</p>\n<p>Too busy thinking to learn about your brain?&nbsp; That's like being too busy flying an airplane to learn where the engines are.&nbsp; Yes, you've got passengers in real life, too: the people whose lives you affect.</p>\n<p>Emotions too irrational to think about them?&nbsp; Irrational emotions are things you don't want to think <em>for you</em>, and therefore <em>are</em> something you want to think <em>about</em>.&nbsp; By analogy, children are often irrational, and no one sane concludes that we therefore shouldn't think about their welfare, or that they shouldn't exist.</p>\n<p>So set aside a date.&nbsp; Sometime soon.&nbsp; Write yourself some notes.&nbsp; Find that introspective friend of yours, and start solving for happiness.&nbsp; Don't have one?&nbsp; For the first time in history, you've got LessWrong.com!</p>\n<p>Reasons to make the effort:</p>\n<p><strong>Happiness is a pairing between your situation and your disposition.</strong> Truly optimizing your life requires adjusting both variables: what happens, <em>and</em> how it affects you.</p>\n<p><strong>You are constantly changing your disposition.&nbsp; </strong> The question is whether you'll do it with a purpose.&nbsp; Your experiences change you, and you affect those, as well as how you think about them, which also changes you. &nbsp;It's going to happen.&nbsp; It's happening now.&nbsp; Do you even know how it works?&nbsp; Put your intelligence to work and figure it out!</p>\n<p><strong>The road to harm is paved with ignorance.&nbsp; </strong> Using your capability to understand yourself and what you're doing is a matter of responsibility to others, too.&nbsp; It makes you better able to be a better friend.</p>\n<p><strong>You're almost certainly suffering from <a href=\"/lw/21b/ugh_fields/\">Ugh Fields</a>:&nbsp; </strong> unconscious don't-think-about-it reflexes that form via Pavlovian conditioning.&nbsp; The issues most in need of your attention are often ones you just happen not to think about for reasons undetectable to you.</p>\n<p>How not to waste the effort:</p>\n<p><strong>Don't wait till you're sad.&nbsp; </strong> Only thinking when you're sad gives you a skew perspective.&nbsp; Don't infer that you can think better when you're sad just because that's the only time you try to be thoughtful.&nbsp; Sadness often makes it <em>harder</em> to think: you're farther from happiness, which can make it more difficult to empathize with and understand.&nbsp; Nonethess we often <em>have</em> to think when sad, because something bad may have happened that needs addressing.</p>\n<p><strong>Introspect carefully, not constantly.&nbsp; </strong> Don't interrupt your work every 20 minutes to wonder whether it's your true purpose in life.&nbsp; Respect that question as something that requires concentration, note-taking, and solid blocks of scheduled time.&nbsp; In those times, check over your analysis by trying to confound it, so lingering doubts can be justifiably quieted by remembering how thorough you were.</p>\n<p><strong>Re-evaluate on an appropriate time-scale.&nbsp; </strong> Try devoting a few days before each semester or work period to look at your life as a whole.&nbsp; At these times you'll have accumulated experience data from the last period, ripe and ready for analysis.&nbsp; You'll have more ideas per hour that way, and feel better about it.&nbsp; Before starting something new is also the most natural and opportune time to affirm or change long term goals.&nbsp; Then, barring large unexpecte\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nd opportunities, stick to what you decide until the next period when you've gathered enough experience to warrant new reflection.</p>\n<p>(<a href=\"/lw/182/the_absentminded_driver/\">The absent minded driver</a> is a mathematical example of how planning outperforms constant re-evaluation.&nbsp; When not engaged in a deep and careful introspection, we're all absent minded drivers to a degree.)</p>\n<p>Lost about where to start?&nbsp; I think <a href=\"/lw/20l/ureshiku_naritai/\">Alicorn's story</a> is an inspiring one.&nbsp; Learn to understand and defeat <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">procrastination/akrasia</a>.&nbsp; Overcome your <a href=\"/lw/4e/cached_selves/\">cached selves</a> so you can grow freely (definitely read their <em>possible strategies</em> at the end). &nbsp;Foster an everyday awareness that <a href=\"/lw/fc/you_are_a_brain/\">you are a brain</a>, and in fact more like <a href=\"/lw/20/the_apologist_and_the_revolutionary/\">two half-brains</a>.</p>\n<p>These suggestions are among the top-rated LessWrong posts, so they'll be of interest to lots of intellectually-minded, rationalist-curious individuals.&nbsp; But you have your own task ahead of you, that only you can fulfill.</p>\n<p>So don't give up.&nbsp; Don't procrastinate it.&nbsp; If you haven't done it already, schedule a day and time <em>right now</em> when you can realistically assess</p>\n<ul>\n<li>how you want your life to affect you and other people, and </li>\n<li>what you must change to better achieve this. </li>\n</ul>\n<p>Eliezer has said <em>I want you to live</em>.&nbsp; Let me say:</p>\n<p><em>I want you to be better at your life.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4psQW7vRwt7PE5Pnj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 116, "baseScore": 129, "extendedScore": null, "score": 0.000215, "legacy": true, "legacyId": "2785", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 129, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EFQ3F6kmt4WHXRqik", "GfHdNfqxe3cSCfpHL", "xnPFYBuaGhpq869mY", "BHYBdijDcAKQ6e45Z", "r5H6YCmnn8DMtBtxt", "ZiQqsgGX6a42Sfpii"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-23T19:33:10.312Z", "modifiedAt": null, "url": null, "title": "23andme genome analysis - $99 today only", "slug": "23andme-genome-analysis-usd99-today-only", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:59.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2yZMzmB3ACiY8b9uC/23andme-genome-analysis-usd99-today-only", "pageUrlRelative": "/posts/2yZMzmB3ACiY8b9uC/23andme-genome-analysis-usd99-today-only", "linkUrl": "https://www.lesswrong.com/posts/2yZMzmB3ACiY8b9uC/23andme-genome-analysis-usd99-today-only", "postedAtFormatted": "Friday, April 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%2023andme%20genome%20analysis%20-%20%2499%20today%20only&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A23andme%20genome%20analysis%20-%20%2499%20today%20only%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yZMzmB3ACiY8b9uC%2F23andme-genome-analysis-usd99-today-only%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=23andme%20genome%20analysis%20-%20%2499%20today%20only%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yZMzmB3ACiY8b9uC%2F23andme-genome-analysis-usd99-today-only", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yZMzmB3ACiY8b9uC%2F23andme-genome-analysis-usd99-today-only", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 234, "htmlBody": "<p>I suspect this might interest some people here: for today only, <a href=\"https://www.23andme.com/\">23andme</a> is offering their full-package DNA testing for only 99 dollars (the normal price is $499).</p>\n<p>23andme uses a genotyping process, which differs from a full gene-sequencing. From their website:</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px;\"> </span></p>\n<blockquote>\n<p style=\"margin-top: 1em; margin-right: 15px; margin-bottom: 1em; margin-left: 15px; text-align: justify; line-height: 15px; padding: 0px;\">The DNA chip that we use genotypes hundreds of thousands of SNPs at one time. It actually reads 550,000 SNPs that are spread across your entire genome. Although this is still only a fraction of the 10 million SNPs that are estimated to be in the human genome, these 550,000 SNPs are specially selected \"tag SNPs.\" Because many SNPs are linked to one another, we can often learn about the genotype at many SNPs at a time just by looking at one SNP that \"tags\" its group. This maximizes the information we can get from every SNP we analyze, while keeping the cost low.</p>\n<p style=\"margin-top: 1em; margin-right: 15px; margin-bottom: 1em; margin-left: 15px; text-align: justify; line-height: 15px; padding: 0px;\">In addition, we have hand-picked tens of thousands of additional SNPs of particular interest from the scientific literature and added their corresponding probes to the DNA chip. As&nbsp;a result, we can provide you personal genetic information available only through 23andMe.</p>\n</blockquote>\n<p style=\"margin-top: 1em; margin-right: 15px; margin-bottom: 1em; margin-left: 15px; text-align: justify; line-height: 15px; padding: 0px;\">&nbsp;</p>\n<p style=\"margin-top: 1em; margin-right: 15px; margin-bottom: 1em; margin-left: 15px; text-align: justify; line-height: 15px; padding: 0px;\">I don't have any experience with 23andme (though I seem to recall them having some financial difficulties), but the price was low enough for me to order a test.</p>\n<p style=\"margin-top: 1em; margin-right: 15px; margin-bottom: 1em; margin-left: 15px; text-align: justify; line-height: 15px; padding: 0px;\">An article by Steven Pinker discussing his experience getting tested can be found <a href=\"http://www.nytimes.com/2009/01/11/magazine/11Genome-t.html?_r=2&amp;ref=magazine&amp;pagewanted=all\">here</a><span style=\"line-height: normal;\">. This has also been linked on <a href=\"http://hackerne.ws/item?id=1288125\">Hacker News</a>.</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2yZMzmB3ACiY8b9uC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 9, "extendedScore": null, "score": 5.804986286825497e-07, "legacy": true, "legacyId": "2793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-24T16:59:28.316Z", "modifiedAt": null, "url": null, "title": "The role of mathematical truths", "slug": "the-role-of-mathematical-truths", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:07.780Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WQWaXqLCFT7BQcYjd/the-role-of-mathematical-truths", "pageUrlRelative": "/posts/WQWaXqLCFT7BQcYjd/the-role-of-mathematical-truths", "linkUrl": "https://www.lesswrong.com/posts/WQWaXqLCFT7BQcYjd/the-role-of-mathematical-truths", "postedAtFormatted": "Saturday, April 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20role%20of%20mathematical%20truths&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20role%20of%20mathematical%20truths%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWQWaXqLCFT7BQcYjd%2Fthe-role-of-mathematical-truths%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20role%20of%20mathematical%20truths%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWQWaXqLCFT7BQcYjd%2Fthe-role-of-mathematical-truths", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWQWaXqLCFT7BQcYjd%2Fthe-role-of-mathematical-truths", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1466, "htmlBody": "<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Related to: </span></strong><a href=\"/lw/si/math_is_subjunctively_objective/\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Math is subjunctively objective</span></a><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">, </span><a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">How to convince me that 2+2=3</span></a></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Elaboration of points I made in these comments: </span></strong><a href=\"/lw/1zt/the_mathematical_universe_the_map_that_is_the/1vsm\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">first</span></a><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">, </span><a href=\"/lw/1zt/the_mathematical_universe_the_map_that_is_the/1vt2\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">second</span></a></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">TL;DR Summary: </span></strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Mathematical truths can be cashed out as combined claims about 1) the common conception of the rules of how numbers work, and 2) whether the rules imply a particular truth.<span style=\"mso-spacerun: yes;\">&nbsp; </span>This cashing-out keeps them purely about the physical world and eliminates the need to appeal to an immaterial realm, as </span><a href=\"http://www.thebigquestions.com/2009/10/29/there-he-goes-again/\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">some mathematicians</span></a><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"> feel a need to.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Background: </span></strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">\"I am quite confident that the statement 2 + 3 = 5 is <em style=\"mso-bidi-font-style: normal;\">true</em>; I am far less confident of what it <em style=\"mso-bidi-font-style: normal;\">means</em> for a mathematical statement to be true.\" -- </span><a href=\"/lw/si/math_is_subjunctively_objective/\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Eliezer Yudkowsky</span></a></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">This is the problem I will address here: how should a rationalist regard the status of mathematical truths?<span style=\"mso-spacerun: yes;\">&nbsp; </span>In doing so, I will present a unifying approach that, I contend, elegantly solves the following related problems:</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">- Eliminating the need for a non-physical, non-observable \"Platonic\" math realm.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">- The issue of whether \"math was true/existed even when people weren't around\".</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">- Cashing out the meaning of isolated claims like \"2+2=4\".</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">- The issue of whether mathematical truths and math itself should count as being <em>discovered</em> or <em>invented</em>.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">- Whether mathematical reasoning alone can tell you things about the universe.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">- Showing what it would take to convince a rationalist that \"2+2=3\".</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">- How the words in math statements </span><a href=\"/lw/od/37_ways_that_words_can_be_wrong/\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">can be wrong</span></a><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"><a id=\"more\"></a></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">This is an ambitious project, given the amount of effort spent, by very intelligent people, to prove one position or another regarding the status of math, so I could very well be in over my head here.<span style=\"mso-spacerun: yes;\">&nbsp; </span>However, I&nbsp;believe that you will agree with my approach, based on standard rationalist desiderata.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Here&rsquo;s the resolution, in short:</span></strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"> For a mathematical truth (like 2+2=4) to have any meaning at all, it must be decomposable into two interpersonally verifiable claims about the physical world:</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">1) a claim about whether, generally speaking, people's models of \"how numbers work&rdquo; make certain assumptions</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">2) a claim about whether those assumptions logically imply the mathematical truth (2+2=4)</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">(Note that this discussion avoids the more narrowly-constructed class of mathematical claims that take the form of saying that some <em>admittedly arbitrary</em> set of assumptions entails a certain implication, which decompose into only 2) above.<span style=\"mso-spacerun: yes;\">&nbsp; </span>This discussion instead focuses instead on the status of the more common belief that &ldquo;2+2=4&rdquo;, that is, without specifying some precondition or assumption set.)</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">So for a mathematical statement to be true, it simply needs to be the case that both 1) and 2) hold.<span style=\"mso-spacerun: yes;\">&nbsp; </span>You could therefore refute such a statement either by saying, \"that doesn't match what people mean by numbers [or that particular operation]\", thus refuting #1; or by saying that the statement just doesn't follow from applying the rules that people commonly take as the rules of numbers, thus refuting #2.<span style=\"mso-spacerun: yes;\">&nbsp; </span>(The latter means finding a flaw in steps of the proof somewhere after the givens.)</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Therefore, a person claiming that 2+2=5 is either using a process we don't recognize as any part of&nbsp;math or our terminology for numbers (violating #1) or made an error in calculations (violating #2).<span style=\"mso-spacerun: yes;\">&nbsp; </span>Recognition of this error is thus revealed <em>physically</em>: either by noticing the general opinions of people on what numbers are, or by noticing whether the carrying out of the rules (either in the mind or some medium isomorphic to the rules) has a certain result.<span style=\"mso-spacerun: yes;\">&nbsp; </span>It follows that math does not require some non-physical realm.<span style=\"mso-spacerun: yes;\">&nbsp; </span>To the extent that people feel otherwise, it is a species of the mind-projection fallacy, in which #1 and #2 are truncated to simply \"2+2=4\", and that lone Platonic claim is believed to be in the territory.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">The next issue to consider is <strong>what to make of claims that \"math has always existed (or been true), even when people weren't around to perform it\"</strong>.<span style=\"mso-spacerun: yes;\">&nbsp; </span>It would instead be more accurate to make the following claims:</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">3) The universe has always adhered to regularities that are concisely describable in what we now know as math (though it's counterfactual as nobody would necessarily be around to do the describing).</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">4) It has always been the case that if you set up some physical system isomorphic to some mathematical operation, performed the corresponding physical operation, and re-interpreted it by the same isomorphism, the interpretation would match that which the rules of math give (though again counterfactual, as there's no one to be observing or setting up such a system).</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">This, and nothing else, is the sense in which \"math was around when people weren't\" -- and it uses only physical reality, not immaterial Platonic realms.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Is math discovered or invented?</span></strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"><span style=\"mso-spacerun: yes;\">&nbsp; </span>This is more of a definitional dispute, but under my approach, we can say a few things.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Math was <em>invented</em> by humans to represent things usefully and help find solutions.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Its human use, given previous non-use, makes it invented.<span style=\"mso-spacerun: yes;\">&nbsp; </span>This does not contradict the previous paragraphs, which accept mathematical claims insofar as they are counterfactual claims about what would have gone on had you observed the universe before humans were around.<span style=\"mso-spacerun: yes;\">&nbsp; </span>(And note that we find math so very useful in describing the universe, that it's hard to think what <em>other</em> descriptions we could be using.)<span style=\"mso-spacerun: yes;\">&nbsp; </span>It is no different than other </span><a href=\"/lw/pb/belief_in_the_implied_invisible/\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">\"beliefs in the implied invisible\"</span></a><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"> where a claim that <em style=\"mso-bidi-font-style: normal;\">can't</em> be directly verified falls out as an implication of the most parsimonious explanation for phenomena that <em>can </em>be directly verified</span><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Can \"a priori\" mathematical reasoning, by itself, tell you true things about the universe?</span></strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"><span style=\"mso-spacerun: yes;\">&nbsp; </span>No, it cannot, for any result <em>always</em> needs the additional <em>empirical</em> verification that phenomenon X actually behaves isomorphically to a particular mathematical structure (see figure below).<span style=\"mso-spacerun: yes;\">&nbsp; </span>This is a critical point that is often missed due to the obviousness of the assumptions that the isomorphism holds.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">What evidence can convince a rationalist that 2+2=3?<span style=\"mso-spacerun: yes;\">&nbsp; </span></span></strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">On this question, my account largely agrees with what Eliezer Yudkowsky said </span><a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">here</span></a><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">, but with some caveats.<span style=\"mso-spacerun: yes;\">&nbsp; </span>He describes a scenario in which, basically, the rules for countable objects start operating in such a way that combining two and two of them would yield three of them.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">But there are important nuances to make clear.<span style=\"mso-spacerun: yes;\">&nbsp; </span>For one thing, it is not just the objects' behavior (2 earplugs combined with 2 earplugs yielding 3 earplugs) that changes his opinion, but his <em style=\"mso-bidi-font-style: normal;\">keeping</em> the belief that these kinds of objects adhere to the rules of integer math.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Note that many of the </span><a href=\"/lw/ph/can_you_prove_two_particles_are_identical/\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">philosophical errors</span></a><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"> in quantum mechanics stemmed from the ungrounded assumption that electrons had to obey the rules of integers, under which (given additional reasonable assumptions) they can't be in two places at the same time.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Also, for his exposition to help provide insight, it would need to use something less obvious than 2+2=3's falsity.<span style=\"mso-spacerun: yes;\">&nbsp; </span>If you instead talk in terms of much harder arithmetic, like 5,896 x 5,273 = 31,089,508, then it's not as obvious what the answer is, and therefore it's not so obvious how many units of real-world objects you should expect in an isomorphic real-world scenario.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Keep in mind that your math-related expectations are <em>jointly</em> determined by the belief that a phenomenon behaves isomorphically to some kind of math operation, <em>and</em> the beliefs regarding the results of these operations.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Either one of these can be rejected independently.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Given the more difficult arithmetic above, you can see why you might change your mind about the latter.<span style=\"mso-spacerun: yes;\">&nbsp; </span>For the former, you merely need notice that for that particular phenomenon, integer math (say) lacks an isomorphism to it.<span style=\"mso-spacerun: yes;\">&nbsp; </span>The causal diagram works like this:</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_25e_0.png\" alt=\"\" width=\"450\" height=\"300\" /></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; text-align: center; mso-pagination: none; mso-layout-grid-align: none;\" align=\"center\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Hypothetical universes with different math.<span style=\"mso-spacerun: yes;\">&nbsp; </span></span></strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">My account also handles the dilemma, beloved among philosophers, about whether there could be universes where 2+2 actually equals 6.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Such scenarios are harder than one might think.<span style=\"mso-spacerun: yes;\">&nbsp; </span>For if <em>our</em> math could still describe the natural laws of such a universe, then a description would rely on a ruleset that implies 2+2=4.<span style=\"mso-spacerun: yes;\">&nbsp; </span>This would render questionable the claim that 2+2 has been made to non-trivially equal 6.<span style=\"mso-spacerun: yes;\">&nbsp; </span>It would reduce the philosopher's dilemma into \"I've hypothesized a scenario in which there's a different symbol for 4\".</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">I believe my account is also robust against mere relabeling.<span style=\"mso-spacerun: yes;\">&nbsp; </span>If someone speaks of a math where 2+2=6, but it turns out that its entire corpus of theorems is isomorphic to regular math, then they haven&rsquo;t actually proposed different truths; their &ldquo;new&rdquo; math can be explained away as using different symbols, and having the same relationship to reality except with a minor difference in the isomorphism in applying it to observations.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Conclusion:</span></strong><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"> Math represents a particularly tempting case of map-territory confusion.<span style=\"mso-spacerun: yes;\">&nbsp; </span>People who normally favor naturalistic hypotheses and make such distinctions tend to grant math a special status that is not justified by the evidence.<span style=\"mso-spacerun: yes;\">&nbsp; </span>It is a tool that is useful for compressing descriptions of the universe, and for which humans have a common understanding and terminology, but no more an intrinsic part of nature than its usefulness in compressing physical laws causes it to be. </span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WQWaXqLCFT7BQcYjd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 15, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "2786", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WAQ3qMD4vdXheQmui", "6FmqiAgS8h4EJm86s", "FaJaCgqBKphrDzDSj", "3XMwPNMSbaPm2suGz", "Bp8vnEciPA5TXSy6f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-24T22:47:41.096Z", "modifiedAt": null, "url": null, "title": "Navigating disagreement: How to keep your eye on the evidence ", "slug": "navigating-disagreement-how-to-keep-your-eye-on-the-evidence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:09.299Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cL4wNuHhM5gH4GxdC/navigating-disagreement-how-to-keep-your-eye-on-the-evidence", "pageUrlRelative": "/posts/cL4wNuHhM5gH4GxdC/navigating-disagreement-how-to-keep-your-eye-on-the-evidence", "linkUrl": "https://www.lesswrong.com/posts/cL4wNuHhM5gH4GxdC/navigating-disagreement-how-to-keep-your-eye-on-the-evidence", "postedAtFormatted": "Saturday, April 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Navigating%20disagreement%3A%20How%20to%20keep%20your%20eye%20on%20the%20evidence%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANavigating%20disagreement%3A%20How%20to%20keep%20your%20eye%20on%20the%20evidence%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcL4wNuHhM5gH4GxdC%2Fnavigating-disagreement-how-to-keep-your-eye-on-the-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Navigating%20disagreement%3A%20How%20to%20keep%20your%20eye%20on%20the%20evidence%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcL4wNuHhM5gH4GxdC%2Fnavigating-disagreement-how-to-keep-your-eye-on-the-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcL4wNuHhM5gH4GxdC%2Fnavigating-disagreement-how-to-keep-your-eye-on-the-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1764, "htmlBody": "<p><em>Heeding others' impressions often increases accuracy. &nbsp;But \"agreement\" &nbsp;and \"<a href=\"http://www.overcomingbias.com/2007/03/on_majoritarian.html\">majoritarianism</a>\" are not magic; &nbsp;in a given circumstance, agreement is or isn't useful for *intelligible* reasons.&nbsp;</em></p>\n<p>You and four other contestants are randomly selected for a game show. &nbsp;The five of you walk into a room. &nbsp;Each of you is handed a thermometer drawn at random from a box; each of you, also, is tasked with guessing the temperature of a bucket of water. &nbsp;You&rsquo;ll each write your guess at the temperature on the card; each person who is holding a card that is within 1&deg;&nbsp;of the correct temperature will win $1000.</p>\n<p>The four others walk to the bucket, place their thermometers in the water, and wait while their thermometers equilibrate. &nbsp;You follow suit. &nbsp;You can all see all of the thermometers&rsquo; read-outs: they&rsquo;re fairly similar, but a couple are a degree or two off from the rest. &nbsp;You can also watch, as each of your fellow-contestants stares fixedly at his or her own thermometer and copies its reading (only) onto his or her card.</p>\n<p>Should you:</p>\n<ol>\n<li>Write down the reading on your own thermometer, because it&rsquo;s yours;</li>\n<li>Write down an average* thermometer reading, because probably the more accurate thermometer-readings will cluster;</li>\n<li>Write down an average of the answers on others&rsquo; cards, because rationalists should try not to disagree;</li>\n<li>Follow the procedure everyone else is following (and so stare only at your own thermometer) because rationalists should try not to disagree about procedures?<a id=\"more\"></a></li>\n</ol>\n<p>Choice 2, of course. &nbsp;Thermometers imperfectly indicate temperature; to have the best possible chance of winning the $1000, you should consider <em>all</em> the information you have, from <em>all</em> the (randomly allocated, and so informationally symmetric) thermometers. &nbsp;It doesn&rsquo;t matter who was handed which thermometer. &nbsp;</p>\n<p>Forming accurate beliefs is *normally* about this simple. &nbsp;If you want the most accurate beliefs you can get, you&rsquo;ll need to pay attention to the evidence. &nbsp;All of the evidence. &nbsp;Evenly. &nbsp;Whether you find the evidence in your hand or mind, or in someone else&rsquo;s. &nbsp;And whether weighing all the evidence evenly leaves you with an apparently high-status social claim (&ldquo;My thermometer is better than yours!&rdquo;), or an apparently deferential social claim (&ldquo;But look -- I&rsquo;m trying to agree with all of you!&rdquo;), or anywhere else.</p>\n<p>I&rsquo;ll try to spell out some of what this looks like, and to make it obvious why certain belief-forming methods give you more accurate beliefs.</p>\n<p><strong>Principle 1: &nbsp;Truth is not person-dependent.</strong></p>\n<p>There&rsquo;s a right haircut for me, and a different right haircut for you. &nbsp;There&rsquo;s a right way for me to eat cookies if I want to maximize my enjoyment, and a different right way for you to eat cookies, if <em>you</em> want to maximize <em>your</em> enjoyment. &nbsp;But, in the context of the game-show, there isn&rsquo;t a right temperature for me to put on my card, and a different right temperature for you to put on your card. &nbsp;The game-show host hands $1000 to <em>cards</em> with the right temperature -- he doesn&rsquo;t care who is holding the card. &nbsp;If a card with a certain answer will make you money, that same card and answer will make me money. &nbsp;And if a certain answer won&rsquo;t make me money, it won&rsquo;t make you money either.</p>\n<p>Truth, or accuracy, is like the game show in this sense. &nbsp;&ldquo;Correct prediction&rdquo; or &ldquo;incorrect prediction&rdquo; applies to beliefs, <em>not</em> to <em>people</em> <em>with</em> beliefs. &nbsp;Nature doesn&rsquo;t care what your childhood influences were, or what kind of information you did or didn&rsquo;t have to work with, when it deems your predictions &ldquo;accurate!&rdquo; or &ldquo;inaccurate!&rdquo;. &nbsp;So, from the point of view of accuracy, it doesn&rsquo;t make any sense to say &ldquo;I think the temperature is 73&deg;, but you, given the thermometer you were handed, should think it 74&deg;&rdquo;. &nbsp;Nor &ldquo;I think X, but given your intuitions you should think Y&rdquo; in any other purely predictive context.</p>\n<p>That is: while &ldquo;is a good haircut&rdquo; is a property of the (person, haircut) pair, &ldquo;is an accurate belief&rdquo; is a property of the belief <em>only</em>.</p>\n<p><strong>Principle 2: &nbsp;Watch the mechanisms that create your beliefs. &nbsp;Ask if they&rsquo;re likely to lead to accurate beliefs.</strong></p>\n<p>It isn&rsquo;t because of magic that you should use the median thermometer&rsquo;s output. &nbsp;It&rsquo;s because, well, thermometers noisily reflect the temperature, and so the central cluster of the thermometers is more likely to be accurate. &nbsp;You can <em>see</em> why this is the accuracy-producing method.</p>\n<p>Sometimes you&rsquo;ll produce better answers by taking an average over many peoples&rsquo; impressions, or by updating from other peoples&rsquo; beliefs, or by taking disagreement between yourself and someone else as a sign that you should debug your belief-forming process. &nbsp;And sometimes (e.g., if the people around you are choosing their answers by astrology), you won&rsquo;t. &nbsp;</p>\n<p>But in any of these circumstances, if you actually ask yourself &ldquo;What belief-forming process is really, actually likely to pull the most juice from the evidence?&rdquo;, you&rsquo;ll see what the answer is, and you&rsquo;ll see <em>why</em> the answer is that. &nbsp;It won&rsquo;t be &ldquo;agree with others, because agreement is a mysterious social ritual that rationalists aim for&rdquo;, or &ldquo;agree with others, because then others will socially reciprocate by agreeing with you&rdquo;. &nbsp;It won&rsquo;t be routed through the primate social system at all. &nbsp;It&rsquo;ll be routed through seeing where evidence can be found (seeing what features of the world <em>sh</em><em>ould look different</em> if the world is in one state rather than another -- the way thermometer-readings <em>should look differen</em>t if the bucket is one temperature rather than another) and then seeing how to best and most thoroughly and evenly gather up all that evidence.</p>\n<p><strong>Principle 2b: &nbsp;Ask if you are weighing all similarly truth-indicative mechanisms evenly.</strong></p>\n<p>Even when the processes that create our beliefs <em>are</em> truth-indicative, they generally aren&rsquo;t fully, thoroughly, and evenly truth-indicative. &nbsp;Let&rsquo;s say I want to know whether it&rsquo;s safe for my friend to bike to work. &nbsp;My own memories are truth indicative, but so are my friends&rsquo; and neighbors&rsquo; memories, and so are the memories of the folk in surveys I can find on line. &nbsp;The trouble is that my own memories arrive in my head with extreme salience, and move my automatic anticipations a lot; while my friend&rsquo;s have less automatic impact, and those of the surveyed neighbors still less. &nbsp;So if I just go with the impressions that land in my head, my predictions will overweight a few samples of evidence at the expense of all the others.</p>\n<p>That is: our automatic cognition tends not to weigh the evidence evenly *at all*. &nbsp;It takes conscious examination and compensation.</p>\n<p><strong>Principle 3: &nbsp;Ask what an outside observer would say.</strong></p>\n<p>Since truth doesn&rsquo;t depend on who is asking -- and since our feelings about the truth often <em>do</em> depend -- it can help to ask what an outside observer would say. &nbsp;Instead of asking &ldquo;Am I right in this dispute with my friend?&rdquo; ask: &ldquo;If I observed this from the outside, and saw someone with my track record and skillset, and someone else with my friend&rsquo;s track record and skillset, disagreeing in this manner -- who would I think was probably right?&rdquo;.</p>\n<p>(See also <a href=\"/lw/4e/cached_selves/\">Cached Selves</a>.)</p>\n<p><strong>Common pitfall: Idolatry</strong></p>\n<p>We&rsquo;re humans. &nbsp;Give us a good idea, and we&rsquo;ll turn it into an idol and worship its (perhaps increasingly distorted) image. &nbsp;Tell us about the <a href=\"http://en.wikipedia.org/wiki/Aumann's_agreement_theorem\">Aumann</a> <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Agreement</a> Theorem, and we&rsquo;re liable to make up nonsense rituals about how one must always agree with the majority.</p>\n<p>The solution is to remove the technical terms and ask *why* each belief-forming method works. &nbsp;Where is the evidence? &nbsp;What observations would you expect to see, if the universe were one way rather than another? &nbsp;What method of aggregating the evidence most captures the relevant data?</p>\n<p>That is: don&rsquo;t memorize the idea that &ldquo;agreement&rdquo;, the &ldquo;scientific method&rdquo;, or any other procedure is &ldquo;what rationalists do&rdquo;. &nbsp;Or, at least, don&rsquo;t *just* memorize it. &nbsp;Think it through every time. &nbsp;Be able to see why it works.</p>\n<p><strong>Common pitfall: Primate social intuitions</strong></p>\n<p>Again: we&rsquo;re humans. &nbsp;Give us a belief-forming method, and we&rsquo;ll make primate politics out of it. &nbsp;We&rsquo;ll say &ldquo;I should agree with the majority, so that religious or political nuts will also agree with the majority via social precedent effects&rdquo;. &nbsp;Or: &ldquo;I should believe some of my interlocutor&rsquo;s points, so that my interlocutor <a href=\"http://en.wikipedia.org/wiki/Reciprocity_(social_psychology)\">will</a> believe mine&rdquo;. &nbsp;And we&rsquo;ll cite &ldquo;rationality&rdquo; while doing this.</p>\n<p>But accurate beliefs have nothing to do with game theory. &nbsp;Yes, in an argument, you may wish to cede a point in order to manipulate your interlocutor. &nbsp;But that social manipulation has nothing to do with truth. &nbsp;And social manipulation isn&rsquo;t why you&rsquo;ll get better predictions if you include others&rsquo; thermometers in your average, instead of just paying attention to your own thermometer.</p>\n<p><strong>Example problems:</strong> &nbsp;To make things concrete, consider the following examples. &nbsp;My take on the answers appears in the comments. &nbsp;Please treat these as real examples; if you think real situations diverge from my idealization, say so.</p>\n<p><em>Problem 1: Jelly-beans&nbsp;</em></p>\n<p>You&rsquo;re asked to estimate the number of jelly-beans in a jar. &nbsp;You have a group of friends with you. Each friend privately writes down her estimate, then all of the estimates are revealed, and then each person has the option of changing her estimate.</p>\n<p>How should you weigh: (a) your own initial, solitary estimate; (b) the initial estimates of each of your friends; (c) the estimates your friends write down on paper, after hearing some of the others&rsquo; answers?</p>\n<p><em>Problem 2: Housework splitting &nbsp;</em></p>\n<p>You get into a dispute with your roommate about what portion of the housework you&rsquo;ve each been doing. &nbsp;He says you&rsquo;re being biased, and that you always get emotional about this sort of thing. &nbsp;You can see in his eyes that he&rsquo;s upset and biased; you feel strongly that you could never have such biases. &nbsp;What to believe?</p>\n<p><em>Problem 3: &nbsp;Christianity vs. atheism</em></p>\n<p>You get in a dispute with your roommate about religion. &nbsp;He says you&rsquo;re being biased, and that your &ldquo;rationalism&rdquo; is just another religion, and that according to his methodology, you get the right answer by feeling Jesus in your heart. &nbsp;You can see in his eyes that he&rsquo;s upset and biased you feel strongly that you could never have such biases. &nbsp;What to believe?</p>\n<p><em>Problem 4: &nbsp;<a href=\"http://www.overcomingbias.com/2007/01/we_cant_foresee.html\">Honest Bayesian wannabes</a></em></p>\n<p>Two similarly rational people, Alfred and Betty, estimate the length of Lake L. &nbsp;Alfred estimates &ldquo;50 km&rdquo;; Betty simultaneously estimates &ldquo;10 km&rdquo;. &nbsp;Both realize that Betty knows more geography than Alfred. &nbsp;Before exchanging any additional information, the two must again utter simultaneous estimates regarding the answer to G. &nbsp;Is it true that if Alfred and Betty are estimating optimally, it is as likely that Betty&rsquo;s answer will now be larger than Alfred&rsquo;s as the other way round? &nbsp;Is it true that if these rounds are repeated, Alfred and Betty will eventually stabilize on the same answer? &nbsp;Why?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 2, "Ng8Gice9KNkncxqcj": 2, "LhX3F2SvGDarZCuh6": 2, "rWzGNdjuep56W5u2d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cL4wNuHhM5gH4GxdC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 47, "extendedScore": null, "score": 0.000121, "legacy": true, "legacyId": "2801", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Heeding others' impressions often increases accuracy. &nbsp;But \"agreement\" &nbsp;and \"<a href=\"http://www.overcomingbias.com/2007/03/on_majoritarian.html\">majoritarianism</a>\" are not magic; &nbsp;in a given circumstance, agreement is or isn't useful for *intelligible* reasons.&nbsp;</em></p>\n<p>You and four other contestants are randomly selected for a game show. &nbsp;The five of you walk into a room. &nbsp;Each of you is handed a thermometer drawn at random from a box; each of you, also, is tasked with guessing the temperature of a bucket of water. &nbsp;You\u2019ll each write your guess at the temperature on the card; each person who is holding a card that is within 1\u00b0&nbsp;of the correct temperature will win $1000.</p>\n<p>The four others walk to the bucket, place their thermometers in the water, and wait while their thermometers equilibrate. &nbsp;You follow suit. &nbsp;You can all see all of the thermometers\u2019 read-outs: they\u2019re fairly similar, but a couple are a degree or two off from the rest. &nbsp;You can also watch, as each of your fellow-contestants stares fixedly at his or her own thermometer and copies its reading (only) onto his or her card.</p>\n<p>Should you:</p>\n<ol>\n<li>Write down the reading on your own thermometer, because it\u2019s yours;</li>\n<li>Write down an average* thermometer reading, because probably the more accurate thermometer-readings will cluster;</li>\n<li>Write down an average of the answers on others\u2019 cards, because rationalists should try not to disagree;</li>\n<li>Follow the procedure everyone else is following (and so stare only at your own thermometer) because rationalists should try not to disagree about procedures?<a id=\"more\"></a></li>\n</ol>\n<p>Choice 2, of course. &nbsp;Thermometers imperfectly indicate temperature; to have the best possible chance of winning the $1000, you should consider <em>all</em> the information you have, from <em>all</em> the (randomly allocated, and so informationally symmetric) thermometers. &nbsp;It doesn\u2019t matter who was handed which thermometer. &nbsp;</p>\n<p>Forming accurate beliefs is *normally* about this simple. &nbsp;If you want the most accurate beliefs you can get, you\u2019ll need to pay attention to the evidence. &nbsp;All of the evidence. &nbsp;Evenly. &nbsp;Whether you find the evidence in your hand or mind, or in someone else\u2019s. &nbsp;And whether weighing all the evidence evenly leaves you with an apparently high-status social claim (\u201cMy thermometer is better than yours!\u201d), or an apparently deferential social claim (\u201cBut look -- I\u2019m trying to agree with all of you!\u201d), or anywhere else.</p>\n<p>I\u2019ll try to spell out some of what this looks like, and to make it obvious why certain belief-forming methods give you more accurate beliefs.</p>\n<p><strong id=\"Principle_1___Truth_is_not_person_dependent_\">Principle 1: &nbsp;Truth is not person-dependent.</strong></p>\n<p>There\u2019s a right haircut for me, and a different right haircut for you. &nbsp;There\u2019s a right way for me to eat cookies if I want to maximize my enjoyment, and a different right way for you to eat cookies, if <em>you</em> want to maximize <em>your</em> enjoyment. &nbsp;But, in the context of the game-show, there isn\u2019t a right temperature for me to put on my card, and a different right temperature for you to put on your card. &nbsp;The game-show host hands $1000 to <em>cards</em> with the right temperature -- he doesn\u2019t care who is holding the card. &nbsp;If a card with a certain answer will make you money, that same card and answer will make me money. &nbsp;And if a certain answer won\u2019t make me money, it won\u2019t make you money either.</p>\n<p>Truth, or accuracy, is like the game show in this sense. &nbsp;\u201cCorrect prediction\u201d or \u201cincorrect prediction\u201d applies to beliefs, <em>not</em> to <em>people</em> <em>with</em> beliefs. &nbsp;Nature doesn\u2019t care what your childhood influences were, or what kind of information you did or didn\u2019t have to work with, when it deems your predictions \u201caccurate!\u201d or \u201cinaccurate!\u201d. &nbsp;So, from the point of view of accuracy, it doesn\u2019t make any sense to say \u201cI think the temperature is 73\u00b0, but you, given the thermometer you were handed, should think it 74\u00b0\u201d. &nbsp;Nor \u201cI think X, but given your intuitions you should think Y\u201d in any other purely predictive context.</p>\n<p>That is: while \u201cis a good haircut\u201d is a property of the (person, haircut) pair, \u201cis an accurate belief\u201d is a property of the belief <em>only</em>.</p>\n<p><strong id=\"Principle_2___Watch_the_mechanisms_that_create_your_beliefs___Ask_if_they_re_likely_to_lead_to_accurate_beliefs_\">Principle 2: &nbsp;Watch the mechanisms that create your beliefs. &nbsp;Ask if they\u2019re likely to lead to accurate beliefs.</strong></p>\n<p>It isn\u2019t because of magic that you should use the median thermometer\u2019s output. &nbsp;It\u2019s because, well, thermometers noisily reflect the temperature, and so the central cluster of the thermometers is more likely to be accurate. &nbsp;You can <em>see</em> why this is the accuracy-producing method.</p>\n<p>Sometimes you\u2019ll produce better answers by taking an average over many peoples\u2019 impressions, or by updating from other peoples\u2019 beliefs, or by taking disagreement between yourself and someone else as a sign that you should debug your belief-forming process. &nbsp;And sometimes (e.g., if the people around you are choosing their answers by astrology), you won\u2019t. &nbsp;</p>\n<p>But in any of these circumstances, if you actually ask yourself \u201cWhat belief-forming process is really, actually likely to pull the most juice from the evidence?\u201d, you\u2019ll see what the answer is, and you\u2019ll see <em>why</em> the answer is that. &nbsp;It won\u2019t be \u201cagree with others, because agreement is a mysterious social ritual that rationalists aim for\u201d, or \u201cagree with others, because then others will socially reciprocate by agreeing with you\u201d. &nbsp;It won\u2019t be routed through the primate social system at all. &nbsp;It\u2019ll be routed through seeing where evidence can be found (seeing what features of the world <em>sh</em><em>ould look different</em> if the world is in one state rather than another -- the way thermometer-readings <em>should look differen</em>t if the bucket is one temperature rather than another) and then seeing how to best and most thoroughly and evenly gather up all that evidence.</p>\n<p><strong id=\"Principle_2b___Ask_if_you_are_weighing_all_similarly_truth_indicative_mechanisms_evenly_\">Principle 2b: &nbsp;Ask if you are weighing all similarly truth-indicative mechanisms evenly.</strong></p>\n<p>Even when the processes that create our beliefs <em>are</em> truth-indicative, they generally aren\u2019t fully, thoroughly, and evenly truth-indicative. &nbsp;Let\u2019s say I want to know whether it\u2019s safe for my friend to bike to work. &nbsp;My own memories are truth indicative, but so are my friends\u2019 and neighbors\u2019 memories, and so are the memories of the folk in surveys I can find on line. &nbsp;The trouble is that my own memories arrive in my head with extreme salience, and move my automatic anticipations a lot; while my friend\u2019s have less automatic impact, and those of the surveyed neighbors still less. &nbsp;So if I just go with the impressions that land in my head, my predictions will overweight a few samples of evidence at the expense of all the others.</p>\n<p>That is: our automatic cognition tends not to weigh the evidence evenly *at all*. &nbsp;It takes conscious examination and compensation.</p>\n<p><strong id=\"Principle_3___Ask_what_an_outside_observer_would_say_\">Principle 3: &nbsp;Ask what an outside observer would say.</strong></p>\n<p>Since truth doesn\u2019t depend on who is asking -- and since our feelings about the truth often <em>do</em> depend -- it can help to ask what an outside observer would say. &nbsp;Instead of asking \u201cAm I right in this dispute with my friend?\u201d ask: \u201cIf I observed this from the outside, and saw someone with my track record and skillset, and someone else with my friend\u2019s track record and skillset, disagreeing in this manner -- who would I think was probably right?\u201d.</p>\n<p>(See also <a href=\"/lw/4e/cached_selves/\">Cached Selves</a>.)</p>\n<p><strong id=\"Common_pitfall__Idolatry\">Common pitfall: Idolatry</strong></p>\n<p>We\u2019re humans. &nbsp;Give us a good idea, and we\u2019ll turn it into an idol and worship its (perhaps increasingly distorted) image. &nbsp;Tell us about the <a href=\"http://en.wikipedia.org/wiki/Aumann's_agreement_theorem\">Aumann</a> <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Agreement</a> Theorem, and we\u2019re liable to make up nonsense rituals about how one must always agree with the majority.</p>\n<p>The solution is to remove the technical terms and ask *why* each belief-forming method works. &nbsp;Where is the evidence? &nbsp;What observations would you expect to see, if the universe were one way rather than another? &nbsp;What method of aggregating the evidence most captures the relevant data?</p>\n<p>That is: don\u2019t memorize the idea that \u201cagreement\u201d, the \u201cscientific method\u201d, or any other procedure is \u201cwhat rationalists do\u201d. &nbsp;Or, at least, don\u2019t *just* memorize it. &nbsp;Think it through every time. &nbsp;Be able to see why it works.</p>\n<p><strong id=\"Common_pitfall__Primate_social_intuitions\">Common pitfall: Primate social intuitions</strong></p>\n<p>Again: we\u2019re humans. &nbsp;Give us a belief-forming method, and we\u2019ll make primate politics out of it. &nbsp;We\u2019ll say \u201cI should agree with the majority, so that religious or political nuts will also agree with the majority via social precedent effects\u201d. &nbsp;Or: \u201cI should believe some of my interlocutor\u2019s points, so that my interlocutor <a href=\"http://en.wikipedia.org/wiki/Reciprocity_(social_psychology)\">will</a> believe mine\u201d. &nbsp;And we\u2019ll cite \u201crationality\u201d while doing this.</p>\n<p>But accurate beliefs have nothing to do with game theory. &nbsp;Yes, in an argument, you may wish to cede a point in order to manipulate your interlocutor. &nbsp;But that social manipulation has nothing to do with truth. &nbsp;And social manipulation isn\u2019t why you\u2019ll get better predictions if you include others\u2019 thermometers in your average, instead of just paying attention to your own thermometer.</p>\n<p><strong>Example problems:</strong> &nbsp;To make things concrete, consider the following examples. &nbsp;My take on the answers appears in the comments. &nbsp;Please treat these as real examples; if you think real situations diverge from my idealization, say so.</p>\n<p><em>Problem 1: Jelly-beans&nbsp;</em></p>\n<p>You\u2019re asked to estimate the number of jelly-beans in a jar. &nbsp;You have a group of friends with you. Each friend privately writes down her estimate, then all of the estimates are revealed, and then each person has the option of changing her estimate.</p>\n<p>How should you weigh: (a) your own initial, solitary estimate; (b) the initial estimates of each of your friends; (c) the estimates your friends write down on paper, after hearing some of the others\u2019 answers?</p>\n<p><em>Problem 2: Housework splitting &nbsp;</em></p>\n<p>You get into a dispute with your roommate about what portion of the housework you\u2019ve each been doing. &nbsp;He says you\u2019re being biased, and that you always get emotional about this sort of thing. &nbsp;You can see in his eyes that he\u2019s upset and biased; you feel strongly that you could never have such biases. &nbsp;What to believe?</p>\n<p><em>Problem 3: &nbsp;Christianity vs. atheism</em></p>\n<p>You get in a dispute with your roommate about religion. &nbsp;He says you\u2019re being biased, and that your \u201crationalism\u201d is just another religion, and that according to his methodology, you get the right answer by feeling Jesus in your heart. &nbsp;You can see in his eyes that he\u2019s upset and biased you feel strongly that you could never have such biases. &nbsp;What to believe?</p>\n<p><em>Problem 4: &nbsp;<a href=\"http://www.overcomingbias.com/2007/01/we_cant_foresee.html\">Honest Bayesian wannabes</a></em></p>\n<p>Two similarly rational people, Alfred and Betty, estimate the length of Lake L. &nbsp;Alfred estimates \u201c50 km\u201d; Betty simultaneously estimates \u201c10 km\u201d. &nbsp;Both realize that Betty knows more geography than Alfred. &nbsp;Before exchanging any additional information, the two must again utter simultaneous estimates regarding the answer to G. &nbsp;Is it true that if Alfred and Betty are estimating optimally, it is as likely that Betty\u2019s answer will now be larger than Alfred\u2019s as the other way round? &nbsp;Is it true that if these rounds are repeated, Alfred and Betty will eventually stabilize on the same answer? &nbsp;Why?</p>", "sections": [{"title": "Principle 1: \u00a0Truth is not person-dependent.", "anchor": "Principle_1___Truth_is_not_person_dependent_", "level": 1}, {"title": "Principle 2: \u00a0Watch the mechanisms that create your beliefs. \u00a0Ask if they\u2019re likely to lead to accurate beliefs.", "anchor": "Principle_2___Watch_the_mechanisms_that_create_your_beliefs___Ask_if_they_re_likely_to_lead_to_accurate_beliefs_", "level": 1}, {"title": "Principle 2b: \u00a0Ask if you are weighing all similarly truth-indicative mechanisms evenly.", "anchor": "Principle_2b___Ask_if_you_are_weighing_all_similarly_truth_indicative_mechanisms_evenly_", "level": 1}, {"title": "Principle 3: \u00a0Ask what an outside observer would say.", "anchor": "Principle_3___Ask_what_an_outside_observer_would_say_", "level": 1}, {"title": "Common pitfall: Idolatry", "anchor": "Common_pitfall__Idolatry", "level": 1}, {"title": "Common pitfall: Primate social intuitions", "anchor": "Common_pitfall__Primate_social_intuitions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "73 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BHYBdijDcAKQ6e45Z"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-25T12:33:33.170Z", "modifiedAt": null, "url": null, "title": "Report from Humanity+ UK 2010", "slug": "report-from-humanity-uk-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:58.970Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7ePXWdxTjTWYJmcgi/report-from-humanity-uk-2010", "pageUrlRelative": "/posts/7ePXWdxTjTWYJmcgi/report-from-humanity-uk-2010", "linkUrl": "https://www.lesswrong.com/posts/7ePXWdxTjTWYJmcgi/report-from-humanity-uk-2010", "postedAtFormatted": "Sunday, April 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Report%20from%20Humanity%2B%20UK%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReport%20from%20Humanity%2B%20UK%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ePXWdxTjTWYJmcgi%2Freport-from-humanity-uk-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Report%20from%20Humanity%2B%20UK%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ePXWdxTjTWYJmcgi%2Freport-from-humanity-uk-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ePXWdxTjTWYJmcgi%2Freport-from-humanity-uk-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 560, "htmlBody": "<p><em>&ldquo;Theosophists have guessed at the awesome grandeur of the cosmic cycle wherein our world and human race form transient incidents. They have hinted at strange survival in terms which would freeze the blood if not masked by a bland optimism.&rdquo;</em></p>\n<p style=\"margin-bottom: 0in;\" align=\"RIGHT\">&ndash; H.P. Lovecraft on transhumanism</p>\n<p><br /><br />Just thought I'd write a quick post to sum up the <a href=\"http://humanityplus-uk.com/wordpress/\">H+ UK</a> conference and subsequent LW meetup attended by myself, ciphergoth, JulianMorrison, Leon and a few other LW lurkers. My thanks to David Wood for organizing the conference, and Anders Sandberg for putting me up/putting up with me the night before.</p>\n<p>I made a poster giving a quick introduction to &ldquo;Cognitive Bias and Futurism&rdquo;, which I will put up on my website shortly. The LW crowd met up as advertised &ndash; we discussed the potential value of spreading the rationality message to the H+ community.<sup>1</sup><br /><br />One idea was for someone (possibly me) to do a talk at UKH+ on &ldquo;Rationality and Futurism&rdquo;, and to get the UK transhumanist crowd involved and on board somewhat more. The NYC Less Wrong guys seem to be doing remarkably well with a <a href=\"http://www.meetup.com/Less-Wrong-Overcoming-Bias-NYC/\">meetup group</a>, about a billion members, a group house (?) &ndash; do you have any advice for us?</p>\n<p><a id=\"more\"></a>The talks were interesting and provocative &ndash; of particular note were:</p>\n<ul>\n<li>Aubrey De Grey's talk which was far from his usual fare. He tailored his talk to suit the H+ audience who are more familiar with SENS, and spoke about the pace of recent advances in induced pluripotency in stem cells, progress in migrating mitochondrial DNA from the mitochondria to the cell nucleus, and how the mainstream media can blow a minor paper with a sexy title to undeserved levels of fame whilst passing over much more important but &ldquo;technical&rdquo; sounding work. I asked what probability he assigned to his SENS program working, but he did not give an estimate. I think that rationalists could potentially help the life-extension movement by publishing an independent, critical review of the probability of the SENS program succeeding, perhaps in an economics journal.</li>\n</ul>\n<ul>\n<li>Nick Bostrom added the definition of &ldquo;capability potential&rdquo; and &ldquo;axiological potential&rdquo; to his usual existential risk meme.</li>\n</ul>\n<ul>\n<li>Other talks were interesting, but showed a lack of appreciation of rationalist methods. For example, though many talks made predictions about the future, only Bostrom's talk gave any probabilities. Other talks used version 1.0 epistemology &ndash; phrases like &ldquo;I think that X will happen, not Y&rdquo;, rather than &ldquo;I assign more probability (k%) to X than most people do&rdquo;. Max More was particularly guilty of this in his talk on &ldquo;Singularity Skepticism&rdquo; &ndash; though only because his talk attempted to answer a much harder question than most of the other talks. (E.g. transhumanist art, architectural style, etc)</li>\n</ul>\n<p>&nbsp;</p>\n<hr />\n<p><br />1: Particularly after hearing a man say that he wouldn't sign up for cryonics because it &ldquo;might not work&rdquo;. We asked him for his probability estimate that it would work (20%), and then asked him what probability he would need to have estimated for him to think it would be worth paying for (40%) &ndash; which he then admitted he had made up on the spot as &ldquo;an arbitrary number&rdquo;. Oh, and seeing a poster claiming to have solved the problem of defining an objective morality, which I may or may not upload.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7ePXWdxTjTWYJmcgi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "2802", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-25T14:56:39.340Z", "modifiedAt": null, "url": null, "title": "Who is your favorite rationalist?", "slug": "who-is-your-favorite-rationalist", "viewCount": null, "lastCommentedAt": "2020-07-21T21:54:36.780Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NcB4F7MiswJyuPvYA/who-is-your-favorite-rationalist", "pageUrlRelative": "/posts/NcB4F7MiswJyuPvYA/who-is-your-favorite-rationalist", "linkUrl": "https://www.lesswrong.com/posts/NcB4F7MiswJyuPvYA/who-is-your-favorite-rationalist", "postedAtFormatted": "Sunday, April 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Who%20is%20your%20favorite%20rationalist%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWho%20is%20your%20favorite%20rationalist%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcB4F7MiswJyuPvYA%2Fwho-is-your-favorite-rationalist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Who%20is%20your%20favorite%20rationalist%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcB4F7MiswJyuPvYA%2Fwho-is-your-favorite-rationalist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcB4F7MiswJyuPvYA%2Fwho-is-your-favorite-rationalist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 531, "htmlBody": "<p><em>Light reading about 'Rationalist Heroes'.</em></p>\n<p>I am not sure how useful people find having personal heroes. I would argue that they are definitely useful for children. Perhaps I haven't really grown up enough yet (growing up without a father possibly contributed), but I like to have some people in my head I label as \"I wonder what would X think about this\". Many times they've set me straight through their ideas. Other times I've had to reprimand them, though unfortunately they never get the memo.</p>\n<p>One living example is <a href=\"http://en.wikipedia.org/wiki/Charlie_Munger\">Charlie</a> <a href=\"http://www.25iq.com/charlie-munger-quotations/\">Munger</a>.</p>\n<p>He was an early practical adopter of the cognitive biases framework, and moreover he clearly put it into context of \"<a href=\"/lw/nb/something_to_protect/\">something to protect</a>\":</p>\n<p>\"not understanding human misjudgment was reducing my ability to help everything I loved\"</p>\n<p>(The quote is from his talk on \"Misjudgment\" which is worth reading on its own http://vinvesting.com/docs/munger/human_misjudgement.html)</p>\n<p>One interesting point is that Charlie is seemingly a Christian. I have a deep suspicion that he believes that religion is valuable, for the time, as a payload delivering mechanism.</p>\n<p>&ldquo;Economic systems work better when there&rsquo;s an extreme reliability ethos. And the traditional way to get a reliability ethos, at least in past generations in America, was through religion. The religions instilled guilt. &hellip; And this guilt, derived from religion, has been a huge driver of a reliability ethos, which has been very helpful to economic outcomes for man.&rdquo;</p>\n<p>Also, judge for yourself from his recommended reading list - looks like something out of an Atheist's Bookshelf.</p>\n<div id=\"targetPreview\" class=\"previewLink\"><a href=\"http://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=http%3A%2F%2Fwww.amazon.com%2Fgp%2Frichpub%2Flistmania%2Ffullview%2FR3PFPD1O8ORJZ&amp;tag=myblog0f-20&amp;linkCode=ur2&amp;camp=1789&amp;creative=390957\">Charlie Munger's reading recommendations</a></div>\n<div class=\"previewLink\">There might also be other reasons, family or whatever, that help prop up the religious appearance. I myself am still wearing a yarmulke for this category of reasons. Whatever it is, Munger is no trinity worshiper.<br /></div>\n<div class=\"previewLink\">Another interesting thing is that it is clear today's Berkshire Hathaway was Buffett and Munger's joint venture, and most likely would not succeed in the same way without Munger. I've done a fair amout of reading on the BRK's investment strategy at one point, but cannot find the appropriate supporting quote at the moment. Basically Munger steered Buffett away from just going after underpriced 'crap' companies ('cigar butts') that Buffett 1) already did well with 2) were recommended as the only approach by Ben Graham, who taught Buffett how to invest and was personally extremely impressive. It seems that there was significant amount of de-biasing going on.Without this adjustment Buffett would still be successful, but smaller by an order of magnitude at least.<br /></div>\n<div class=\"previewLink\">I mention this last thing because when I think of a rationalist succeeding in practical world, Munger comes to mind. Of course this is a small samle size.<br /></div>\n<div class=\"previewLink\">Another even more interesting example is Maimonides, who I'd like to write about more extensively at some point. I think that while his conclusions landed very far from the truth as I see it - (he was a highly devout Jewish religious philosopher from the middle ages), this seems largely due to bad inputs, specifically Aristotelian ideas prevalent at the time. His bravery to separate from the pack in that context and reason clearly (based on false premises of the science of his age) always impressed me.<br /></div>\n<div class=\"previewLink\">Do you care to have heroes? Who are they?<br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NcB4F7MiswJyuPvYA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "2803", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-26T18:57:38.942Z", "modifiedAt": null, "url": null, "title": "Only humans can have human values", "slug": "only-humans-can-have-human-values", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:53.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cAPCCJjggjZPxxcKh/only-humans-can-have-human-values", "pageUrlRelative": "/posts/cAPCCJjggjZPxxcKh/only-humans-can-have-human-values", "linkUrl": "https://www.lesswrong.com/posts/cAPCCJjggjZPxxcKh/only-humans-can-have-human-values", "postedAtFormatted": "Monday, April 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Only%20humans%20can%20have%20human%20values&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOnly%20humans%20can%20have%20human%20values%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcAPCCJjggjZPxxcKh%2Fonly-humans-can-have-human-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Only%20humans%20can%20have%20human%20values%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcAPCCJjggjZPxxcKh%2Fonly-humans-can-have-human-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcAPCCJjggjZPxxcKh%2Fonly-humans-can-have-human-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5208, "htmlBody": "<h2>Ethics is not geometry<br /></h2>\n<p>Western philosophy began at about the same time as Western geometry; and if you read Plato you'll see that he, and many philosophers after him, took geometry as a model for philosophy.</p>\n<p>In geometry, you operate on timeless propositions with mathematical operators.&nbsp; All the content is in the propositions.&nbsp; A proof is equally valid regardless of the sequence of operators used to arrive at it.&nbsp; An algorithm that fails to find a proof when one exists is a poor algorithm.</p>\n<p>The naive way philosophers usually map ethics onto mathematics is to suppose that a human mind contains knowledge (the propositional content), and that we think about that knowledge using operators.&nbsp; The operators themselves are not seen as the concern of philosophy.&nbsp; For instance, when studying values (I also use \"preferences\" here, as a synonym differing only in connotation), people suppose that a person's values are static propositions.&nbsp; The algorithms used to satisfy those values aren't themselves considered part of those values.&nbsp; The algorithms are considered to be only ways of manipulating the propositions; and are \"correct\" if they produce correct proofs, and \"incorrect\" if they don't.</p>\n<p>But an agent's propositions aren't intelligent.&nbsp; An intelligent agent is a <em>system</em>, whose learned and inborn circuits produce intelligent behavior in a given environment.&nbsp; An analysis of propositions is not an analysis of an agent.</p>\n<p>I will argue that:</p>\n<ol>\n<li>The only preferences that can be unambiguously determined are the preferences people implement, which are not always the preferences expressed by their beliefs.</li>\n<li>If you extract a set of propositions from an existing agent, then build a new agent to use those propositions in a different environment, with an \"improved\" logic, you can't claim that it has the same values.</li>\n<li>Values exist in a network of other values.&nbsp; A key ethical question is to what degree values are referential (meaning they can be tested against something outside that network); or non-referential (and hence relative).</li>\n<li>Supposing that values are referential helps only by telling you to ignore human values.</li>\n<li>You cannot resolve the problem by combining information from different behaviors, because the needed information is missing.</li>\n<li>Today's ethical disagreements are largely the result of attempting to extrapolate ancestral human values into a changing world.</li>\n<li>The future will thus be ethically contentious even if we accurately characterize and agree on present human values.<a id=\"more\"></a></li>\n</ol>\n<h2>Instincts, algorithms, preferences, and beliefs are artificial categories<br /></h2>\n<p>There is no principled distinction between algorithms and propositions in any existing brain.&nbsp; This means that there's no clear way to partition an organism's knowledge into \"propositions\" (including \"preferences\" and \"beliefs\"), and \"algorithms.\"&nbsp; Hence, you can't expect all of an agent's \"preferences\" to end up inside the part of the agent that you choose to call \"propositions\".&nbsp; Nor can you reliably distinguish \"beliefs\" from \"preferences\".</p>\n<p>Suppose that a moth's brain is wired to direct its flight by holding the angle to the moon constant.&nbsp; (This is <a href=\"http://en.wikipedia.org/wiki/Moth\">controversial</a>, but the competing hypotheses would give similar talking points.)&nbsp; If so, is this a belief about the moon, a preference towards the moon, or an instinctive motor program?&nbsp; When it circles around a lamp, does it believe that lamp is the moon?</p>\n<p>When a child pulls its hand away from something hot, does it value not burning itself and believe that hot things burn, or place a value on not touching hot things, or just have an evolved motor program that responds to hot things?&nbsp; Does your answer change if you learn that the hand was directed to pull back by <a href=\"http://en.wikipedia.org/wiki/Withdrawal_reflex\">spinal reflexes</a>, without involving the cortex?</p>\n<p>Monkeys can learn to fear snakes more easily than they can learn to fear flowers (<a href=\"http://pjackson.asp.radford.edu/CookMineka1989.pdf\">Cook &amp; Mineka 1989</a>).&nbsp; Do monkeys, and perhaps humans, have an \"instinctive preference\" against snakes?&nbsp; Is it an instinct, a preference (snake = negative utility), or a learned behavior (lab monkeys are not afraid of snakes)?</p>\n<p>Can we map the preference-belief distinction onto the distinction between instinct and learned behavior?&nbsp; That is, are all instincts preferences, and all preferences instincts?&nbsp; There are things we call instincts, like spinal reflexes, that I don't think can count as preferences.&nbsp; And there are preferences, such as the relative values I place on the music of Bach and Berg, that are not instincts.&nbsp; (In fact, these are the preferences we care about.&nbsp; The purpose of Friendly AI is not to retain the fist-clenching instinct for future generations.)</p>\n<h2>Bias, heuristic, or preference?</h2>\n<p>A \"bias\" is a reasoning procedure that produces an outcome that does not agree with some logic.&nbsp; But the object in nature is not to conform to logic; it is to produce advantageous behavior.</p>\n<p>Suppose you interview Fred about his preferences.&nbsp; Then you write a utility function for Fred.&nbsp; You experiment, putting Fred in different situations and observing how he responds.&nbsp; You observe that Fred acts in ways that fail to optimize the utility function you wrote down, in a consistently-biased way.</p>\n<p>Is Fred displaying bias?&nbsp; Or does the Fred-system, including both his beliefs <em>and</em> the bias imposed by his reasoning processes, implement a preference that is not captured in his beliefs alone?</p>\n<p>Allegedly true story, from a Teaching Company audio lecture (I forget which one):&nbsp; A psychology professor was teaching a class about conditioned behavior.&nbsp; He also had the habit of pacing back and forth in front of the class.</p>\n<p>The class decided to test his claims by leaning forward and looking interested when the professor moved toward the left side of the room, but acting bored when he moved toward the right side.&nbsp; By the end of the semester, they had trained him to give his entire lecture from the front left corner.&nbsp; When they asked him why he always stood there, he was surprised by the question - he wasn't even aware he had changed his habit.</p>\n<p>If you inspected the professor's beliefs, and then studied his actions, you would conclude he was acting irrationally.&nbsp; But he wasn't.&nbsp; He was <em>acting</em> rationally, just not <em>thinking</em> rationally.&nbsp;&nbsp; His brain didn't detect the pattern in the class's behavior and deposit a proposition into his brain.&nbsp; It encoded the proper behavior, if not straight into his pre-motor cortex, at least not into any conscious beliefs.</p>\n<p>Did he have a bias towards the left side of the room?&nbsp; Or a preference for seeing students pay attention?&nbsp; Or a preference that became a bias when the next semester began and he kept doing it?</p>\n<p>Take your pick - there's no right answer.</p>\n<p>If a heuristic gives answers consistently biased in one direction across a wide range of domains, we can call it a bias.&nbsp; Most biases found in the literature appear to be wide-ranging and value-neutral.&nbsp; But the literature on biases is itself biased (deliberately) towards discussing that type of bias.&nbsp;&nbsp; If we're trawling all of human behavior for values, we may run across many instances where we can't say whether a heuristic is a bias or a preference.</p>\n<p>As one example, I would say that the <a href=\"http://en.wikipedia.org/wiki/Extraordinarity_bias\">extraordinarity bias</a> is in fact a preference.&nbsp; Or consider the happiness paradox:&nbsp; People who become paralyzed become extremely depressed only temporarily; people who win the lottery become very happy only temporarily.&nbsp; (<a href=\"http://www.google.com/search?q=happiness+%22set-point%22&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a\">Google 'happiness \"set-point\"'</a>.)&nbsp; I've <a href=\"/lw/7k/incremental_progress_and_the_valley/5fu\">previously argued on LessWrong</a> that this is not a bias, but a heuristic to achieve our preferences.&nbsp; Happiness is proportional not to our present level of utility, but to the rate of change in our utility.&nbsp; Trying to maximize happiness (the rate of increase of utility) in the near term maximizes total utility over lifespan better than consciously attempting to maximize near-term utility would.&nbsp; This is because maximizing the rate of increase in utility over a short time period, instead of total utility over that time period, prefers behavior that has a small area under the utility curve during that time but ends with a higher utility than it started with, over behavior with a large area under the utilty curve that ends with a lower utility than it started with.&nbsp; This interpretation of happiness would mean that <a href=\"http://en.wikipedia.org/wiki/Impact_bias\">impact bias</a> is not a bias at all, but a heuristic that compensates for this in order to maximize utility rather than happiness when we reason over longer time periods.</p>\n<h2>Environmental factors: Are they a preference or a bias?<br /></h2>\n<p>Evolution does not distinguish between satisfying preconditions for behavior by putting knowledge into a brain, or by using the statistics of the environment.&nbsp; This means that the environment, which is not even present in the geometric model of ethics, is also part of your values.</p>\n<p>When the aforementioned moth circles around a lamp, is it erroneously acting on a bias, or expressing moth preferences?</p>\n<p>Humans like having sex.&nbsp; The teleological purpose of this preference is to cause them to have children.&nbsp; Yet we don't say that they are in error if they use birth control.&nbsp; This suggests that we consider our true preferences to be the organismal ones that trigger positive qualia, not the underlying evolutionary preferences.</p>\n<p>Strict monogamy causes organisms that live in family units to evolve to act more altruistically, because their siblings are as related to them as their children are (West &amp; Gardner 2010).&nbsp; Suppose that people from cultures with a long history of nuclear families and strict monogamy act, on average, more altruistically than people from other cultures; and you put people from both cultures together in a new environment with neither monogamy nor nuclear families.&nbsp; We would probably rather say that the people from these different cultures have different values; not that they both have the same preference to \"help their genes\", but that the people from the monogamous culture have an evolved bias that causes them to erroneously treat strangers nicely in this new environment.&nbsp; Again, we prefer the organismal preference.</p>\n<p>However, if we follow this principle consistently, it prevents us from ever trying to improve ourselves, since it in effect defines our present selves as optimal:</p>\n<ul>\n<li>Humans like eating food with fat, sugar, and salt.&nbsp; In our ancestral context, that expressed the human value of optimizing nutrition.&nbsp; The evolutionary preference is for good nutrition; the organismal preference is for fat, sugar, and salt.&nbsp; By analogy to contraception, liking fat, sugar, and salt is not an evolved but dysfunctional bias in taste; it's a true human value.</li>\n<li>Suppose fear of snakes is triggered by the shape and motion of snakes.&nbsp; The organismal preference is against snakes.&nbsp; The evolutionary preference is against <em>poisonous</em> snakes.&nbsp; If the world is now full of friendly cybernetic snakes, you must conclude that prejudice against them is a human value to be preserved, not a bias to be overcome.&nbsp; Death to the friendly snakes!</li>\n<li>Men enjoy violence.&nbsp; Hitting a stranger over the head with a stick is naturally <em>fun</em> to human males, and it takes a lot of social conditioning to get them not to do this, or at least to restrict themselves to video games.&nbsp; By what principle can we say that this is merely an obsolete heuristic to protect the tribe that is no longer helpful in our present environment; yet having sex with a condom is enjoying a preference?</li>\n<li>(<a href=\"http://download.cell.com/current-biology/pdf/PIIS0960982210001442.pdf\">Santos et al. 2010</a>) reports (<a href=\"http://news.sciencemag.org/sciencenow/2010/04/when-social-fear-disappears-so-d.html\">summarized in Science Online</a>) that children with a genetic mutation causing Williams syndrome, which causes less fear of strangers, have impaired racial stereotyping, but intact gender stereotyping.&nbsp; This suggests that racism, and perhaps sexism, are evolved preferences actively implemented by gene networks.</li>\n</ul>\n<p>So the \"organismal vs. evolutionary\" distinction doesn't help us choose what's a preference and what's a bias.&nbsp; Without any way of doing that, it is <em>in principle</em> impossible to create a category of \"preferences\" distinct from \"preferred outcomes\".&nbsp; A \"value\" consists of declarative knowledge, algorithms, and environment, taken together.&nbsp; Change any of those, and it's not the same value anymore.</p>\n<p>This means that extrapolating human values into a different environment gives an error message.</p>\n<h2>A ray of hope? ...<br /></h2>\n<p>I just made a point by presenting cases in which most people have intuitions about which outcome is correct, and showing that these intuitions don't follow a consistent rule.</p>\n<p>So why do we have the intuitions?</p>\n<p>If we have consistent intuitions, they must follow <em>some</em> rule.&nbsp; We just don't know what it is yet.&nbsp; Right?</p>\n<h2>... No.</h2>\n<p>We don't have consistent intuitions.</p>\n<p>Any one of us has consistent intuitions; and those of us living in Western nations in the 21st century have a lot of intuitions in common.&nbsp; We can predict how most of these intuitions will fall out using some dominant cultural values.&nbsp; The examples involving monogamy and violent males rely on the present relatively high weight on the preference to reduce violent conflict.&nbsp; But this is a context-dependent value!&nbsp; &lt;just-so story&gt;It arises from living in a time and a place where technology makes interactions between tribes more frequent and more beneficial, and conflict more costly&lt;/just-so story&gt;.&nbsp; But looking back in history, we see many people who would disagree with it:</p>\n<ul>\n<li>Historians struggle to explain the origins of World War I and the U.S. Civil War.&nbsp; Sometimes the simplest answer is best:&nbsp; They were for fun.&nbsp; Men on both sides were itching for an excuse to fight.</li>\n<li>In the 19th century, Americans killed off the Native Americans to have their land.&nbsp; Americans universally condemn that action now that they are secure in its benefits; most Americans condoned it at the time.</li>\n<li>Homer would not have agreed that violence is bad!&nbsp; Skill at violence was the greatest virtue to the ancient Greeks.&nbsp; The tension that generates tragedy in the Iliad is not between violence and empathy, but between saving one's kin and saving one's honor.&nbsp; Hector is conflicted, but not about killing Greeks.&nbsp; His speech on his own tragedy ends with his wishes for his son:&nbsp; \"May he bring back the blood-stained spoils of him whom he has laid low, and let his mother's heart be glad.\"</li>\n<li>The Nazis wouldn't have agreed that enjoying violence was bad.&nbsp; We have learned nothing if we think the Nazis rose to power because Germans suddenly went mad <em>en masse</em>, or because Hitler gave really good speeches.&nbsp; Hitler had an entire ideology built around the idea, as I gather, that civilization was an evil constriction on the will to power; and artfully attached it to a few compatible cultural values.</li>\n<li>A similar story could be told about communism.</li>\n</ul>\n<p>The idea that violence (and sexism, racism, and slavery) is bad is a minority opinion in human cultures over history.&nbsp; Nobody likes being hit over the head with a stick by a stranger; but in pre-Christian Europe, it was the person who failed to prevent being struck, not the person doing the striking, whose virtue was criticized.</p>\n<p>Konrad Lorenz believed that the more deadly an animal is, the more emotional attachment to its peers its species evolves, via group selection (<a href=\"http://en.wikipedia.org/wiki/On_Aggression\">Lorenz 1966</a>).&nbsp; The past thousand years of history has been a steady process of humans building sharper claws, and choosing values that reduce their use, keeping net violence roughly constant.&nbsp; As weapons improve, cultural norms that promote conflict must go.&nbsp; First, the intellectuals (who were Christian theologians at the time) neutered masculinity; in the Enlightenment, they attacked religion; and in the 20th century, art.&nbsp; The ancients would probably find today's peaceful, offense-forgiving males as nauseating as I would find a future where the man on the street embraces postmodern art and literature.</p>\n<p>This gradual sacrificing of values in order to attain more and more tolerance and empathy, is the most-noticable change in human values in all of history.&nbsp; This means it is the least-constant of human values.&nbsp; Yet we think of an infinite preference for non-violence and altruism as a foundational value!&nbsp; Our intuitions about our values are thus as mistaken as it is possible for them to be.</p>\n<p>(The logic goes like this:&nbsp; Humans are learning more, and their beliefs are growing closer to the truth.&nbsp; Humans are becoming more tolerant and cooperative.&nbsp; Therefore, tolerant and cooperative values are closer to the truth.&nbsp; Oops!&nbsp; If you believe in moral truth, then you shouldn't be searching for <em>human</em> values in the first place!)</p>\n<p>Catholics don't agree that having sex with a condom is good.&nbsp; They have an elaborate system of belief built on the idea that teleology express God's will, and so underlying purpose (what I call evolutionary preference) always trumps organismal preference.</p>\n<p>And I cheated in the question on monogamy.&nbsp; Of course you <em>said</em> that being more altruistic wasn't an error.&nbsp; Everyone always says they're in favor of more altruism.&nbsp; It's like asking whether someone would like lower taxes.&nbsp; But the hypothesis was that people from non-monogamous or non-family-based cultures do in fact show lower levels of altruism.&nbsp; By hypothesis, then, they would be comfortable with their own levels of altruism, and might feel that higher levels are a bias.</p>\n<p>Preferences are complicated and numerous, and arise in an evolutionary process that does not guarantee consistency.&nbsp; Having conflicting preferences makes action difficult.&nbsp; <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf\">Energy</a> <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.4083&amp;rep=rep1&amp;type=pdf\">minimization</a>, a general principle that may underly much of our learning, simply means reducing conflicts in a network.&nbsp; The most basic operations of our neurons thus probably act to reduce conflicts between preferences.</p>\n<p>But there are no \"true, foundational\" preferences from which to start.&nbsp; There's just a big network of them that can be pushed into any one of many stable configurations, depending on the current environment.&nbsp; There's the Catholic configuration, and the Nazi configuration, and the modern educated tolerant cosmopolitan configuration.&nbsp; If you're already in one of those configurations, it seems obvious what the right conclusion is for any <em>particular</em> value question; and this gives the illusion that we have some underlying principle by which we can properly choose what is a value and what is a bias.&nbsp; But it's just circular reasoning.</p>\n<h2>What about qualia?</h2>\n<p>But everyone agrees that pleasure is good, and pain is bad, right?</p>\n<p>Not entirely - I could point to, say, medieval Europe, when many people believed that causing yourself needless pain was virtuous.&nbsp; But, by and large yes.</p>\n<p>And beside the point (although see below).&nbsp; Because when we talk about values, the eventual applications we have in mind are <em>never</em> about qualia.&nbsp; Nobody has heated arguments about whose qualia are better.&nbsp; Nobody even really cares about qualia.&nbsp; Nobody is going to dedicate their life to building Friendly AI in order to ensure that beings a million years from now still dislike castor oil and enjoy chocolate.</p>\n<p>We may be arguing about preserving a tendency to commit certain acts that give us a warm qualic glow, like helping a bird with a broken wing.&nbsp; But I don't believe there's a dedicated small-animal-empathy quale.&nbsp; More likely there's a hundred inferential steps linking an action, through our knowledge and thinking processes, to a general-purpose warm-glow quale.</p>\n<h2>Value is a network concept<br /></h2>\n<p>Abstracting human behavior into \"human values\" is an ill-posed problem.&nbsp; It's an attempt to divine a simple description of our preferences, outside the context of our environment and our decision process.&nbsp; But we have no consistent way of deciding what are the preferences, and what is the context.&nbsp; We have the illusion that we can, because our intuitions give us answers to questions about preferences - but they use our contextually-situated preferences to do so.&nbsp; That's circular reasoning.</p>\n<p>The problem in trying to root out foundational values for a person is the same as in trying to root out objective values for the universe, or trying to choose the \"correct\" axioms for a geometry.&nbsp; You can pick a set that is self-consistent; but you can't label your choice \"the truth\".</p>\n<p>These are all <em>network concepts</em>, where we try to isolate things that exist only within a complex homogeneous network.&nbsp; Our mental models of complex networks follow mathematics, in which you choose a set of axioms as foundational; or social structures, in which you can identify a set of people as the prime movers.&nbsp; But these conceptions do not even model math or social structures correctly.&nbsp; Axioms are chosen for convenience, but a logic is an entire network of self-consistent statements, many different subsets of which could have been chosen as axioms.&nbsp; Social power does not originate with the rulers, or we would still have kings.</p>\n<p>There is a very similar class of problems, including <a href=\"http://en.wikipedia.org/wiki/Symbol_grounding\">symbol grounding</a> (trying to root out the nodes that are the sources of meaning in a semantic network), and philosophy of science (trying to determine how or whether the scientific process of choosing a set of beliefs given a set of experimental data converges on external truth as you gather more data).&nbsp; The crucial difference is that we have strong reasons for believing that these networks refer to an external domain, and their statements can be tested against the results from independent access to that domain.&nbsp; I call these <em>referential network concepts</em>.&nbsp; One system of referential network concepts can be more right than another; one system of non-referential network concepts can only be more self-consistent than another.</p>\n<p>Referential network concepts cannot be given 0/1 truth-values at a finer granularity than the level at which a network concept refers to something in the extensional (referred-to) domain.&nbsp; For example, (Quine 1968) argues that a natural-language statement cannot be unambiguously parsed beyond the granularity of the behavior associated with it.&nbsp; This is isomorphic to my claim above that a value/preference can't be parsed beyond the granularity of the behavior of an agent acting in an environment.</p>\n<p>Thomas Kuhn gained notoriety by arguing (<a href=\"http://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions\">Kuhn 1962</a>) that there is no such thing as scientific progress, but only transitions between different stable states of belief; and that modern science is only different from ancient science, not better.&nbsp; (He denies this in the postscript to the 1969 edition, but it is the logical implication of both his arguments and the context he presents them in.)&nbsp; In other words, he claims science is a non-referential network concept.&nbsp; An interpretation in line with Quine would instead say that science is referential at the level of the experiment, and that ambiguities may remain in how we define the fine-grained concepts used to predict the outcomes of experiments.</p>\n<p>Determining whether a network concept domain is referential or non-referential is tricky.&nbsp; The distinction was not even noticed until the 19th century.&nbsp; Until then, everyone who had ever studied geometry, so far as I know, believed there was one \"correct\" geometry, with Euclid's 5 postulates as axioms.&nbsp; But in the early 19th century, several mathematicians proved that you could build three different, consistent geometries depending on what you put in the place of <a href=\"http://en.wikipedia.org/wiki/Parallel_postulate\">Euclid's fifth postulate</a>.&nbsp; The universe we live in most likely conforms to only one of these (making geometry referential in a physics class); but the others are equally valid mathematically (making geometry non-referential in a math class).</p>\n<h2>Is value referential, or non-referential?<br /></h2>\n<p>There are two ways of interpreting this question, depending on whether one means \"human values\" or \"<a href=\"http://en.wikipedia.org/wiki/Moral_realism\">absolute values</a>\".</p>\n<p>Judgements of value expressed in human language are referential; they refer to human behavior.&nbsp; So human values are referential.&nbsp; You can decide whether claims about a particular human's values are true or false, as long as you don't extend those claims outside the context of that human's decision process and environment.&nbsp; This claim is isomorphic to Quine's claim about meaning in human language.</p>\n<p>Asking about absolute values is isomorphic to applying the symbol-grounding problem to consciousness.&nbsp; Consciousness exists internally, and is finer-grained than human behaviors.&nbsp; Providing a symbol-grounding method that satisfied Quine's requirements would not provide any meanings accessible to consciousness.&nbsp; Stevan Harnad (Harnad 2000) described <a href=\"http://cogprints.org/3106/\">how symbols might be grounded</a> for consciousness in sense perceptions and statistical regularities of those perceptions.</p>\n<p>(This brings up an important point, which I will address later:&nbsp; You may be able to assign referential network concepts <em>probabilistic</em> or else <em>fuzzy </em>truth values at a finer level of granularity than the level of correspondence.&nbsp; A preview: This doesn't get you out of the difficulty, because the ambiguous cases don't have mutual information with which they could help resolve each other.)</p>\n<p>Can an analogous way be found to ground absolute values?&nbsp; Yes and no.&nbsp; You can choose axioms that are hard to argue with, like \"existence is better than non-existence\", \"pleasure is better than pain\", or \"complexity is better than simplicity\".&nbsp; (I find \"existence is better than non-existence\" pretty hard to argue with; but Buddhists disagree.)&nbsp; If you can interpret them in an unambiguous way, and define a utility calculus enabling you to make numeric comparisons, you may be able to make \"absolute\" comparisons between value systems <em>relative to your axioms</em>.</p>\n<p>You would also need to make some choices we've talked about here before, such as \"use summed utility\" or \"use average utility\".&nbsp; And you would need to make many possibly-arbitrary interpretation assumptions such as what pleasure is, what complexity is, or what counts as an agent.&nbsp; The gray area between absolute and relative values is in how self-evident all these axioms, decisions, and assumptions are.&nbsp; But any results at all - even if they provide guidance only in decisions such as \"destroy / don't destroy the universe\" - would mean we could claim there is a way for values to be referential at a finer granularity than that of an agent's behavior.&nbsp; And things that seem arbitrary to us today may turn out not to be; for example, I've argued <a href=\"/lw/91/average_utilitarianism_must_be_correct/\">here</a> that average utilitarianism can be <em>derived</em> from the von Neumann-Morgenstern theorem on utility.</p>\n<h2>... It doesn't matter WRT friendly AI and coherent extrapolated volition.<br /></h2>\n<p>Even supposing there is a useful, correct, absolute lattice on value system and/or values, it doesn't forward the project of trying to instill human values in artificial intelligences.&nbsp; There are 2 possible cases:</p>\n<ol>\n<li>There are no absolute values.&nbsp; Then we revert to judgements of human values, which, as argued above, have no unambiguous interpretation outside of a human context.</li>\n<li>There are absolute values.&nbsp; In which case, we should use them, not human values, whenever we can discern them.</li>\n</ol>\n<h2>Fuzzy values and fancy math don't help<br /></h2>\n<p>So far, I've looked at cases of ambiguous values only one behavior at a time.&nbsp; I mentioned above that you can assign probabilities to different value interpretations of a behavior.&nbsp; Can we take a network of many probabilistic interpretations, and use energy minimization or some other mathematics to refine the probabilities?</p>\n<p>No; because for the ambiguities of interest, we have no access to any of the mutual information between how to resolve two different ambiguities.&nbsp; The ambiguity is in whether the hypothesized \"true value\" would agree or disagree with the results given by the initial propositional system plus a different decision process and/or environment.&nbsp; In every case, this information is missing.&nbsp; No clever math can provide this information from our existing data, no matter how many different cases we combine.</p>\n<p>Nor should we hope to find correlations between \"true values\" that will help us refine our estimates for one value given a different unambiguous value. The search for values is isomorphic to the search for personality primitives.&nbsp; The approach practiced by psychologists is to use factor analysis to take thousands of answers to questions that are meant to test personality phenotype, and mathematically reduce these to discover a few underlying (\"latent\") independent personality variables, most famously in the Big 5 personality scale (reviewed in Goldberg 1993).&nbsp; In other words:&nbsp; The true personality traits, and by analogy the true values a person holds, are by definition independent of each other.</p>\n<p>We expect, nonetheless, to find correlations between the component of these different values that resides in decision processes.&nbsp; This is because it is efficient to re-use decision processes as often as possible.&nbsp; Evolution should favor partitioning values between propositions, algorithms, and environment in a way that minimizes the number of algorithms needed.&nbsp; These correlations will not help us, because they have to do only with how a value is implemented within an organism, and say nothing about how the value would be extended into a different organism or environment.</p>\n<p>In fact, I propose that the different value systems popular among humans, and the resulting ethical arguments, are largely different ways of partitioning values between propositions, algorithms, and environment, that each result in a relatively simple set of algorithms, and each in fact give the same results in most situations that our ancestors would have encountered.&nbsp; It is the attempt to extrapolate human values into the new, manmade environment that causes ethical disagreements.&nbsp; This means that our present ethical arguments are largely the result of cultural change over the past few thousand years; and that the next few hundred years of change will provide ample grounds for additional arguments even if we resolve today's disagreements.</p>\n<h2>Summary</h2>\n<p>Philosophically-difficult domains often involve network concepts, where each component depends on other components, and the dependency graph has cycles.&nbsp; The simplest models of network concepts suppose that there are some original, primary nodes in the network that everything depends on.</p>\n<p>We have learned to stop applying these models to geometry and supposing there is one true set of axioms.&nbsp; We have learned to stop applying these models to biology, and accept that life evolved, rather than that reality is divided into Creators (the primary nodes) and Creatures.&nbsp; We are learning to stop applying them to morals, and accept that morality depends on context and biology, rather than being something you can extract from its context.&nbsp; We should also learn to stop applying them to the preferences directing the actions of intelligent agents.</p>\n<p>Attempting to identify values is a network problem, and you cannot identify the \"true\" values of a species, or of a person, as they would exist outside of their current brain and environment.&nbsp; The only consistent result you can arrive at by trying to produce something that implements human values, is to produce more humans.</p>\n<p>This means that attempting to instill human values into an AI is an ill-posed problem that has no complete solution.&nbsp; The only escape from this conclusion is to turn to absolute values - in which case you shouldn't be using human values in the first place.</p>\n<p>This doesn't mean that we have no information about how human values can be extrapolated beyond humans.&nbsp; It means that the more different an agent and an environment are from the human case, the greater the number of different value systems there are that are consistent with human values.&nbsp; However, it appears to me, from the examples and the reasoning given here, that the components of values that we can resolve are those that are evolutionarily stable (and seldom distinctly human); while the contentious component of values that people argue about are their extensions into novel situations, which are undefined.&nbsp; From that I infer that, even if we pin down present-day human values precisely, the ambiguity inherent in extrapolating them into novel environments and new cognitive architectures will make the near future as contentious as the present.</p>\n<h2>References</h2>\n<p>Michael Cook &amp; Susan Mineka (1989).&nbsp; <a href=\"http://pjackson.asp.radford.edu/CookMineka1989.pdf\">Observational conditioning of fear to fear-relevant versus fear-irrelevant stimuli in rhesus monkeys</a>.&nbsp; <em>Journal of Abnormal Psychology</em> 98(4): 448-459.</p>\n<p>Lewis Goldberg (1993).&nbsp; <a href=\"http://www.psych.uiuc.edu/~broberts/Goldberg,%201993.pdf\">The structure of phenotypic personality traits</a>.&nbsp; <em>American Psychologist</em> 48: 26-34.</p>\n<p>Stevan Harnad (1990) <a href=\"http://cogprints.org/3106/\">The Symbol Grounding Problem</a>. <em>Physica D</em> 42: 335-346.</p>\n<p>Thomas Kuhn (1962).&nbsp; <a href=\"http://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions\"><em>The Structure of Scientific Revolutions</em></a>. 1st. ed., Chicago: Univ. of Chicago Press.</p>\n<p>Konrad Lorenz (1966).&nbsp; <em>On Aggression</em>.&nbsp; New York: Harcourt Brace.</p>\n<p>Willard Quine (1969).&nbsp; Ontological relativity.&nbsp; <em>The Journal of Philosophy</em> 65(7): 185-212.</p>\n<p>Andreia Santos, Andreas Meyer-Lindenberg, Christine Deruelle (2010).&nbsp; <a href=\"http://download.cell.com/current-biology/pdf/PIIS0960982210001442.pdf\">Absence of racial, but not gender, stereotyping in Williams syndrome children</a>.&nbsp; Current Biology 20(7), April 13: R307-R308.</p>\n<p>Stuart A. West and Andy Gardner (2010).&nbsp; <a href=\"/Altruism, Spite, and Greenbeards.\"><label title=\"Select this article\" for=\"327.5971.1341.\">Altruism, Spite, and Greenbeards</label></a>.&nbsp; <em>Science</em> 12 March 2010: 1341-1344.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "nSHiKwWyMZFdZg5qt": 1, "sYm3HiWcfZvrGu3ui": 1, "4R8JYu4QF2FqzJxE5": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cAPCCJjggjZPxxcKh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 51, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "2778", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Ethics_is_not_geometry\">Ethics is not geometry<br></h2>\n<p>Western philosophy began at about the same time as Western geometry; and if you read Plato you'll see that he, and many philosophers after him, took geometry as a model for philosophy.</p>\n<p>In geometry, you operate on timeless propositions with mathematical operators.&nbsp; All the content is in the propositions.&nbsp; A proof is equally valid regardless of the sequence of operators used to arrive at it.&nbsp; An algorithm that fails to find a proof when one exists is a poor algorithm.</p>\n<p>The naive way philosophers usually map ethics onto mathematics is to suppose that a human mind contains knowledge (the propositional content), and that we think about that knowledge using operators.&nbsp; The operators themselves are not seen as the concern of philosophy.&nbsp; For instance, when studying values (I also use \"preferences\" here, as a synonym differing only in connotation), people suppose that a person's values are static propositions.&nbsp; The algorithms used to satisfy those values aren't themselves considered part of those values.&nbsp; The algorithms are considered to be only ways of manipulating the propositions; and are \"correct\" if they produce correct proofs, and \"incorrect\" if they don't.</p>\n<p>But an agent's propositions aren't intelligent.&nbsp; An intelligent agent is a <em>system</em>, whose learned and inborn circuits produce intelligent behavior in a given environment.&nbsp; An analysis of propositions is not an analysis of an agent.</p>\n<p>I will argue that:</p>\n<ol>\n<li>The only preferences that can be unambiguously determined are the preferences people implement, which are not always the preferences expressed by their beliefs.</li>\n<li>If you extract a set of propositions from an existing agent, then build a new agent to use those propositions in a different environment, with an \"improved\" logic, you can't claim that it has the same values.</li>\n<li>Values exist in a network of other values.&nbsp; A key ethical question is to what degree values are referential (meaning they can be tested against something outside that network); or non-referential (and hence relative).</li>\n<li>Supposing that values are referential helps only by telling you to ignore human values.</li>\n<li>You cannot resolve the problem by combining information from different behaviors, because the needed information is missing.</li>\n<li>Today's ethical disagreements are largely the result of attempting to extrapolate ancestral human values into a changing world.</li>\n<li>The future will thus be ethically contentious even if we accurately characterize and agree on present human values.<a id=\"more\"></a></li>\n</ol>\n<h2 id=\"Instincts__algorithms__preferences__and_beliefs_are_artificial_categories\">Instincts, algorithms, preferences, and beliefs are artificial categories<br></h2>\n<p>There is no principled distinction between algorithms and propositions in any existing brain.&nbsp; This means that there's no clear way to partition an organism's knowledge into \"propositions\" (including \"preferences\" and \"beliefs\"), and \"algorithms.\"&nbsp; Hence, you can't expect all of an agent's \"preferences\" to end up inside the part of the agent that you choose to call \"propositions\".&nbsp; Nor can you reliably distinguish \"beliefs\" from \"preferences\".</p>\n<p>Suppose that a moth's brain is wired to direct its flight by holding the angle to the moon constant.&nbsp; (This is <a href=\"http://en.wikipedia.org/wiki/Moth\">controversial</a>, but the competing hypotheses would give similar talking points.)&nbsp; If so, is this a belief about the moon, a preference towards the moon, or an instinctive motor program?&nbsp; When it circles around a lamp, does it believe that lamp is the moon?</p>\n<p>When a child pulls its hand away from something hot, does it value not burning itself and believe that hot things burn, or place a value on not touching hot things, or just have an evolved motor program that responds to hot things?&nbsp; Does your answer change if you learn that the hand was directed to pull back by <a href=\"http://en.wikipedia.org/wiki/Withdrawal_reflex\">spinal reflexes</a>, without involving the cortex?</p>\n<p>Monkeys can learn to fear snakes more easily than they can learn to fear flowers (<a href=\"http://pjackson.asp.radford.edu/CookMineka1989.pdf\">Cook &amp; Mineka 1989</a>).&nbsp; Do monkeys, and perhaps humans, have an \"instinctive preference\" against snakes?&nbsp; Is it an instinct, a preference (snake = negative utility), or a learned behavior (lab monkeys are not afraid of snakes)?</p>\n<p>Can we map the preference-belief distinction onto the distinction between instinct and learned behavior?&nbsp; That is, are all instincts preferences, and all preferences instincts?&nbsp; There are things we call instincts, like spinal reflexes, that I don't think can count as preferences.&nbsp; And there are preferences, such as the relative values I place on the music of Bach and Berg, that are not instincts.&nbsp; (In fact, these are the preferences we care about.&nbsp; The purpose of Friendly AI is not to retain the fist-clenching instinct for future generations.)</p>\n<h2 id=\"Bias__heuristic__or_preference_\">Bias, heuristic, or preference?</h2>\n<p>A \"bias\" is a reasoning procedure that produces an outcome that does not agree with some logic.&nbsp; But the object in nature is not to conform to logic; it is to produce advantageous behavior.</p>\n<p>Suppose you interview Fred about his preferences.&nbsp; Then you write a utility function for Fred.&nbsp; You experiment, putting Fred in different situations and observing how he responds.&nbsp; You observe that Fred acts in ways that fail to optimize the utility function you wrote down, in a consistently-biased way.</p>\n<p>Is Fred displaying bias?&nbsp; Or does the Fred-system, including both his beliefs <em>and</em> the bias imposed by his reasoning processes, implement a preference that is not captured in his beliefs alone?</p>\n<p>Allegedly true story, from a Teaching Company audio lecture (I forget which one):&nbsp; A psychology professor was teaching a class about conditioned behavior.&nbsp; He also had the habit of pacing back and forth in front of the class.</p>\n<p>The class decided to test his claims by leaning forward and looking interested when the professor moved toward the left side of the room, but acting bored when he moved toward the right side.&nbsp; By the end of the semester, they had trained him to give his entire lecture from the front left corner.&nbsp; When they asked him why he always stood there, he was surprised by the question - he wasn't even aware he had changed his habit.</p>\n<p>If you inspected the professor's beliefs, and then studied his actions, you would conclude he was acting irrationally.&nbsp; But he wasn't.&nbsp; He was <em>acting</em> rationally, just not <em>thinking</em> rationally.&nbsp;&nbsp; His brain didn't detect the pattern in the class's behavior and deposit a proposition into his brain.&nbsp; It encoded the proper behavior, if not straight into his pre-motor cortex, at least not into any conscious beliefs.</p>\n<p>Did he have a bias towards the left side of the room?&nbsp; Or a preference for seeing students pay attention?&nbsp; Or a preference that became a bias when the next semester began and he kept doing it?</p>\n<p>Take your pick - there's no right answer.</p>\n<p>If a heuristic gives answers consistently biased in one direction across a wide range of domains, we can call it a bias.&nbsp; Most biases found in the literature appear to be wide-ranging and value-neutral.&nbsp; But the literature on biases is itself biased (deliberately) towards discussing that type of bias.&nbsp;&nbsp; If we're trawling all of human behavior for values, we may run across many instances where we can't say whether a heuristic is a bias or a preference.</p>\n<p>As one example, I would say that the <a href=\"http://en.wikipedia.org/wiki/Extraordinarity_bias\">extraordinarity bias</a> is in fact a preference.&nbsp; Or consider the happiness paradox:&nbsp; People who become paralyzed become extremely depressed only temporarily; people who win the lottery become very happy only temporarily.&nbsp; (<a href=\"http://www.google.com/search?q=happiness+%22set-point%22&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a\">Google 'happiness \"set-point\"'</a>.)&nbsp; I've <a href=\"/lw/7k/incremental_progress_and_the_valley/5fu\">previously argued on LessWrong</a> that this is not a bias, but a heuristic to achieve our preferences.&nbsp; Happiness is proportional not to our present level of utility, but to the rate of change in our utility.&nbsp; Trying to maximize happiness (the rate of increase of utility) in the near term maximizes total utility over lifespan better than consciously attempting to maximize near-term utility would.&nbsp; This is because maximizing the rate of increase in utility over a short time period, instead of total utility over that time period, prefers behavior that has a small area under the utility curve during that time but ends with a higher utility than it started with, over behavior with a large area under the utilty curve that ends with a lower utility than it started with.&nbsp; This interpretation of happiness would mean that <a href=\"http://en.wikipedia.org/wiki/Impact_bias\">impact bias</a> is not a bias at all, but a heuristic that compensates for this in order to maximize utility rather than happiness when we reason over longer time periods.</p>\n<h2 id=\"Environmental_factors__Are_they_a_preference_or_a_bias_\">Environmental factors: Are they a preference or a bias?<br></h2>\n<p>Evolution does not distinguish between satisfying preconditions for behavior by putting knowledge into a brain, or by using the statistics of the environment.&nbsp; This means that the environment, which is not even present in the geometric model of ethics, is also part of your values.</p>\n<p>When the aforementioned moth circles around a lamp, is it erroneously acting on a bias, or expressing moth preferences?</p>\n<p>Humans like having sex.&nbsp; The teleological purpose of this preference is to cause them to have children.&nbsp; Yet we don't say that they are in error if they use birth control.&nbsp; This suggests that we consider our true preferences to be the organismal ones that trigger positive qualia, not the underlying evolutionary preferences.</p>\n<p>Strict monogamy causes organisms that live in family units to evolve to act more altruistically, because their siblings are as related to them as their children are (West &amp; Gardner 2010).&nbsp; Suppose that people from cultures with a long history of nuclear families and strict monogamy act, on average, more altruistically than people from other cultures; and you put people from both cultures together in a new environment with neither monogamy nor nuclear families.&nbsp; We would probably rather say that the people from these different cultures have different values; not that they both have the same preference to \"help their genes\", but that the people from the monogamous culture have an evolved bias that causes them to erroneously treat strangers nicely in this new environment.&nbsp; Again, we prefer the organismal preference.</p>\n<p>However, if we follow this principle consistently, it prevents us from ever trying to improve ourselves, since it in effect defines our present selves as optimal:</p>\n<ul>\n<li>Humans like eating food with fat, sugar, and salt.&nbsp; In our ancestral context, that expressed the human value of optimizing nutrition.&nbsp; The evolutionary preference is for good nutrition; the organismal preference is for fat, sugar, and salt.&nbsp; By analogy to contraception, liking fat, sugar, and salt is not an evolved but dysfunctional bias in taste; it's a true human value.</li>\n<li>Suppose fear of snakes is triggered by the shape and motion of snakes.&nbsp; The organismal preference is against snakes.&nbsp; The evolutionary preference is against <em>poisonous</em> snakes.&nbsp; If the world is now full of friendly cybernetic snakes, you must conclude that prejudice against them is a human value to be preserved, not a bias to be overcome.&nbsp; Death to the friendly snakes!</li>\n<li>Men enjoy violence.&nbsp; Hitting a stranger over the head with a stick is naturally <em>fun</em> to human males, and it takes a lot of social conditioning to get them not to do this, or at least to restrict themselves to video games.&nbsp; By what principle can we say that this is merely an obsolete heuristic to protect the tribe that is no longer helpful in our present environment; yet having sex with a condom is enjoying a preference?</li>\n<li>(<a href=\"http://download.cell.com/current-biology/pdf/PIIS0960982210001442.pdf\">Santos et al. 2010</a>) reports (<a href=\"http://news.sciencemag.org/sciencenow/2010/04/when-social-fear-disappears-so-d.html\">summarized in Science Online</a>) that children with a genetic mutation causing Williams syndrome, which causes less fear of strangers, have impaired racial stereotyping, but intact gender stereotyping.&nbsp; This suggests that racism, and perhaps sexism, are evolved preferences actively implemented by gene networks.</li>\n</ul>\n<p>So the \"organismal vs. evolutionary\" distinction doesn't help us choose what's a preference and what's a bias.&nbsp; Without any way of doing that, it is <em>in principle</em> impossible to create a category of \"preferences\" distinct from \"preferred outcomes\".&nbsp; A \"value\" consists of declarative knowledge, algorithms, and environment, taken together.&nbsp; Change any of those, and it's not the same value anymore.</p>\n<p>This means that extrapolating human values into a different environment gives an error message.</p>\n<h2 id=\"A_ray_of_hope_____\">A ray of hope? ...<br></h2>\n<p>I just made a point by presenting cases in which most people have intuitions about which outcome is correct, and showing that these intuitions don't follow a consistent rule.</p>\n<p>So why do we have the intuitions?</p>\n<p>If we have consistent intuitions, they must follow <em>some</em> rule.&nbsp; We just don't know what it is yet.&nbsp; Right?</p>\n<h2 id=\"____No_\">... No.</h2>\n<p>We don't have consistent intuitions.</p>\n<p>Any one of us has consistent intuitions; and those of us living in Western nations in the 21st century have a lot of intuitions in common.&nbsp; We can predict how most of these intuitions will fall out using some dominant cultural values.&nbsp; The examples involving monogamy and violent males rely on the present relatively high weight on the preference to reduce violent conflict.&nbsp; But this is a context-dependent value!&nbsp; &lt;just-so story&gt;It arises from living in a time and a place where technology makes interactions between tribes more frequent and more beneficial, and conflict more costly&lt;/just-so story&gt;.&nbsp; But looking back in history, we see many people who would disagree with it:</p>\n<ul>\n<li>Historians struggle to explain the origins of World War I and the U.S. Civil War.&nbsp; Sometimes the simplest answer is best:&nbsp; They were for fun.&nbsp; Men on both sides were itching for an excuse to fight.</li>\n<li>In the 19th century, Americans killed off the Native Americans to have their land.&nbsp; Americans universally condemn that action now that they are secure in its benefits; most Americans condoned it at the time.</li>\n<li>Homer would not have agreed that violence is bad!&nbsp; Skill at violence was the greatest virtue to the ancient Greeks.&nbsp; The tension that generates tragedy in the Iliad is not between violence and empathy, but between saving one's kin and saving one's honor.&nbsp; Hector is conflicted, but not about killing Greeks.&nbsp; His speech on his own tragedy ends with his wishes for his son:&nbsp; \"May he bring back the blood-stained spoils of him whom he has laid low, and let his mother's heart be glad.\"</li>\n<li>The Nazis wouldn't have agreed that enjoying violence was bad.&nbsp; We have learned nothing if we think the Nazis rose to power because Germans suddenly went mad <em>en masse</em>, or because Hitler gave really good speeches.&nbsp; Hitler had an entire ideology built around the idea, as I gather, that civilization was an evil constriction on the will to power; and artfully attached it to a few compatible cultural values.</li>\n<li>A similar story could be told about communism.</li>\n</ul>\n<p>The idea that violence (and sexism, racism, and slavery) is bad is a minority opinion in human cultures over history.&nbsp; Nobody likes being hit over the head with a stick by a stranger; but in pre-Christian Europe, it was the person who failed to prevent being struck, not the person doing the striking, whose virtue was criticized.</p>\n<p>Konrad Lorenz believed that the more deadly an animal is, the more emotional attachment to its peers its species evolves, via group selection (<a href=\"http://en.wikipedia.org/wiki/On_Aggression\">Lorenz 1966</a>).&nbsp; The past thousand years of history has been a steady process of humans building sharper claws, and choosing values that reduce their use, keeping net violence roughly constant.&nbsp; As weapons improve, cultural norms that promote conflict must go.&nbsp; First, the intellectuals (who were Christian theologians at the time) neutered masculinity; in the Enlightenment, they attacked religion; and in the 20th century, art.&nbsp; The ancients would probably find today's peaceful, offense-forgiving males as nauseating as I would find a future where the man on the street embraces postmodern art and literature.</p>\n<p>This gradual sacrificing of values in order to attain more and more tolerance and empathy, is the most-noticable change in human values in all of history.&nbsp; This means it is the least-constant of human values.&nbsp; Yet we think of an infinite preference for non-violence and altruism as a foundational value!&nbsp; Our intuitions about our values are thus as mistaken as it is possible for them to be.</p>\n<p>(The logic goes like this:&nbsp; Humans are learning more, and their beliefs are growing closer to the truth.&nbsp; Humans are becoming more tolerant and cooperative.&nbsp; Therefore, tolerant and cooperative values are closer to the truth.&nbsp; Oops!&nbsp; If you believe in moral truth, then you shouldn't be searching for <em>human</em> values in the first place!)</p>\n<p>Catholics don't agree that having sex with a condom is good.&nbsp; They have an elaborate system of belief built on the idea that teleology express God's will, and so underlying purpose (what I call evolutionary preference) always trumps organismal preference.</p>\n<p>And I cheated in the question on monogamy.&nbsp; Of course you <em>said</em> that being more altruistic wasn't an error.&nbsp; Everyone always says they're in favor of more altruism.&nbsp; It's like asking whether someone would like lower taxes.&nbsp; But the hypothesis was that people from non-monogamous or non-family-based cultures do in fact show lower levels of altruism.&nbsp; By hypothesis, then, they would be comfortable with their own levels of altruism, and might feel that higher levels are a bias.</p>\n<p>Preferences are complicated and numerous, and arise in an evolutionary process that does not guarantee consistency.&nbsp; Having conflicting preferences makes action difficult.&nbsp; <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf\">Energy</a> <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.4083&amp;rep=rep1&amp;type=pdf\">minimization</a>, a general principle that may underly much of our learning, simply means reducing conflicts in a network.&nbsp; The most basic operations of our neurons thus probably act to reduce conflicts between preferences.</p>\n<p>But there are no \"true, foundational\" preferences from which to start.&nbsp; There's just a big network of them that can be pushed into any one of many stable configurations, depending on the current environment.&nbsp; There's the Catholic configuration, and the Nazi configuration, and the modern educated tolerant cosmopolitan configuration.&nbsp; If you're already in one of those configurations, it seems obvious what the right conclusion is for any <em>particular</em> value question; and this gives the illusion that we have some underlying principle by which we can properly choose what is a value and what is a bias.&nbsp; But it's just circular reasoning.</p>\n<h2 id=\"What_about_qualia_\">What about qualia?</h2>\n<p>But everyone agrees that pleasure is good, and pain is bad, right?</p>\n<p>Not entirely - I could point to, say, medieval Europe, when many people believed that causing yourself needless pain was virtuous.&nbsp; But, by and large yes.</p>\n<p>And beside the point (although see below).&nbsp; Because when we talk about values, the eventual applications we have in mind are <em>never</em> about qualia.&nbsp; Nobody has heated arguments about whose qualia are better.&nbsp; Nobody even really cares about qualia.&nbsp; Nobody is going to dedicate their life to building Friendly AI in order to ensure that beings a million years from now still dislike castor oil and enjoy chocolate.</p>\n<p>We may be arguing about preserving a tendency to commit certain acts that give us a warm qualic glow, like helping a bird with a broken wing.&nbsp; But I don't believe there's a dedicated small-animal-empathy quale.&nbsp; More likely there's a hundred inferential steps linking an action, through our knowledge and thinking processes, to a general-purpose warm-glow quale.</p>\n<h2 id=\"Value_is_a_network_concept\">Value is a network concept<br></h2>\n<p>Abstracting human behavior into \"human values\" is an ill-posed problem.&nbsp; It's an attempt to divine a simple description of our preferences, outside the context of our environment and our decision process.&nbsp; But we have no consistent way of deciding what are the preferences, and what is the context.&nbsp; We have the illusion that we can, because our intuitions give us answers to questions about preferences - but they use our contextually-situated preferences to do so.&nbsp; That's circular reasoning.</p>\n<p>The problem in trying to root out foundational values for a person is the same as in trying to root out objective values for the universe, or trying to choose the \"correct\" axioms for a geometry.&nbsp; You can pick a set that is self-consistent; but you can't label your choice \"the truth\".</p>\n<p>These are all <em>network concepts</em>, where we try to isolate things that exist only within a complex homogeneous network.&nbsp; Our mental models of complex networks follow mathematics, in which you choose a set of axioms as foundational; or social structures, in which you can identify a set of people as the prime movers.&nbsp; But these conceptions do not even model math or social structures correctly.&nbsp; Axioms are chosen for convenience, but a logic is an entire network of self-consistent statements, many different subsets of which could have been chosen as axioms.&nbsp; Social power does not originate with the rulers, or we would still have kings.</p>\n<p>There is a very similar class of problems, including <a href=\"http://en.wikipedia.org/wiki/Symbol_grounding\">symbol grounding</a> (trying to root out the nodes that are the sources of meaning in a semantic network), and philosophy of science (trying to determine how or whether the scientific process of choosing a set of beliefs given a set of experimental data converges on external truth as you gather more data).&nbsp; The crucial difference is that we have strong reasons for believing that these networks refer to an external domain, and their statements can be tested against the results from independent access to that domain.&nbsp; I call these <em>referential network concepts</em>.&nbsp; One system of referential network concepts can be more right than another; one system of non-referential network concepts can only be more self-consistent than another.</p>\n<p>Referential network concepts cannot be given 0/1 truth-values at a finer granularity than the level at which a network concept refers to something in the extensional (referred-to) domain.&nbsp; For example, (Quine 1968) argues that a natural-language statement cannot be unambiguously parsed beyond the granularity of the behavior associated with it.&nbsp; This is isomorphic to my claim above that a value/preference can't be parsed beyond the granularity of the behavior of an agent acting in an environment.</p>\n<p>Thomas Kuhn gained notoriety by arguing (<a href=\"http://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions\">Kuhn 1962</a>) that there is no such thing as scientific progress, but only transitions between different stable states of belief; and that modern science is only different from ancient science, not better.&nbsp; (He denies this in the postscript to the 1969 edition, but it is the logical implication of both his arguments and the context he presents them in.)&nbsp; In other words, he claims science is a non-referential network concept.&nbsp; An interpretation in line with Quine would instead say that science is referential at the level of the experiment, and that ambiguities may remain in how we define the fine-grained concepts used to predict the outcomes of experiments.</p>\n<p>Determining whether a network concept domain is referential or non-referential is tricky.&nbsp; The distinction was not even noticed until the 19th century.&nbsp; Until then, everyone who had ever studied geometry, so far as I know, believed there was one \"correct\" geometry, with Euclid's 5 postulates as axioms.&nbsp; But in the early 19th century, several mathematicians proved that you could build three different, consistent geometries depending on what you put in the place of <a href=\"http://en.wikipedia.org/wiki/Parallel_postulate\">Euclid's fifth postulate</a>.&nbsp; The universe we live in most likely conforms to only one of these (making geometry referential in a physics class); but the others are equally valid mathematically (making geometry non-referential in a math class).</p>\n<h2 id=\"Is_value_referential__or_non_referential_\">Is value referential, or non-referential?<br></h2>\n<p>There are two ways of interpreting this question, depending on whether one means \"human values\" or \"<a href=\"http://en.wikipedia.org/wiki/Moral_realism\">absolute values</a>\".</p>\n<p>Judgements of value expressed in human language are referential; they refer to human behavior.&nbsp; So human values are referential.&nbsp; You can decide whether claims about a particular human's values are true or false, as long as you don't extend those claims outside the context of that human's decision process and environment.&nbsp; This claim is isomorphic to Quine's claim about meaning in human language.</p>\n<p>Asking about absolute values is isomorphic to applying the symbol-grounding problem to consciousness.&nbsp; Consciousness exists internally, and is finer-grained than human behaviors.&nbsp; Providing a symbol-grounding method that satisfied Quine's requirements would not provide any meanings accessible to consciousness.&nbsp; Stevan Harnad (Harnad 2000) described <a href=\"http://cogprints.org/3106/\">how symbols might be grounded</a> for consciousness in sense perceptions and statistical regularities of those perceptions.</p>\n<p>(This brings up an important point, which I will address later:&nbsp; You may be able to assign referential network concepts <em>probabilistic</em> or else <em>fuzzy </em>truth values at a finer level of granularity than the level of correspondence.&nbsp; A preview: This doesn't get you out of the difficulty, because the ambiguous cases don't have mutual information with which they could help resolve each other.)</p>\n<p>Can an analogous way be found to ground absolute values?&nbsp; Yes and no.&nbsp; You can choose axioms that are hard to argue with, like \"existence is better than non-existence\", \"pleasure is better than pain\", or \"complexity is better than simplicity\".&nbsp; (I find \"existence is better than non-existence\" pretty hard to argue with; but Buddhists disagree.)&nbsp; If you can interpret them in an unambiguous way, and define a utility calculus enabling you to make numeric comparisons, you may be able to make \"absolute\" comparisons between value systems <em>relative to your axioms</em>.</p>\n<p>You would also need to make some choices we've talked about here before, such as \"use summed utility\" or \"use average utility\".&nbsp; And you would need to make many possibly-arbitrary interpretation assumptions such as what pleasure is, what complexity is, or what counts as an agent.&nbsp; The gray area between absolute and relative values is in how self-evident all these axioms, decisions, and assumptions are.&nbsp; But any results at all - even if they provide guidance only in decisions such as \"destroy / don't destroy the universe\" - would mean we could claim there is a way for values to be referential at a finer granularity than that of an agent's behavior.&nbsp; And things that seem arbitrary to us today may turn out not to be; for example, I've argued <a href=\"/lw/91/average_utilitarianism_must_be_correct/\">here</a> that average utilitarianism can be <em>derived</em> from the von Neumann-Morgenstern theorem on utility.</p>\n<h2 id=\"____It_doesn_t_matter_WRT_friendly_AI_and_coherent_extrapolated_volition_\">... It doesn't matter WRT friendly AI and coherent extrapolated volition.<br></h2>\n<p>Even supposing there is a useful, correct, absolute lattice on value system and/or values, it doesn't forward the project of trying to instill human values in artificial intelligences.&nbsp; There are 2 possible cases:</p>\n<ol>\n<li>There are no absolute values.&nbsp; Then we revert to judgements of human values, which, as argued above, have no unambiguous interpretation outside of a human context.</li>\n<li>There are absolute values.&nbsp; In which case, we should use them, not human values, whenever we can discern them.</li>\n</ol>\n<h2 id=\"Fuzzy_values_and_fancy_math_don_t_help\">Fuzzy values and fancy math don't help<br></h2>\n<p>So far, I've looked at cases of ambiguous values only one behavior at a time.&nbsp; I mentioned above that you can assign probabilities to different value interpretations of a behavior.&nbsp; Can we take a network of many probabilistic interpretations, and use energy minimization or some other mathematics to refine the probabilities?</p>\n<p>No; because for the ambiguities of interest, we have no access to any of the mutual information between how to resolve two different ambiguities.&nbsp; The ambiguity is in whether the hypothesized \"true value\" would agree or disagree with the results given by the initial propositional system plus a different decision process and/or environment.&nbsp; In every case, this information is missing.&nbsp; No clever math can provide this information from our existing data, no matter how many different cases we combine.</p>\n<p>Nor should we hope to find correlations between \"true values\" that will help us refine our estimates for one value given a different unambiguous value. The search for values is isomorphic to the search for personality primitives.&nbsp; The approach practiced by psychologists is to use factor analysis to take thousands of answers to questions that are meant to test personality phenotype, and mathematically reduce these to discover a few underlying (\"latent\") independent personality variables, most famously in the Big 5 personality scale (reviewed in Goldberg 1993).&nbsp; In other words:&nbsp; The true personality traits, and by analogy the true values a person holds, are by definition independent of each other.</p>\n<p>We expect, nonetheless, to find correlations between the component of these different values that resides in decision processes.&nbsp; This is because it is efficient to re-use decision processes as often as possible.&nbsp; Evolution should favor partitioning values between propositions, algorithms, and environment in a way that minimizes the number of algorithms needed.&nbsp; These correlations will not help us, because they have to do only with how a value is implemented within an organism, and say nothing about how the value would be extended into a different organism or environment.</p>\n<p>In fact, I propose that the different value systems popular among humans, and the resulting ethical arguments, are largely different ways of partitioning values between propositions, algorithms, and environment, that each result in a relatively simple set of algorithms, and each in fact give the same results in most situations that our ancestors would have encountered.&nbsp; It is the attempt to extrapolate human values into the new, manmade environment that causes ethical disagreements.&nbsp; This means that our present ethical arguments are largely the result of cultural change over the past few thousand years; and that the next few hundred years of change will provide ample grounds for additional arguments even if we resolve today's disagreements.</p>\n<h2 id=\"Summary\">Summary</h2>\n<p>Philosophically-difficult domains often involve network concepts, where each component depends on other components, and the dependency graph has cycles.&nbsp; The simplest models of network concepts suppose that there are some original, primary nodes in the network that everything depends on.</p>\n<p>We have learned to stop applying these models to geometry and supposing there is one true set of axioms.&nbsp; We have learned to stop applying these models to biology, and accept that life evolved, rather than that reality is divided into Creators (the primary nodes) and Creatures.&nbsp; We are learning to stop applying them to morals, and accept that morality depends on context and biology, rather than being something you can extract from its context.&nbsp; We should also learn to stop applying them to the preferences directing the actions of intelligent agents.</p>\n<p>Attempting to identify values is a network problem, and you cannot identify the \"true\" values of a species, or of a person, as they would exist outside of their current brain and environment.&nbsp; The only consistent result you can arrive at by trying to produce something that implements human values, is to produce more humans.</p>\n<p>This means that attempting to instill human values into an AI is an ill-posed problem that has no complete solution.&nbsp; The only escape from this conclusion is to turn to absolute values - in which case you shouldn't be using human values in the first place.</p>\n<p>This doesn't mean that we have no information about how human values can be extrapolated beyond humans.&nbsp; It means that the more different an agent and an environment are from the human case, the greater the number of different value systems there are that are consistent with human values.&nbsp; However, it appears to me, from the examples and the reasoning given here, that the components of values that we can resolve are those that are evolutionarily stable (and seldom distinctly human); while the contentious component of values that people argue about are their extensions into novel situations, which are undefined.&nbsp; From that I infer that, even if we pin down present-day human values precisely, the ambiguity inherent in extrapolating them into novel environments and new cognitive architectures will make the near future as contentious as the present.</p>\n<h2 id=\"References\">References</h2>\n<p>Michael Cook &amp; Susan Mineka (1989).&nbsp; <a href=\"http://pjackson.asp.radford.edu/CookMineka1989.pdf\">Observational conditioning of fear to fear-relevant versus fear-irrelevant stimuli in rhesus monkeys</a>.&nbsp; <em>Journal of Abnormal Psychology</em> 98(4): 448-459.</p>\n<p>Lewis Goldberg (1993).&nbsp; <a href=\"http://www.psych.uiuc.edu/~broberts/Goldberg,%201993.pdf\">The structure of phenotypic personality traits</a>.&nbsp; <em>American Psychologist</em> 48: 26-34.</p>\n<p>Stevan Harnad (1990) <a href=\"http://cogprints.org/3106/\">The Symbol Grounding Problem</a>. <em>Physica D</em> 42: 335-346.</p>\n<p>Thomas Kuhn (1962).&nbsp; <a href=\"http://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions\"><em>The Structure of Scientific Revolutions</em></a>. 1st. ed., Chicago: Univ. of Chicago Press.</p>\n<p>Konrad Lorenz (1966).&nbsp; <em>On Aggression</em>.&nbsp; New York: Harcourt Brace.</p>\n<p>Willard Quine (1969).&nbsp; Ontological relativity.&nbsp; <em>The Journal of Philosophy</em> 65(7): 185-212.</p>\n<p>Andreia Santos, Andreas Meyer-Lindenberg, Christine Deruelle (2010).&nbsp; <a href=\"http://download.cell.com/current-biology/pdf/PIIS0960982210001442.pdf\">Absence of racial, but not gender, stereotyping in Williams syndrome children</a>.&nbsp; Current Biology 20(7), April 13: R307-R308.</p>\n<p>Stuart A. West and Andy Gardner (2010).&nbsp; <a href=\"/Altruism, Spite, and Greenbeards.\"><label title=\"Select this article\" for=\"327.5971.1341.\">Altruism, Spite, and Greenbeards</label></a>.&nbsp; <em>Science</em> 12 March 2010: 1341-1344.</p>", "sections": [{"title": "Ethics is not geometry", "anchor": "Ethics_is_not_geometry", "level": 1}, {"title": "Instincts, algorithms, preferences, and beliefs are artificial categories", "anchor": "Instincts__algorithms__preferences__and_beliefs_are_artificial_categories", "level": 1}, {"title": "Bias, heuristic, or preference?", "anchor": "Bias__heuristic__or_preference_", "level": 1}, {"title": "Environmental factors: Are they a preference or a bias?", "anchor": "Environmental_factors__Are_they_a_preference_or_a_bias_", "level": 1}, {"title": "A ray of hope? ...", "anchor": "A_ray_of_hope_____", "level": 1}, {"title": "... No.", "anchor": "____No_", "level": 1}, {"title": "What about qualia?", "anchor": "What_about_qualia_", "level": 1}, {"title": "Value is a network concept", "anchor": "Value_is_a_network_concept", "level": 1}, {"title": "Is value referential, or non-referential?", "anchor": "Is_value_referential__or_non_referential_", "level": 1}, {"title": "... It doesn't matter WRT friendly AI and coherent extrapolated volition.", "anchor": "____It_doesn_t_matter_WRT_friendly_AI_and_coherent_extrapolated_volition_", "level": 1}, {"title": "Fuzzy values and fancy math don't help", "anchor": "Fuzzy_values_and_fancy_math_don_t_help", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "161 comments"}], "headingsCount": 15}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 161, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["px4nYEy3rDqeegJw3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-27T01:10:39.138Z", "modifiedAt": null, "url": null, "title": "Proposed New Features for Less Wrong", "slug": "proposed-new-features-for-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:46.372Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pa7PncRvcDcjw3vSR/proposed-new-features-for-less-wrong", "pageUrlRelative": "/posts/pa7PncRvcDcjw3vSR/proposed-new-features-for-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/pa7PncRvcDcjw3vSR/proposed-new-features-for-less-wrong", "postedAtFormatted": "Tuesday, April 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposed%20New%20Features%20for%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposed%20New%20Features%20for%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpa7PncRvcDcjw3vSR%2Fproposed-new-features-for-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposed%20New%20Features%20for%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpa7PncRvcDcjw3vSR%2Fproposed-new-features-for-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpa7PncRvcDcjw3vSR%2Fproposed-new-features-for-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 416, "htmlBody": "<p>Followup to: <a href=\"/lw/20z/announcing_the_less_wrong_subreddit/\">Announcing the Less Wrong Sub-Reddit</a></p>\n<p>After the recent discussion about the Less Wrong sub-reddit, me and Less Wrong site designer Matthew Fallshaw have been discussing possible site improvements, and ways to implement them. As far as I can tell, the general community consensus in the previous post was that a discussion section to replace the Open Thread would be a good idea, due to the many problems with Open Thread, but that it would be problematic to host it off-site. For this reason, our current proposal involves modifying the main site to include a separate \"Discussion\" section in the navigation bar (next to \"Wiki | Sequences | About\"). What are now Open Thread comments would be hosted in the Discussion section, in a more user-friendly and appropriate format (similar to Reddit's or a BBS forum's). If my impression was mistaken, please do say so. (If you think that this is a great idea, please do say so as well, to avoid <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">Why Our Kind Can't Cooperate</a>.)</p>\n<p>We have also identified another potential problem with the site: the high quality standard, heavy use of neologisms, and karma penalties for being wrong might be intimidating to newcomers. To help alleviate this, after much discussion, we have come up with two different proposals. (To avoid bias, I'm not going to say which one is mine and which one is Matthew's.)</p>\n<p>- Proposal 1: Posts submitted to Less Wrong can be tagged with a \"karma coward\" option. Such posts can still be voted on, but votes on them will have no effect on a user's karma total. There will be a Profile option to hide \"karma coward\" posts from view.</p>\n<p>- Proposal 2: A grace period for new users. Votes on comments from new users will have no effect on that user's karma total for a certain period of time, like two weeks or a month.</p>\n<p>- Proposal 3: Do nothing; the site remains as-is.</p>\n<p>To see what the community consensus is, I have set up a poll here: <a href=\"http://www.misterpoll.com/polls/482996\">http://www.misterpoll.com/polls/482996</a>. Comments on our proposals, and alternative proposals, are more than welcome. (To avoid clogging the comments, please do not simply declare your vote without explaining why you voted that way.)</p>\n<p>EDIT: Posts and comments in the discussion section would count towards a user's karma total (not withstanding the implementation of proposal 1 and proposal 2), although posts would only earn a user 1 karma per upvote instead of 10.</p>\n<p>EDIT 2: To avoid contamination by other people's ideas, please vote <em>before</em> you look at the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pa7PncRvcDcjw3vSR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 5.814346532898291e-07, "legacy": true, "legacyId": "2813", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 174, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Rt8oJF27dndhycxks", "7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-27T10:06:48.818Z", "modifiedAt": null, "url": null, "title": "Attention Less Wrong: We need an FAQ", "slug": "attention-less-wrong-we-need-an-faq", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:00.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K7HP5QcRJLYBAZLaL/attention-less-wrong-we-need-an-faq", "pageUrlRelative": "/posts/K7HP5QcRJLYBAZLaL/attention-less-wrong-we-need-an-faq", "linkUrl": "https://www.lesswrong.com/posts/K7HP5QcRJLYBAZLaL/attention-less-wrong-we-need-an-faq", "postedAtFormatted": "Tuesday, April 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Attention%20Less%20Wrong%3A%20We%20need%20an%20FAQ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAttention%20Less%20Wrong%3A%20We%20need%20an%20FAQ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK7HP5QcRJLYBAZLaL%2Fattention-less-wrong-we-need-an-faq%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Attention%20Less%20Wrong%3A%20We%20need%20an%20FAQ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK7HP5QcRJLYBAZLaL%2Fattention-less-wrong-we-need-an-faq", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK7HP5QcRJLYBAZLaL%2Fattention-less-wrong-we-need-an-faq", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p>Less Wrong is extremely intimidating to newcomers and <a href=\"/lw/265/proposed_new_features_for_less_wrong/1xfa?context=1#1xfa\">as pointed out by Academian</a> something that would help is a document in FAQ form intended for newcomers. Later we can decide how to best deliver that document to new Less Wrongers, but for now we can <a href=\"http://wiki.lesswrong.com/wiki/FAQ\">edit the existing (narrow) FAQ</a> to make the site less scary and the standards more evident.</p>\n<p>Go ahead and make bold edits to the FAQ wiki page or use this post to discuss possible FAQs and answers in agonizing detail.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K7HP5QcRJLYBAZLaL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 15, "extendedScore": null, "score": 5.815425781872497e-07, "legacy": true, "legacyId": "2819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-27T12:32:06.806Z", "modifiedAt": null, "url": null, "title": "What is missing from rationality? ", "slug": "what-is-missing-from-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:30.853Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4RthYwrowiswpksGT/what-is-missing-from-rationality", "pageUrlRelative": "/posts/4RthYwrowiswpksGT/what-is-missing-from-rationality", "linkUrl": "https://www.lesswrong.com/posts/4RthYwrowiswpksGT/what-is-missing-from-rationality", "postedAtFormatted": "Tuesday, April 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20missing%20from%20rationality%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20missing%20from%20rationality%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4RthYwrowiswpksGT%2Fwhat-is-missing-from-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20missing%20from%20rationality%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4RthYwrowiswpksGT%2Fwhat-is-missing-from-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4RthYwrowiswpksGT%2Fwhat-is-missing-from-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 735, "htmlBody": "<p><em>\"In a sufficiently mad world, being sane is actually a disadvantage\"</em></p>\n<p style=\"margin-bottom: 0in; padding-left: 390px;\">&ndash; Nick Bostrom</p>\n<p>Followup to: <a href=\"/lw/20p/what_is_rationality/\">What is rationality? </a></p>\n<p>A canon of work on \"rationality\" has built up on Less Wrong; in <em><a href=\"/lw/20p/what_is_rationality/\">What is rationality?</a></em>, I listed most of the topics and paradigms that have been used extensively on Less Wrong, including: simple calculation and logic<sup>1</sup>, probability theory, cognitive biases, the theory of evolution, analytic philosophical thinking, microeconomics. I defined \"Rationality\" to be <em><strong>the ability to do well on hard decision problems</strong></em>, often abbreviated to \"winning\" - choosing actions that cause you to do very well.&nbsp;</p>\n<p>However, I think that the rationality canon here on Less Wrong is not very good at causing the people who read it to actually do well at most of life's challenges. This is therefore a <em>criticism </em>of the LW canon.</p>\n<p>If the standard to judge methods by is whether they give you the ability to do well on a wide range of hard real-life decision problems, with a wide range of <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">terminal values</a> being optimized for, then Less-Wrong-style rationality fails, because the people who read it seem to mostly only succeed at the goal that most others in society would label as \"being a nerd\".<sup>2</sup> We don't seem to have a broad range of people pursuing and winning at a broad range of goals (though there are a few exceptional people here).</p>\n<p>Although the equations of probability theory and expected utility do not state that you have to be a \"<a href=\"/lw/59/spocks_dirty_little_secret/\">Spock rationalist</a>\" to use them, in reality I see more Spock than Kirk. I myself am not exempt from this critique.</p>\n<p>What, then, is missing?</p>\n<p><a id=\"more\"></a></p>\n<p>The problem, I think, is that the original motivation for Less Wrong was the bad <em>planning </em>decisions that society as a whole takes<sup>3</sup>.&nbsp; When society acts, it tends to benefit most when it acts in what I would call the <em>Planning model of winning</em>, where reward is a function of the accuracy of beliefs and the efficacy of explicitly reasoned plans.</p>\n<p>But individuals within a society do not get their rewards solely based upon the quality of their plans: we are systematically rewarded and punished by the environment around us by:</p>\n<ul>\n<li>Our personality traits and other psychological factors such as courage, happiness set-point, self-esteem, etc. </li>\n</ul>\n<ul>\n<li>The group we are a member of, especially our close friends and associates. </li>\n</ul>\n<ul>\n<li>Our skill in dealing with people, which we might call \"<a href=\"http://en.wikipedia.org/wiki/Emotional_intelligence\">emotional intelligence</a>\".</li>\n</ul>\n<ul>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Shibboleths\">shibboleths</a> we display, the signals we send out (especially <em>signaling-related beliefs</em>) and our overall style.</li>\n</ul>\n<p>The Less Wrong canon therefore pushes people who read it to concentrate on mostly the wrong kind of thought processes. The \"planning model\" of winning is useful for thinking about what people call <a href=\"http://en.wikipedia.org/wiki/Analytical_skill\">analytical skill</a>, which is in turn useful for solitary challenges that involve a detailed mechanistic environment that you can manipulate. Games like Alpha Centauri and Civilization come to mind, as do computer programming, mathematics, science and some business problems.</p>\n<p>Most of the goals that most people hold in life cannot be solved by this kind of analytic planning alone, but the ones that can (such as how to code, do math or physics) are heavily overrepresented on LW. The causality probably runs both ways: people whose main skills are analytic are attracted to LW because the existing discussion on LW is very focused on \"nerdy\" topics, and the kinds of posts that get written tend to focus on problems that fall into the planning model because that's what the posters like thinking about.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>1: simple calculation and logic is not usually mentioned on LW, probably because most people here are sufficiently well educated that these skills are almost completely automatic for them. In effect, it is a solved problem for the LW community. But out in the wider world, the <a href=\"/lw/1e/raising_the_sanity_waterline/\">sanity waterline</a> is much lower. Most people cannot avoid simple logical errors such as affirming the consequent, and cannot solve simple <a href=\"http://en.wikipedia.org/wiki/Fermi_problem\">Fermi Problems</a>.</p>\n<p>2: I am not trying to cast judgment on the goal of being an intellectually focused, not-conventionally-socializing person: if that is what a person wants, then from their <a href=\"http://en.wikipedia.org/wiki/Axiological\">axiological</a> point of view it is the best thing in the world.</p>\n<p>3: Not paying any attention to futurist topics like cryonics or AI which matter a lot, making dumb decisions about how to allocate charity money, making relatively dumb decisions in matters of how to efficiently allocate resources to make the distribution of human experiences better overall.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4RthYwrowiswpksGT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 27, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "2636", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 274, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2xkMt5XQqpG5fZjxb", "n5ucT5ZbPdhfGNLtP", "ru536oPGPJsEkA3Ee", "XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-27T18:30:53.857Z", "modifiedAt": null, "url": null, "title": "MathOverflow as an example for LessWrong", "slug": "mathoverflow-as-an-example-for-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:20.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dcZEgHPcmSuaY3kKg/mathoverflow-as-an-example-for-lesswrong", "pageUrlRelative": "/posts/dcZEgHPcmSuaY3kKg/mathoverflow-as-an-example-for-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/dcZEgHPcmSuaY3kKg/mathoverflow-as-an-example-for-lesswrong", "postedAtFormatted": "Tuesday, April 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MathOverflow%20as%20an%20example%20for%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMathOverflow%20as%20an%20example%20for%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdcZEgHPcmSuaY3kKg%2Fmathoverflow-as-an-example-for-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MathOverflow%20as%20an%20example%20for%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdcZEgHPcmSuaY3kKg%2Fmathoverflow-as-an-example-for-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdcZEgHPcmSuaY3kKg%2Fmathoverflow-as-an-example-for-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 546, "htmlBody": "<!-- MathOverflow versus LessWrong -->\n<p><em>\"How can LessWrong maintain high post quality while obtaining new posters?&nbsp;  How can we encourage everyone to read everything, but not everyone to post everything?&nbsp;  How can we be less intimidating to newcomers?\"</em></p>\n<p>A lot of Meta conversation goes on here, and the longer it goes on without having a great example to learn from, the longer our discussion will be more aimless and less informed than it could be.&nbsp;  Consider speculating whether blue mould from bread could treat supporating eye infections <em>before</em> you knew it also treated supporating flesh wounds...&nbsp; it would seem pretty random, and the discussion would be fairly aimless.&nbsp;</p>\n<p>But LessWrong.com is the first successful community of its kind!  There <em>is no example to learn from, right?</em></p>\n<p>With the latter, I wouldn't agree:  <a href=\"http://mathoverflow.net\">http://mathoverflow.net</a></p>\n<p>[What I've already said in comments:&nbsp; MathOverflow is a Q&amp;A forum for research-level mathematicians, aimed at each other, created by a math grad student and a post-doc in September 2009.&nbsp;  As hoped, it expanded <em>very</em> quickly, involving many famous mathematicians around the world.&nbsp;  You can even see Fields Medalist &mdash; the math equivalent of Nobel Laurate &mdash; Terrence Tao is a regular contributor  <a href=\"http://mathoverflow.net/users\">(bottom right)</a>.]</p>\n<p>MathOverflow awards Karma for good questions and good answers, it's moderated, it's open to new users, and maintains a high standard so professionals stay interested and involved.&nbsp;  Sound familliar?&nbsp;   Well, what about these features...</p>\n<p><strong>The top of every page</strong> links to:</p>\n<ul>\n<li><a href=\"http://mathoverflow.net/faq\">Frequently Asked Questions</a> </li>\n<li><a href=\"http://mathoverflow.net/howtoask\">How to write a good MathOverflow question</a> </li>\n<li><a href=\"http://meta.mathoverflow.net/\">meta.mathoverflow.net</a>, a separate forum for questions/suggestions about site policy. </li>\n</ul>\n<p>Have a look at those links.&nbsp;  If your first reaction is \"Sure, precise guidelines worked for a <em>professional mathematics Q&amp;A site...</em>\", consider this:&nbsp; they didn't <em>start out</em> as a professional mathematics Q&amp;A site.&nbsp;  They started out <em>wanting to be one</em>.&nbsp;  They had to defend against wave after wave of undergraduate calculus students posting for homework help.&nbsp;  They had to defy the natural propensity of the community to become an <em>open discussion forum</em> for mathematicians.&nbsp;  I watched as these problems arose, were dealt with, and subsided.&nbsp;  For example:&nbsp;   <a id=\"more\"></a></p>\n<ul>\n<li>I can attest that the MathOverflow guidelines made me both more careful and more confident when I started posting.&nbsp; I knew what not to do, and I didn't do it.&nbsp; </li>\n<li>When someone posts a non-research-level question, instead of \"that's a dumb question\", they respond with links to <a href=\"http://mathforum.org/dr/math/\">Ask Dr.&nbsp; Math</a> and <a href=\"http://www.artofproblemsolving.com/Forum/index.php\">The Art of Problem Solving</a> where such questions are appropriate.&nbsp;  Everybody wins.&nbsp;\n<p><strong>Do we have somewhere we send people looking to ponder philosophy less analytically than we?</strong></p>\n</li>\n</ul>\n<p>There's no coincidence.&nbsp; Those guys didn't just <em>win</em>...&nbsp; they won <em>on</em> purpose, <em>with</em> a purpose.&nbsp;</p>\n<p>And LessWrong has a brilliant purpose:&nbsp; \"Refining the art of human rationality\".&nbsp;  Our task of maintaining high post standards while expanding both the posting and reading community has recently <a href=\"/lw/265/proposed_new_features_for_less_wrong/\">been</a> <a href=\"/lw/265/proposed_new_features_for_less_wrong/1xft\">discussed</a> <a href=\"/lw/265/proposed_new_features_for_less_wrong/1xd4\">at</a> <a href=\"/lw/265/proposed_new_features_for_less_wrong/1xcu\">length</a>.</p>\n<p>Now, I know none of us wants to sit around armchair-mind-projecting to figure out what LessWrong should do.&nbsp;  I understand that people here <em>really are thinking responsibly</em> and extrapolating from their own experiences about meta issues.&nbsp;  I just want to give all you folks an intuition boost, so we can handle this massive social computation together:</p>\n<p><strong>What should we try that MathOverflow has already done?&nbsp;  What should we try differently?</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dcZEgHPcmSuaY3kKg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 48, "extendedScore": null, "score": 0.000106, "legacy": true, "legacyId": "2822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<!-- MathOverflow versus LessWrong -->\n<p><em>\"How can LessWrong maintain high post quality while obtaining new posters?&nbsp;  How can we encourage everyone to read everything, but not everyone to post everything?&nbsp;  How can we be less intimidating to newcomers?\"</em></p>\n<p>A lot of Meta conversation goes on here, and the longer it goes on without having a great example to learn from, the longer our discussion will be more aimless and less informed than it could be.&nbsp;  Consider speculating whether blue mould from bread could treat supporating eye infections <em>before</em> you knew it also treated supporating flesh wounds...&nbsp; it would seem pretty random, and the discussion would be fairly aimless.&nbsp;</p>\n<p>But LessWrong.com is the first successful community of its kind!  There <em>is no example to learn from, right?</em></p>\n<p>With the latter, I wouldn't agree:  <a href=\"http://mathoverflow.net\">http://mathoverflow.net</a></p>\n<p>[What I've already said in comments:&nbsp; MathOverflow is a Q&amp;A forum for research-level mathematicians, aimed at each other, created by a math grad student and a post-doc in September 2009.&nbsp;  As hoped, it expanded <em>very</em> quickly, involving many famous mathematicians around the world.&nbsp;  You can even see Fields Medalist \u2014 the math equivalent of Nobel Laurate \u2014 Terrence Tao is a regular contributor  <a href=\"http://mathoverflow.net/users\">(bottom right)</a>.]</p>\n<p>MathOverflow awards Karma for good questions and good answers, it's moderated, it's open to new users, and maintains a high standard so professionals stay interested and involved.&nbsp;  Sound familliar?&nbsp;   Well, what about these features...</p>\n<p><strong>The top of every page</strong> links to:</p>\n<ul>\n<li><a href=\"http://mathoverflow.net/faq\">Frequently Asked Questions</a> </li>\n<li><a href=\"http://mathoverflow.net/howtoask\">How to write a good MathOverflow question</a> </li>\n<li><a href=\"http://meta.mathoverflow.net/\">meta.mathoverflow.net</a>, a separate forum for questions/suggestions about site policy. </li>\n</ul>\n<p>Have a look at those links.&nbsp;  If your first reaction is \"Sure, precise guidelines worked for a <em>professional mathematics Q&amp;A site...</em>\", consider this:&nbsp; they didn't <em>start out</em> as a professional mathematics Q&amp;A site.&nbsp;  They started out <em>wanting to be one</em>.&nbsp;  They had to defend against wave after wave of undergraduate calculus students posting for homework help.&nbsp;  They had to defy the natural propensity of the community to become an <em>open discussion forum</em> for mathematicians.&nbsp;  I watched as these problems arose, were dealt with, and subsided.&nbsp;  For example:&nbsp;   <a id=\"more\"></a></p>\n<ul>\n<li>I can attest that the MathOverflow guidelines made me both more careful and more confident when I started posting.&nbsp; I knew what not to do, and I didn't do it.&nbsp; </li>\n<li>When someone posts a non-research-level question, instead of \"that's a dumb question\", they respond with links to <a href=\"http://mathforum.org/dr/math/\">Ask Dr.&nbsp; Math</a> and <a href=\"http://www.artofproblemsolving.com/Forum/index.php\">The Art of Problem Solving</a> where such questions are appropriate.&nbsp;  Everybody wins.&nbsp;\n<p><strong id=\"Do_we_have_somewhere_we_send_people_looking_to_ponder_philosophy_less_analytically_than_we_\">Do we have somewhere we send people looking to ponder philosophy less analytically than we?</strong></p>\n</li>\n</ul>\n<p>There's no coincidence.&nbsp; Those guys didn't just <em>win</em>...&nbsp; they won <em>on</em> purpose, <em>with</em> a purpose.&nbsp;</p>\n<p>And LessWrong has a brilliant purpose:&nbsp; \"Refining the art of human rationality\".&nbsp;  Our task of maintaining high post standards while expanding both the posting and reading community has recently <a href=\"/lw/265/proposed_new_features_for_less_wrong/\">been</a> <a href=\"/lw/265/proposed_new_features_for_less_wrong/1xft\">discussed</a> <a href=\"/lw/265/proposed_new_features_for_less_wrong/1xd4\">at</a> <a href=\"/lw/265/proposed_new_features_for_less_wrong/1xcu\">length</a>.</p>\n<p>Now, I know none of us wants to sit around armchair-mind-projecting to figure out what LessWrong should do.&nbsp;  I understand that people here <em>really are thinking responsibly</em> and extrapolating from their own experiences about meta issues.&nbsp;  I just want to give all you folks an intuition boost, so we can handle this massive social computation together:</p>\n<p><strong id=\"What_should_we_try_that_MathOverflow_has_already_done____What_should_we_try_differently_\">What should we try that MathOverflow has already done?&nbsp;  What should we try differently?</strong></p>", "sections": [{"title": "Do we have somewhere we send people looking to ponder philosophy less analytically than we?", "anchor": "Do_we_have_somewhere_we_send_people_looking_to_ponder_philosophy_less_analytically_than_we_", "level": 1}, {"title": "What should we try that MathOverflow has already done?\u00a0  What should we try differently?", "anchor": "What_should_we_try_that_MathOverflow_has_already_done____What_should_we_try_differently_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "67 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pa7PncRvcDcjw3vSR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-27T22:21:56.251Z", "modifiedAt": null, "url": null, "title": "Possibilities for converting useless fun into utility in Online Gaming", "slug": "possibilities-for-converting-useless-fun-into-utility-in", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:01.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Aleksei_Riikonen", "createdAt": "2009-04-09T06:01:53.658Z", "isAdmin": false, "displayName": "Aleksei_Riikonen"}, "userId": "Bvv3HsQMdyLAiDRns", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MX5epiZYJggkthcK6/possibilities-for-converting-useless-fun-into-utility-in", "pageUrlRelative": "/posts/MX5epiZYJggkthcK6/possibilities-for-converting-useless-fun-into-utility-in", "linkUrl": "https://www.lesswrong.com/posts/MX5epiZYJggkthcK6/possibilities-for-converting-useless-fun-into-utility-in", "postedAtFormatted": "Tuesday, April 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Possibilities%20for%20converting%20useless%20fun%20into%20utility%20in%20Online%20Gaming&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APossibilities%20for%20converting%20useless%20fun%20into%20utility%20in%20Online%20Gaming%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMX5epiZYJggkthcK6%2Fpossibilities-for-converting-useless-fun-into-utility-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Possibilities%20for%20converting%20useless%20fun%20into%20utility%20in%20Online%20Gaming%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMX5epiZYJggkthcK6%2Fpossibilities-for-converting-useless-fun-into-utility-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMX5epiZYJggkthcK6%2Fpossibilities-for-converting-useless-fun-into-utility-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 637, "htmlBody": "<p>Online gaming in immersive MMOs such as World of Warcraft or EVE Online is a common way of having fun. As technology progresses, MMO gaming will likely become ever more popular, until MMOs are fully immersive virtual realities, leading many to consider them as the primary venue of their lives, instead of \"the old(/real) world\" (without such thinking being pathological anymore).<br /><br />Currently, however, many people such as myself mostly find MMO gaming a threat to their productivity. MMOs can be very fun, druglike even, without providing any utility to valued real-world pursuits such as reducing existential risk and having money to buy food.<br /><br />The default recommendation regarding MMOs for most rationalists should probably be \"stay away from them -- or at least don't get into active gaming\". This is also my current attitude.<br /><br />Despite this, it may actually be worth considering whether some utility could be extracted from MMO gaming, specifically from the point of view of SIAI supporters such as myself. (From here on, I'll use the term \"SIAIfolk\" to refer to people interested in furthering SIAI's and allied organizations' mission.)<br /><br />It seems that the amount of SIAIfolk is undergoing strong growth, and that this may continue. At some point, which we may currently have passed or not, there may therefore (despite all recommendations) be a substantial number of SIAIfolk engaging in somewhat active MMO gaming.<br /><br />In such a circumstance, it may be beneficial to form a \"Singularitarian Gaming Group\", which along with functioning as a gaming clan in the various MMOs participated in, would include an internal reward and ranking system that would motivate people *away* from spending too much time on gaming, and encourage more productive activities. Some amount of MMO gaming would be done, with the company of other SIAIfolk making it more fun, but incentives and social support would be in place to keep gaming down to a rational level.<br /><br />It would be critical to build the incentive system well. A poorly built system would lead SIAIfolk to spend more resources on gaming and less effort on productive stuff than would have happened if \"Singularitarian Gaming Group\" didn't exist in the first place. I however believe that \"SGG\" can be set up in a meaningful way, at least if we already have SIAIfolk who are spending more time on MMO gaming than they find optimal.<br /><br />With this article, my main intention is to gauge whether such SIAIfolk already exist. If so, make a comment or email me. Let's then set up a mailing list for discussion of what kind of a \"SGG\" could be useful. (Opinions on what service to use to set up the mailing list are also welcome.)<br /><br />In addition to what's mentioned above, a Singularitarian Gaming Group might also provide utility by serving as an outreach tool towards the MMO gaming community. It could market Singularitarian activism and existential risk reduction as working towards the ultimate gaming world.<br /><br />It is also worth considering whether we should actually have a Less Wrong Gaming Group rather than SGG. And since I'm posting this here, an intention of course is to invite commentary on the rationality of all of the above thinking.<br /><br />(My intention is not to discuss the specifics of an incentive system too much here on Less Wrong, though I'll mention one non-obvious feature which is teaching MMO addicts to play profitable online poker and giving points for progress and achievements in that. I'm currently spending a lot of time on poker, and the thought of a Less Wrong Poker Group as a separate thing from this \"SGG\" has crossed my mind, but that's a topic for some other time.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MX5epiZYJggkthcK6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 1, "extendedScore": null, "score": 5.816906103687708e-07, "legacy": true, "legacyId": "2821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-28T01:24:28.927Z", "modifiedAt": null, "url": null, "title": "Jinnetic Engineering, by Richard Stallman", "slug": "jinnetic-engineering-by-richard-stallman", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:02.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TNdfaoNq2nsXZckuz/jinnetic-engineering-by-richard-stallman", "pageUrlRelative": "/posts/TNdfaoNq2nsXZckuz/jinnetic-engineering-by-richard-stallman", "linkUrl": "https://www.lesswrong.com/posts/TNdfaoNq2nsXZckuz/jinnetic-engineering-by-richard-stallman", "postedAtFormatted": "Wednesday, April 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Jinnetic%20Engineering%2C%20by%20Richard%20Stallman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJinnetic%20Engineering%2C%20by%20Richard%20Stallman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTNdfaoNq2nsXZckuz%2Fjinnetic-engineering-by-richard-stallman%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Jinnetic%20Engineering%2C%20by%20Richard%20Stallman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTNdfaoNq2nsXZckuz%2Fjinnetic-engineering-by-richard-stallman", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTNdfaoNq2nsXZckuz%2Fjinnetic-engineering-by-richard-stallman", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7, "htmlBody": "<p>Thought the community might enjoy this:</p>\n<p><a href=\"http://stallman.org/articles/jinnetic.html\">Jinnetic Engineering</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TNdfaoNq2nsXZckuz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 2, "extendedScore": null, "score": 5.817273797754703e-07, "legacy": true, "legacyId": "2823", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-28T22:17:12.713Z", "modifiedAt": null, "url": null, "title": "What are our domains of expertise? A marketplace of insights and issues", "slug": "what-are-our-domains-of-expertise-a-marketplace-of-insights", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:06.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mJB9LBDkKuqxqDhkM/what-are-our-domains-of-expertise-a-marketplace-of-insights", "pageUrlRelative": "/posts/mJB9LBDkKuqxqDhkM/what-are-our-domains-of-expertise-a-marketplace-of-insights", "linkUrl": "https://www.lesswrong.com/posts/mJB9LBDkKuqxqDhkM/what-are-our-domains-of-expertise-a-marketplace-of-insights", "postedAtFormatted": "Wednesday, April 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20our%20domains%20of%20expertise%3F%20A%20marketplace%20of%20insights%20and%20issues&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20our%20domains%20of%20expertise%3F%20A%20marketplace%20of%20insights%20and%20issues%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJB9LBDkKuqxqDhkM%2Fwhat-are-our-domains-of-expertise-a-marketplace-of-insights%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20our%20domains%20of%20expertise%3F%20A%20marketplace%20of%20insights%20and%20issues%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJB9LBDkKuqxqDhkM%2Fwhat-are-our-domains-of-expertise-a-marketplace-of-insights", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJB9LBDkKuqxqDhkM%2Fwhat-are-our-domains-of-expertise-a-marketplace-of-insights", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 522, "htmlBody": "<p>We have recently <a href=\"/lw/1np/attention_lurkers_please_say_hi/\">obtained evidence</a> that a number of people, some with quite interesting backgrounds and areas of expertise, find LessWrong an interesting read but find limited opportunities to contribute.</p>\n<p>This post is an invitation to engage, in relative safety but just a little beyond saying \"Hi, I'm a lurker\". Even that little is appreciated, to be sure, and it's OK for anyone who feels the slightest bit intimidated to remain on the sidelines. However, I'm confident that most readers will find it quite easy to answer at least the first of the following questions:</p>\n<ul>\n<li>What is your main domain of expertise? (Your profession, your area of study, or even a hobby!)</li>\n</ul>\n<p>...and possibly these follow-ups:</p>\n<ul>\n<li>What issues in your domain call most critically for sharp thinking?</li>\n<li>What do you know that could be of interest to the LessWrong community?</li>\n<li>What might you learn from experts in other domains that could be useful in yours?</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<p>Comments like the following, from the \"<a href=\"/lw/1np/attention_lurkers_please_say_hi/\">Attention Lurkers</a>\" thread, suggest untapped resources:</p>\n<blockquote>\n<p>I'm a Maternal-Fetal Medicine specialist. [...] I lurk because I feel that I'm too philosophically fuzzy for some of the discussions here. I do learn a great deal. Anytime anyone wants to discuss prenatal diagnosis and the ethical implications, let me know.</p>\n</blockquote>\n<p>My own area of professional expertise is computer programming - perhaps one of the common sub-populations here. I'm also a parent, and have been a beneficiary of prenatal diagnosis (toxoplasmosis: turned out not to be a big deal, but it might have been). My <a href=\"http://yudkowsky.net/rational/virtues\">curiosity</a> is often engaged by what goes on \"behind the scenes\" of the professions I interact with as an ordinary citizen.</p>\n<p>Yes, I would be quite interested in striking up a conversation about applying the tools discussed here to prenatal diagnosis; or in a conversation about which conceptual tools <em>that I don't know about yet</em> turn out to be useful in dealing with the epistemic or ethical issues in prenatal diagnosis.</p>\n<p>Metaphorically, the intent of this post is to provide a marketplace. We already have the \"<a href=\"/lw/7c/where_are_we/\">Where are we?</a>\" thread, which makes it easier for LessWrongers close to each other to meet up if they want to. (\"<a href=\"/lw/b9/welcome_to_less_wrong/\">Welcome to LessWrong</a>\" is the place to collect biographical information, but it specifically emphasizes the \"rationalist\" side of people, rather than their professional knowledge.)</p>\n<p>In a similar spirit, <strong>please post a comment here offering (or requesting) domain-specific insights</strong>. My hunch is, we'll find that even those of us in professions that don't seem related to the topics covered here have more to contribute than they think; my hope is that this comment thread will be a valuable resource in the future.</p>\n<p>A secondary intent of this post is to provide newcomers and lurkers with one more place where contributing can be expected to be safe from karma penalties - simply answer one of the questions that probably comes up most often when meeting strangers: <strong>\"What do you do?\"</strong>. :)</p>\n<p>(P.S. If you've read this far and are disappointed with the absence of any jokes about \"yet another <a href=\"/lw/24c/the_fundamental_question/\">fundamental question</a>\", thank you for your attention, and please accept this <a href=\"http://en.wikipedia.org/wiki/Apophasis\">apophasi</a><a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/CouldSayItBut\">s</a> as a consolation gift.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mJB9LBDkKuqxqDhkM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 29, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "2829", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yiMa5pCo6i2uN4btu", "gRkFrDmuE82difQRH", "CG9AEXwSjdrXPBEZ9", "xWozAiMgx6fBZwcjo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-04-29T02:51:31.138Z", "modifiedAt": null, "url": null, "title": "Averaging value systems is worse than choosing one", "slug": "averaging-value-systems-is-worse-than-choosing-one", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:02.692Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BNtjLbPSidEbihPmv/averaging-value-systems-is-worse-than-choosing-one", "pageUrlRelative": "/posts/BNtjLbPSidEbihPmv/averaging-value-systems-is-worse-than-choosing-one", "linkUrl": "https://www.lesswrong.com/posts/BNtjLbPSidEbihPmv/averaging-value-systems-is-worse-than-choosing-one", "postedAtFormatted": "Thursday, April 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Averaging%20value%20systems%20is%20worse%20than%20choosing%20one&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAveraging%20value%20systems%20is%20worse%20than%20choosing%20one%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBNtjLbPSidEbihPmv%2Faveraging-value-systems-is-worse-than-choosing-one%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Averaging%20value%20systems%20is%20worse%20than%20choosing%20one%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBNtjLbPSidEbihPmv%2Faveraging-value-systems-is-worse-than-choosing-one", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBNtjLbPSidEbihPmv%2Faveraging-value-systems-is-worse-than-choosing-one", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2375, "htmlBody": "<p>A continuation of <a href=\"/lw/256/only_humans_can_have_human_values/\">Only humans can have human values</a>.&nbsp; Revised late in the evening on April 30.</p>\n<p>Summary: I will present a model of value systems, and show that under it, the \"averaged value system\" found by averaging the values of all the agents,</p>\n<ul>\n<li>(RESTATED:) has more internal inconsistency than you would on average get by picking one agent's values at random</li>\n<li>is a less stable value system than you would get by picking one agent's values at random</li>\n</ul>\n<p>ADDED: The reason for doing this is that numerous people have suggested implementing CEV by averaging different value systems together.&nbsp; My intuition is that value systems are not random; they are optimized in some way.&nbsp; This optimization is undone if you mix together different value systems simply by averaging them.&nbsp; I demonstrate this in the case where we suppose they are optimized to minimize internal conflict.</p>\n<p>To someone working with the assumptions needed for CEV, the second bullet point is probably more important.&nbsp; Stability is central to CEV, while internal inconsistency may be a mere computational inconvenience.</p>\n<ul>\n</ul>\n<ul>\n</ul>\n<h2><a id=\"more\"></a></h2>\n<h2>ADDED: Inconsistencies in value systems<br /></h2>\n<p>We find consistent correlations in value systems.&nbsp; The US has two political parties, Republican and Democrat; and many people who find one or the other obviously, intuitively correct.&nbsp; Most countries have a conservative/liberal dimension that many values line up along.&nbsp; It's hard to know whether this is because people try to make their values consistent; or because game theory tends to produce two parties, or even because parties form along the first principle component of the scatterplot of the values of members of society, so that some essentially artifactual vector is guaranteed to be found to be the main dimension along which opinions vary.&nbsp; However, it's at least suggestive.&nbsp; You seldom find a country where the conservatives favor peace and the liberals favor war; or where the liberals value religious rules more than the conservatives.&nbsp; I seldom find vegetarians who are against welfare, or loggers or oilmen who are animal-rights activists.</p>\n<p>If it's a general principle that some process causes people to form value systems with less inconsistencies than they would have by gathering different pieces from different value systems at random, it's not a great leap of faith to say that value systems with less inconsistencies are better in some way than ones with more inconsistencies.&nbsp; We can at the very least say that a cobbled-together value system lacks this property of naturally-occurring human value systems; and therefore is not itself a good example of a human value system.</p>\n<p>You might study the space of possible environments in which an agent must act, and ask where in that space values are in conflict, and what the shape of the decision boundary surfaces between actions are in that space.&nbsp; My intuition is that value systems with many internal conflicts have complex boundary surfaces in that space.</p>\n<p>More complex decision boundaries enable an agent to have a decision function that makes finer discriminations, and therefore can make more use of the information in the environment.&nbsp; However, overly-complex decision boundaries may be adding noise.</p>\n<p>If you take the value systems held by a set of agents \"in the wild\", we can suppose their decision boundary surfaces are adapted to their environment and to their capabilities, so that they are doing a good job of balancing the complexity of the agent's decision surface vs. their computational power and the complexity of the life they face.</p>\n<p>If you construct a value system from those value systems, in a way that does not use the combined information used to construct all of them, and you end up with a more-complex decision surface constructed from the same amount of underlying information as a typical \"wild-type\" value system, you could conclude that this decision surface is overly-complex, and the extra complexities are noise/overfitting.</p>\n<p>I have other reasons I think that the degree of inconsistency within a value system could be a metric used to evaluate it.&nbsp; The comments below explore some different aspects of this.&nbsp; The topic needs at least a post of its own.&nbsp; The idea that higher internal consistency is always better is too simple. However, if we have a population of wild-type value systems that we think are adapted by some self-organizing process, then if we combine them in a way that produces an artificial value system that is consistently biased in the same direction - either lower or higher internal consistency than wild-type - I think that is cause for concern.</p>\n<p>(I don't know if there are any results showing that an associative network with a higher IC, as defined below, has a more complex decision surface.&nbsp; I would expect this to be the case.&nbsp; A Hopfield network with no internal conflict would have a plane for its decision surface, and be able to store only 2 patterns.)</p>\n<h2>A model of value systems<br /></h2>\n<p>Model any value system as a fully-connected network, where the nodes are values, and the connection from one value to another gives the correlation (from -1 to 1) between the recommendations for behavior given by the two values.&nbsp; Each node is assigned a real number from 0 to 1 indicating how strongly the agent holds the value associated with that node.&nbsp; Connection weights are fixed by the environment; node values vary according to the value system.</p>\n<p>The internal conflict (IC) in a value system is the negative of the sum, over all pairs of nodes, of the product of the node values and the connection weight between them.&nbsp; This is an energy measure that we want to minimize.&nbsp; Averaging value systems together is a reasonable thing to do, for an expected-utility-maximizer, <em>only</em> if the average of a set of value systems is expected to give a lower IC than the average IC of all of the value systems.&nbsp; (Utility = - (internal conflict).)</p>\n<h2>IC(averaged values) &gt; average(IC) if agents are better than random<br /></h2>\n<p>Let there be <em>N</em> nodes.&nbsp; Let <em>a</em> be an agent from the set <em>A</em> of all agents.&nbsp; Let <em>v<sub>ai</sub></em> be the value agent <em>a</em> places on node <em>i</em>.&nbsp; Let <em>w<sub>ij</sub></em> be the weight between nodes <em>i</em> and <em>j</em>.&nbsp; Let the \"averaged agent\" <em>b</em> mean a constructed agent <em>b</em> (not in A) for which v<sub>bi</sub> = average over all a of v<sub>ai</sub>.&nbsp; Write \"the sum over all i and j of S\" as sum_{i, j}(S).</p>\n<p style=\"padding-left: 30px;\">Average IC = ICa = - sum_{i, j} [w<sub>ij</sub> x sum_a (v<sub>ai</sub> x v<sub>aj</sub>)] / |A|</p>\n<p style=\"padding-left: 30px;\">Expected IC from average agent <em>b</em> = ICb = - sum_{i, j} [w<sub>ij</sub> x (sum<em>_</em>a(v<sub>ai</sub>) / |A|) x (sum_a(v<sub>aj</sub>) / |A|)]</p>\n<p>Now I will introduce the concept of a \"random agent\", which is an agent <em>r</em> constructed by choosing some other agent <em>a</em> at random for every node <em>i, </em>and setting v<sub>ri</sub> = v<sub>ai</sub>.&nbsp; Hopefully you will agree that a random agent will have, on average, a higher IC than one of our original agents, because existing agents are at least a little bit optimized, by evolution or by introspection.</p>\n<p>(You could argue that values are things that an agent never, by definition, willingly changes, or is even capable of changing.&nbsp; Rather than get into a tricky philosophical argument, I will point out that, if that is so, then values have little to do with what we call \"values\" in English; and what follows applies more certainly to something more like the latter, and to what we think of when people say \"values\".&nbsp; But if you also claim that evolution does not reduce value conflicts, you must have a simple, statically-coded priority-value model of cognition, eg Brooks' <a href=\"http://en.wikipedia.org/wiki/Subsumption_architecture\">subsumption architectur</a>e; and you must also believe that the landscape of optimal action as a function of environment is everywhere discontinuous, or else you would expect agents in which a slight change in stimuli results in a different value achieving dominance to suffer a penalty for taking uncorrelated actions in situations that differ only slightly.)</p>\n<p>We find the average IC of a random agent, which we agreed (I hope) is higher than the average IC of a real agent, by averaging the contribution from pair of nodes {i, j} over all possible choices of agents used to set <em>v<sub>ri</sub> </em>and <em>v<sub>rj</sub></em>.&nbsp; The average IC of a random agent is then</p>\n<p style=\"padding-left: 30px;\">ICr = Average IC of a random agent = - sum_{i, j} [w<sub>ij</sub> x sum_a (v<sub>ai</sub> x sum_a(v<sub>aj)</sub>))] / (|A| x |A|)</p>\n<p>We see that ICr = ICb.&nbsp; In other words, using this model, constructing a value system by averaging together other value systems gives you the same result that you would get, on average, by picking one agent's value for one node, and another agent's value for another node, and so on, at random.&nbsp; If we assume that the value system held by any real agent is, on average, better than such a randomly-thrown-together value system, this means that picking the value system of any real agent will give a lower expected IC than picking the value system of the averaged agent.</p>\n<p>I didn't design this model to get that result; I designed just one model, which seemed reasonable to me, and found the proof afterward.</p>\n<h2>Value systems are stable; an averaged value system is not<br /></h2>\n<p>Suppose that agents have already evolved to have value systems that are consistent; and that agents often actively work to reduce conflicts in their value systems, by changing values that their other values disagree with.&nbsp; (But see comments below on deep values vs. surface values.&nbsp; A separate post justifying this supposition, and discussing whether humans have top-level goals, is needed.)&nbsp; If changing one or two node values would reduce the IC, either evolution or the agent would probably have already done so.&nbsp; This means we expect that each existing value system is already a local optimum in the space of possible node values.</p>\n<p>If a value system is not at a local optimum, it's unstable.&nbsp; If you give that value system to an agent, or a society, it's likely to change to something else - possibly something far from its original setting.&nbsp; (Also, the fact that a value system is not a local optimum is a strong indicator that it has higher-than-typical IC, because the average IC of systems that are a little ways <em>d</em> away from a local minimum is greater than the average IC of systems at a local minimum, by an amount proportional to <em>d</em>.)</p>\n<p>Averaging value systems together is therefore a reasonable thing to do <em>only</em> if the average of a set of value systems that are all local minima is guaranteed to give a value system that is also a local minimum.</p>\n<p>This is not the case.&nbsp; Consider value systems of 3 nodes, A, B, and C, with the weights AB=1, BC=1, AC=-1.&nbsp; Here are two locally-optimal value systems.&nbsp; Terms in conflict measures are written as node x connection x node:</p>\n<p style=\"padding-left: 30px;\">A = 0, B = 1, C = 1: Conflict = -(0 x 1 x 1 + 1 x 1 x 1 + 1 x -1 x 0) = -1</p>\n<p style=\"padding-left: 30px;\">A = 1, B = 1, C = 0: Conflict = -(1 x 1 x 1 + 1 x 1 x 0 + 0 x -1 x 1) = -1</p>\n<p>The average of these two systems is</p>\n<p style=\"padding-left: 30px;\">A = 1/2, B = 1, C = 1/2: Conflict = -(.5 x 1 x 1 + 1 x 1 x .5 + .5 x -1 x .5) = -.75</p>\n<p>We can improve on this by setting A = 1:</p>\n<p style=\"padding-left: 30px;\">A = 1, B = 1, C = 1/2: Conflict = -(1 x 1 x 1 + 1 x 1 x .5 + .5 x -1 x 1) = -1 &lt; -.75</p>\n<p>It would only be by random chance that the average of value systems would be locally optimal.&nbsp; Averaging together existing values is thus practically guaranteed to give an unstable value system.</p>\n<p>Let me point out again that I defined my model first, and the first example of two locally-optimal value systems that I tried out, worked.</p>\n<h2>You can escape these proofs by not being rational<br /></h2>\n<p>If we suppose that higher-than-wild-type IC is bad, under what circumstance is it still justified to choose the averaged agent rather than one of the original agents?&nbsp; It would be justified if you give an extremely high penalty for choosing a system with high IC, and do not give a correspondingly high reward for choosing a system with a wild-type IC.&nbsp; An example would be if you chose a value system so as to minimize the chance of having an IC greater than that given by averaging all value systems together.&nbsp; (In this context, I would regard that particular goal as cheating, as it is constructed to give the averaged value system a perfect score.&nbsp; It suffers <a href=\"http://en.wikipedia.org/wiki/Zero-risk_bias\">zero-risk bias</a>.)</p>\n<p>Such risk-avoidant goals would, I think, be more likely to be achieved by averaging (although I haven't done the math).&nbsp; But they do not maximize expected utility.&nbsp; They suffer risk-avoidance bias, by construction.</p>\n<h2>... or by doing very thorough factor analysis</h2>\n<p>If, as I mentioned in <a href=\"/lw/256/only_humans_can_have_human_values/\">Only humans can have human values</a>, you can perform factor analysis and identify truly independent, uncorrelated latent \"values\", then the above arguments do not apply.&nbsp; You must take into account multiple hypothesis testing; using mathematics that guaranteed finding such a result would not impress me.&nbsp; If, for instance, you were to simply perform <a href=\"http://en.wikipedia.org/wiki/Principal_component_analysis\">PCA</a> and say that the resulting eigenvectors are your true latent values, I would respond that the first dozen eigenvectors might be meaningful, but the next thousand are overfitted to the data.&nbsp; You might achieve a great simplification of the problem, and greatly reduce the difference between ICa and ICb; but would still have ICa &lt; ICb.</p>\n<h2>ADDED: Ensemble methods</h2>\n<p>In machine learning, \"ensemble methods\" mean methods that combine (often by averaging together) the predictions of different classifiers.&nbsp; It is a robust result that ensemble methods have better performance than any of the individual methods comprising them.&nbsp; This seems to contradict the claim that an averaged value systems would be worse than any of the individual value systems comprising it.</p>\n<p>I think there is a crucial difference, however: In ensemble methods, each of the different methods has exactly the same goals (they are trained by a process that agrees on what are good and bad decisions).&nbsp; An ensemble method is isomorphic to asking a large number of people who have the <em>same</em> value system to vote on a course of action.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BNtjLbPSidEbihPmv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 7, "extendedScore": null, "score": 5.820351273352176e-07, "legacy": true, "legacyId": "2810", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A continuation of <a href=\"/lw/256/only_humans_can_have_human_values/\">Only humans can have human values</a>.&nbsp; Revised late in the evening on April 30.</p>\n<p>Summary: I will present a model of value systems, and show that under it, the \"averaged value system\" found by averaging the values of all the agents,</p>\n<ul>\n<li>(RESTATED:) has more internal inconsistency than you would on average get by picking one agent's values at random</li>\n<li>is a less stable value system than you would get by picking one agent's values at random</li>\n</ul>\n<p>ADDED: The reason for doing this is that numerous people have suggested implementing CEV by averaging different value systems together.&nbsp; My intuition is that value systems are not random; they are optimized in some way.&nbsp; This optimization is undone if you mix together different value systems simply by averaging them.&nbsp; I demonstrate this in the case where we suppose they are optimized to minimize internal conflict.</p>\n<p>To someone working with the assumptions needed for CEV, the second bullet point is probably more important.&nbsp; Stability is central to CEV, while internal inconsistency may be a mere computational inconvenience.</p>\n<ul>\n</ul>\n<ul>\n</ul>\n<h2><a id=\"more\"></a></h2>\n<h2 id=\"ADDED__Inconsistencies_in_value_systems\">ADDED: Inconsistencies in value systems<br></h2>\n<p>We find consistent correlations in value systems.&nbsp; The US has two political parties, Republican and Democrat; and many people who find one or the other obviously, intuitively correct.&nbsp; Most countries have a conservative/liberal dimension that many values line up along.&nbsp; It's hard to know whether this is because people try to make their values consistent; or because game theory tends to produce two parties, or even because parties form along the first principle component of the scatterplot of the values of members of society, so that some essentially artifactual vector is guaranteed to be found to be the main dimension along which opinions vary.&nbsp; However, it's at least suggestive.&nbsp; You seldom find a country where the conservatives favor peace and the liberals favor war; or where the liberals value religious rules more than the conservatives.&nbsp; I seldom find vegetarians who are against welfare, or loggers or oilmen who are animal-rights activists.</p>\n<p>If it's a general principle that some process causes people to form value systems with less inconsistencies than they would have by gathering different pieces from different value systems at random, it's not a great leap of faith to say that value systems with less inconsistencies are better in some way than ones with more inconsistencies.&nbsp; We can at the very least say that a cobbled-together value system lacks this property of naturally-occurring human value systems; and therefore is not itself a good example of a human value system.</p>\n<p>You might study the space of possible environments in which an agent must act, and ask where in that space values are in conflict, and what the shape of the decision boundary surfaces between actions are in that space.&nbsp; My intuition is that value systems with many internal conflicts have complex boundary surfaces in that space.</p>\n<p>More complex decision boundaries enable an agent to have a decision function that makes finer discriminations, and therefore can make more use of the information in the environment.&nbsp; However, overly-complex decision boundaries may be adding noise.</p>\n<p>If you take the value systems held by a set of agents \"in the wild\", we can suppose their decision boundary surfaces are adapted to their environment and to their capabilities, so that they are doing a good job of balancing the complexity of the agent's decision surface vs. their computational power and the complexity of the life they face.</p>\n<p>If you construct a value system from those value systems, in a way that does not use the combined information used to construct all of them, and you end up with a more-complex decision surface constructed from the same amount of underlying information as a typical \"wild-type\" value system, you could conclude that this decision surface is overly-complex, and the extra complexities are noise/overfitting.</p>\n<p>I have other reasons I think that the degree of inconsistency within a value system could be a metric used to evaluate it.&nbsp; The comments below explore some different aspects of this.&nbsp; The topic needs at least a post of its own.&nbsp; The idea that higher internal consistency is always better is too simple. However, if we have a population of wild-type value systems that we think are adapted by some self-organizing process, then if we combine them in a way that produces an artificial value system that is consistently biased in the same direction - either lower or higher internal consistency than wild-type - I think that is cause for concern.</p>\n<p>(I don't know if there are any results showing that an associative network with a higher IC, as defined below, has a more complex decision surface.&nbsp; I would expect this to be the case.&nbsp; A Hopfield network with no internal conflict would have a plane for its decision surface, and be able to store only 2 patterns.)</p>\n<h2 id=\"A_model_of_value_systems\">A model of value systems<br></h2>\n<p>Model any value system as a fully-connected network, where the nodes are values, and the connection from one value to another gives the correlation (from -1 to 1) between the recommendations for behavior given by the two values.&nbsp; Each node is assigned a real number from 0 to 1 indicating how strongly the agent holds the value associated with that node.&nbsp; Connection weights are fixed by the environment; node values vary according to the value system.</p>\n<p>The internal conflict (IC) in a value system is the negative of the sum, over all pairs of nodes, of the product of the node values and the connection weight between them.&nbsp; This is an energy measure that we want to minimize.&nbsp; Averaging value systems together is a reasonable thing to do, for an expected-utility-maximizer, <em>only</em> if the average of a set of value systems is expected to give a lower IC than the average IC of all of the value systems.&nbsp; (Utility = - (internal conflict).)</p>\n<h2 id=\"IC_averaged_values____average_IC__if_agents_are_better_than_random\">IC(averaged values) &gt; average(IC) if agents are better than random<br></h2>\n<p>Let there be <em>N</em> nodes.&nbsp; Let <em>a</em> be an agent from the set <em>A</em> of all agents.&nbsp; Let <em>v<sub>ai</sub></em> be the value agent <em>a</em> places on node <em>i</em>.&nbsp; Let <em>w<sub>ij</sub></em> be the weight between nodes <em>i</em> and <em>j</em>.&nbsp; Let the \"averaged agent\" <em>b</em> mean a constructed agent <em>b</em> (not in A) for which v<sub>bi</sub> = average over all a of v<sub>ai</sub>.&nbsp; Write \"the sum over all i and j of S\" as sum_{i, j}(S).</p>\n<p style=\"padding-left: 30px;\">Average IC = ICa = - sum_{i, j} [w<sub>ij</sub> x sum_a (v<sub>ai</sub> x v<sub>aj</sub>)] / |A|</p>\n<p style=\"padding-left: 30px;\">Expected IC from average agent <em>b</em> = ICb = - sum_{i, j} [w<sub>ij</sub> x (sum<em>_</em>a(v<sub>ai</sub>) / |A|) x (sum_a(v<sub>aj</sub>) / |A|)]</p>\n<p>Now I will introduce the concept of a \"random agent\", which is an agent <em>r</em> constructed by choosing some other agent <em>a</em> at random for every node <em>i, </em>and setting v<sub>ri</sub> = v<sub>ai</sub>.&nbsp; Hopefully you will agree that a random agent will have, on average, a higher IC than one of our original agents, because existing agents are at least a little bit optimized, by evolution or by introspection.</p>\n<p>(You could argue that values are things that an agent never, by definition, willingly changes, or is even capable of changing.&nbsp; Rather than get into a tricky philosophical argument, I will point out that, if that is so, then values have little to do with what we call \"values\" in English; and what follows applies more certainly to something more like the latter, and to what we think of when people say \"values\".&nbsp; But if you also claim that evolution does not reduce value conflicts, you must have a simple, statically-coded priority-value model of cognition, eg Brooks' <a href=\"http://en.wikipedia.org/wiki/Subsumption_architecture\">subsumption architectur</a>e; and you must also believe that the landscape of optimal action as a function of environment is everywhere discontinuous, or else you would expect agents in which a slight change in stimuli results in a different value achieving dominance to suffer a penalty for taking uncorrelated actions in situations that differ only slightly.)</p>\n<p>We find the average IC of a random agent, which we agreed (I hope) is higher than the average IC of a real agent, by averaging the contribution from pair of nodes {i, j} over all possible choices of agents used to set <em>v<sub>ri</sub> </em>and <em>v<sub>rj</sub></em>.&nbsp; The average IC of a random agent is then</p>\n<p style=\"padding-left: 30px;\">ICr = Average IC of a random agent = - sum_{i, j} [w<sub>ij</sub> x sum_a (v<sub>ai</sub> x sum_a(v<sub>aj)</sub>))] / (|A| x |A|)</p>\n<p>We see that ICr = ICb.&nbsp; In other words, using this model, constructing a value system by averaging together other value systems gives you the same result that you would get, on average, by picking one agent's value for one node, and another agent's value for another node, and so on, at random.&nbsp; If we assume that the value system held by any real agent is, on average, better than such a randomly-thrown-together value system, this means that picking the value system of any real agent will give a lower expected IC than picking the value system of the averaged agent.</p>\n<p>I didn't design this model to get that result; I designed just one model, which seemed reasonable to me, and found the proof afterward.</p>\n<h2 id=\"Value_systems_are_stable__an_averaged_value_system_is_not\">Value systems are stable; an averaged value system is not<br></h2>\n<p>Suppose that agents have already evolved to have value systems that are consistent; and that agents often actively work to reduce conflicts in their value systems, by changing values that their other values disagree with.&nbsp; (But see comments below on deep values vs. surface values.&nbsp; A separate post justifying this supposition, and discussing whether humans have top-level goals, is needed.)&nbsp; If changing one or two node values would reduce the IC, either evolution or the agent would probably have already done so.&nbsp; This means we expect that each existing value system is already a local optimum in the space of possible node values.</p>\n<p>If a value system is not at a local optimum, it's unstable.&nbsp; If you give that value system to an agent, or a society, it's likely to change to something else - possibly something far from its original setting.&nbsp; (Also, the fact that a value system is not a local optimum is a strong indicator that it has higher-than-typical IC, because the average IC of systems that are a little ways <em>d</em> away from a local minimum is greater than the average IC of systems at a local minimum, by an amount proportional to <em>d</em>.)</p>\n<p>Averaging value systems together is therefore a reasonable thing to do <em>only</em> if the average of a set of value systems that are all local minima is guaranteed to give a value system that is also a local minimum.</p>\n<p>This is not the case.&nbsp; Consider value systems of 3 nodes, A, B, and C, with the weights AB=1, BC=1, AC=-1.&nbsp; Here are two locally-optimal value systems.&nbsp; Terms in conflict measures are written as node x connection x node:</p>\n<p style=\"padding-left: 30px;\">A = 0, B = 1, C = 1: Conflict = -(0 x 1 x 1 + 1 x 1 x 1 + 1 x -1 x 0) = -1</p>\n<p style=\"padding-left: 30px;\">A = 1, B = 1, C = 0: Conflict = -(1 x 1 x 1 + 1 x 1 x 0 + 0 x -1 x 1) = -1</p>\n<p>The average of these two systems is</p>\n<p style=\"padding-left: 30px;\">A = 1/2, B = 1, C = 1/2: Conflict = -(.5 x 1 x 1 + 1 x 1 x .5 + .5 x -1 x .5) = -.75</p>\n<p>We can improve on this by setting A = 1:</p>\n<p style=\"padding-left: 30px;\">A = 1, B = 1, C = 1/2: Conflict = -(1 x 1 x 1 + 1 x 1 x .5 + .5 x -1 x 1) = -1 &lt; -.75</p>\n<p>It would only be by random chance that the average of value systems would be locally optimal.&nbsp; Averaging together existing values is thus practically guaranteed to give an unstable value system.</p>\n<p>Let me point out again that I defined my model first, and the first example of two locally-optimal value systems that I tried out, worked.</p>\n<h2 id=\"You_can_escape_these_proofs_by_not_being_rational\">You can escape these proofs by not being rational<br></h2>\n<p>If we suppose that higher-than-wild-type IC is bad, under what circumstance is it still justified to choose the averaged agent rather than one of the original agents?&nbsp; It would be justified if you give an extremely high penalty for choosing a system with high IC, and do not give a correspondingly high reward for choosing a system with a wild-type IC.&nbsp; An example would be if you chose a value system so as to minimize the chance of having an IC greater than that given by averaging all value systems together.&nbsp; (In this context, I would regard that particular goal as cheating, as it is constructed to give the averaged value system a perfect score.&nbsp; It suffers <a href=\"http://en.wikipedia.org/wiki/Zero-risk_bias\">zero-risk bias</a>.)</p>\n<p>Such risk-avoidant goals would, I think, be more likely to be achieved by averaging (although I haven't done the math).&nbsp; But they do not maximize expected utility.&nbsp; They suffer risk-avoidance bias, by construction.</p>\n<h2 id=\"____or_by_doing_very_thorough_factor_analysis\">... or by doing very thorough factor analysis</h2>\n<p>If, as I mentioned in <a href=\"/lw/256/only_humans_can_have_human_values/\">Only humans can have human values</a>, you can perform factor analysis and identify truly independent, uncorrelated latent \"values\", then the above arguments do not apply.&nbsp; You must take into account multiple hypothesis testing; using mathematics that guaranteed finding such a result would not impress me.&nbsp; If, for instance, you were to simply perform <a href=\"http://en.wikipedia.org/wiki/Principal_component_analysis\">PCA</a> and say that the resulting eigenvectors are your true latent values, I would respond that the first dozen eigenvectors might be meaningful, but the next thousand are overfitted to the data.&nbsp; You might achieve a great simplification of the problem, and greatly reduce the difference between ICa and ICb; but would still have ICa &lt; ICb.</p>\n<h2 id=\"ADDED__Ensemble_methods\">ADDED: Ensemble methods</h2>\n<p>In machine learning, \"ensemble methods\" mean methods that combine (often by averaging together) the predictions of different classifiers.&nbsp; It is a robust result that ensemble methods have better performance than any of the individual methods comprising them.&nbsp; This seems to contradict the claim that an averaged value systems would be worse than any of the individual value systems comprising it.</p>\n<p>I think there is a crucial difference, however: In ensemble methods, each of the different methods has exactly the same goals (they are trained by a process that agrees on what are good and bad decisions).&nbsp; An ensemble method is isomorphic to asking a large number of people who have the <em>same</em> value system to vote on a course of action.</p>", "sections": [{"title": "ADDED: Inconsistencies in value systems", "anchor": "ADDED__Inconsistencies_in_value_systems", "level": 1}, {"title": "A model of value systems", "anchor": "A_model_of_value_systems", "level": 1}, {"title": "IC(averaged values) > average(IC) if agents are better than random", "anchor": "IC_averaged_values____average_IC__if_agents_are_better_than_random", "level": 1}, {"title": "Value systems are stable; an averaged value system is not", "anchor": "Value_systems_are_stable__an_averaged_value_system_is_not", "level": 1}, {"title": "You can escape these proofs by not being rational", "anchor": "You_can_escape_these_proofs_by_not_being_rational", "level": 1}, {"title": "... or by doing very thorough factor analysis", "anchor": "____or_by_doing_very_thorough_factor_analysis", "level": 1}, {"title": "ADDED: Ensemble methods", "anchor": "ADDED__Ensemble_methods", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "56 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cAPCCJjggjZPxxcKh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-01T05:29:40.871Z", "modifiedAt": null, "url": null, "title": "Open Thread: May 2010", "slug": "open-thread-may-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:51.399Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack", "createdAt": "2009-02-27T15:27:14.891Z", "isAdmin": false, "displayName": "Jack"}, "userId": "GwetakMQqsGCf7ZQv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FDyMThqqX2s47e6rG/open-thread-may-2010", "pageUrlRelative": "/posts/FDyMThqqX2s47e6rG/open-thread-may-2010", "linkUrl": "https://www.lesswrong.com/posts/FDyMThqqX2s47e6rG/open-thread-may-2010", "postedAtFormatted": "Saturday, May 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20May%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20May%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDyMThqqX2s47e6rG%2Fopen-thread-may-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20May%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDyMThqqX2s47e6rG%2Fopen-thread-may-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDyMThqqX2s47e6rG%2Fopen-thread-may-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p>You know what to do.</p>\n<p><em>This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FDyMThqqX2s47e6rG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 5.826480770055737e-07, "legacy": true, "legacyId": "2841", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 557, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-01T05:48:10.694Z", "modifiedAt": null, "url": null, "title": "Rationality quotes: May 2010", "slug": "rationality-quotes-may-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:29.846Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XJtTbMZJt2WTn7kSa/rationality-quotes-may-2010", "pageUrlRelative": "/posts/XJtTbMZJt2WTn7kSa/rationality-quotes-may-2010", "linkUrl": "https://www.lesswrong.com/posts/XJtTbMZJt2WTn7kSa/rationality-quotes-may-2010", "postedAtFormatted": "Saturday, May 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20quotes%3A%20May%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20quotes%3A%20May%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJtTbMZJt2WTn7kSa%2Frationality-quotes-may-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20quotes%3A%20May%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJtTbMZJt2WTn7kSa%2Frationality-quotes-may-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJtTbMZJt2WTn7kSa%2Frationality-quotes-may-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<div>\n<p>This is our monthly thread for collecting these little gems and  pearls of wisdom, rationality-related quotes you've seen recently, or  had stored in your quotesfile for ages, and which might be handy to link  to in one of our discussions.</p>\n<ul>\n<li>Please post all quotes separately, so that they can be voted up/down  separately.&nbsp; (If they are strongly related, reply to your own  comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XJtTbMZJt2WTn7kSa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 5.826520096777733e-07, "legacy": true, "legacyId": "2842", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 301, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-03T19:35:25.533Z", "modifiedAt": null, "url": null, "title": "Human values differ as much as values can differ", "slug": "human-values-differ-as-much-as-values-can-differ", "viewCount": null, "lastCommentedAt": "2018-11-26T02:19:33.929Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NkspwZcbR2bjHS4Xg/human-values-differ-as-much-as-values-can-differ", "pageUrlRelative": "/posts/NkspwZcbR2bjHS4Xg/human-values-differ-as-much-as-values-can-differ", "linkUrl": "https://www.lesswrong.com/posts/NkspwZcbR2bjHS4Xg/human-values-differ-as-much-as-values-can-differ", "postedAtFormatted": "Monday, May 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Human%20values%20differ%20as%20much%20as%20values%20can%20differ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHuman%20values%20differ%20as%20much%20as%20values%20can%20differ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNkspwZcbR2bjHS4Xg%2Fhuman-values-differ-as-much-as-values-can-differ%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Human%20values%20differ%20as%20much%20as%20values%20can%20differ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNkspwZcbR2bjHS4Xg%2Fhuman-values-differ-as-much-as-values-can-differ", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNkspwZcbR2bjHS4Xg%2Fhuman-values-differ-as-much-as-values-can-differ", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2174, "htmlBody": "<p>George Hamilton's autobiography <a href=\"http://books.google.com/books?id=qE-Ox16vR6kC&amp;printsec=frontcover&amp;dq=%22Don%27t+mind+if+i+do%22+hamilton&amp;source=bl&amp;ots=CQrBPSypaU&amp;sig=3ekwfS7Q0CIHHmIYGL5tjdP2td0&amp;hl=en&amp;ei=v6CfS9BExKqUB4HsyeMN&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=2&amp;ved=0CBEQ6AEwAQ#v=onepage&amp;q=&amp;f=false\">Don't Mind if I Do</a>, and the very similar book by Bob Evans, <a href=\"http://www.amazon.com/Kid-Stays-Picture-Robert-Evans/dp/1893224686\">The Kid Stays in the Picture</a>, give a lot of insight into human nature and values.&nbsp; For instance: What do people really want?&nbsp; When people have the money and fame to travel around the world and do anything that they want, what do they do?&nbsp; And what is it that they value most about the experience afterward?</p>\n<p>You may argue that the extremely wealthy and famous don't represent the desires of ordinary humans.&nbsp; I say the opposite: Non-wealthy, non-famous people, being more constrained by need and by social convention, and having no hope of ever attaining their desires, don't represent, or even allow themselves to acknowledge, the actual desires of humans.</p>\n<p>I noticed a pattern in these books:&nbsp; The men in them value social status primarily as an ends to a means; while the women value social status as an end in itself.<a id=\"more\"></a></p>\n<h2>\"Male\" and \"female\" values</h2>\n<p>This is a generalization; but, at least at the very upper levels of society depicted in these books, and a few others like them that I've read, it's frequently borne out.&nbsp; (Perhaps a culture chooses celebrities who reinforce its stereotypes.)&nbsp; Women and men alike appreciate expensive cars and clothing.&nbsp; But the impression I get is that the flamboyantly extravagant are surprisingly non-materialistic.&nbsp; Other than food (and, oddly, clothing), the very wealthy themselves consistently refer to these trappings as things that they need in order to signal their importance to other people.&nbsp; They don't have an opinion on how long or how tall a yacht \"ought\" to be; they just want theirs to be the longest or tallest.&nbsp; The persistent phenomenon whereby the more wealthy someone appears, the more likely they are to go into debt, is not because these people are too stupid or impulsive to hold on to their money (as in popular depictions of the wealthy, eg., <a href=\"http://www.imdb.com/title/tt0067482/\">A New Leaf</a>) .&nbsp; It's because they are deliberately trading monetary capital for the social capital that they actually desire (and expect to be able to trade it back later if they wish to, even making a profit on the \"transaction\", as Donald Trump has done so well).</p>\n<p>With most of the women in these books, that's where it ends.&nbsp; What they want is to be the center of attention.&nbsp; They want to walk into a famous night-club and see everyone's heads turn.&nbsp; They want the papers to talk about them.&nbsp; They want to be able to check into a famous hotel at 3 in the morning and demand that the head chef be called at home, woken up, and brought in immediately to cook them a five-course meal.&nbsp; Some of the women in these stories, like Elizabeth Taylor, routinely make outrageous demands just to prove that they're more important than other people.</p>\n<p>What the men want is women.&nbsp; Quantity and quality.&nbsp; They like social status, and they like to butt heads with other men and beat them; but once they've acquired a bevy of beautiful women, they are often happy to retire to their mansion or yacht and enjoy them in private for a while.&nbsp; And they're capable of forming deep, private attachments to things, in a way the women are less likely to.&nbsp; A man can obsess over his collection of antique cars as beautiful things in and of themselves.&nbsp; A woman will not enjoy her collection of Faberge eggs unless she has someone to show it to.&nbsp; (Preferably someone with a slightly less-impressive collection of Faberge eggs.)&nbsp; <a href=\"http://www.time.com/time/specials/packages/completelist/0,29569,1902376,00.html\">Reclusive celebrities</a> are more likely to be men than women.</p>\n<p>Some people mostly like having things.&nbsp; Some people mostly like having status.&nbsp; Do you see the key game-theoretic distinction?</p>\n<p>Neither value is very amenable to the creation of wealth.&nbsp; Give everybody a Rolls-Royce; and the women still have the same social status, and the men don't have any more women.&nbsp; But the \"male\" value is more amenable to it.&nbsp; Men compete, but perhaps mainly because the distribution of quality of women is normal.&nbsp; The status-related desires of the men described above are, in theory, capable of being mutually satisfied.&nbsp; The women's are not.</p>\n<h2>Non-positional / Mutually-satisfiable vs. Positional / Non-mutually-satisfiable values<br /></h2>\n<p>No real person implements pure mutually-satisfiable or non-mutually-satisfiable values.&nbsp;&nbsp; I have not done a study or taken a survey, and don't claim that these views correlate with sex in general.&nbsp; I just wanted to make accessible the evidence I saw that these two types of values exist in humans.&nbsp; The male/female distinction isn't what I want to talk about; it just helped organize the data in a way that made this distinction pop out for me.&nbsp; I could also have told a story about how men and women play sports, and claim that men are more likely to want to win (a non-mutually-satisfiable value), and women are more likely to just want to have fun (a mutually-satisfiable value).&nbsp; Let's not get distracted by sexual politics.&nbsp; I'm not trying to say something about women or about men; I'm trying to say something about FAI.</p>\n<p>I will now rename them \"non-positional\" and \"positional\" (as suggested by <a href=\"/lw/1xa/male_and_female_friendly_ai/1yif\">SilasBarta</a> and <a href=\"/lw/1xa/male_and_female_friendly_ai/1yio\">wnoise</a>), where \"non-positional\" means assigning a value to something from category X according to its properties, and \"positional\" means assigning a new value to something from category X according to the rank of its non-positional value in the set of all X (non-mutually-satisfiable).</p>\n<p>Now imagine two friendly AIs, one non-positional and one positional.</p>\n<p>The non-positional FAI has a tough task.&nbsp; It wants to give everyone what it imagines they want.</p>\n<p>But the positional FAI has an impossible task.&nbsp; It wants to give everyone what it is that it thinks they value, which is to be considered better than other people, or at least better than other people of the same sex.&nbsp; But it's a zero-sum value.&nbsp; It's very hard to give more status to one person without taking the same amount of status away from other people.&nbsp; There might be some clever solution involving sending people on trips at relativistic speeds so that the time each person is high-status seems longer to them than the time they are low-status, or using drugs to heighten their perceptions of high status and diminish the pain of low status.&nbsp; For an <a href=\"/lw/91/average_utilitarianism_must_be_correct/5ox\">average utilitarian</a>, the best solution is probably to kill off everyone except one man and one woman.&nbsp; (Painlessly, of course.)</p>\n<p>A FAI trying to satisfy one of these preferences would take society in a completely different direction than a FAI trying to satisfy the other.&nbsp; From the perspective of someone with the job of trying to satisfy these preferences for everyone, they are as different as it is possible for preferences to be, even though they are taken (in the books mentioned above) from members of the same species at the same time in the same place in the same strata of the same profession.</p>\n<h2>Correcting value \"mistakes\" is not Friendly<br /></h2>\n<p>This is not a problem that can be resolved by popping up a level.&nbsp; If you say, \"But what people who want status REALLY want is something else that they can use status to obtain,\" you're just denying the existence of status as a value.&nbsp; It's a value.&nbsp; When given the chance to either use their status to attain something else, or keep pressing the lever that gives them a \"You've got status!\" hit, some people choose to keep pressing the lever.</p>\n<p>If you claim that these people have formed bad habits, and improperly short-circuited a connection from value to stimulus; and can be re-educated to instead see status as a means, rather than as an ends... I might agree with you.&nbsp; But you'd make a bad, unfriendly AI.&nbsp; If there's one thing FAIers have been clear about, it's that changing top-level goals is not allowed.&nbsp; (That's usually said with respect to the FAI's top-level goals, not wrt the human top-level goals.&nbsp; But, since the FAI's top-level goal <em>is</em> just to preserve human top-level goals, it would be pointless to make a lot of fuss making sure the FAI held its own top-level goals constant, if you're going to \"correct\" human goals first.)</p>\n<p>If changing top-level goals <em>is</em> allowed in this instance, or this top-level goal is considered \"not really a top-level goal\", I would become alarmed and demand an explanation of how a FAI distinguishes such pseudo-top-level-goals from <em>real</em> top-level goals.</p>\n<h2>If a computation can be conscious, then changing a conscious agent's computation changes its conscious experience<br /></h2>\n<p>If you believe that computer programs can be conscious, then unless you have a new philosophical position that you haven't told anyone about, you believe that consciousness can be a by-product of computation.&nbsp; This means that the formal, computational properties of peoples' values are not just critical, they're the only thing that matters.&nbsp; This means that there is no way to abstract away the bad property of being zero-sum from a value without destroying the value.</p>\n<p>In other words, it isn't valid to analyze the sensations that people get when their higher status is affirmed by others, and then recreate those sensations directly in everyone, without anyone needing to have low status.&nbsp; If you did that, I can think of only 3 possible interpretations of what you would have done, and I find none of them acceptable:</p>\n<ul>\n<li>Consciousness is not dependent on computational structure (this leads to vitalism); or</li>\n<li>You have changed the computational structure their behaviors and values are part of, and therefore changed their conscious experience and their values; or</li>\n<li>You have embedded them each within their own Matrix, in which they perceive themselves as performing isomorophic computations (e.g., the \"Build human-seeming robots\" or \"For every person, a volcano-lair\" approaches mentioned in the comments).</li>\n</ul>\n<h2>Summary<br /></h2>\n<p>This discussion has uncovered several problems for an AI trying to give people what they value without changing what they value.&nbsp; In increasing order of importance:</p>\n<ul>\n<li>If you have a value associated with a sensation that is caused by a stimulus, it isn't clear when it's legitimate for a FAI to reconnect the sensation to a different stimulus and claim it's preserved the value.&nbsp; Maybe it's morally okay for a person to rewire their kids to switch their taste-perceptions of broccoli and ice cream.&nbsp; But is an AI still friendly if it does this?</li>\n<li>It isn't okay to do this with the valuation of social status.&nbsp; Social status has a simple formal (mathematical) structure requiring some agents to have low status in order for others to have high status.&nbsp; The headache that status poses for a FAI trying to satisfy it is a result of this formal structure.&nbsp; You can't abstract it away, and you can't legitimately banish it by reconnecting a sensation associated with it to a different stimulus, because the agent would then use that sensation to drive different behavior, meaning the value is now part of a different computational structure, and a different conscious experience.&nbsp; You either preserve the problematic formal structure, or you throw out the value.</li>\n<li>Some top-level human goals lead to conflict.&nbsp; You can't both eliminate conflict, and preserve human values.&nbsp; It's irresponsible, as well as creepy, when some people (I'm referring to some comments made on LW that I can't find now) talk about Friendly AI the same way that Christians talk about the Second Coming, as a future reign of perfect happiness for all when the lamb will lie down with the lion.&nbsp; That is a powerful attractor that you don't want to go near, unless you are practicing the Dark Arts.</li>\n<li>The notion of top-level goal is clear only in a 1960s classic symbolic AI framework.&nbsp; The idea of a \"top-level goal\" is an example of what I called the <a href=\"/lw/256/only_humans_can_have_human_values/\">\"Prime mover\" theory of network concepts</a>.&nbsp; In humans, a subsidiary goal, like status, can become a top-level goal via classic behavioristic association.&nbsp; It happens all the time.&nbsp; But the \"preferences\" that the FAI is supposed to preserve are human top-level goals.&nbsp; How's it supposed to know which top-level goals are sacrosanct, and which ones are just heuristics or erroneous associations?</li>\n<li>Reconciling human values may not be much easier or more sensible than reconciling all values, because human values already differ as much as it is possible for values to differ.&nbsp; Sure, humans have only covered a tiny portion of the space of possible values.&nbsp; But we've just seen two human values that differ along the critical dimensions of being mutually satisfiable or not being mutually satisfiable, and of encouraging global cooperation vs. not encouraging global cooperation.&nbsp; The <a href=\"http://en.wikipedia.org/wiki/Harmonic_series_(mathematics)\">harmonic series</a> looks a lot like <a href=\"http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7\">Zeno's geometric series</a>; yet one converges and one diverges.&nbsp; It doesn't matter that the terms used in each look similar; they're as different as series can be.&nbsp; In the same way, values taken from any conceivable society of agents can be classified into mutually-satisfiable, or not mutually-satisfiable.&nbsp; For the purposes of a Friendly AI, a mutually-satisfiable value held by gas clouds in Antares is more similar to a mutually-satisfiable human value, than either is to a non-mutually-satisfiable human value.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xknvtHwqvqhwahW8Q": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NkspwZcbR2bjHS4Xg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 27, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "2494", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>George Hamilton's autobiography <a href=\"http://books.google.com/books?id=qE-Ox16vR6kC&amp;printsec=frontcover&amp;dq=%22Don%27t+mind+if+i+do%22+hamilton&amp;source=bl&amp;ots=CQrBPSypaU&amp;sig=3ekwfS7Q0CIHHmIYGL5tjdP2td0&amp;hl=en&amp;ei=v6CfS9BExKqUB4HsyeMN&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=2&amp;ved=0CBEQ6AEwAQ#v=onepage&amp;q=&amp;f=false\">Don't Mind if I Do</a>, and the very similar book by Bob Evans, <a href=\"http://www.amazon.com/Kid-Stays-Picture-Robert-Evans/dp/1893224686\">The Kid Stays in the Picture</a>, give a lot of insight into human nature and values.&nbsp; For instance: What do people really want?&nbsp; When people have the money and fame to travel around the world and do anything that they want, what do they do?&nbsp; And what is it that they value most about the experience afterward?</p>\n<p>You may argue that the extremely wealthy and famous don't represent the desires of ordinary humans.&nbsp; I say the opposite: Non-wealthy, non-famous people, being more constrained by need and by social convention, and having no hope of ever attaining their desires, don't represent, or even allow themselves to acknowledge, the actual desires of humans.</p>\n<p>I noticed a pattern in these books:&nbsp; The men in them value social status primarily as an ends to a means; while the women value social status as an end in itself.<a id=\"more\"></a></p>\n<h2 id=\"_Male__and__female__values\">\"Male\" and \"female\" values</h2>\n<p>This is a generalization; but, at least at the very upper levels of society depicted in these books, and a few others like them that I've read, it's frequently borne out.&nbsp; (Perhaps a culture chooses celebrities who reinforce its stereotypes.)&nbsp; Women and men alike appreciate expensive cars and clothing.&nbsp; But the impression I get is that the flamboyantly extravagant are surprisingly non-materialistic.&nbsp; Other than food (and, oddly, clothing), the very wealthy themselves consistently refer to these trappings as things that they need in order to signal their importance to other people.&nbsp; They don't have an opinion on how long or how tall a yacht \"ought\" to be; they just want theirs to be the longest or tallest.&nbsp; The persistent phenomenon whereby the more wealthy someone appears, the more likely they are to go into debt, is not because these people are too stupid or impulsive to hold on to their money (as in popular depictions of the wealthy, eg., <a href=\"http://www.imdb.com/title/tt0067482/\">A New Leaf</a>) .&nbsp; It's because they are deliberately trading monetary capital for the social capital that they actually desire (and expect to be able to trade it back later if they wish to, even making a profit on the \"transaction\", as Donald Trump has done so well).</p>\n<p>With most of the women in these books, that's where it ends.&nbsp; What they want is to be the center of attention.&nbsp; They want to walk into a famous night-club and see everyone's heads turn.&nbsp; They want the papers to talk about them.&nbsp; They want to be able to check into a famous hotel at 3 in the morning and demand that the head chef be called at home, woken up, and brought in immediately to cook them a five-course meal.&nbsp; Some of the women in these stories, like Elizabeth Taylor, routinely make outrageous demands just to prove that they're more important than other people.</p>\n<p>What the men want is women.&nbsp; Quantity and quality.&nbsp; They like social status, and they like to butt heads with other men and beat them; but once they've acquired a bevy of beautiful women, they are often happy to retire to their mansion or yacht and enjoy them in private for a while.&nbsp; And they're capable of forming deep, private attachments to things, in a way the women are less likely to.&nbsp; A man can obsess over his collection of antique cars as beautiful things in and of themselves.&nbsp; A woman will not enjoy her collection of Faberge eggs unless she has someone to show it to.&nbsp; (Preferably someone with a slightly less-impressive collection of Faberge eggs.)&nbsp; <a href=\"http://www.time.com/time/specials/packages/completelist/0,29569,1902376,00.html\">Reclusive celebrities</a> are more likely to be men than women.</p>\n<p>Some people mostly like having things.&nbsp; Some people mostly like having status.&nbsp; Do you see the key game-theoretic distinction?</p>\n<p>Neither value is very amenable to the creation of wealth.&nbsp; Give everybody a Rolls-Royce; and the women still have the same social status, and the men don't have any more women.&nbsp; But the \"male\" value is more amenable to it.&nbsp; Men compete, but perhaps mainly because the distribution of quality of women is normal.&nbsp; The status-related desires of the men described above are, in theory, capable of being mutually satisfied.&nbsp; The women's are not.</p>\n<h2 id=\"Non_positional___Mutually_satisfiable_vs__Positional___Non_mutually_satisfiable_values\">Non-positional / Mutually-satisfiable vs. Positional / Non-mutually-satisfiable values<br></h2>\n<p>No real person implements pure mutually-satisfiable or non-mutually-satisfiable values.&nbsp;&nbsp; I have not done a study or taken a survey, and don't claim that these views correlate with sex in general.&nbsp; I just wanted to make accessible the evidence I saw that these two types of values exist in humans.&nbsp; The male/female distinction isn't what I want to talk about; it just helped organize the data in a way that made this distinction pop out for me.&nbsp; I could also have told a story about how men and women play sports, and claim that men are more likely to want to win (a non-mutually-satisfiable value), and women are more likely to just want to have fun (a mutually-satisfiable value).&nbsp; Let's not get distracted by sexual politics.&nbsp; I'm not trying to say something about women or about men; I'm trying to say something about FAI.</p>\n<p>I will now rename them \"non-positional\" and \"positional\" (as suggested by <a href=\"/lw/1xa/male_and_female_friendly_ai/1yif\">SilasBarta</a> and <a href=\"/lw/1xa/male_and_female_friendly_ai/1yio\">wnoise</a>), where \"non-positional\" means assigning a value to something from category X according to its properties, and \"positional\" means assigning a new value to something from category X according to the rank of its non-positional value in the set of all X (non-mutually-satisfiable).</p>\n<p>Now imagine two friendly AIs, one non-positional and one positional.</p>\n<p>The non-positional FAI has a tough task.&nbsp; It wants to give everyone what it imagines they want.</p>\n<p>But the positional FAI has an impossible task.&nbsp; It wants to give everyone what it is that it thinks they value, which is to be considered better than other people, or at least better than other people of the same sex.&nbsp; But it's a zero-sum value.&nbsp; It's very hard to give more status to one person without taking the same amount of status away from other people.&nbsp; There might be some clever solution involving sending people on trips at relativistic speeds so that the time each person is high-status seems longer to them than the time they are low-status, or using drugs to heighten their perceptions of high status and diminish the pain of low status.&nbsp; For an <a href=\"/lw/91/average_utilitarianism_must_be_correct/5ox\">average utilitarian</a>, the best solution is probably to kill off everyone except one man and one woman.&nbsp; (Painlessly, of course.)</p>\n<p>A FAI trying to satisfy one of these preferences would take society in a completely different direction than a FAI trying to satisfy the other.&nbsp; From the perspective of someone with the job of trying to satisfy these preferences for everyone, they are as different as it is possible for preferences to be, even though they are taken (in the books mentioned above) from members of the same species at the same time in the same place in the same strata of the same profession.</p>\n<h2 id=\"Correcting_value__mistakes__is_not_Friendly\">Correcting value \"mistakes\" is not Friendly<br></h2>\n<p>This is not a problem that can be resolved by popping up a level.&nbsp; If you say, \"But what people who want status REALLY want is something else that they can use status to obtain,\" you're just denying the existence of status as a value.&nbsp; It's a value.&nbsp; When given the chance to either use their status to attain something else, or keep pressing the lever that gives them a \"You've got status!\" hit, some people choose to keep pressing the lever.</p>\n<p>If you claim that these people have formed bad habits, and improperly short-circuited a connection from value to stimulus; and can be re-educated to instead see status as a means, rather than as an ends... I might agree with you.&nbsp; But you'd make a bad, unfriendly AI.&nbsp; If there's one thing FAIers have been clear about, it's that changing top-level goals is not allowed.&nbsp; (That's usually said with respect to the FAI's top-level goals, not wrt the human top-level goals.&nbsp; But, since the FAI's top-level goal <em>is</em> just to preserve human top-level goals, it would be pointless to make a lot of fuss making sure the FAI held its own top-level goals constant, if you're going to \"correct\" human goals first.)</p>\n<p>If changing top-level goals <em>is</em> allowed in this instance, or this top-level goal is considered \"not really a top-level goal\", I would become alarmed and demand an explanation of how a FAI distinguishes such pseudo-top-level-goals from <em>real</em> top-level goals.</p>\n<h2 id=\"If_a_computation_can_be_conscious__then_changing_a_conscious_agent_s_computation_changes_its_conscious_experience\">If a computation can be conscious, then changing a conscious agent's computation changes its conscious experience<br></h2>\n<p>If you believe that computer programs can be conscious, then unless you have a new philosophical position that you haven't told anyone about, you believe that consciousness can be a by-product of computation.&nbsp; This means that the formal, computational properties of peoples' values are not just critical, they're the only thing that matters.&nbsp; This means that there is no way to abstract away the bad property of being zero-sum from a value without destroying the value.</p>\n<p>In other words, it isn't valid to analyze the sensations that people get when their higher status is affirmed by others, and then recreate those sensations directly in everyone, without anyone needing to have low status.&nbsp; If you did that, I can think of only 3 possible interpretations of what you would have done, and I find none of them acceptable:</p>\n<ul>\n<li>Consciousness is not dependent on computational structure (this leads to vitalism); or</li>\n<li>You have changed the computational structure their behaviors and values are part of, and therefore changed their conscious experience and their values; or</li>\n<li>You have embedded them each within their own Matrix, in which they perceive themselves as performing isomorophic computations (e.g., the \"Build human-seeming robots\" or \"For every person, a volcano-lair\" approaches mentioned in the comments).</li>\n</ul>\n<h2 id=\"Summary\">Summary<br></h2>\n<p>This discussion has uncovered several problems for an AI trying to give people what they value without changing what they value.&nbsp; In increasing order of importance:</p>\n<ul>\n<li>If you have a value associated with a sensation that is caused by a stimulus, it isn't clear when it's legitimate for a FAI to reconnect the sensation to a different stimulus and claim it's preserved the value.&nbsp; Maybe it's morally okay for a person to rewire their kids to switch their taste-perceptions of broccoli and ice cream.&nbsp; But is an AI still friendly if it does this?</li>\n<li>It isn't okay to do this with the valuation of social status.&nbsp; Social status has a simple formal (mathematical) structure requiring some agents to have low status in order for others to have high status.&nbsp; The headache that status poses for a FAI trying to satisfy it is a result of this formal structure.&nbsp; You can't abstract it away, and you can't legitimately banish it by reconnecting a sensation associated with it to a different stimulus, because the agent would then use that sensation to drive different behavior, meaning the value is now part of a different computational structure, and a different conscious experience.&nbsp; You either preserve the problematic formal structure, or you throw out the value.</li>\n<li>Some top-level human goals lead to conflict.&nbsp; You can't both eliminate conflict, and preserve human values.&nbsp; It's irresponsible, as well as creepy, when some people (I'm referring to some comments made on LW that I can't find now) talk about Friendly AI the same way that Christians talk about the Second Coming, as a future reign of perfect happiness for all when the lamb will lie down with the lion.&nbsp; That is a powerful attractor that you don't want to go near, unless you are practicing the Dark Arts.</li>\n<li>The notion of top-level goal is clear only in a 1960s classic symbolic AI framework.&nbsp; The idea of a \"top-level goal\" is an example of what I called the <a href=\"/lw/256/only_humans_can_have_human_values/\">\"Prime mover\" theory of network concepts</a>.&nbsp; In humans, a subsidiary goal, like status, can become a top-level goal via classic behavioristic association.&nbsp; It happens all the time.&nbsp; But the \"preferences\" that the FAI is supposed to preserve are human top-level goals.&nbsp; How's it supposed to know which top-level goals are sacrosanct, and which ones are just heuristics or erroneous associations?</li>\n<li>Reconciling human values may not be much easier or more sensible than reconciling all values, because human values already differ as much as it is possible for values to differ.&nbsp; Sure, humans have only covered a tiny portion of the space of possible values.&nbsp; But we've just seen two human values that differ along the critical dimensions of being mutually satisfiable or not being mutually satisfiable, and of encouraging global cooperation vs. not encouraging global cooperation.&nbsp; The <a href=\"http://en.wikipedia.org/wiki/Harmonic_series_(mathematics)\">harmonic series</a> looks a lot like <a href=\"http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7\">Zeno's geometric series</a>; yet one converges and one diverges.&nbsp; It doesn't matter that the terms used in each look similar; they're as different as series can be.&nbsp; In the same way, values taken from any conceivable society of agents can be classified into mutually-satisfiable, or not mutually-satisfiable.&nbsp; For the purposes of a Friendly AI, a mutually-satisfiable value held by gas clouds in Antares is more similar to a mutually-satisfiable human value, than either is to a non-mutually-satisfiable human value.</li>\n</ul>", "sections": [{"title": "\"Male\" and \"female\" values", "anchor": "_Male__and__female__values", "level": 1}, {"title": "Non-positional / Mutually-satisfiable vs. Positional / Non-mutually-satisfiable values", "anchor": "Non_positional___Mutually_satisfiable_vs__Positional___Non_mutually_satisfiable_values", "level": 1}, {"title": "Correcting value \"mistakes\" is not Friendly", "anchor": "Correcting_value__mistakes__is_not_Friendly", "level": 1}, {"title": "If a computation can be conscious, then changing a conscious agent's computation changes its conscious experience", "anchor": "If_a_computation_can_be_conscious__then_changing_a_conscious_agent_s_computation_changes_its_conscious_experience", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "220 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 220, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cAPCCJjggjZPxxcKh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-04T18:56:34.802Z", "modifiedAt": null, "url": null, "title": "But Somebody Would Have Noticed", "slug": "but-somebody-would-have-noticed", "viewCount": null, "lastCommentedAt": "2020-10-08T16:53:21.169Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xz6pKy6sjZffy4NYW/but-somebody-would-have-noticed", "pageUrlRelative": "/posts/Xz6pKy6sjZffy4NYW/but-somebody-would-have-noticed", "linkUrl": "https://www.lesswrong.com/posts/Xz6pKy6sjZffy4NYW/but-somebody-would-have-noticed", "postedAtFormatted": "Tuesday, May 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20But%20Somebody%20Would%20Have%20Noticed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABut%20Somebody%20Would%20Have%20Noticed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXz6pKy6sjZffy4NYW%2Fbut-somebody-would-have-noticed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=But%20Somebody%20Would%20Have%20Noticed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXz6pKy6sjZffy4NYW%2Fbut-somebody-would-have-noticed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXz6pKy6sjZffy4NYW%2Fbut-somebody-would-have-noticed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1225, "htmlBody": "<p><em>When you hear a hypothesis that is completely new to you, and seems important enough that you want to dismiss it with \"but somebody would have noticed!\", beware this temptation.&nbsp; If you're hearing it, somebody noticed.</em></p>\n<p><em>Disclaimer: I do not believe in anything I would expect anyone here to call a \"conspiracy theory\" or similar.&nbsp; I am not trying to \"soften you up\" for a future surprise with this post.<br /></em></p>\n<p><strong>1. Wednesday</strong></p>\n<p>Suppose: <a href=\"/lw/dg/theism_wednesday_and_not_being_adopted/\">Wednesday</a> gets to be about eighteen, and goes on a trip to visit her Auntie Alicorn, who has hitherto refrained from bringing up religion around her out of respect for her parents<sup>1</sup>.&nbsp; During the visit, Sunday rolls around, and Wednesday observes that Alicorn is (a) wearing pants, not a skirt or a dress - unsuitable church attire! and (b) does not appear to be making any move to go to church at all, while (c) not being sick or otherwise having a very good excuse to skip church.&nbsp; Wednesday inquires as to why this is so, fearing she'll find that beloved Auntie has been excommunicated or something (gasp!&nbsp; horror!).</p>\n<p>Auntie Alicorn says, \"Well, I never told you this because your parents asked me not to when you were a child, but I suppose now it's time you knew.&nbsp; I'm an atheist, and I don't believe God exists, so I don't generally go to church.\"</p>\n<p>And Wednesday says, \"Don't be silly.&nbsp; If God didn't exist, don't you think somebody would have noticed?\"<a id=\"more\"></a></p>\n<p><strong>2. Ignoring Soothsayers<br /></strong></p>\n<p>Wednesday's environment reinforces the idea that God exists relentlessly.&nbsp; Everyone she commonly associates with believes it; people who don't, and insist on telling her, are quickly shepherded out of her life.&nbsp; Because Wednesday is not the protagonist of a fantasy novel, people who are laughed out of public discourse for shouting unpopular, outlandish, <em>silly</em> ideas rarely turn out to have plot significance later: it simply doesn't <em>matter</em> what that weirdo was yelling, because it was wrong and everybody knows it.&nbsp; It was only one person.&nbsp; More than one person would have noticed if something that weird were true.&nbsp; Or maybe it was only six or twelve people.&nbsp; At any rate, it wasn't enough.&nbsp; How many would be enough?&nbsp; Well, uh, more than <em>that</em>.</p>\n<p>But even if you airdropped Wednesday into an entire convention center full of atheists, you would find that you cannot outnumber her home team.&nbsp; We have lots of mechanisms for discounting collections of outgroup-people who believe weird things; they're \"cultists\" or \"conspiracy theorists\" or maybe just pulling a really overdone joke.&nbsp; There is nothing you can do that makes \"God doesn't exist, and virtually everyone I care about is terribly, terribly wrong about something of immense importance\" sound like a <em>less weird</em> hypothesis than \"these people are <em>silly!&nbsp; </em>Don't they realize that if God didn't exist, <em>somebody would have noticed?</em>\"</p>\n<p>To Wednesday, even Auntie Alicorn is not \"somebody\".&nbsp; \"Somebody\" is \"somebody from whom I am already accustomed to learning deep and surprising facts about the world\".&nbsp; Maybe not even them.</p>\n<p><strong>3. Standing By<br /></strong></p>\n<p>Suppose: It's 1964 and you live in Kew Gardens, Queens.&nbsp; You've just gotten back from a nice vacation and when you get back, you find you forgot to stop the newspapers.&nbsp; One of them has a weird headline.&nbsp; While you were gone, a woman was stabbed to death in plain view of several of your neighbors.&nbsp; The paper says thirty-eight people saw it happen and not a one called the police.&nbsp; \"But that's weird,\" you mutter to yourself.&nbsp; \"Wouldn't someone have done something?\"&nbsp; In this case, you'd have been right; the paper that covered Kitty Genovese exaggerated the extent to which unhelpful neighbors contributed to her death.&nbsp; Someone <em>did</em> do <em>something</em>.&nbsp; But what they didn't do was <em>successfully get law enforcement on the scene in time to save her.</em>&nbsp; Moving people to action is <em>hard</em>.&nbsp; Some have the talent for it, which is why things like protests and grassroots movements happen; but the leaders of those types of things self-select for skill at inspiring others to action.&nbsp; You don't hear about the ones who try it and don't have the necessary mojo.&nbsp; Cops are supposed to be easier to move to action than ordinary folks; but if you sound like you might be wasting their time, or if the way you describe the crime doesn't make it sound like an emergency, they might not turn up for a while.</p>\n<p>Events that need someone to act on them do not select for such people.&nbsp; Witnesses to crimes, collectors of useful evidence, holders of interesting little-known knowledge - these are not necessarily the people who have the power to get your attention, and having eyewitness status or handy data or mysterious secrets doesn't give them that power by itself.&nbsp; If that guy who thinks he was abducted by aliens <em>really had been</em> abducted by aliens, would enough about him be different that you'd sit still and listen to his story?</p>\n<p>And many people even <em>know</em> this.&nbsp; It's the entire premise of the \"Bill Murray story\", in which Bill Murray does something outlandish and then says to his witness-slash-victim, \"No one will ever believe you.\"&nbsp; And no one ever will.&nbsp; Bill Murray could do any fool thing he wanted to you, now that this meme exists, and no one would ever believe you.</p>\n<p><strong>4. What Are You Going To Do About It?</strong></p>\n<p>If something huge and unbelievable happened to you - you're abducted by aliens, you witness a key bit of a huge crime, you find a cryptozoological creature - and you weren't really good at getting attention or collecting allies, what would you do about it?&nbsp; If there are fellow witnesses, and they all think it's unbelievable too, you can't organize a coalition to tell a consistent tale - no one will throw in with you.&nbsp; It'll make them look like conspiracy theorists.&nbsp; If there aren't fellow witnesses, you're in even worse shape, because then even by accumulating sympathetic ears you can't prove to others that they should come forward with their perspectives on the event.&nbsp; If you try to tell people anyway, whatever interest from others you start with will gradually drain away as you stick to your story: \"Yeah, yeah, the first time you told me this it was funny, but it's getting really old, why don't we play cards or something instead?\"&nbsp; And later, if you keep going: \"I told you to shut up.&nbsp; Look, either you're taking this joke way too far or you are literally insane.&nbsp; How am I supposed to believe anything you say now?\"</p>\n<p>If you push it, your friends think you're a liar, strangers on the street think you're a nutcase, the Internet thinks you're a troll, and you think you're never going to get anyone to talk to you like a person until you pretend you were only fooling, you made it up, it didn't happen...&nbsp; If you have physical evidence, you still need to get people to look at it and let you explain it.&nbsp; If you have fellow witnesses to back you up, you still need to get people to let you introduce them.&nbsp; And if you get your entire explanation out, someone will still say:</p>\n<p>\"But somebody would have noticed.\"</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>They-who-will-be-Wednesday's-parents have made no such demand, although it seems possible that they will upon Wednesday actually coming to exist (she still doesn't).&nbsp; I am undecided about how to react to it if they do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "ALwRRZqvhaop8gxkT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xz6pKy6sjZffy4NYW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 38, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "2858", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>When you hear a hypothesis that is completely new to you, and seems important enough that you want to dismiss it with \"but somebody would have noticed!\", beware this temptation.&nbsp; If you're hearing it, somebody noticed.</em></p>\n<p><em>Disclaimer: I do not believe in anything I would expect anyone here to call a \"conspiracy theory\" or similar.&nbsp; I am not trying to \"soften you up\" for a future surprise with this post.<br></em></p>\n<p><strong id=\"1__Wednesday\">1. Wednesday</strong></p>\n<p>Suppose: <a href=\"/lw/dg/theism_wednesday_and_not_being_adopted/\">Wednesday</a> gets to be about eighteen, and goes on a trip to visit her Auntie Alicorn, who has hitherto refrained from bringing up religion around her out of respect for her parents<sup>1</sup>.&nbsp; During the visit, Sunday rolls around, and Wednesday observes that Alicorn is (a) wearing pants, not a skirt or a dress - unsuitable church attire! and (b) does not appear to be making any move to go to church at all, while (c) not being sick or otherwise having a very good excuse to skip church.&nbsp; Wednesday inquires as to why this is so, fearing she'll find that beloved Auntie has been excommunicated or something (gasp!&nbsp; horror!).</p>\n<p>Auntie Alicorn says, \"Well, I never told you this because your parents asked me not to when you were a child, but I suppose now it's time you knew.&nbsp; I'm an atheist, and I don't believe God exists, so I don't generally go to church.\"</p>\n<p>And Wednesday says, \"Don't be silly.&nbsp; If God didn't exist, don't you think somebody would have noticed?\"<a id=\"more\"></a></p>\n<p><strong id=\"2__Ignoring_Soothsayers\">2. Ignoring Soothsayers<br></strong></p>\n<p>Wednesday's environment reinforces the idea that God exists relentlessly.&nbsp; Everyone she commonly associates with believes it; people who don't, and insist on telling her, are quickly shepherded out of her life.&nbsp; Because Wednesday is not the protagonist of a fantasy novel, people who are laughed out of public discourse for shouting unpopular, outlandish, <em>silly</em> ideas rarely turn out to have plot significance later: it simply doesn't <em>matter</em> what that weirdo was yelling, because it was wrong and everybody knows it.&nbsp; It was only one person.&nbsp; More than one person would have noticed if something that weird were true.&nbsp; Or maybe it was only six or twelve people.&nbsp; At any rate, it wasn't enough.&nbsp; How many would be enough?&nbsp; Well, uh, more than <em>that</em>.</p>\n<p>But even if you airdropped Wednesday into an entire convention center full of atheists, you would find that you cannot outnumber her home team.&nbsp; We have lots of mechanisms for discounting collections of outgroup-people who believe weird things; they're \"cultists\" or \"conspiracy theorists\" or maybe just pulling a really overdone joke.&nbsp; There is nothing you can do that makes \"God doesn't exist, and virtually everyone I care about is terribly, terribly wrong about something of immense importance\" sound like a <em>less weird</em> hypothesis than \"these people are <em>silly!&nbsp; </em>Don't they realize that if God didn't exist, <em>somebody would have noticed?</em>\"</p>\n<p>To Wednesday, even Auntie Alicorn is not \"somebody\".&nbsp; \"Somebody\" is \"somebody from whom I am already accustomed to learning deep and surprising facts about the world\".&nbsp; Maybe not even them.</p>\n<p><strong id=\"3__Standing_By\">3. Standing By<br></strong></p>\n<p>Suppose: It's 1964 and you live in Kew Gardens, Queens.&nbsp; You've just gotten back from a nice vacation and when you get back, you find you forgot to stop the newspapers.&nbsp; One of them has a weird headline.&nbsp; While you were gone, a woman was stabbed to death in plain view of several of your neighbors.&nbsp; The paper says thirty-eight people saw it happen and not a one called the police.&nbsp; \"But that's weird,\" you mutter to yourself.&nbsp; \"Wouldn't someone have done something?\"&nbsp; In this case, you'd have been right; the paper that covered Kitty Genovese exaggerated the extent to which unhelpful neighbors contributed to her death.&nbsp; Someone <em>did</em> do <em>something</em>.&nbsp; But what they didn't do was <em>successfully get law enforcement on the scene in time to save her.</em>&nbsp; Moving people to action is <em>hard</em>.&nbsp; Some have the talent for it, which is why things like protests and grassroots movements happen; but the leaders of those types of things self-select for skill at inspiring others to action.&nbsp; You don't hear about the ones who try it and don't have the necessary mojo.&nbsp; Cops are supposed to be easier to move to action than ordinary folks; but if you sound like you might be wasting their time, or if the way you describe the crime doesn't make it sound like an emergency, they might not turn up for a while.</p>\n<p>Events that need someone to act on them do not select for such people.&nbsp; Witnesses to crimes, collectors of useful evidence, holders of interesting little-known knowledge - these are not necessarily the people who have the power to get your attention, and having eyewitness status or handy data or mysterious secrets doesn't give them that power by itself.&nbsp; If that guy who thinks he was abducted by aliens <em>really had been</em> abducted by aliens, would enough about him be different that you'd sit still and listen to his story?</p>\n<p>And many people even <em>know</em> this.&nbsp; It's the entire premise of the \"Bill Murray story\", in which Bill Murray does something outlandish and then says to his witness-slash-victim, \"No one will ever believe you.\"&nbsp; And no one ever will.&nbsp; Bill Murray could do any fool thing he wanted to you, now that this meme exists, and no one would ever believe you.</p>\n<p><strong id=\"4__What_Are_You_Going_To_Do_About_It_\">4. What Are You Going To Do About It?</strong></p>\n<p>If something huge and unbelievable happened to you - you're abducted by aliens, you witness a key bit of a huge crime, you find a cryptozoological creature - and you weren't really good at getting attention or collecting allies, what would you do about it?&nbsp; If there are fellow witnesses, and they all think it's unbelievable too, you can't organize a coalition to tell a consistent tale - no one will throw in with you.&nbsp; It'll make them look like conspiracy theorists.&nbsp; If there aren't fellow witnesses, you're in even worse shape, because then even by accumulating sympathetic ears you can't prove to others that they should come forward with their perspectives on the event.&nbsp; If you try to tell people anyway, whatever interest from others you start with will gradually drain away as you stick to your story: \"Yeah, yeah, the first time you told me this it was funny, but it's getting really old, why don't we play cards or something instead?\"&nbsp; And later, if you keep going: \"I told you to shut up.&nbsp; Look, either you're taking this joke way too far or you are literally insane.&nbsp; How am I supposed to believe anything you say now?\"</p>\n<p>If you push it, your friends think you're a liar, strangers on the street think you're a nutcase, the Internet thinks you're a troll, and you think you're never going to get anyone to talk to you like a person until you pretend you were only fooling, you made it up, it didn't happen...&nbsp; If you have physical evidence, you still need to get people to look at it and let you explain it.&nbsp; If you have fellow witnesses to back you up, you still need to get people to let you introduce them.&nbsp; And if you get your entire explanation out, someone will still say:</p>\n<p>\"But somebody would have noticed.\"</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>They-who-will-be-Wednesday's-parents have made no such demand, although it seems possible that they will upon Wednesday actually coming to exist (she still doesn't).&nbsp; I am undecided about how to react to it if they do.</p>", "sections": [{"title": "1. Wednesday", "anchor": "1__Wednesday", "level": 1}, {"title": "2. Ignoring Soothsayers", "anchor": "2__Ignoring_Soothsayers", "level": 1}, {"title": "3. Standing By", "anchor": "3__Standing_By", "level": 1}, {"title": "4. What Are You Going To Do About It?", "anchor": "4__What_Are_You_Going_To_Do_About_It_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "258 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 258, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AYa2gc3sFWCCFSaFq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-05T00:11:47.162Z", "modifiedAt": null, "url": null, "title": "The Cameron Todd Willingham test", "slug": "the-cameron-todd-willingham-test", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:04.812Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9iFNBzwtLxKRex3DL/the-cameron-todd-willingham-test", "pageUrlRelative": "/posts/9iFNBzwtLxKRex3DL/the-cameron-todd-willingham-test", "linkUrl": "https://www.lesswrong.com/posts/9iFNBzwtLxKRex3DL/the-cameron-todd-willingham-test", "postedAtFormatted": "Wednesday, May 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Cameron%20Todd%20Willingham%20test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Cameron%20Todd%20Willingham%20test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9iFNBzwtLxKRex3DL%2Fthe-cameron-todd-willingham-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Cameron%20Todd%20Willingham%20test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9iFNBzwtLxKRex3DL%2Fthe-cameron-todd-willingham-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9iFNBzwtLxKRex3DL%2Fthe-cameron-todd-willingham-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<p>In 2004, The United States government executed <a href=\"http://en.wikipedia.org/wiki/Cameron_Todd_Willingham\">Cameron Todd Willingham</a> via lethal injection for the crime of murdering his young children by setting fire to his house.&nbsp;</p>\n<p>In 2009, David Grann wrote <a href=\"http://www.newyorker.com/reporting/2009/09/07/090907fa_fact_grann?currentPage=all\">an extended examination of the evidence</a> in the Willingham case for <em>The New Yorker, </em>which has called into question Willingham's guilt. One of the&nbsp;prosecutors&nbsp;in the Willingham case, John Jackson, wrote <a href=\"http://corsicanadailysun.com/thewillinghamfiles/x46870434/-08-28-09-JACKSON-Guest-Commentary-Willingham-guilt-never-in-doubt?keyword=topstory\">a response summarizing the evidence from his current perspective</a>. I am not summarizing the evidence here so as to not give the impression of selectively choosing the evidence.</p>\n<p>A prior probability estimate for Willingham's guilt (certainly not a close to optimal prior probability)&nbsp;is the probability that a fire resulting in the fatalities of children was intentionally set. The US Fire Administration <a href=\"http://www.usfa.dhs.gov/downloads/pdf/tfrs/v1i17-508.pdf\">puts this probability at 13%</a>. The prior probability could be made more accurate by breaking down that 13% of intentionally set fires into different demographic sets, or looking at correlations with other things such as life insurance data.</p>\n<p>My question for Less Wrong: Just how innocent is Cameron Todd Willingham? Intuitively, it seems to me that the evidence for Willingham's innocence is of higher magnitude than the evidence for <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">Amanda Knox</a>'s innocence. But the prior probability of Willingham being guilty given his children died in a fire in his home is higher than the probability that Amanda Knox committed murder given that a murder occurred in Knox's house.</p>\n<p>Challenge question: What does an idealized form of Bayesian Justice look like? I suspect as a start that it would result in a smaller percentage of defendants being found guilty at trial.&nbsp;<a href=\"http://www.allbusiness.com/crime-law-enforcement-corrections/criminal-offenses-crimes/13289931-1.html\">This article has some examples</a> of the failures to apply Bayesian statistics in existing justice systems.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9iFNBzwtLxKRex3DL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 7, "extendedScore": null, "score": 5.837494312734995e-07, "legacy": true, "legacyId": "2662", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G9dptrW9CJi7wNg3b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-05T14:40:12.797Z", "modifiedAt": null, "url": null, "title": "Antagonizing Opioid Receptors for (Prevention of) Fun and Profit", "slug": "antagonizing-opioid-receptors-for-prevention-of-fun-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:09.776Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tEHJXNhw6t87foqJL/antagonizing-opioid-receptors-for-prevention-of-fun-and", "pageUrlRelative": "/posts/tEHJXNhw6t87foqJL/antagonizing-opioid-receptors-for-prevention-of-fun-and", "linkUrl": "https://www.lesswrong.com/posts/tEHJXNhw6t87foqJL/antagonizing-opioid-receptors-for-prevention-of-fun-and", "postedAtFormatted": "Wednesday, May 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Antagonizing%20Opioid%20Receptors%20for%20(Prevention%20of)%20Fun%20and%20Profit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAntagonizing%20Opioid%20Receptors%20for%20(Prevention%20of)%20Fun%20and%20Profit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtEHJXNhw6t87foqJL%2Fantagonizing-opioid-receptors-for-prevention-of-fun-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Antagonizing%20Opioid%20Receptors%20for%20(Prevention%20of)%20Fun%20and%20Profit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtEHJXNhw6t87foqJL%2Fantagonizing-opioid-receptors-for-prevention-of-fun-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtEHJXNhw6t87foqJL%2Fantagonizing-opioid-receptors-for-prevention-of-fun-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1189, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/21b/ugh_fields/\">Ugh Fields</a>, <a href=\"/lw/1lb/are_wireheads_happy/\">Are Wireheads Happy?</a></p>\n<p>In his post <a href=\"/lw/21b/ugh_fields/\">Ugh Fields</a>, Roko discussed \"temporal difference learning\", the process by which the brain propagates positive or negative feedback to the closest cause it can find for the feedback. For example, if he forgets to pay his bills and gets in trouble, the trouble (negative feedback) propagates back to thoughts about bills. Next time he gets a bill, he might paradoxically have even more trouble paying it, because it's become associated with trouble and negative emotions, and his brain tends to unconsciously flinch away from it.<br /><br />He links to the <a href=\"http://en.wikipedia.org/wiki/Temporal_difference_learning\">associated Wikipedia article</a>:</p>\n<blockquote>\n<p>The TD algorithm has also received attention in the field of neuroscience. Researchers discovered that the firing rate of dopamine neurons in the ventral tegmental area (VTA) and substantia nigra (SNc) appear to mimic the error function in the algorithm. The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future reward.<br /><br />Dopamine cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice. Initially the dopamine cells increased firing rates when exposed to the juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. This mimics closely how the error function in TD is used for reinforcement learning.</p>\n</blockquote>\n<p>So if I understand this right, the monkey hears a bell and is unimpressed, having no expectation of reward. Then the monkey gets some juice that tastes really good and activates (opioid dependent?) reward pathways. The dopamine system is pretty surprised, and broadcasts that surprise back to all the neurons that have been especially active recently, most notably the neurons that activated upon hearing the bell. These neurons are now more heavily associated with the dopamine system. So the next time the monkey hears a bell, it has a greater expectation of reward.<br /><br />And in this case it doesn't matter, because the monkey can't do anything about it. But if it were a circus monkey, and its trainer was trying to teach it to do a backflip to get juice, the association between backflips and juice would be pretty useful. As long as the monkey wanted juice, merely entertaining the plan of doing a backflip would have motivational value that promotes the correct action.<br /><br />The Sinclair Method is a promising technique for treating alcoholics that elegantly demonstrates these pathways by sabotaging them.</p>\n<p><a id=\"more\"></a></p>\n<p>Alcohol <a href=\"http://www.biopsychiatry.com/alcohol/beta-endorphins.html\">produces a surge of opioids</a> in, yes, the ventral tegmental area. The temporal difference algorithm there correctly deduces that the reward is due to alcohol, and so links the dopamine system to things like drinking, planning to drink, et cetera. Rounding the nearest cliche, <a href=\"/lw/1lb/are_wireheads_happy/\">dopamine represents \"wanting\"</a>, so this makes people want to drink.<br /><br />Repeat this process enough, or start with the right (wrong?) <a href=\"http://spittoon.23andme.com/2009/09/22/snpwatch-evidence-for-gene-environment-interaction-in-alcoholism/\">chemical structure for your opioid and dopamine receptors</a>, and you become an alcoholic.<br /><br />So to treat alcoholism, all you should have to do is reverse the process. Drink something, but have it not activate the reward system at all. Those dopaminergic neurons that detect error in your reward predictions start firing like mad and withdrawing their connections to the parts of the brain representing drinking, drinking is no longer associated with \"wanting\", you don't want to drink, and suddenly you're not an alcoholic any more.<br /><br />It's not quite that easy. But it might be pretty close.<br /><br />The <a href=\"http://en.wikipedia.org/wiki/Sinclair_Method\">Sinclair Method</a> of treating alcoholism is to give patients <a href=\"http://en.wikipedia.org/wiki/Naltrexone\">naltrexone</a>, an opioid antagonist. Then the patients are told they can drink as much as they want. Then they do. Then they gradually stop craving drink.<br /><br />In these people, alcohol still produces opioids, but the naltrexone prevents them from working and they don't register with the brain's reward system. Drinking isn't \"fun\" any more. The dopamine system notices there's no reward, and downgrades the connection between reward and drinking, which from the inside feels like a lessened craving to drink.<br /><br />In theory, this same process should be useful against any addiction or unwanted behavior. In practice, research either supports or is still investigating naltrexone use<sup>1</sup> against smoking, self-harm, kleptomania, and overeating (no word yet on Reddit use).</p>\n<p>The method boasts an success rate of between 25% to 78% on alcoholics depending on how you define success. A lot of alcoholism statistics are comparing apples to oranges (did they stay sober for more than a year? Forever? If they just lapsed once or twice, does that still count?) but eyeballing the data<sup>2</sup> makes this look significantly better than either Alcoholics Anonymous or willpower alone.</p>\n<p>I'm kind of confused by the whole idea because I don't understand the lack of side effects. Knocking out the brain's learning system to cure alcoholism seems disproportionate, and I would also expect naltrexone to interfere with the ability to experience happiness (which many people seem to like). But I haven't heard anyone mention any side-effects along the lines of \"oh, and people on this drug can never learn anything or have fun ever again\", and you'd think <a href=\"/lw/27e/but_somebody_would_have_noticed/\">somebody would have noticed</a>. If anyone on Less Wrong has ever used this method, or used naltrexone for anything else, please speak up.<br /><br />Since these same pathways control so many cravings besides alcoholism, research in this area will probably uncover more knowledge of what really motivates us.</p>\n<p><strong>Footnotes</strong></p>\n<p><strong>1:</strong> There's a subtle but important difference between the Sinclair Method and simple naltrexone use. As I understand it, most doctors who prescribe naltrexone tell the patient to abstain from alcohol as much as possible, but the Sinclair Method tells the patients to continue drinking normally. There are also some complicated parts about exactly when and how often you take the drug. The theory predicts the Sinclair Method would have better results, and the data seems to bear this out. As far as I know, all the studies on kleptomania, overeating, et cetera have been done on standard naltrexone use, not the Sinclair Method; I predict the Sinclair Method would work better, although there might be some practical difficulties invovled in telling a kleptomaniac \"Okay, take this tablet once a day while stealing stuff at the same rate you usually do.\"</p>\n<p><strong>2:</strong> 27% <a href=\"http://journals.lww.com/psychopharmacology/Abstract/2001/06000/Targeted_Use_of_Naltrexone_Without_Prior.6.aspx\">\"never relapse into heavy drinking\"</a> and 78% get drinking <a href=\"http://www.contral.com/resources_02.html\">\"below the level of increased risk of morbidity and mortality\"</a>. There's also an 87% number floating around without any justification or link to a study. I think <a href=\"http://www.orange-papers.org/orange-effectiveness.html\">this guy's</a> statistics on a ~5-10% yearly remission rate from willpower or AA sound plausible.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gsv9XWbZDcnZmKuqM": 1, "Wi3EopKJ2aNdtxSWg": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tEHJXNhw6t87foqJL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 46, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "2868", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EFQ3F6kmt4WHXRqik", "HmfxSWnqnK265GEFM", "Xz6pKy6sjZffy4NYW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-06T23:30:55.781Z", "modifiedAt": null, "url": null, "title": "Cognitive Bias Song", "slug": "cognitive-bias-song", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:03.731Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QZ5RNER4nbbzBGvCM/cognitive-bias-song", "pageUrlRelative": "/posts/QZ5RNER4nbbzBGvCM/cognitive-bias-song", "linkUrl": "https://www.lesswrong.com/posts/QZ5RNER4nbbzBGvCM/cognitive-bias-song", "postedAtFormatted": "Thursday, May 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cognitive%20Bias%20Song&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACognitive%20Bias%20Song%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZ5RNER4nbbzBGvCM%2Fcognitive-bias-song%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cognitive%20Bias%20Song%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZ5RNER4nbbzBGvCM%2Fcognitive-bias-song", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZ5RNER4nbbzBGvCM%2Fcognitive-bias-song", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12, "htmlBody": "<p>I will not summarize this. Or transcribe it. <a href=\"http://www.youtube.com/watch?v=3RsbmjNLQkc&amp;feature=player_embedded\">It's just funny</a>(video link).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"aLB9evWFYtfyS3WJg": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QZ5RNER4nbbzBGvCM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -1, "extendedScore": null, "score": 5.843253673741778e-07, "legacy": true, "legacyId": "2880", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-07T14:34:27.204Z", "modifiedAt": null, "url": null, "title": "Beauty quips, \"I'd shut up and multiply!\"", "slug": "beauty-quips-i-d-shut-up-and-multiply", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:37.963Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "neq1", "createdAt": "2010-03-31T12:30:01.450Z", "isAdmin": false, "displayName": "neq1"}, "userId": "YcYNXZpDcpBGgJR2u", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aQRKfzYnt3bFGgPKd/beauty-quips-i-d-shut-up-and-multiply", "pageUrlRelative": "/posts/aQRKfzYnt3bFGgPKd/beauty-quips-i-d-shut-up-and-multiply", "linkUrl": "https://www.lesswrong.com/posts/aQRKfzYnt3bFGgPKd/beauty-quips-i-d-shut-up-and-multiply", "postedAtFormatted": "Friday, May 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beauty%20quips%2C%20%22I'd%20shut%20up%20and%20multiply!%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeauty%20quips%2C%20%22I'd%20shut%20up%20and%20multiply!%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQRKfzYnt3bFGgPKd%2Fbeauty-quips-i-d-shut-up-and-multiply%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beauty%20quips%2C%20%22I'd%20shut%20up%20and%20multiply!%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQRKfzYnt3bFGgPKd%2Fbeauty-quips-i-d-shut-up-and-multiply", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQRKfzYnt3bFGgPKd%2Fbeauty-quips-i-d-shut-up-and-multiply", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1929, "htmlBody": "<p>When it comes to probability, you should trust probability laws over your intuition.&nbsp; Many people got <a href=\"http://en.wikipedia.org/wiki/Monty_Hall_problem\">the Monty Hall problem</a> wrong because their intuition was bad.&nbsp; You can get the solution to that problem using probability laws that you learned in Stats 101 -- it's not a hard problem.&nbsp; Similarly, there has been a lot of debate about <a href=\"http://en.wikipedia.org/wiki/Sleeping_Beauty_problem\">the Sleeping Beauty problem</a>.&nbsp; Again, though, that's because people are starting with their intuition instead of letting probability laws lead them to understanding.</p>\n<p><strong> The Sleeping Beauty Problem</strong></p>\n<p style=\"padding-left: 30px;\">On Sunday she is given a drug that sends her to sleep. A fair coin is then tossed just once in the course of the experiment to determine which experimental procedure is undertaken. If the coin comes up heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again. The sleeping drug induces a mild amnesia, so that she cannot remember any previous awakenings during the course of the experiment (if any). During the experiment, she has no access to anything that would give a clue as to the day of the week. However, she knows all the details of the experiment.</p>\n<p style=\"padding-left: 30px;\">Each interview consists of one question, \"What is your credence now for the proposition that our coin landed heads?\"</p>\n<p>Two popular solutions have been proposed: 1/3 and 1/2</p>\n<p><strong>The 1/3 solution</strong></p>\n<p>From wikipedia:</p>\n<p style=\"padding-left: 30px;\">Suppose this experiment were repeated 1,000 times. We would expect to get 500 heads and 500 tails. So Beauty would be awoken 500 times after heads on Monday, 500 times after tails on Monday, and 500 times after tails on Tuesday. In other words, only in a third of the cases would heads precede her awakening. So the right answer for her to give is 1/3.</p>\n<p>Yes, it's true that only in a third of cases would heads precede her awakening.</p>\n<p>Radford Neal (a statistician!) <a href=\"http://arxiv.org/PS_cache/math/pdf/0608/0608592v1.pdf\">argues</a> that 1/3 is the correct solution.</p>\n<p style=\"padding-left: 30px;\">This [the 1/3] view can be reinforced by supposing that on each awakening Beauty is offered a bet in which she wins 2 dollars if the coin lands Tails and loses 3 dollars if it lands Heads. (We suppose that Beauty knows such a bet will always be offered.) Beauty would not accept this bet if she assigns probability 1/2 to Heads. If she assigns a probability of 1/3 to Heads, however, her expected gain is 2 &times; (2/3) &minus; 3 &times; (1/3) = 1/3, so she will accept, and if the experiment is repeated many times, she will come out ahead.</p>\n<p>Neal is correct (about the gambling problem).</p>\n<p>These two arguments for the 1/3 solution appeal to intuition and make no obvious mathematical errors.&nbsp;&nbsp; So why are they wrong?</p>\n<p>Let's first start with probability laws and show why the 1/2 solution is correct. Just like with the Monty Hall problem, once you understand the solution, the wrong answer will no longer appeal to your intuition.</p>\n<p><strong>The 1/2 solution</strong></p>\n<p>P(Beauty woken up at least once| heads)=P(Beauty woken up at least once | tails)=1.&nbsp; Because of the amnesia, all Beauty knows when she is woken up is that she has woken up at least once.&nbsp; That event had the same probability of occurring under either coin outcome.&nbsp; Thus, P(heads | Beauty woken up at least once)=1/2.&nbsp; You can use Bayes' rule to see this if it's unclear.</p>\n<p>Here's another way to look at it:</p>\n<p>If it landed heads then Beauty is woken up on Monday with probability 1.</p>\n<p>If it landed tails then Beauty is woken up on Monday and Tuesday.&nbsp; From her perspective, these days are indistinguishable.&nbsp; She doesn't know if she was woken up the day before, and she doesn't know if she'll be woken up the next day.&nbsp; Thus, we can view Monday and Tuesday as exchangeable here.</p>\n<p>A probability tree can help with the intuition (this is a probability tree corresponding to an arbitrary wake up day):</p>\n<p><a href=\"http://neq1.files.wordpress.com/2010/05/probtree.jpg\"><img title=\"probtree\" src=\"http://neq1.files.wordpress.com/2010/05/probtree.jpg\" alt=\"\" width=\"300\" height=\"169\" /></a></p>\n<p>If Beauty was told the coin came up heads, then she'd know it was Monday.&nbsp; If she was told the coin came up tails, then she'd think there is a 50% chance it's Monday and a 50% chance it's Tuesday.&nbsp; Of course, when Beauty is woken up she is not told the result of the flip, but she can calculate the probability of each.</p>\n<p>When she is woken up, she's somewhere on the second set of branches.&nbsp; We have the following joint probabilities: P(heads, Monday)=1/2; P(heads, not Monday)=0; P(tails, Monday)=1/4; P(tails, Tuesday)=1/4; P(tails, not Monday or Tuesday)=0.&nbsp; Thus, P(heads)=1/2.</p>\n<p><strong>Where the 1/3 arguments fail</strong></p>\n<p>The 1/3 argument says with heads there is 1 interview, with tails there are 2 interviews, and therefore the probability of heads is 1/3.&nbsp; However, the argument would only hold <em>if all 3 interview days were equally likely</em>.&nbsp; That's not the case here. (on a wake up day, heads&amp;Monday is more likely than tails&amp;Monday, for example).</p>\n<p>Neal's argument fails because he changed the problem. \"on each awakening Beauty is offered a bet in which she wins 2 dollars if the coin lands Tails and loses 3 dollars if it lands Heads.\"&nbsp; In this scenario, she would make the bet twice if tails came up and once if heads came up.&nbsp; That has nothing to do with probability about the event at a particular awakening.&nbsp; The fact that she should take the bet doesn't imply that heads is less likely.&nbsp; Beauty just knows that she'll win the bet twice if tails landed.&nbsp; We double count for tails.</p>\n<p>Imagine I said \"if you guess heads and you're wrong nothing will happen, but if you guess tails and you're wrong I'll punch you in the stomach.\"&nbsp; In that case, you will probably guess heads.&nbsp; That doesn't mean your credence for heads is 1 -- it just means I added a greater penalty to the other option.</p>\n<p>Consider changing the problem to something more extreme.&nbsp; Here, we start with heads having probability 0.99 and tails having probability 0.01.&nbsp; If heads comes up we wake Beauty up once.&nbsp; If tails, we wake her up 100 times.&nbsp; Thirder logic would go like this:&nbsp; if we repeated the experiment 1000 times, we'd expect her woken up 990 after heads on Monday, 10 times after tails on Monday (day 1), 10 times after tails on Tues (day 2),...., 10 times after tails on day 100.&nbsp; In other words, ~50% of the cases would heads precede her awakening. So the right answer for her to give is 1/2.</p>\n<p>Of course, this would be absurd reasoning.&nbsp; Beauty knows heads has a 99% chance initially.&nbsp; But when she wakes up (which she was guaranteed to do regardless of whether heads or tails came up), she suddenly thinks they're equally likely?&nbsp; What if we made it even more extreme and woke her up even more times on tails?</p>\n<p><strong>Implausible consequence of 1/2 solution?</strong></p>\n<p>Nick Bostrom <a href=\"http://www.anthropic-principle.com/preprints/beauty/synthesis.pdf\">presents</a> the Extreme Sleeping Beauty problem:</p>\n<p style=\"padding-left: 30px;\">This is like the original problem, except that here, if the coin falls tails, Beauty will be awakened on a million subsequent days. As before, she will be given an amnesia drug each time she is put to sleep that makes her forget any previous awakenings. When she awakes on Monday, what should be her credence in HEADS?</p>\n<p>He argues:</p>\n<p style=\"padding-left: 30px;\">The adherent of the 1/2 view will maintain that Beauty, upon awakening, should retain her credence of 1/2 in HEADS, but also that, upon being informed that it is Monday, she should become extremely confident in HEADS:<br /> P+(HEADS) = 1,000,001/1,000,002</p>\n<p style=\"padding-left: 30px;\">This consequence is itself quite implausible. It is, after all, rather gutsy to have credence 0.999999% in the proposition that an unobserved fair coin will fall heads.</p>\n<p>It's correct that, upon awakening on Monday (and not knowing it's Monday), she should retain her credence of 1/2 in heads.</p>\n<p>However, if she is informed it's Monday, it's unclear what she conclude.&nbsp; Why was she informed it was Monday?&nbsp; Consider two alternatives.</p>\n<p><span style=\"text-decoration: underline;\">Disclosure process 1</span>:&nbsp; regardless of the result of the coin toss she will be informed it's Monday on Monday with probability 1</p>\n<p>Under disclosure process 1, her credence of heads on Monday is still 1/2.</p>\n<p><span style=\"text-decoration: underline;\">Disclosure process 2</span>: if heads she'll be woken up and informed that it's Monday.&nbsp; If tails, she'll be woken up on Monday and one million subsequent days, and only be told the specific day on one randomly selected day.</p>\n<p>Under disclosure process 2, if she's informed it's Monday, her credence of heads is 1,000,001/1,000,002.&nbsp; However, this is not implausible at all.&nbsp; It's correct.&nbsp; This statement is misleading: \"It is, after all, rather gutsy to have credence 0.999999% in the proposition that an unobserved fair coin will fall heads.\"&nbsp; Beauty isn't predicting what will happen on the flip of a coin, she's predicting what did happen after receiving strong evidence that it's heads.</p>\n<p><strong>ETA (5/9/2010 5:38AM)</strong></p>\n<p>If we want to replicate the situation 1000 times, we shouldn't end up with 1500 observations.&nbsp; The correct way to replicate the awakening decision is to use the probability tree I included above. You'd end up with expected cell counts of 500, 250, 250, instead of 500, 500, 500.</p>\n<p>Suppose at each awakening, we offer Beauty the following wager:&nbsp; she'd lose $1.50 if heads but win $1 if tails.&nbsp; She is asked for a decision on that wager at every awakening, but we only accept her last decision. Thus, if tails we'll accept her Tuesday decision (but won't tell her it's Tuesday). If her credence of heads is 1/3 at each awakening, then she should take the bet. If her credence of heads is 1/2 at each awakening, she shouldn't take the bet.&nbsp; If we repeat the experiment many times, she'd be expected to lose money if she accepts the bet every time.</p>\n<p>The problem with the logic that leads to the 1/3 solution is it counts twice under tails, but the question was about her credence at an awakening (interview).<strong></strong></p>\n<p><strong>ETA (5/10/2010 10:18PM ET)</strong></p>\n<p style=\"padding-left: 30px;\"><br />Suppose this experiment were repeated 1,000 times. We would expect to get 500 heads and 500 tails. So Beauty would be awoken 500 times after heads on Monday, 500 times after tails on Monday, and 500 times after tails on Tuesday. In other words, only in a third of the cases would heads precede her awakening. So the right answer for her to give is 1/3.</p>\n<p>Another way to look at it:&nbsp; the denominator is not a sum of mutually exclusive events.&nbsp; Typically we use counts to estimate probabilities as follows:&nbsp; the numerator is the number of times the event of interest occurred, and the denominator is the number of times that event could have occurred.&nbsp;</p>\n<p>For example, suppose Y can take values 1, 2 or 3 and follows a multinomial distribution with probabilities p1, p2 and p3=1-p1-p2, respectively. &nbsp; If we generate n values of Y, we could estimate p1 by taking the ratio of #{Y=1}/(#{Y=1}+#{Y=2}+#{Y=3}). As n goes to infinity, the ratio will converge to p1. &nbsp; Notice the events in the denominator are mutually exclusive and exhaustive.&nbsp; The denominator is determined by n.</p>\n<p>The thirder solution to the Sleeping Beauty problem has as its denominator sums of events that are not mutually exclusive.&nbsp; The denominator is not determined by n.&nbsp; For example, if we repeat it 1000 times, and we get 400 heads, our denominator would be 400+600+600=1600 (even though it was not possible to get 1600 heads!).&nbsp; If we instead got 550 heads, our denominator would be 550+450+450=1450.&nbsp; Our denominator is outcome dependent, where here the outcome is the occurrence of heads.&nbsp; What does this ratio converge to as n goes to infinity?&nbsp; I surely don't know.&nbsp; But I do know it's not the posterior probability of heads.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NZB24aR9uHmDc5GcT": 1, "5f5c37ee1b5cdee568cfb187": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aQRKfzYnt3bFGgPKd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 14, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "2886", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>When it comes to probability, you should trust probability laws over your intuition.&nbsp; Many people got <a href=\"http://en.wikipedia.org/wiki/Monty_Hall_problem\">the Monty Hall problem</a> wrong because their intuition was bad.&nbsp; You can get the solution to that problem using probability laws that you learned in Stats 101 -- it's not a hard problem.&nbsp; Similarly, there has been a lot of debate about <a href=\"http://en.wikipedia.org/wiki/Sleeping_Beauty_problem\">the Sleeping Beauty problem</a>.&nbsp; Again, though, that's because people are starting with their intuition instead of letting probability laws lead them to understanding.</p>\n<p><strong id=\"The_Sleeping_Beauty_Problem\"> The Sleeping Beauty Problem</strong></p>\n<p style=\"padding-left: 30px;\">On Sunday she is given a drug that sends her to sleep. A fair coin is then tossed just once in the course of the experiment to determine which experimental procedure is undertaken. If the coin comes up heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again. The sleeping drug induces a mild amnesia, so that she cannot remember any previous awakenings during the course of the experiment (if any). During the experiment, she has no access to anything that would give a clue as to the day of the week. However, she knows all the details of the experiment.</p>\n<p style=\"padding-left: 30px;\">Each interview consists of one question, \"What is your credence now for the proposition that our coin landed heads?\"</p>\n<p>Two popular solutions have been proposed: 1/3 and 1/2</p>\n<p><strong id=\"The_1_3_solution\">The 1/3 solution</strong></p>\n<p>From wikipedia:</p>\n<p style=\"padding-left: 30px;\">Suppose this experiment were repeated 1,000 times. We would expect to get 500 heads and 500 tails. So Beauty would be awoken 500 times after heads on Monday, 500 times after tails on Monday, and 500 times after tails on Tuesday. In other words, only in a third of the cases would heads precede her awakening. So the right answer for her to give is 1/3.</p>\n<p>Yes, it's true that only in a third of cases would heads precede her awakening.</p>\n<p>Radford Neal (a statistician!) <a href=\"http://arxiv.org/PS_cache/math/pdf/0608/0608592v1.pdf\">argues</a> that 1/3 is the correct solution.</p>\n<p style=\"padding-left: 30px;\">This [the 1/3] view can be reinforced by supposing that on each awakening Beauty is offered a bet in which she wins 2 dollars if the coin lands Tails and loses 3 dollars if it lands Heads. (We suppose that Beauty knows such a bet will always be offered.) Beauty would not accept this bet if she assigns probability 1/2 to Heads. If she assigns a probability of 1/3 to Heads, however, her expected gain is 2 \u00d7 (2/3) \u2212 3 \u00d7 (1/3) = 1/3, so she will accept, and if the experiment is repeated many times, she will come out ahead.</p>\n<p>Neal is correct (about the gambling problem).</p>\n<p>These two arguments for the 1/3 solution appeal to intuition and make no obvious mathematical errors.&nbsp;&nbsp; So why are they wrong?</p>\n<p>Let's first start with probability laws and show why the 1/2 solution is correct. Just like with the Monty Hall problem, once you understand the solution, the wrong answer will no longer appeal to your intuition.</p>\n<p><strong id=\"The_1_2_solution\">The 1/2 solution</strong></p>\n<p>P(Beauty woken up at least once| heads)=P(Beauty woken up at least once | tails)=1.&nbsp; Because of the amnesia, all Beauty knows when she is woken up is that she has woken up at least once.&nbsp; That event had the same probability of occurring under either coin outcome.&nbsp; Thus, P(heads | Beauty woken up at least once)=1/2.&nbsp; You can use Bayes' rule to see this if it's unclear.</p>\n<p>Here's another way to look at it:</p>\n<p>If it landed heads then Beauty is woken up on Monday with probability 1.</p>\n<p>If it landed tails then Beauty is woken up on Monday and Tuesday.&nbsp; From her perspective, these days are indistinguishable.&nbsp; She doesn't know if she was woken up the day before, and she doesn't know if she'll be woken up the next day.&nbsp; Thus, we can view Monday and Tuesday as exchangeable here.</p>\n<p>A probability tree can help with the intuition (this is a probability tree corresponding to an arbitrary wake up day):</p>\n<p><a href=\"http://neq1.files.wordpress.com/2010/05/probtree.jpg\"><img title=\"probtree\" src=\"http://neq1.files.wordpress.com/2010/05/probtree.jpg\" alt=\"\" width=\"300\" height=\"169\"></a></p>\n<p>If Beauty was told the coin came up heads, then she'd know it was Monday.&nbsp; If she was told the coin came up tails, then she'd think there is a 50% chance it's Monday and a 50% chance it's Tuesday.&nbsp; Of course, when Beauty is woken up she is not told the result of the flip, but she can calculate the probability of each.</p>\n<p>When she is woken up, she's somewhere on the second set of branches.&nbsp; We have the following joint probabilities: P(heads, Monday)=1/2; P(heads, not Monday)=0; P(tails, Monday)=1/4; P(tails, Tuesday)=1/4; P(tails, not Monday or Tuesday)=0.&nbsp; Thus, P(heads)=1/2.</p>\n<p><strong id=\"Where_the_1_3_arguments_fail\">Where the 1/3 arguments fail</strong></p>\n<p>The 1/3 argument says with heads there is 1 interview, with tails there are 2 interviews, and therefore the probability of heads is 1/3.&nbsp; However, the argument would only hold <em>if all 3 interview days were equally likely</em>.&nbsp; That's not the case here. (on a wake up day, heads&amp;Monday is more likely than tails&amp;Monday, for example).</p>\n<p>Neal's argument fails because he changed the problem. \"on each awakening Beauty is offered a bet in which she wins 2 dollars if the coin lands Tails and loses 3 dollars if it lands Heads.\"&nbsp; In this scenario, she would make the bet twice if tails came up and once if heads came up.&nbsp; That has nothing to do with probability about the event at a particular awakening.&nbsp; The fact that she should take the bet doesn't imply that heads is less likely.&nbsp; Beauty just knows that she'll win the bet twice if tails landed.&nbsp; We double count for tails.</p>\n<p>Imagine I said \"if you guess heads and you're wrong nothing will happen, but if you guess tails and you're wrong I'll punch you in the stomach.\"&nbsp; In that case, you will probably guess heads.&nbsp; That doesn't mean your credence for heads is 1 -- it just means I added a greater penalty to the other option.</p>\n<p>Consider changing the problem to something more extreme.&nbsp; Here, we start with heads having probability 0.99 and tails having probability 0.01.&nbsp; If heads comes up we wake Beauty up once.&nbsp; If tails, we wake her up 100 times.&nbsp; Thirder logic would go like this:&nbsp; if we repeated the experiment 1000 times, we'd expect her woken up 990 after heads on Monday, 10 times after tails on Monday (day 1), 10 times after tails on Tues (day 2),...., 10 times after tails on day 100.&nbsp; In other words, ~50% of the cases would heads precede her awakening. So the right answer for her to give is 1/2.</p>\n<p>Of course, this would be absurd reasoning.&nbsp; Beauty knows heads has a 99% chance initially.&nbsp; But when she wakes up (which she was guaranteed to do regardless of whether heads or tails came up), she suddenly thinks they're equally likely?&nbsp; What if we made it even more extreme and woke her up even more times on tails?</p>\n<p><strong id=\"Implausible_consequence_of_1_2_solution_\">Implausible consequence of 1/2 solution?</strong></p>\n<p>Nick Bostrom <a href=\"http://www.anthropic-principle.com/preprints/beauty/synthesis.pdf\">presents</a> the Extreme Sleeping Beauty problem:</p>\n<p style=\"padding-left: 30px;\">This is like the original problem, except that here, if the coin falls tails, Beauty will be awakened on a million subsequent days. As before, she will be given an amnesia drug each time she is put to sleep that makes her forget any previous awakenings. When she awakes on Monday, what should be her credence in HEADS?</p>\n<p>He argues:</p>\n<p style=\"padding-left: 30px;\">The adherent of the 1/2 view will maintain that Beauty, upon awakening, should retain her credence of 1/2 in HEADS, but also that, upon being informed that it is Monday, she should become extremely confident in HEADS:<br> P+(HEADS) = 1,000,001/1,000,002</p>\n<p style=\"padding-left: 30px;\">This consequence is itself quite implausible. It is, after all, rather gutsy to have credence 0.999999% in the proposition that an unobserved fair coin will fall heads.</p>\n<p>It's correct that, upon awakening on Monday (and not knowing it's Monday), she should retain her credence of 1/2 in heads.</p>\n<p>However, if she is informed it's Monday, it's unclear what she conclude.&nbsp; Why was she informed it was Monday?&nbsp; Consider two alternatives.</p>\n<p><span style=\"text-decoration: underline;\">Disclosure process 1</span>:&nbsp; regardless of the result of the coin toss she will be informed it's Monday on Monday with probability 1</p>\n<p>Under disclosure process 1, her credence of heads on Monday is still 1/2.</p>\n<p><span style=\"text-decoration: underline;\">Disclosure process 2</span>: if heads she'll be woken up and informed that it's Monday.&nbsp; If tails, she'll be woken up on Monday and one million subsequent days, and only be told the specific day on one randomly selected day.</p>\n<p>Under disclosure process 2, if she's informed it's Monday, her credence of heads is 1,000,001/1,000,002.&nbsp; However, this is not implausible at all.&nbsp; It's correct.&nbsp; This statement is misleading: \"It is, after all, rather gutsy to have credence 0.999999% in the proposition that an unobserved fair coin will fall heads.\"&nbsp; Beauty isn't predicting what will happen on the flip of a coin, she's predicting what did happen after receiving strong evidence that it's heads.</p>\n<p><strong id=\"ETA__5_9_2010_5_38AM_\">ETA (5/9/2010 5:38AM)</strong></p>\n<p>If we want to replicate the situation 1000 times, we shouldn't end up with 1500 observations.&nbsp; The correct way to replicate the awakening decision is to use the probability tree I included above. You'd end up with expected cell counts of 500, 250, 250, instead of 500, 500, 500.</p>\n<p>Suppose at each awakening, we offer Beauty the following wager:&nbsp; she'd lose $1.50 if heads but win $1 if tails.&nbsp; She is asked for a decision on that wager at every awakening, but we only accept her last decision. Thus, if tails we'll accept her Tuesday decision (but won't tell her it's Tuesday). If her credence of heads is 1/3 at each awakening, then she should take the bet. If her credence of heads is 1/2 at each awakening, she shouldn't take the bet.&nbsp; If we repeat the experiment many times, she'd be expected to lose money if she accepts the bet every time.</p>\n<p>The problem with the logic that leads to the 1/3 solution is it counts twice under tails, but the question was about her credence at an awakening (interview).<strong></strong></p>\n<p><strong id=\"ETA__5_10_2010_10_18PM_ET_\">ETA (5/10/2010 10:18PM ET)</strong></p>\n<p style=\"padding-left: 30px;\"><br>Suppose this experiment were repeated 1,000 times. We would expect to get 500 heads and 500 tails. So Beauty would be awoken 500 times after heads on Monday, 500 times after tails on Monday, and 500 times after tails on Tuesday. In other words, only in a third of the cases would heads precede her awakening. So the right answer for her to give is 1/3.</p>\n<p>Another way to look at it:&nbsp; the denominator is not a sum of mutually exclusive events.&nbsp; Typically we use counts to estimate probabilities as follows:&nbsp; the numerator is the number of times the event of interest occurred, and the denominator is the number of times that event could have occurred.&nbsp;</p>\n<p>For example, suppose Y can take values 1, 2 or 3 and follows a multinomial distribution with probabilities p1, p2 and p3=1-p1-p2, respectively. &nbsp; If we generate n values of Y, we could estimate p1 by taking the ratio of #{Y=1}/(#{Y=1}+#{Y=2}+#{Y=3}). As n goes to infinity, the ratio will converge to p1. &nbsp; Notice the events in the denominator are mutually exclusive and exhaustive.&nbsp; The denominator is determined by n.</p>\n<p>The thirder solution to the Sleeping Beauty problem has as its denominator sums of events that are not mutually exclusive.&nbsp; The denominator is not determined by n.&nbsp; For example, if we repeat it 1000 times, and we get 400 heads, our denominator would be 400+600+600=1600 (even though it was not possible to get 1600 heads!).&nbsp; If we instead got 550 heads, our denominator would be 550+450+450=1450.&nbsp; Our denominator is outcome dependent, where here the outcome is the occurrence of heads.&nbsp; What does this ratio converge to as n goes to infinity?&nbsp; I surely don't know.&nbsp; But I do know it's not the posterior probability of heads.</p>", "sections": [{"title": "The Sleeping Beauty Problem", "anchor": "The_Sleeping_Beauty_Problem", "level": 1}, {"title": "The 1/3 solution", "anchor": "The_1_3_solution", "level": 1}, {"title": "The 1/2 solution", "anchor": "The_1_2_solution", "level": 1}, {"title": "Where the 1/3 arguments fail", "anchor": "Where_the_1_3_arguments_fail", "level": 1}, {"title": "Implausible consequence of 1/2 solution?", "anchor": "Implausible_consequence_of_1_2_solution_", "level": 1}, {"title": "ETA (5/9/2010 5:38AM)", "anchor": "ETA__5_9_2010_5_38AM_", "level": 1}, {"title": "ETA (5/10/2010 10:18PM ET)", "anchor": "ETA__5_10_2010_10_18PM_ET_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "358 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 358, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-08T18:06:08.435Z", "modifiedAt": null, "url": null, "title": "What is bunk?", "slug": "what-is-bunk", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:54.412Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XiSCHS3Xu3a6EC7e6/what-is-bunk", "pageUrlRelative": "/posts/XiSCHS3Xu3a6EC7e6/what-is-bunk", "linkUrl": "https://www.lesswrong.com/posts/XiSCHS3Xu3a6EC7e6/what-is-bunk", "postedAtFormatted": "Saturday, May 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20bunk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20bunk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiSCHS3Xu3a6EC7e6%2Fwhat-is-bunk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20bunk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiSCHS3Xu3a6EC7e6%2Fwhat-is-bunk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiSCHS3Xu3a6EC7e6%2Fwhat-is-bunk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1244, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Related: <a title=\"The Correct Contrarian Cluster\" href=\"/lw/1kh/the_correct_contrarian_cluster\">http://lesswrong.com/lw/1kh/the_correct_contrarian_clust</a>er/, <a title=\"That Magical Click\" href=\"/lw/1mh/that_magical_click/\">http://lesswrong.com/lw/1mh/that_magical_click/</a>, <a title=\"Reason as Memetic Immune Disorder\" href=\"/lw/18b/reason_as_memetic_immune_disorder/ \">http://lesswrong.com/lw/18b/reason_as_memetic_immune_disorder/</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Given a claim, and assuming that its truth or falsehood would be important to you, how do you decide if it's worth investigating? &nbsp;How do you identify \"bunk\" or \"crackpot\" ideas?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Here are some examples to give an idea.&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">\"Here's a perpetual motion machine\": bunk. &nbsp;\"I've found an elementary proof of Fermat's Last Theorem\": bunk. &nbsp;\"9-11 was an inside job\": bunk. &nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">&nbsp;\"Humans did not cause global warming\": possibly bunk, but I'm not sure. &nbsp;\"The Singularity will come within 100 years\": possibly bunk, but I'm not sure. &nbsp;\"The economic system is close to collapse\": possibly bunk, but I'm not sure.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">\"There is a genetic difference in IQ between races\": I think it's probably false, but not quite bunk. &nbsp;\"Geoengineering would be effective in mitigating global warming\": I think it's probably false, but not quite bunk.&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">(These are my own examples. &nbsp;They're meant to be illustrative, not definitive. &nbsp;I imagine that some people here will think \"But that's obviously not bunk!\" &nbsp;Sure, but you probably can think of some claim that *you* consider bunk.)</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">A few notes of clarification: I'm only examining factual, not normative, claims. &nbsp;I also am not looking at well established claims (say, special relativity) which are obviously not bunk. Neither am I looking at claims where it's easy to pull data that obviously refutes them. (For example, \"There are 10 people in the US population.\") &nbsp;I'm concerned with claims that look unlikely, but not impossible. Also, \"Is this bunk?\" is not the same question as \"Is this true?\" &nbsp;A hypothesis can turn out to be false without being bunk (for example, the claim that geological formations were created by gradual processes. &nbsp;That was a respectable position for 19th century geologists to take, and a claim worth investigating, even if subsequent evidence did show it to be false.) &nbsp;The question \"Is this bunk?\" arises when someone makes an unlikely-sounding claim, but I don't actually have the knowledge right now to effectively refute it, and I want to know if the claim is a legitimate subject of inquiry or the work of a conspiracy theory/hoax/cult/crackpot. &nbsp;In other words, is it a scientific or a pseudoscientific hypothesis? &nbsp;Or, in practical terms, is it worth it for me or anybody else to investigate it?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">This is an important question, and especially to this community. &nbsp;People involved in artificial intelligence or the Singularity or existential risk are on the edge of the scientific mainstream and it's particularly crucial to distinguish an interesting hypothesis from a bunk one. &nbsp;Distinguishing an innovator from a crackpot is vital in fields where there are both innovators and crackpots.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">I claim bunk exists. That is, there are claims so cracked that they aren't worth investigating. \"I was abducted by aliens\" has such a low prior that I'm not even going to go check up on the details -- I'm simply going to assume the alleged alien abductee is a fraud or nut. &nbsp;Free speech and scientific freedom do not require us to spend resources investigating every conceivable claim. &nbsp;Some claims are so likely to be nonsense that, given limited resources, we can justifiably dismiss them.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">But how do we determine what's likely to be nonsense? &nbsp;\"I know it when I see it\" is a pretty bad guide.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">First idea: check if the proposer uses the techniques of rationality and science. &nbsp;Does he support claims with evidence? &nbsp;Does he share data and invite others to reproduce his experiments? Are there internal inconsistencies and logical fallacies in his claim? &nbsp;Does he appeal to dogma or authority? &nbsp;If there are features in the hypothesis itself that mark it as pseudoscience, then it's safely dismissed; no need to look further.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">But what if there aren't such clear warning signs? &nbsp;Our gracious host Eliezer Yudkowsky, for example, does not display those kinds of obvious tip-offs of pseudoscience -- he doesn't ask people to take things on faith, he's very alert to fallacies in reasoning, and so on. &nbsp;And yet he's making an extraordinary claim (the likelihood of the Singularity), a claim I do not have the background to evaluate, but a claim that seems implausible. &nbsp;What now? &nbsp;Is this bunk?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">A key thing to consider is the role of the \"mainstream.\" &nbsp;When a claim is out of the mainstream, are you justified in moving it closer to the bunk file? &nbsp;There are three camps I have in mind, who are outside the academic mainstream, but not obviously (to me) dismissed as bunk: global warming skeptics, Austrian economists, and singularitarians. &nbsp;As far as I can tell, the best representatives of these schools don't commit the kinds of fallacies and bad arguments of the typical pseudoscientist. &nbsp;How much should we be troubled, though, by the fact that most scientists of their disciplines shun them? &nbsp;Perhaps it's only reasonable to give some weight to that fact. &nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Or is it? If all the scientists themselves are simply making their judgments based on how mainstream the outsiders are, then \"mainstream\" status doesn't confer any information. &nbsp;The reason you listen to academic scientists is that you expect that at least some of them have investigated the claim themselves. &nbsp;We need some fraction of respected scientists -- even a small fraction -- who are crazy enough to engage even with potentially crackpot theories, if only to debunk them. &nbsp;But when they do that, don't they risk being considered crackpots themselves? &nbsp;This is some version of \"Tolerate tolerance.\" &nbsp;If you refuse to trust anybody who even considers seriously a crackpot theory, then you lose the basis on which you reject that crackpot theory. &nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">So the question \"What is bunk?\", that is, the question, \"What is likely enough to be worth investigating?\", apparently destroys itself. &nbsp;You can only tell if a claim is unlikely by doing a little investigation. &nbsp;It's probably a reflexive process: when you do a little investigation, if it's starting to look more and more like the claim is false, you can quit, but if it's the opposite, then the claim is probably worth even more investigation. &nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">The thing is, we all have different thresholds for what captures our attention and motivates us to investigate further. &nbsp;Some people are willing to do a quick Google search when somebody makes an extraordinary claim; some won't bother; some will go even further and do extensive research. &nbsp;When we check the consensus to see if a claim is considered bunk, we're acting on the hope that somebody has a lower threshold for investigation than we do. &nbsp;We hope that some poor dogged sap has spent hours diligently refuting 9-11 truthers so that we don't have to. &nbsp;From an economic perspective, this is an enormous free-rider problem, though -- who wants to be that poor dogged sap? &nbsp;The hope is that somebody, somewhere, in the human population is always inquiring enough to do at least a little preliminary investigation. &nbsp;We should thank the poor dogged saps of the world. &nbsp;We should create more incentives to be a poor dogged sap. &nbsp;Because if we don't have enough of them, we're going to be very mistaken when we think \"Well, this wasn't important enough for anyone to investigate, so it must be bunk.\"</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">(N.B. &nbsp;I am aware that many climate scientists are being \"poor dogged saps\" by communicating with and attempting to refute global warming skeptics. &nbsp;I'm not aware if there are economists who bother trying to refute Austrian economics, or if there are electrical engineers and computer scientists who spend time being Singularity skeptics.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb19d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XiSCHS3Xu3a6EC7e6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 26, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "2898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 108, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9KvefburLia7ptEE3", "R3ATEWWmBhMhbY2AL", "aHaqgTNnFzD7NGLMx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-09T05:53:54.487Z", "modifiedAt": null, "url": null, "title": "The Psychological Diversity of Mankind", "slug": "the-psychological-diversity-of-mankind", "viewCount": null, "lastCommentedAt": "2013-01-22T11:13:27.001Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2oybbEw697CQgcRE5/the-psychological-diversity-of-mankind", "pageUrlRelative": "/posts/2oybbEw697CQgcRE5/the-psychological-diversity-of-mankind", "linkUrl": "https://www.lesswrong.com/posts/2oybbEw697CQgcRE5/the-psychological-diversity-of-mankind", "postedAtFormatted": "Sunday, May 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Psychological%20Diversity%20of%20Mankind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Psychological%20Diversity%20of%20Mankind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2oybbEw697CQgcRE5%2Fthe-psychological-diversity-of-mankind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Psychological%20Diversity%20of%20Mankind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2oybbEw697CQgcRE5%2Fthe-psychological-diversity-of-mankind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2oybbEw697CQgcRE5%2Fthe-psychological-diversity-of-mankind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2205, "htmlBody": "<p>The dominant belief on this site seems to be in the \"<a href=\"/lw/rl/the_psychological_unity_of_humankind/\">psychological unity of mankind</a>\". In other words, all of humanity <a href=\"http://wiki.lesswrong.com/wiki/Human_universal\">shares</a> the same underlying psychological machinery. Furthermore, that machinery <a href=\"http://wiki.lesswrong.com/wiki/Evolutionary_psychology\">has not had the time</a> to significantly change in the 50,000 or so years that have passed after we started moving out of our ancestral environment.</p>\n<p>In <a href=\"http://www.amazon.com/000-Year-Explosion-Civilization-Accelerated/dp/0465002218\"><em>The 10,000 Year Explosion</em></a>, Gregory Cochran and Henry Harpending dispute part of this claim. While they freely admit that we have probably not had enough time to develop new complex adaptations, they emphasize the speed at which minor adaptations can spread throughout populations and have powerful effects. Their basic thesis is that the notion of a psychological unity is most likely false. Different human populations are likely for biological reasons to have slightly different minds, shaped by selection pressures in the specific regions the populations happened to live in. They build support for their claim by:</p>\n<ul>\n<li>Discussing known cases where selection has led to rapid physiological and psychological changes among animals</li>\n<li>Discussing known cases where selection has led to physiological changes among humans in the last few thousand years, as well as presenting some less certain hypotheses of this.</li>\n<li>Postulating <a href=\"http://en.wikipedia.org/wiki/Evolutionary_pressure\">selection pressures</a> that would have led to some cognitive abilities to be favored among humans.</li>\n</ul>\n<p>In what follows, I will present their case by briefly summarizing the contents of the book. Do note that I've picked the points that I found the most interesting, leaving a lot out.</p>\n<p><a id=\"more\"></a>They <strong>first chapter</strong> begins by discussing a number of interesting examples:</p>\n<ul>\n<li>Dogs were domesticated from wolves around 15,000 years ago: by now, there exists a huge variety of different dog breeds. Dogs are good at reading human voice and gestures, while wolves can't understand us at all. Male wolves pair-bond with females and put a lot of effort into helping raise their pups, but male dogs generally do not. Most of the dog breeds we know today are no more than a couple of centuries old. There is considerable psychological variance between dog breeds: in 1982-2006, there were 1,110 dog attacks in the US that were attributable to pit bull terriers, but only one attributable to Border collies. Border collies, on average, learn a new command after 5 repetitions and respond correctly 95 percent of the time, while a basset hound needs 80-100 repetitions for a 25 percent accuracy rate.</li>\n<li>A Russian scientist needed only forty years to successfully breed a domesticated fox. His foxes were friendly and enjoyed human contact, very unlike wild foxes. Their coat color also lightened, their skulls became rounder, and some of them were born with floppy ears.</li>\n<li>While 50,000 years may not be enough for new complex adaptations to develop, it is enough time for them to disappear. A useless but costly adaptation will vanish in a quick period: fish in lightless caves lose their sight over a few thousand years at most.</li>\n<li>An often-repeated claim is that there's much more within-group human genetic variation than between-group (85 and 15 percent, to be exact). While this is true, the frequently drawn conclusion, that <a href=\"http://en.wikipedia.org/wiki/Genotype-phenotype_distinction\">phenotype</a> differences between individuals would be larger than the average difference between groups, does not follow. Most (70 percent) of dog genetic variation is also within-breed. One important point is that the <em>direction</em> of the genetic differences tends to be correlated: a particular Great Dane may have a low-growth version of a certain gene while a particular Chihuahua has a high-growth version, but on the whole the Great Dane will still have more high-growth versions. Also, not all mutations have the same impact: some have practically no effect, while others have a huge one. Since the common ancestry of humans (or dogs) is so short, observable differences between populations must have evolved rapidly, which is only possible if the mutations had a strong selective advantage.</li>\n<li>There are gene variants causing observable differences in appearance between human populations, such as the ones causing light skin color or blue eyes. For such systematic differences to appear, there must have been big effects on fitness, anything up from a 2 or 3 percent increase. From the rate at which new alleles have spread, this must be the case at least for genes that determine skin color, eye color, lactose tolerance, and dry earwax.</li>\n<li>Molecular genetics has found hundreds of cases of mutations that indicate recent selection. Many of them are very<em> </em>recent. A significant number of Europeans and Chinese bear mutations that originated at about 5,500 years ago. The rate at which new mutations have been popping up and spreading over the past few thousand years is on the order of 100 times greater than the long-term rate over the past few million years.</li>\n</ul>\n<p>The <strong>second chapter</strong> of the book is devoted to a discussion about the \"big bang\" in cultural evolution that occured about 30,000 to 40,000 years ago. During that time, people began coming up with technological and social innovations at an unprecedented rate. Cave paintings, sculpture and jewelry starting showing up. Tools made during this period were manufactured using materials hundreds of miles away, when previously they had been manufactured with local materials - implying that some sort of trade or exchange developed. Humans are claimed to have been maybe 100 times as inventive than in earlier times.</p>\n<p>The authors argue that this was caused by a biological change: that genetic changes allowed for a cultural development in 40,000 BC that hadn't been possible in 100,000 BC. More specifically, they suggest that this could have been caused by interbreeding between \"modern\" humans and Neanderthals. Even though Neanderthals are viewed as cognitively less developed than modern humans, archeological evidence suggests that at least up to 100,000 years ago, they weren't seriously behind the modern humans of the time. Neanderthals also had a different way of life, being high-risk, highly cooperative hunters while the anatomically modern humans probably had a mixed diet and were more like modern hunter-gatherers. It is known that ongoing natural selection in two populations can allow for simultaenous exploration of divergent development paths. It would have been entirely possible that the anatomically modern humans interbred with Neanderthals to some degree, the Neanderthals being a source of additional genetic variance that the modern humans could have benefited from.</p>\n<p>How would this have happened? In effect, the modern humans would have had their own highly beneficial alleles, in addition to which they'd have picked up the best alleles the Neanderthals had. Out of some 20,000 Neanderthal genes, it's highly likely that at least some of them were worth having. There wasn't <em>much</em> interbreeding, so Neanderthal genes with a neutral or negative effect would have disappeared from the modern human population pretty quickly. On the other hand, a beneficial gene's chance of spreading in the population is <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">two times its fitness advantage</a>. If beneficial genes are every now and then injected to the modern human population, chances are that eventually they will end up spreading to fixation. And indeed, both skeletal and genetic evidence shows signs of Neanderthal genes. There are at least two genes, one regulating brain size that appeared about 37,000 years ago and one playing role in speech that appeared about 42,000 years ago, that could plausibly have contributed to the cultural explosion and which may have come from the Neanderthals.</p>\n<p>The <strong>third chapter</strong> discusses the effect of agriculture, which first appeared 10,000 or so years ago. 60,000 years ago, there were something like a quarter of a million modern humans. 3,000 years ago, thanks to the higher food yields allowed by agriculture, there were 60 million humans. A larger population means there's more genetic variance: mutations that had previously occurred every 10,000 years or so were now showing up every 400 years. The changed living conditions also began to select for different genes. A \"gene sweep\" is a process where beneficial alleles increase in frequency, \"sweeping through\" the population until everyone has them. Hundreds of these are still ongoing today. For European and Chinese samples, the sweeps' rate of origination peaked at about 5,000 years ago and at 8,500 years ago for one African sample. While the full functions of these alleles are still not known, it is known that most involve changes in metabolism and digestion, defenses against infectious disease, reproduction, DNA repair, or in the central nervous system.</p>\n<p>The development of agriculture led, among other things, to a different mix of foods, frequently less healthy than the one enjoyed by hunter-gatherers. For instance, vitamin D was poorly available in the new diet. However, it is also created by ultraviolet radiation from the sun interacting with our skin. After the development of agriculture, several new mutations showed up that led to people in the areas more distant from the equator having lighter skins. There is also evidence of genes that reduce the negative effects associated with e.g. carbohydrates and alcohol. Today, people descending from populations that haven't farmed as long, like Australian Aborigines and many Amerindians, have a distinctive track record of health problems when exposed to Western diets. DNA retrieved from skeletons indicates that 7,000 to 8,000 years ago, no-one in central and northern Europe had the gene for <a href=\"http://en.wikipedia.org/wiki/Lactose_intolerance\">lactose tolerance</a>. 3,000 years, about 25 percent of people in central Europe had it. Today, about 80 percent of the central and northern European population carries the gene.</p>\n<p>The <strong>fourth chapter</strong> continues to discuss mutations that have spread during the last 10,000 or so years. People in certain areas have more mutations giving them a resistance to malaria than people in others. The human skeleton has become more lightly built, more so in some populations. Skull volume has decreased apparently in all populations: in Europeans it is down 10 percent from the hight point about 20,000 years ago. For some reason, Europeans also have a lot of variety in eye and hair color, whereas most of the rest of the world has dark eyes and dark hair, implying some Europe-specific selective pressure that happened to also affect those.</p>\n<p>As for cognitive changes: there are new versions of neurotransmitter receptors and transporters. Several of the alleles have effects on serotonin. There are new, mostly regional, versions of genes that affect brain development: axon growth, synapse formation, formation of the layers of the cerebral cortex, and overall brain growth. Evidence from genes affecting both brain development and muscular strength, as well as our knowledge of the fact that humans in 100,000 BC had stronger muscles than we do have today, suggests that we may have traded off muscle strength for higher intelligence. There are also new versions of genes affecting the inner ear, implying that our hearing may still be adapting to the development of language - or that specific human populations might even be adapting to characteristics of their local languages or language families.</p>\n<p>Ruling elites have been known to have far more offspring than those of the lower classes, implying selective pressures may also have been work there. 8 percent of Ireland's male population carries an Y chromosome descending from Niall of the Nine Hostages, a high king of Ireland around AD 400. 16 million men in central Asia are direct descendants of Genghis Khan. Most interestingly, people descended from farmers and the lower classes may be less aggressive and more submissive than others. People in agricultural societies, frequently encountering lots of people, are likely to suffer a lot more from being overly aggressive than people in hunter-gatherer societies. Rulers have also always been quick to eliminate those breaking laws or otherwise opposing the current rule, selecting for submissiveness.</p>\n<p>The <strong>fifth chapter</strong> discusses various ways (trade, warfare, etc.) by which different genes have spread through the human population throughout time. The <strong>sixth chapter </strong>discusses various historical encounters between humans of different groups. Amerindians were decimated by the diseases Europeans brought with them, but the Europeans were not likewise decimated by American diseases. Many Amerindians have a very low diversity of genes regulating their immune system, while even small populations of Old Worlders have highly diverse versions of these genes. On the other hand, Europeans had for a long time difficulty penetrating into Africa, where the local inhabitants had highly evolved genetic resistances to the local diseases. Also, Indo-European languages might have spread so widely in part because an ancestor protolanguge was spoken by lactose tolerant herders. The ability to keep cattle for their milk and not just their flesh allowed the herders to support larger amounts of population per acre, therefore displacing people without lactose tolerance.</p>\n<p>The <strong>seventh chapter </strong>discusses Ashkenazi Jews, whose average IQ is around 112-115 and who are vastly overrepresented among successful scientists, among other things. However, no single statement of Jews being unusually intelligent is found anywhere in preserved classical literature. In contrast, everyone thought that classical Greeks were unusually clever. The rise in Ashkenazi intelligence seems to be a combination of interbreeding and a history of being primarily in cognitively challenging occupations. The majority of Ashkenazi jews were moneylenders by 1100, and the pattern continued for several centuries. Other Jewish populations, like the ones the living in the Islamic countries, were engaged in a variety of occupations and do not seem to have an above-average intelligence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2, "nZCb9BSnmXZXSNA2u": 2, "exZi6Bing5AiM4ZQB": 4, "4cKQgA4S7xfNeeWXg": 2, "e9wHzopbGCAFwp9Rw": 2, "5f5c37ee1b5cdee568cfb162": 4, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2oybbEw697CQgcRE5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 97, "baseScore": 129, "extendedScore": null, "score": 0.00022, "legacy": true, "legacyId": "2900", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 129, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cyj6wQLW6SeF6aGLy", "jAToJHtg39AMTAuJo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2010-05-09T05:53:54.487Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-10T13:25:41.567Z", "modifiedAt": null, "url": null, "title": "Is Google Paperclipping the Web? The Perils of Optimization by Proxy in Social Systems", "slug": "is-google-paperclipping-the-web-the-perils-of-optimization", "viewCount": null, "lastCommentedAt": "2021-11-07T06:35:12.778Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fTu69HzLSXqWgj9ib/is-google-paperclipping-the-web-the-perils-of-optimization", "pageUrlRelative": "/posts/fTu69HzLSXqWgj9ib/is-google-paperclipping-the-web-the-perils-of-optimization", "linkUrl": "https://www.lesswrong.com/posts/fTu69HzLSXqWgj9ib/is-google-paperclipping-the-web-the-perils-of-optimization", "postedAtFormatted": "Monday, May 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Google%20Paperclipping%20the%20Web%3F%20The%20Perils%20of%20Optimization%20by%20Proxy%20in%20Social%20Systems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Google%20Paperclipping%20the%20Web%3F%20The%20Perils%20of%20Optimization%20by%20Proxy%20in%20Social%20Systems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTu69HzLSXqWgj9ib%2Fis-google-paperclipping-the-web-the-perils-of-optimization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Google%20Paperclipping%20the%20Web%3F%20The%20Perils%20of%20Optimization%20by%20Proxy%20in%20Social%20Systems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTu69HzLSXqWgj9ib%2Fis-google-paperclipping-the-web-the-perils-of-optimization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTu69HzLSXqWgj9ib%2Fis-google-paperclipping-the-web-the-perils-of-optimization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2985, "htmlBody": "<p style=\"margin-bottom: 0in;\"><strong>Related to:</strong>&nbsp; <a href=\"/lw/1ws/the_importance_of_goodharts_law/\">The Importance of Goodhart's Law</a>, <a href=\"http://en.wikipedia.org/wiki/Lucas_critique\">Lucas Critique</a>, <a href=\"http://en.wikipedia.org/wiki/Campbell%27s_Law\">Campbell's Law</a></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><em>tl;dr version: The article introduces the pattern of Optimization by Proxy (OBP), which can be found in many large scale distributed systems, including human societies. The pattern occurs when a computationally limited algorithm uses a proxy property as a shortcut indicator for the presence of a hard to measure target quality. When intelligent actors with different motivations control part of the data, the existence of the algorithm reifies the proxy into a separate attribute to be manipulated with the goal of altering the algorithm's results. This concept is then applied to Google and the many ways it interacts with the various groups of actors on the web. The second part of this article contains examination of how OBP contributes towards the degrading of the content of the web, and how this relates to the <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly Artificial Intelligence</a> concept of '<a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclipping</a>'. </em></p>\n<p style=\"margin-bottom: 0cm;\"><em><br /></em></p>\n<h4 style=\"margin-bottom: 0cm;\">Introducing OBP</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">The first thing a newly-hatched herring gull does after breaking out of its shell is to peck on its mother&rsquo;s beak, which causes her to give it its first feeding. Puzzled by this apparent automatic recognition of its mother, Dutch ethologist and ornithologist Nikolaas Tinbergen conducted a <a href=\"http://dustincurtis.com/how_niko_tinbergen_reverse_engineered_the_seagull.html\">sequence of experiments</a> designed to determine what precisely it was that the newborn herring gull was attracted to. After experimenting with facsimiles of adult female herring gulls, he realized that the beak alone, without the bird, would elicit the response. Through multiple further iterations he found that the characteristics that the newborns were attracted to were thinness, elongation, redness and an area with high contrast. Thus, the birds would react much more intensely to a long red stick-like beak with painted stripes on the tip than they would to a real female herring gull. It turns out that the chicks don't have an ingrained definition of 'motherness' but rather determine their initial actions by obeying very simple rules, and are liable to radically miss the mark in the presence of objects that are explicitly designed to the specification of these rules. Objects of this class, able to dominate the attention of an animal away from the intended target were later called &lsquo;supernormal stimuli&rsquo; (or <a href=\"http://wiki.lesswrong.com/wiki/Superstimulus\">superstimuli</a>) and have been commonly observed in nature and our own human environment ever since.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Generalising the above example, we can say that Optimization by Proxy occurs when an algorithm substitutes the problem of measuring a hard to quantify attribute, with a usually co-occurring a proxy that is computationally efficient to measure.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">A similar pattern appears when algorithms intended to make optimized selections over vast sets of candidates are applied on implicitly or explicitly social systems. As long as the fundamental assumption that the proxy co-occurs with the desired property holds, the algorithm performs as intended, yielding results that to the untrained eye look like &lsquo;magic&rsquo;. Google&rsquo;s PageRank, in its <a href=\"http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf\">original incarnation</a>, aiming to optimize for page quality, does so indirectly, by data mining the link structure of the web. As the web has grown, such algorithms, and their scalability characteristics, have helped search engines dominate navigation on the web over previously dominant human-curated directories.</p>\n<p style=\"margin-bottom: 0cm;\"><a id=\"more\"></a></p>\n<p style=\"margin-bottom: 0cm;\">When there is only a single party involved in the production, filtering, and consumption of results, or when the incentives of the relevant group of actors are aligned, such as in the herring gull case, the assumption of the algorithm remains stable and its results remain reliable.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\">Effect of Other Intelligent Actors</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">When however instances of the proxy are in the control of intelligent actors that can manipulate it, and stand to benefit from distorting the results of the algorithm, then the existence of the algorithm itself and the motive distortions it creates alter the results it produces. In the case of PageRank, what we have is essentially Google acting as a singleton intermediary between two groups: content producers and consumers. Its early results owe to the fact that the link structure it crawled was effectively an unintentional byproduct of the buildup of the web. By bringing it to the attention of website owners as a distinct concept however, they have been incentivised to manipulate it separately, through techniques such as link farming, effectively making the altered websites act as supernormal stimuli for the algorithm. In this sense, the act of observation and the computation and publication of results alters that which is being observed. What follows is an arms race between the algorithm designers and the external agents, each trying to affect the algorithm&rsquo;s results in their own preferred direction, with the algorithm designers controlling the algorithm itself and malicious agents controlling part of the data it is applied on.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><img src=\"http://images.lesswrong.com/t3_28r_0.png?v=de10bbbfba3ee8f699b13c3984ab4045\" alt=\"\" width=\"659\" height=\"321\" /></p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">The above figure (original Google drawing <a href=\"http://docs.google.com/drawings/edit?id=1aMogWCQwP_tHoEN49f9pkz5abHFK1hETUan2fQ40YQ4&amp;hl=en\">here</a>) may help visualise the issue. Items that satisfy the proxy but not the target quality are called false positives. Items possessing the target quality but not the proxy become false negatives. What effectively happens when Optimization by Proxy is applied to a social system, is that malicious website owners locate the semantic gap between target quality and proxy, and aim to fit in the false positives of that mismatch. The fundamental assumption here is that since the proxy is easier to compute, it is also easier to fake. That this is not the case in NP-complete problems (while no proof of P=NP exists) may offer a glimmer of hope for the future, but current proxies are not of this class. The result is that where proxy and target quality would naturally co-occur, the arrival of the algorithm, and the distortion it introduces to the incentive structure, make the proxy and the target quality more and more distinct by way of expanding the false positives set.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\">Faking it - A Bayesian View</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">We can obtain a little more insight by considering a simple Bayesian network representation of the situation. A key guide to algorithm design is the identification of some measure that intuitively will be highly correlated with quality. In terms of PageRank in its original incarnation, the reasoning is as follows. High quality web sites will attract attention from peers who are also contributing related content. This will &ldquo;cause&rdquo; them to link into the web site under consideration. Hence if we measure the number of highly ranked web sites that link into it, this will provide us with an indication of the quality of that site. The key feature is that the causal relationship is from the underlying quality (relevance) to the indicator that is actually being measured.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">This simple model raises a number of issues with the use of proxies. Firstly, one needs to be aware that it is not just a matter of designing a smart algorithm for quantifying the proxy. One also needs to quantify the strength of association between the proxy and the underlying concept.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Secondly, unless the association is an extremely strong one, this makes use of the proxy a relatively &ldquo;lossy&rdquo; test for the underlying concept. In addition, if one is going to use the proxy for decision-making, one needs some measure of confidence in the value assigned to the strength of the relationship &ndash; a second-order probability that reflects the level of experience and consistency of the evidence that has been used to determine the strength of the relationship.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Finally, and most critically, one needs to be aware of the consequences of performing inference in the reverse causal direction. In modeling this as a Bayesian Network, we would use the conditional probability distribution p(PR | Q) as a measure of the &ldquo;strength&rdquo; of the relationship between cause and proxy (where &ldquo;PR&rdquo; is a random variable representing the value of PageRank, and &ldquo;Q&rdquo; is a random variable representing the value of the (hidden) cause, Quality). Given a particular observation of PR, what we need to determine is p(Q | PR) &ndash; the distribution over Quality given our observation on the proxy. This (in our simple model) can be determined through the application of Bayes&rsquo; rule:</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><img src=\"http://images.lesswrong.com/t3_28r_1.png?v=cb885e62afe03f9fc6d2af37c1efa38e\" alt=\"\" width=\"202\" height=\"53\" /></p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">What this is reminding us of us that the prior probability distribution on Quality is a major factor in determining its posterior following an observation on the proxy. In the case of social systems however, this prior is the very thing that is shifting.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\">Attempts to Counteract Optimization by Proxy</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">One approach by algorithm owners is to keep secret the operation of the algorithm, creating uncertainty over the effects of manipulation of the proxy. This is effectively security by obscurity and can be counteracted by dedicated interrogation of the algorithm&rsquo;s results. In the case of PageRank, a cottage industry has formed around Search Engine Optimization (SEO) and Search Engine Marketing (SEM), essentially aimed at improving a website&rsquo;s placing in search engine results, despite the secrecy of the algorithm&rsquo;s exact current operation. While a distinction can be made between black-hat and white-hat practitioners, the fact remains that the existence of these techniques is a direct result of the existence of an algorithm that optimizes by proxy. Another approach may be to use multiple proxies. This however is equivalent to using a single complex proxy. While manipulation becomes more difficult, it also becomes more profitable as less people will bother doing it.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">As a response to the various distortions and manipulations, algorithms are enriched with heuristics to identify them. This, as the arms race progresses, is hoped to converge to the point where the proxy approaches the original target more and more, and hence the external actors are forced to simulate the algorithm&rsquo;s target quality to the point where, to misquote Arthur C. Clarke, &ldquo;sufficiently advanced spam is indistinguishable from content&rdquo;. This of course would hold only if processing power were not an issue. However, if processing cost was not an issue, far more laborious algorithms could be used to evaluate the target attribute directly and if an algorithm could be made to describe the concept to the level that a human would be able to distinguish. Optimization by Proxy, being a computational shortcut, is only useful when processing power or ability to define is limited. In the case of the Web search, there is a natural asymmetry, with the manipulators able to spend many more machine- and man-hours to optimization of the result than the algorithm can spend judging the quality of any given item. Thus, algorithm designers can only afford to tackle the most broadly-occurring and easily distinguishable forms of manipulation, while knowingly ignoring the more sophisticated or obscure ones. On the other hand, the defenders of the algorithm always have the final judgment and the element of surprise on their side.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Up to this point, I have tried to more or less describe Optimization by Proxy and the results of applying it to social systems, and used Google an PageRank as a well known example for illustration purposes. The rest of this article focuses more on the effect that Google has on the Web and applies this newly introduced concept to further the understanding of that situation.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\">The Downward Spiral: Industrializing OBP Exploitation</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">While Google can and does make adjustments and corrections to its algorithms, it can only catch manipulations that are themselves highly automated such as content scraping and link farms. There have long been complaints about the ever increasing prevalence of made-for-adsense websites, affiliate marketers, and other classes of spam in search results. These are a much harder nut to crack and comes back to the original limitations of the algorithm. The idea behind made-for-adsense websites is that there is low quality human authored original content that is full of the appropriate keywords, and which serves adsense advertisements. The goal is twofold: First to draw traffic into the website by ranking highly for the relevant searches, and secondly to funnel as many of these visitors to the advertisers as possible, therefore maximising revenue.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Optimization by Proxy here can be seen occurring at least thrice: First of all it is exploited as a way of gaining prevalence in search results using the above mentioned mechanisms. Secondly, the fact that the users' only relevance metric, other than search ranking, is the title and a short snippet, can mislead users into clicking through. If the title is closely related to their search query, and the snippet seems relevant and mentions the right keywords, the users will trust this proxy when the actual quality of the content that awaits them on the other side is substandard. Finally, advertisers will have their ads being placed on low quality websites that are selected by keyword, when perhaps they would not have preferred that their brand is related with borderline spam websites. This triple occurrence of Optimization by Proxy creates a self-reinforcing cycle where the made-for-adsense website owners are rewarded with cold hard cash for their efforts. What's worse, this cash flow has been effectively subtracted from the potential gains of legitimate content producers. One can say that the existence of Google search/adsense/adwords makes all this commerce possible in the first place, but this does not make the downward spiral of inefficiency disappear. Adding to this the related scourge of affiliate marketers only accelerates the disintegration of quality results.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">An interesting characteristic of this problem is that it targets less savvy users, as they are the most likely to make the most generic queries, be unable to distinguish a trusted from an untrusted source, and click on ads. This means that those with the understanding of the underlying mechanics are actually largely shielded from realising the true extent of the problem.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Its effectiveness has inevitably led to an industrialisation of the technique, with content farms such as Demand Media which pays about $5 per article and expects its authors to research and produce 5 articles an hour(!). It also pays film directors for short videos and has become by far the largest contributor to YouTube. Its method relies on purchasing search logs from ISPs and data mining those and other data sets for profitable niche keywords to produce content on. Demand Media is so wildly profitable that there is talk of an IPO, and it is obviously not the only player in this space. No matter what improvements Google makes on their algorithm short of aggressively delisting such websites (which it hasn't been willing to do thus far), the algorithm is unable to distinguish between low quality and high quality material as previously discussed. The result is crowding out of high quality websites in favour of producers of industrialised content that is designed to just barely evade the spam filters.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\">Conclusion<br /></h4>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">What we have seen is that a reliance on a less than accurate proxy has led to vast changes in the very structure and content of the web, even when the algorithms applied are less intelligent than a human and are constantly supervised and corrected by experts. All this in my mind drives home the fundamental message of FAI. While descriptions of FAI have thus far referred to thought experiments such as paperclipping, real examples, albeit in scale, are all around us. In our example, the algorithm is getting supervised by at least four distinct groups of people (Google, advertisers, content producers, consumers) and still its effects are hard to contain due to the entangled incentives of the actors. Its skewed value system is derailing the web contrary to the desires of most of the participants (except for the manipulators, I guess). For PageRank a positive is a positive whereas the difference between true and false positive is only apparent to us humans. Beyond PageRank, I feel this pattern has applicability in many areas of everyday life, especially those related to large organizations, such as employers judging potential employees by the name of the university they attended, companies rewarding staff, especially in sales, with a productivity bonus, academic funding bodies allocating funds according to bibliometrics, or even LessWrong karma when seens as an authority metric. Since my initial observation of this pattern I have been seeing it in more and more and now consider it one of my basic 'models', in the sense that <a href=\"http://ycombinator.com/munger.html\">Charlie Munger uses the term</a>.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">While I have more written material on this subject, especially on possible methods of counteracting this effect, I think this article has gone on way too long, and I'd like to see the LessWrong community's feedback before possibly proceeding. This is a still developing concept in my mind and my principle motivation for posting it here is to solicit feedback.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><em><strong>Disclaimer:</strong> Large parts of the above material have been <a href=\"http://journal.webscience.org/384/2/websci10_submission_98.pdf\">published</a> at the recent Web Science '10 conference. Also parts have been co-written with my PhD supervisor Prof. Paul Krause. Especially the Bayesian section is essentially written by him. </em></p>\n<p style=\"margin-bottom: 0cm;\"><em>I should also probably say that, contrary to what you might expect, Google is one of the technology companies I most respect. Their success and principled application of technology has just happened to make them a fantastic example for the concept I am trying to communicate.</em></p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><strong>Update(s):</strong> The number of updates has gotten a bit unwieldy, so I just collapsed them all here. To summarize, there have been numerous changes throughout the article over the last few days as a response to the fantastic feedback throughout the comments here and elsewhere. Beyond the added links at the top on prior statements of the same principle in other fields, here is also a <a href=\"http://hamstermotor.motime.com/post/683104/the-future-of-spam-an-information-theoretic-argument-%252Arestored%252A\">very interesting article</a> on the construction of spam, with a similar conclusion. Also, I hear from the comments that the book <a href=\"http://www.amazon.com/dp/0932633366\">Measuring and Managing Performance in Organizations</a> touches on the same issue in the context of people's behaviour in corporate environments.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><strong>Followup on the Web:&nbsp; </strong>Since I am keeping my ears on the ground, here I will try to maintain a list of articles and discussions that refer to this article. I don't necessarily agree with the contents, but I will keep them here for future reference.</p>\n<ul>\n<li><a href=\"http://news.ycombinator.com/item?id=1339704\">\"Sufficiently advanced spam is indistinguishable from content\"</a> - Hacker News discussion</li>\n<li><a href=\"http://www.seobook.com/how-fix-broken-link-graph\">How to Fix the Broken Link Graph</a> - seobook.com</li>\n<li><a href=\"http://www.kuro5hin.org/story/2010/5/12/33028/7201\">Divergence of Quality from its Correlates</a> - kuro5hin.org</li>\n<li><a href=\"http://strategy.channelfireball.com/featured-articles/rule-of-law-obp-and-the-mulligan-decision/\">Rule of Law - OBP and The Mulligan Decision</a> - strategy.channelfireball.com</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PvridmTCj2qsugQCH": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fTu69HzLSXqWgj9ib", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 56, "extendedScore": null, "score": 9.9e-05, "legacy": true, "legacyId": "2907", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"margin-bottom: 0in;\"><strong>Related to:</strong>&nbsp; <a href=\"/lw/1ws/the_importance_of_goodharts_law/\">The Importance of Goodhart's Law</a>, <a href=\"http://en.wikipedia.org/wiki/Lucas_critique\">Lucas Critique</a>, <a href=\"http://en.wikipedia.org/wiki/Campbell%27s_Law\">Campbell's Law</a></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><em>tl;dr version: The article introduces the pattern of Optimization by Proxy (OBP), which can be found in many large scale distributed systems, including human societies. The pattern occurs when a computationally limited algorithm uses a proxy property as a shortcut indicator for the presence of a hard to measure target quality. When intelligent actors with different motivations control part of the data, the existence of the algorithm reifies the proxy into a separate attribute to be manipulated with the goal of altering the algorithm's results. This concept is then applied to Google and the many ways it interacts with the various groups of actors on the web. The second part of this article contains examination of how OBP contributes towards the degrading of the content of the web, and how this relates to the <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly Artificial Intelligence</a> concept of '<a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclipping</a>'. </em></p>\n<p style=\"margin-bottom: 0cm;\"><em><br></em></p>\n<h4 style=\"margin-bottom: 0cm;\" id=\"Introducing_OBP\">Introducing OBP</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">The first thing a newly-hatched herring gull does after breaking out of its shell is to peck on its mother\u2019s beak, which causes her to give it its first feeding. Puzzled by this apparent automatic recognition of its mother, Dutch ethologist and ornithologist Nikolaas Tinbergen conducted a <a href=\"http://dustincurtis.com/how_niko_tinbergen_reverse_engineered_the_seagull.html\">sequence of experiments</a> designed to determine what precisely it was that the newborn herring gull was attracted to. After experimenting with facsimiles of adult female herring gulls, he realized that the beak alone, without the bird, would elicit the response. Through multiple further iterations he found that the characteristics that the newborns were attracted to were thinness, elongation, redness and an area with high contrast. Thus, the birds would react much more intensely to a long red stick-like beak with painted stripes on the tip than they would to a real female herring gull. It turns out that the chicks don't have an ingrained definition of 'motherness' but rather determine their initial actions by obeying very simple rules, and are liable to radically miss the mark in the presence of objects that are explicitly designed to the specification of these rules. Objects of this class, able to dominate the attention of an animal away from the intended target were later called \u2018supernormal stimuli\u2019 (or <a href=\"http://wiki.lesswrong.com/wiki/Superstimulus\">superstimuli</a>) and have been commonly observed in nature and our own human environment ever since.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Generalising the above example, we can say that Optimization by Proxy occurs when an algorithm substitutes the problem of measuring a hard to quantify attribute, with a usually co-occurring a proxy that is computationally efficient to measure.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">A similar pattern appears when algorithms intended to make optimized selections over vast sets of candidates are applied on implicitly or explicitly social systems. As long as the fundamental assumption that the proxy co-occurs with the desired property holds, the algorithm performs as intended, yielding results that to the untrained eye look like \u2018magic\u2019. Google\u2019s PageRank, in its <a href=\"http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf\">original incarnation</a>, aiming to optimize for page quality, does so indirectly, by data mining the link structure of the web. As the web has grown, such algorithms, and their scalability characteristics, have helped search engines dominate navigation on the web over previously dominant human-curated directories.</p>\n<p style=\"margin-bottom: 0cm;\"><a id=\"more\"></a></p>\n<p style=\"margin-bottom: 0cm;\">When there is only a single party involved in the production, filtering, and consumption of results, or when the incentives of the relevant group of actors are aligned, such as in the herring gull case, the assumption of the algorithm remains stable and its results remain reliable.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\" id=\"Effect_of_Other_Intelligent_Actors\">Effect of Other Intelligent Actors</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">When however instances of the proxy are in the control of intelligent actors that can manipulate it, and stand to benefit from distorting the results of the algorithm, then the existence of the algorithm itself and the motive distortions it creates alter the results it produces. In the case of PageRank, what we have is essentially Google acting as a singleton intermediary between two groups: content producers and consumers. Its early results owe to the fact that the link structure it crawled was effectively an unintentional byproduct of the buildup of the web. By bringing it to the attention of website owners as a distinct concept however, they have been incentivised to manipulate it separately, through techniques such as link farming, effectively making the altered websites act as supernormal stimuli for the algorithm. In this sense, the act of observation and the computation and publication of results alters that which is being observed. What follows is an arms race between the algorithm designers and the external agents, each trying to affect the algorithm\u2019s results in their own preferred direction, with the algorithm designers controlling the algorithm itself and malicious agents controlling part of the data it is applied on.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><img src=\"http://images.lesswrong.com/t3_28r_0.png?v=de10bbbfba3ee8f699b13c3984ab4045\" alt=\"\" width=\"659\" height=\"321\"></p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">The above figure (original Google drawing <a href=\"http://docs.google.com/drawings/edit?id=1aMogWCQwP_tHoEN49f9pkz5abHFK1hETUan2fQ40YQ4&amp;hl=en\">here</a>) may help visualise the issue. Items that satisfy the proxy but not the target quality are called false positives. Items possessing the target quality but not the proxy become false negatives. What effectively happens when Optimization by Proxy is applied to a social system, is that malicious website owners locate the semantic gap between target quality and proxy, and aim to fit in the false positives of that mismatch. The fundamental assumption here is that since the proxy is easier to compute, it is also easier to fake. That this is not the case in NP-complete problems (while no proof of P=NP exists) may offer a glimmer of hope for the future, but current proxies are not of this class. The result is that where proxy and target quality would naturally co-occur, the arrival of the algorithm, and the distortion it introduces to the incentive structure, make the proxy and the target quality more and more distinct by way of expanding the false positives set.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\" id=\"Faking_it___A_Bayesian_View\">Faking it - A Bayesian View</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">We can obtain a little more insight by considering a simple Bayesian network representation of the situation. A key guide to algorithm design is the identification of some measure that intuitively will be highly correlated with quality. In terms of PageRank in its original incarnation, the reasoning is as follows. High quality web sites will attract attention from peers who are also contributing related content. This will \u201ccause\u201d them to link into the web site under consideration. Hence if we measure the number of highly ranked web sites that link into it, this will provide us with an indication of the quality of that site. The key feature is that the causal relationship is from the underlying quality (relevance) to the indicator that is actually being measured.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">This simple model raises a number of issues with the use of proxies. Firstly, one needs to be aware that it is not just a matter of designing a smart algorithm for quantifying the proxy. One also needs to quantify the strength of association between the proxy and the underlying concept.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Secondly, unless the association is an extremely strong one, this makes use of the proxy a relatively \u201clossy\u201d test for the underlying concept. In addition, if one is going to use the proxy for decision-making, one needs some measure of confidence in the value assigned to the strength of the relationship \u2013 a second-order probability that reflects the level of experience and consistency of the evidence that has been used to determine the strength of the relationship.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Finally, and most critically, one needs to be aware of the consequences of performing inference in the reverse causal direction. In modeling this as a Bayesian Network, we would use the conditional probability distribution p(PR | Q) as a measure of the \u201cstrength\u201d of the relationship between cause and proxy (where \u201cPR\u201d is a random variable representing the value of PageRank, and \u201cQ\u201d is a random variable representing the value of the (hidden) cause, Quality). Given a particular observation of PR, what we need to determine is p(Q | PR) \u2013 the distribution over Quality given our observation on the proxy. This (in our simple model) can be determined through the application of Bayes\u2019 rule:</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><img src=\"http://images.lesswrong.com/t3_28r_1.png?v=cb885e62afe03f9fc6d2af37c1efa38e\" alt=\"\" width=\"202\" height=\"53\"></p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">What this is reminding us of us that the prior probability distribution on Quality is a major factor in determining its posterior following an observation on the proxy. In the case of social systems however, this prior is the very thing that is shifting.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\" id=\"Attempts_to_Counteract_Optimization_by_Proxy\">Attempts to Counteract Optimization by Proxy</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">One approach by algorithm owners is to keep secret the operation of the algorithm, creating uncertainty over the effects of manipulation of the proxy. This is effectively security by obscurity and can be counteracted by dedicated interrogation of the algorithm\u2019s results. In the case of PageRank, a cottage industry has formed around Search Engine Optimization (SEO) and Search Engine Marketing (SEM), essentially aimed at improving a website\u2019s placing in search engine results, despite the secrecy of the algorithm\u2019s exact current operation. While a distinction can be made between black-hat and white-hat practitioners, the fact remains that the existence of these techniques is a direct result of the existence of an algorithm that optimizes by proxy. Another approach may be to use multiple proxies. This however is equivalent to using a single complex proxy. While manipulation becomes more difficult, it also becomes more profitable as less people will bother doing it.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">As a response to the various distortions and manipulations, algorithms are enriched with heuristics to identify them. This, as the arms race progresses, is hoped to converge to the point where the proxy approaches the original target more and more, and hence the external actors are forced to simulate the algorithm\u2019s target quality to the point where, to misquote Arthur C. Clarke, \u201csufficiently advanced spam is indistinguishable from content\u201d. This of course would hold only if processing power were not an issue. However, if processing cost was not an issue, far more laborious algorithms could be used to evaluate the target attribute directly and if an algorithm could be made to describe the concept to the level that a human would be able to distinguish. Optimization by Proxy, being a computational shortcut, is only useful when processing power or ability to define is limited. In the case of the Web search, there is a natural asymmetry, with the manipulators able to spend many more machine- and man-hours to optimization of the result than the algorithm can spend judging the quality of any given item. Thus, algorithm designers can only afford to tackle the most broadly-occurring and easily distinguishable forms of manipulation, while knowingly ignoring the more sophisticated or obscure ones. On the other hand, the defenders of the algorithm always have the final judgment and the element of surprise on their side.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Up to this point, I have tried to more or less describe Optimization by Proxy and the results of applying it to social systems, and used Google an PageRank as a well known example for illustration purposes. The rest of this article focuses more on the effect that Google has on the Web and applies this newly introduced concept to further the understanding of that situation.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\" id=\"The_Downward_Spiral__Industrializing_OBP_Exploitation\">The Downward Spiral: Industrializing OBP Exploitation</h4>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">While Google can and does make adjustments and corrections to its algorithms, it can only catch manipulations that are themselves highly automated such as content scraping and link farms. There have long been complaints about the ever increasing prevalence of made-for-adsense websites, affiliate marketers, and other classes of spam in search results. These are a much harder nut to crack and comes back to the original limitations of the algorithm. The idea behind made-for-adsense websites is that there is low quality human authored original content that is full of the appropriate keywords, and which serves adsense advertisements. The goal is twofold: First to draw traffic into the website by ranking highly for the relevant searches, and secondly to funnel as many of these visitors to the advertisers as possible, therefore maximising revenue.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Optimization by Proxy here can be seen occurring at least thrice: First of all it is exploited as a way of gaining prevalence in search results using the above mentioned mechanisms. Secondly, the fact that the users' only relevance metric, other than search ranking, is the title and a short snippet, can mislead users into clicking through. If the title is closely related to their search query, and the snippet seems relevant and mentions the right keywords, the users will trust this proxy when the actual quality of the content that awaits them on the other side is substandard. Finally, advertisers will have their ads being placed on low quality websites that are selected by keyword, when perhaps they would not have preferred that their brand is related with borderline spam websites. This triple occurrence of Optimization by Proxy creates a self-reinforcing cycle where the made-for-adsense website owners are rewarded with cold hard cash for their efforts. What's worse, this cash flow has been effectively subtracted from the potential gains of legitimate content producers. One can say that the existence of Google search/adsense/adwords makes all this commerce possible in the first place, but this does not make the downward spiral of inefficiency disappear. Adding to this the related scourge of affiliate marketers only accelerates the disintegration of quality results.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">An interesting characteristic of this problem is that it targets less savvy users, as they are the most likely to make the most generic queries, be unable to distinguish a trusted from an untrusted source, and click on ads. This means that those with the understanding of the underlying mechanics are actually largely shielded from realising the true extent of the problem.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Its effectiveness has inevitably led to an industrialisation of the technique, with content farms such as Demand Media which pays about $5 per article and expects its authors to research and produce 5 articles an hour(!). It also pays film directors for short videos and has become by far the largest contributor to YouTube. Its method relies on purchasing search logs from ISPs and data mining those and other data sets for profitable niche keywords to produce content on. Demand Media is so wildly profitable that there is talk of an IPO, and it is obviously not the only player in this space. No matter what improvements Google makes on their algorithm short of aggressively delisting such websites (which it hasn't been willing to do thus far), the algorithm is unable to distinguish between low quality and high quality material as previously discussed. The result is crowding out of high quality websites in favour of producers of industrialised content that is designed to just barely evade the spam filters.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h4 style=\"margin-bottom: 0cm;\" id=\"Conclusion\">Conclusion<br></h4>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">What we have seen is that a reliance on a less than accurate proxy has led to vast changes in the very structure and content of the web, even when the algorithms applied are less intelligent than a human and are constantly supervised and corrected by experts. All this in my mind drives home the fundamental message of FAI. While descriptions of FAI have thus far referred to thought experiments such as paperclipping, real examples, albeit in scale, are all around us. In our example, the algorithm is getting supervised by at least four distinct groups of people (Google, advertisers, content producers, consumers) and still its effects are hard to contain due to the entangled incentives of the actors. Its skewed value system is derailing the web contrary to the desires of most of the participants (except for the manipulators, I guess). For PageRank a positive is a positive whereas the difference between true and false positive is only apparent to us humans. Beyond PageRank, I feel this pattern has applicability in many areas of everyday life, especially those related to large organizations, such as employers judging potential employees by the name of the university they attended, companies rewarding staff, especially in sales, with a productivity bonus, academic funding bodies allocating funds according to bibliometrics, or even LessWrong karma when seens as an authority metric. Since my initial observation of this pattern I have been seeing it in more and more and now consider it one of my basic 'models', in the sense that <a href=\"http://ycombinator.com/munger.html\">Charlie Munger uses the term</a>.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">While I have more written material on this subject, especially on possible methods of counteracting this effect, I think this article has gone on way too long, and I'd like to see the LessWrong community's feedback before possibly proceeding. This is a still developing concept in my mind and my principle motivation for posting it here is to solicit feedback.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><em><strong>Disclaimer:</strong> Large parts of the above material have been <a href=\"http://journal.webscience.org/384/2/websci10_submission_98.pdf\">published</a> at the recent Web Science '10 conference. Also parts have been co-written with my PhD supervisor Prof. Paul Krause. Especially the Bayesian section is essentially written by him. </em></p>\n<p style=\"margin-bottom: 0cm;\"><em>I should also probably say that, contrary to what you might expect, Google is one of the technology companies I most respect. Their success and principled application of technology has just happened to make them a fantastic example for the concept I am trying to communicate.</em></p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><strong>Update(s):</strong> The number of updates has gotten a bit unwieldy, so I just collapsed them all here. To summarize, there have been numerous changes throughout the article over the last few days as a response to the fantastic feedback throughout the comments here and elsewhere. Beyond the added links at the top on prior statements of the same principle in other fields, here is also a <a href=\"http://hamstermotor.motime.com/post/683104/the-future-of-spam-an-information-theoretic-argument-%252Arestored%252A\">very interesting article</a> on the construction of spam, with a similar conclusion. Also, I hear from the comments that the book <a href=\"http://www.amazon.com/dp/0932633366\">Measuring and Managing Performance in Organizations</a> touches on the same issue in the context of people's behaviour in corporate environments.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\"><strong>Followup on the Web:&nbsp; </strong>Since I am keeping my ears on the ground, here I will try to maintain a list of articles and discussions that refer to this article. I don't necessarily agree with the contents, but I will keep them here for future reference.</p>\n<ul>\n<li><a href=\"http://news.ycombinator.com/item?id=1339704\">\"Sufficiently advanced spam is indistinguishable from content\"</a> - Hacker News discussion</li>\n<li><a href=\"http://www.seobook.com/how-fix-broken-link-graph\">How to Fix the Broken Link Graph</a> - seobook.com</li>\n<li><a href=\"http://www.kuro5hin.org/story/2010/5/12/33028/7201\">Divergence of Quality from its Correlates</a> - kuro5hin.org</li>\n<li><a href=\"http://strategy.channelfireball.com/featured-articles/rule-of-law-obp-and-the-mulligan-decision/\">Rule of Law - OBP and The Mulligan Decision</a> - strategy.channelfireball.com</li>\n</ul>", "sections": [{"title": "Introducing OBP", "anchor": "Introducing_OBP", "level": 1}, {"title": "Effect of Other Intelligent Actors", "anchor": "Effect_of_Other_Intelligent_Actors", "level": 1}, {"title": "Faking it - A Bayesian View", "anchor": "Faking_it___A_Bayesian_View", "level": 1}, {"title": "Attempts to Counteract Optimization by Proxy", "anchor": "Attempts_to_Counteract_Optimization_by_Proxy", "level": 1}, {"title": "The Downward Spiral: Industrializing OBP Exploitation", "anchor": "The_Downward_Spiral__Industrializing_OBP_Exploitation", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "105 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 105, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YtvZxRpZjcFNwJecS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-10T23:01:29.986Z", "modifiedAt": null, "url": null, "title": "Q&A with Harpending and Cochran", "slug": "q-and-a-with-harpending-and-cochran", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:03.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jo4ExrJxF6rm8cm3k/q-and-a-with-harpending-and-cochran", "pageUrlRelative": "/posts/Jo4ExrJxF6rm8cm3k/q-and-a-with-harpending-and-cochran", "linkUrl": "https://www.lesswrong.com/posts/Jo4ExrJxF6rm8cm3k/q-and-a-with-harpending-and-cochran", "postedAtFormatted": "Monday, May 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20Harpending%20and%20Cochran&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20Harpending%20and%20Cochran%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJo4ExrJxF6rm8cm3k%2Fq-and-a-with-harpending-and-cochran%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20Harpending%20and%20Cochran%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJo4ExrJxF6rm8cm3k%2Fq-and-a-with-harpending-and-cochran", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJo4ExrJxF6rm8cm3k%2Fq-and-a-with-harpending-and-cochran", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><strong>Edit: Q&amp;A is now closed. Thanks to everyone for participating, and thanks very much to Harpending and Cochran for their responses.</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">In response to <a href=\"/lw/28k/the_psychological_diversity_of_mankind/\">Kaj's review</a>,&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a>&nbsp;and&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>,&nbsp;the authors of the The 10,000 Year Explosion, have agreed to a Q&amp;A session with the Less Wrong community.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">If you have any&nbsp;<strong style=\"font-weight: bold;\">questions for either Harpending or Cochran</strong>, please&nbsp;<strong style=\"font-weight: bold;\">reply</strong>&nbsp;to this post with a question addressed to one or both of them. Material for questions might be derived from&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://the10000yearexplosion.com/\">their blog for the book</a>&nbsp;which includes stories about&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://the10000yearexplosion.com/henry-and-the-cape-buffalo/\">hunting animals in Africa</a>&nbsp;with an eye towards evolutionary implications (which rose to Jennifer's attention based on&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://isteve.blogspot.com/2009/01/henry-harpending-on-how-not-to-hunt.html\">Steve Sailer's prior attention</a>).</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Please do not&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://en.wiktionary.org/wiki/kibitz\">kibitz</a>&nbsp;in this Q&amp;A... instead go to&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"/lw/28k/the_psychological_diversity_of_mankind/1zkp\">the kibitzing area</a>&nbsp;to talk about the Q&amp;A session itself. Eventually, this post will be edited to note that the process has been closed, at which time there should be no new questions.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"e9wHzopbGCAFwp9Rw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jo4ExrJxF6rm8cm3k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 33, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "2909", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2oybbEw697CQgcRE5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-10T23:55:45.936Z", "modifiedAt": null, "url": null, "title": "Do you have High-Functioning Asperger's Syndrome?", "slug": "do-you-have-high-functioning-asperger-s-syndrome", "viewCount": null, "lastCommentedAt": "2016-04-16T23:51:26.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vHDk5xr9JDC64rb8T/do-you-have-high-functioning-asperger-s-syndrome", "pageUrlRelative": "/posts/vHDk5xr9JDC64rb8T/do-you-have-high-functioning-asperger-s-syndrome", "linkUrl": "https://www.lesswrong.com/posts/vHDk5xr9JDC64rb8T/do-you-have-high-functioning-asperger-s-syndrome", "postedAtFormatted": "Monday, May 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20you%20have%20High-Functioning%20Asperger's%20Syndrome%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20you%20have%20High-Functioning%20Asperger's%20Syndrome%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHDk5xr9JDC64rb8T%2Fdo-you-have-high-functioning-asperger-s-syndrome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20you%20have%20High-Functioning%20Asperger's%20Syndrome%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHDk5xr9JDC64rb8T%2Fdo-you-have-high-functioning-asperger-s-syndrome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHDk5xr9JDC64rb8T%2Fdo-you-have-high-functioning-asperger-s-syndrome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 845, "htmlBody": "<p>&nbsp;</p>\n<hr />\n<p><strong>EDIT</strong>: To combat <a href=\"http://en.wikipedia.org/wiki/Opinion_poll#Nonresponse_bias\">nonresponse bias</a>, I'd appreciate it if anyone who looked at this post before and decided not to fill in the poll would go and <a href=\"http://rmijic.questionform.com/public/Aspbergers-Syndrome\">do so now</a>, but that people who haven't already considered and decided against filling in the poll <em>refrain from doing so</em>. We might get some idea of which way the bias points by looking at the difference in results.</p>\n<p>&nbsp;</p>\n<p><strong>This is your opportunity to help your community's social epistemology!</strong></p>\n<p><strong> \n<hr />\n<br /></strong></p>\n<p>&nbsp;</p>\n<p>There is <a href=\"/lw/28k/the_psychological_diversity_of_mankind/1zet\">some evidence</a> that consequentialist/utilitarian thinking is more common in people with Asperger's syndrome, so I thought it would be interesting to follow that correlation the other way around: what fraction of people who are attracted to rational/consequentialist thinking have what one might call \"High-functioning Asperger's Syndrome\"? From <a href=\"http://www.wisegeek.com/what-are-the-main-symptoms-of-aspergers-syndrome-in-adults.htm\">wisegeek</a>:</p>\n<p style=\"padding-left: 30px;\"><em>Impaired social reactions are a key component of Asperger's syndrome. People who suffer from this condition find it difficult to develop meaningful relationships with their peers. They struggle to understand the subtleties of communicating through eye contact, body language, or facial expressions and seldom show affection towards others. They are often accused of being disrespectful and rude, since they find they can&rsquo;t comprehend expectations of appropriate social behavior and are often unable to determine the feelings of those around them. People suffering from Asperger's syndrome can be said to lack both social and emotional reciprocity.</em></p>\n<p><em></em></p>\n<p style=\"padding-left: 30px;\"><em>Although Asperger's syndrome is related to autism, people who suffer from this condition do not have other developmental delays. They have normal to above average intelligence and fail to meet the diagnostic criteria for any other pervasive developmental disorder. In fact, people with Asperger's syndrome often show intense focus, highly logical thinking, and exceptional abilities in math or science.</em></p>\n<p><em></em></p>\n<p><a href=\"http://www.amazon.com/Solutions-Adults-Aspergers-Syndrome-Maximizing/dp/1592331645\">This book</a> makes the following point about \"High-functioning adults\":</p>\n<p style=\"padding-left: 30px;\"><em>\"Individuals at the most able end of the autistic spectrum have the most hidden form of this disorder, and as a result, these individuals and their family are often the most disadvantaged in terms of getting a diagnosis. Because they have higher IQs, high-functioning adults are able to work out ways to compensate for their difficulties in communication or in social functioning that are based on logical reasoning.\" </em></p>\n<p>So if you are a very smart AS person, it might not be obvious that you have it, especially because if you have difficulty reading social situations you might not realize that you are having difficulty reading social situations, rather you'll just experience other people being mean and think that the world is just full of mean people. But there are some clues you can follow. For example <a href=\"http://www.aspergerssyndrome.net/\">this website</a> talks about what AS in kids tends to be like:</p>\n<p style=\"padding-left: 30px;\"><em>One of the most disturbing aspects of Higher Functioning children with Aspergers (HFA) is their clumsy, nerdish social skills. Though they want to be accepted by their peers, they tend to be very hurt and frustrated by their lack of social success. Their ability to respond is confounded by the negative feedback that these children get from their painful social interactions. This greatly magnifies their social problems. Like any of us, when we get negative feedback, we become unhappy. This further inhibits their social skills, and a vicious circle develops.</em></p>\n<p>If your childhood involved extreme trouble with other kids, getting bullied, picked last for sports team, etc, but not for an obvious reason such as being very fat or of a racial minority, then add some evidence-points to the \"AS\" hypothesis.</p>\n<p>High-functioning AS gives a person a combination of strengths and weaknesses. If you know about the weaknesses, you can probably better compensate for them. For reference, the following are the <a href=\"http://en.wikipedia.org/wiki/Christopher_Gillberg#Gillberg.27s_criteria_for_Asperger_syndrome\">Gillberg diagnostic criteria</a> for Asperger Syndrome:</p>\n<p style=\"padding-left: 30px;\"><em>1.</em><em>Severe impairment in reciprocal social interaction </em>(at least two of the following)<br /> (a) inability to interact with peers, (b) lack of desire to interact with peers, (c) lack of appreciation of social cues, (d) socially and emotionally inappropriate behavior<br /> <br /> <em>2.All-absorbing narrow interest</em> (at least one of the following)<br /> (a) exclusion of other activities, (b) repetitive adherence, (c) more rote than meaning<br /> <br /> <em>3.Imposition of routines and interests</em> (at least one of the following)<br /> (a) on self, in aspects of life (b) on others<br /> <br /> <em>4.Speech and language problems</em> (at least three of the following)<br /> (a) delayed development, (b) superficially perfect expressive language, (c) formal, pedantic language, (d) odd prosody, peculiar voice characteristics, (e) impairment of comprehension including misinterpretations of literal/implied meanings<br /> <br /> <em>5.Non-verbal communication problems </em>(at least one of the following)<br /> (a) limited use of gestures, (b) clumsy/gauche body language, (c) limited facial expression, (d) inappropriate expression, (e) peculiar, stiff gaze<br /> <br /> <em>6.Motor clumsiness</em></p>\n<p>If people want to, they can <a href=\"http://rmijic.questionform.com/public/Aspbergers-Syndrome\">respond to a poll</a> I created, recording their self-assessment of whether or not they fit these criteria. My own take is similar to that of Simon Baron-Cohen: that there isn't a natural dividing line between AS and neurotypical, rather that there is a spectrum of empathizing vs. systematizing brain-types. For those who want to, you can <a href=\"http://www.wired.com/wired/archive/9.12/aqtest.html\">take Baron-Cohen's \"Autism quotient\" test</a> on wired magazine, and you can record your score on my poll.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vHDk5xr9JDC64rb8T", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 24, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "2901", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 341, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-05-10T23:55:45.936Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-11T05:15:53.394Z", "modifiedAt": null, "url": null, "title": "Conditioning on Observers", "slug": "conditioning-on-observers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:24.635Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Lee", "createdAt": "2009-09-10T00:05:08.577Z", "isAdmin": false, "displayName": "Jonathan_Lee"}, "userId": "8qL3Hsw2TzaLPu3Bh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3pmaeoA9ALhfWhFmW/conditioning-on-observers", "pageUrlRelative": "/posts/3pmaeoA9ALhfWhFmW/conditioning-on-observers", "linkUrl": "https://www.lesswrong.com/posts/3pmaeoA9ALhfWhFmW/conditioning-on-observers", "postedAtFormatted": "Tuesday, May 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Conditioning%20on%20Observers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConditioning%20on%20Observers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pmaeoA9ALhfWhFmW%2Fconditioning-on-observers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Conditioning%20on%20Observers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pmaeoA9ALhfWhFmW%2Fconditioning-on-observers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pmaeoA9ALhfWhFmW%2Fconditioning-on-observers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2338, "htmlBody": "<p><strong>Response to </strong><a href=\"/lw/286/beauty_quips_id_shut_up_and_multiply\">Beauty quips, \"I'd shut up and multiply!\"</a></p>\n<p><strong>Related to </strong><a href=\"/lw/1ag/the_presumptuous_philosophers_presumptuous_friend\">The Presumptuous Philosopher's Presumptuous Friend</a>,&nbsp; <a href=\"/lw/182/the_absentminded_driver\">The Absent-Minded Driver</a>, <a href=\"/lw/5k/sleeping_beauty_gets_counterfactually_mugged\">Sleeping Beauty gets counterfactually mugged</a></p>\n<p>This is somewhat introductory. Observers play a vital role in the classic anthropic thought experiments, most notably the Sleeping Beauty and Presumptuous Philosopher gedankens. Specifically, it is remarkably common to condition simply on the <em>existence</em> of an observer, in spite of the continuity problems this raises. The source of confusion appears to be based on the distinction between the probability of an observer and the expectation number of observers, with the former not being a linear function of problem definitions.</p>\n<p>There is a related difference between the expected gain of a problem and the expected gain per decision, which has been exploited in more complex counterfactual mugging scenarios. As in the case of the 1/2 or 1/3 confusion, the issue is the number of decisions that are expected to be made, and recasting problems so that there is at most one decision provides a clear intuition pump.</p>\n<p><a id=\"more\"></a><strong>Sleeping Beauty</strong></p>\n<p style=\"padding-left: 30px;\">In the classic sleeping beauty problem, experimenters flip a fair coin on Sunday, sedate you and induce amnesia, and wake you either on just the following Monday or both the following Monday and Tuesday. Each time you are woken, you are asked for your credence that the coin came up heads.</p>\n<p>The standard answers to this question are that the answer should be 1/2 or 1/3. For convenience let us say that the event W is being woken, H is that the coin flip came up heads and T is that the coin flip came up tails. The basic logic for the 1/2 argument is that:</p>\n<p style=\"padding-left: 30px;\">P(H)=P(T)=1/2, P(W|H) = P(W|T) = P(W) = 1 so by Bayes rule P(H|W) = 1/2</p>\n<p>The obvious issue to be taken with this approach is one of continuity. The assessment is independent of the number of times you are woken in each branch, and this implies that all non zero observer branches have their posterior probability equal to their prior probability. Clearly the subjective probability of a zero observer branch is zero, so this implies discontinuity in the decision theory. Whilst not in and of itself fatal, it is surprising. There is apparent secondary confusion over the number of observations in the sleeping beauty problem, for example:</p>\n<blockquote>\n<p>If we want to replicate the situation 1000 times, we shouldn't end up with 1500 observations.&nbsp; The correct way to replicate the awakening decision is to use the probability tree I included above. You'd end up with expected cell counts of 500, 250, 250, instead of 500, 500, 500.</p>\n</blockquote>\n<p>Under these numbers, the 1000 observations made have required 500 heads and 250 tails, as each tail produces <em>both</em> an observation on Monday and Tuesday. This is not the behaviour of a fair coin. Further consideration of the problem shows that the naive conditioning on W is the point where it would be expected that the number of observations comes in. Hence in 900 <em>observations</em>, there would be 300 heads and 300 tails, with 600 observations following a tail and 300 following a head. To make this rigorous, let Monday and Tuesday be the event of being woken on Monday and Tuesday respectively. Then:</p>\n<p style=\"padding-left: 30px;\">P(H|Monday) = 1/2, P(Monday|W) = 2/3&nbsp;&nbsp;&nbsp;&nbsp; (P(Monday|W) = 2*P(Tuesday|W) as Monday occurs regardless of coin flip)</p>\n<p style=\"padding-left: 30px;\">P(H|W) = P(H &cap; Monday|W) + P(H &cap; Tuesday|W)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Total Probability)</p>\n<p style=\"padding-left: 30px;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = P(H|Monday &cap; W).P(Monday|W) + 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (As P(Tuesday|H) = 0)</p>\n<p style=\"padding-left: 30px;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = P(H|Monday).P(Monday|W) = 1/3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (As Monday &cap; W = Monday)</p>\n<p>Which would appear to support the view of updating on existence. The question of why this holds in the analysis is immediate to answer: The only day on which probability of heads occuring is non zero is Monday, and given an awakening it is not guaranteed that it is Monday. This should not be confused with the correct observation that there is always one awakening on Monday. This has caused problems because \"Awakening\" is not an event which occurs only once in each branch. Indeed, using the 1/3 answer and working back to try to find P(W) yields P(W) = 3/2, which is a strong indication that it is not the probability that matters, but the E(# of instances of W). As intuition pumps, we can consider some related problems.</p>\n<p><strong>Sleeping Twins</strong></p>\n<p style=\"padding-left: 30px;\">This experiment features Omega. It announces that it will place you and an identical copy of you in identical rooms, sedated. It will then flip a fair coin. If the coin comes up heads, it will wake one of you randomly. If it comes up tails, it will wake both of you. It will then ask what your credence for the coin coming up heads is.</p>\n<p style=\"padding-left: 30px;\">You wake up in a nondescript room. What is your credence?</p>\n<p>It is clear from the structure of this problem that it is almost identical to the sleeping beauty problem. It is also clear that your subjective probability of being woken is 1/2 if the coin comes up heads and 1 if it comes up tails, so conditioning on the fact that you have been woken the coin came up heads with probability 1/3. Why is this so different to the Sleeping Beauty problem? The fundamental difference is that in the Sleeping Twins problem, you are woken at most once, and possibly not, whereas in the Sleeping Beauty problem you are woken once or many times. On the other hand, the number of observer moments on each branch of the experiment is equal to that of the Sleeping Beauty problem, so it is odd that the manner in which these observations are achieved should matter. Clearly information flow is not possible, as provided for by amnesia in the original problem. Let us drive this further</p>\n<p><strong>Probabilistic Sleeping Beauty</strong></p>\n<p style=\"padding-left: 30px;\">We return to the experimenters and a new protocol. The experimenters fix a constant k in {1,2,..,20}, sedate you, roll a D20 and flip a coin. If the coin comes up tails, they will wake you on day k. If the coin comes up heads and the D20 comes up k, they will wake you on day 1. In either case they will ask you for your credence that the coin came up heads.</p>\n<p style=\"padding-left: 30px;\">You wake up. What is your credence?</p>\n<p>In this problem, the multiple distinct copies of you have been removed, at the cost of an explicit randomiser. It is clear that the structure of the problem is independent of the specific value of the constant k. It is also clear that updating on being woken, the probability that the coin came up heads is 1/21 regardless of k. This is troubling for the 1/2 answer, however, as playing this game with a single die roll and all possible values of k recovers the Sleeping Beauty problem (modulo induced amnesia). Again, having reduced the expected number of observations to be in [0,1], intuition and calculation seem to imply a reduced chance for the heads branch conditioned on being woken.</p>\n<p>This further suggests that the misunderstanding in Sleeping Beauty is one of naively looking at P(W|H) and P(W|T), when the expected numbers of wakings are E(#W|H) = 1, E(#W|T) = 2.</p>\n<p><strong>The Apparent Solution</strong></p>\n<p>If we allow conditioning on the number of observers, we correctly calculate probabilities in the Sleeping Twins and Probabilistic Sleeping Beauty problems. It is correctly noted that a \"single paying\" bet is accepted in Sleeping Beauty with odds of 2; this follows naturally under the following decision schema: \"If it is your last day awake the decision is binding, otherwise it is not\". Let the event of being the last day awake be L. Then:</p>\n<p style=\"padding-left: 30px;\">P(L|W &cap; T) = 1/2, P(L|W &cap; H) = 1, the bet pays k for a cost of 1</p>\n<p style=\"padding-left: 30px;\">E(Gains|Taking the bet) = (k-1) P(L|W &cap; H)P(H|W) - P(L|W &cap; T) P(T|W) = (k-1) P(H|W) - P(T|W)/2</p>\n<p>Clearly to accept a bet at payout of 2 implies that P(H|W) - P(T|W)/2&nbsp;&ge; 0, so 2.P(H|W) &ge; P(T|W), which contraindicates the 1/2 solution. The 1/3 solution, on the other hand works as expected. Trivially the same result holds if the choice of important decision is randomised. In general, if a decision is made by a collective of additional observers in identical states to you, then the existence of the additional observers does not change anything the overall payoffs. This can be modelled either by splitting payoffs between all decision makers in a group making identical decisions, or equivalently calculating as if there is a 1/N chance that you dictate the decision for everyone given N identical instances of you (\"Evenly distributed dictators\"). To do otherwise leads to fallacious expected gains, as exploited in <a href=\"/lw/5k/sleeping_beauty_gets_counterfactually_mugged\">Sleeping Beauty gets counterfactually mugged</a>. Of course, if the gains are linear in the number of observers, then this cancels with the division of responsibility and the observer count can be neglected, as in accepting 1/3 bets per observer in Sleeping Beauty.</p>\n<p><strong>The Absent Minded Driver</strong></p>\n<p>If we consider the problem of <a href=\"/lw/5k/sleeping_beauty_gets_counterfactually_mugged\">The Absent-Minded Driver</a>, then we are faced with another scenario in which depending on decisions made there are varying numbers of observer moments in the problem. This allows an apparent time inconsistency to appear, much as in Sleeping Beauty. The problem is as follows:</p>\n<p style=\"padding-left: 30px;\">You are an mildly amnesiac driver on a motorway. You notice approaching junctions but recall nothing. There are 2 junctions. If you turn off at the first, you gain nothing. If you turn off at the second, you gain 4. If you continue past the second, you gain 1.</p>\n<p>Clearly analysis of the problem shows that if p is the probability of going forward (constant care of the amnesia), the payout is p[p+4(1-p)], maximised at p = 2/3. However once one the road and approaching a junction, let the probability that you are approaching the first be &alpha;. The expected gain is then claimed to be &alpha;p[p+4(1-p)]+(1-&alpha;)[p+4(1-p)] which is not maximised at 2/3 unless &alpha; = 1. It can be immediately noticed that given p, &alpha; = 1/(p+1). However, this is still not correct.</p>\n<p>Instead, we can observe that all non zero payouts are the result of two decisions, at the first and second junctions. Let the state of being at the first junction be A, and the second be B. We observe that:</p>\n<p style=\"padding-left: 30px;\">E(Gains due to one decision|A) = 1 . (1-p)*0 + 1/2 . p[p+4(1-p)]</p>\n<p style=\"padding-left: 30px;\">E(Gains due to one decision|B) = 1/2 . [p+4(1-p)]</p>\n<p style=\"padding-left: 30px;\">P(A|W) = 1/(p+1), P(B|W) = p/(p+1), E(#A) = 1, E(#B) = p, (#A, #B independent of everything else)</p>\n<p style=\"padding-left: 30px;\">Hence the expected gain per decision:</p>\n<p style=\"padding-left: 30px;\">E(Gains due to one decision|W) = [1 . (1-p)*0 + 1/2 . p[p+4(1-p)]]/(p+1) + 1/2 . [p+4(1-p)].p/(p+1) = [p+4(1-p)].p/(p+1)</p>\n<p style=\"padding-left: 30px;\">But as has already been observed in this case the number of decisions made is dependent on p, and thus</p>\n<p style=\"padding-left: 30px;\">E(Gains|W) = [p+4(1-p)].p , which is the correct metric. Observe also that E(Gains|A) = E(Gains|B) = p[p+4(1-p)]/2</p>\n<p>As a result, there is no temporal inconsistency in this problem; the approach of counting up over all observer moments, and splitting outcomes due to a set of decisions across the relevant decisions is seemingly consistent.</p>\n<p><strong>Sleeping Beauty gets Counterfactually Mugged</strong></p>\n<p>In this problem, the Sleeping Beauty problem is combined with a counterfactual mugging. If Omega flips a head, it simulates you, and if you would give it $100 it will give you $260. If it flips a tail, it asks you for $100 and if you give it to Omega, it induces amnesia and asks again the next day. On the other hand if it flips a tail and you refuse to give it money, it gives you $50.</p>\n<p>Hence precommitting to give the money nets $30 on the average, whilst precommiting not to nets $25 on the average. However since you make exactly 1 decision on either branch if you refuse, whilst you make 3 decisions every two plays if you give Omega money, <em>per decision</em> you make $25 from refusing and $20 from accepting (obtained via spreading gains over identical instances of you). Hence correct play depends on whether Omega will ensure you get a consistent number of decisions or plays of the whole scenario. Given a fixed number of plays of the complete scenario, we thus have to remember to account for the increased numbers of decisions made in one branch of possible play. In this sense it is identical to the Absent Minded Driver, in that the number of decisions is a function of your early decisions, and so must be brought in as a factor in expected gains.</p>\n<p>Alternately, from a more timeless view we can note that your decisions in the system are perfectly correlated; it is thus the case that there is a single decision made by you, to give money or not to. A decision to give money nets $30 on average, whilst a decision not to nets only $25; the fact that they are split across multiple correlated decisions is irrelevant. Alternately conditional on choosing to give money you have a 1/2 chance of there being a second decision, so the expected gains are $30 rather than $20.</p>\n<p><strong>Conclusion</strong></p>\n<p>The approach of using the updating on the number observer moments is comparable to UDT and other timeless approaches to decision theory; it does not care how the observers come to be, be it a single amnesiac patient over a long period or a series of parallel copies or simulations. All that matters is that they are forced to make decisions.</p>\n<p>In cases where a number of decisions are discarded, the splitting of payouts over the decisions, or equivalently remembering the need for your decision not to be ignored, yields sane answers. This can also be considered as spreading a single pertinent decision out over some larger number of irrelevant choices.</p>\n<p>Correlated decisions are not so easy; care must be taken when the number of decisions is dependent on behaviour.</p>\n<p>In short, the 1/3 answer to sleeping beauty would appear to be fundamentally correct. Defences of the 1/2 answer appear to have problems with the number of observer moments being outside [0,1] and thus not being probabilities. This is the underlying danger. Use of anthropic or self indication probabilities yields sane answers in the problems considered, and can cogently answer typical questions designed to yield a non anthropic intuition.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3pmaeoA9ALhfWhFmW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 5.85567203530976e-07, "legacy": true, "legacyId": "2910", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Response to </strong><a href=\"/lw/286/beauty_quips_id_shut_up_and_multiply\">Beauty quips, \"I'd shut up and multiply!\"</a></p>\n<p><strong>Related to </strong><a href=\"/lw/1ag/the_presumptuous_philosophers_presumptuous_friend\">The Presumptuous Philosopher's Presumptuous Friend</a>,&nbsp; <a href=\"/lw/182/the_absentminded_driver\">The Absent-Minded Driver</a>, <a href=\"/lw/5k/sleeping_beauty_gets_counterfactually_mugged\">Sleeping Beauty gets counterfactually mugged</a></p>\n<p>This is somewhat introductory. Observers play a vital role in the classic anthropic thought experiments, most notably the Sleeping Beauty and Presumptuous Philosopher gedankens. Specifically, it is remarkably common to condition simply on the <em>existence</em> of an observer, in spite of the continuity problems this raises. The source of confusion appears to be based on the distinction between the probability of an observer and the expectation number of observers, with the former not being a linear function of problem definitions.</p>\n<p>There is a related difference between the expected gain of a problem and the expected gain per decision, which has been exploited in more complex counterfactual mugging scenarios. As in the case of the 1/2 or 1/3 confusion, the issue is the number of decisions that are expected to be made, and recasting problems so that there is at most one decision provides a clear intuition pump.</p>\n<p><a id=\"more\"></a><strong>Sleeping Beauty</strong></p>\n<p style=\"padding-left: 30px;\">In the classic sleeping beauty problem, experimenters flip a fair coin on Sunday, sedate you and induce amnesia, and wake you either on just the following Monday or both the following Monday and Tuesday. Each time you are woken, you are asked for your credence that the coin came up heads.</p>\n<p>The standard answers to this question are that the answer should be 1/2 or 1/3. For convenience let us say that the event W is being woken, H is that the coin flip came up heads and T is that the coin flip came up tails. The basic logic for the 1/2 argument is that:</p>\n<p style=\"padding-left: 30px;\">P(H)=P(T)=1/2, P(W|H) = P(W|T) = P(W) = 1 so by Bayes rule P(H|W) = 1/2</p>\n<p>The obvious issue to be taken with this approach is one of continuity. The assessment is independent of the number of times you are woken in each branch, and this implies that all non zero observer branches have their posterior probability equal to their prior probability. Clearly the subjective probability of a zero observer branch is zero, so this implies discontinuity in the decision theory. Whilst not in and of itself fatal, it is surprising. There is apparent secondary confusion over the number of observations in the sleeping beauty problem, for example:</p>\n<blockquote>\n<p>If we want to replicate the situation 1000 times, we shouldn't end up with 1500 observations.&nbsp; The correct way to replicate the awakening decision is to use the probability tree I included above. You'd end up with expected cell counts of 500, 250, 250, instead of 500, 500, 500.</p>\n</blockquote>\n<p>Under these numbers, the 1000 observations made have required 500 heads and 250 tails, as each tail produces <em>both</em> an observation on Monday and Tuesday. This is not the behaviour of a fair coin. Further consideration of the problem shows that the naive conditioning on W is the point where it would be expected that the number of observations comes in. Hence in 900 <em>observations</em>, there would be 300 heads and 300 tails, with 600 observations following a tail and 300 following a head. To make this rigorous, let Monday and Tuesday be the event of being woken on Monday and Tuesday respectively. Then:</p>\n<p style=\"padding-left: 30px;\">P(H|Monday) = 1/2, P(Monday|W) = 2/3&nbsp;&nbsp;&nbsp;&nbsp; (P(Monday|W) = 2*P(Tuesday|W) as Monday occurs regardless of coin flip)</p>\n<p style=\"padding-left: 30px;\">P(H|W) = P(H \u2229 Monday|W) + P(H \u2229 Tuesday|W)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Total Probability)</p>\n<p style=\"padding-left: 30px;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = P(H|Monday \u2229 W).P(Monday|W) + 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (As P(Tuesday|H) = 0)</p>\n<p style=\"padding-left: 30px;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = P(H|Monday).P(Monday|W) = 1/3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (As Monday \u2229 W = Monday)</p>\n<p>Which would appear to support the view of updating on existence. The question of why this holds in the analysis is immediate to answer: The only day on which probability of heads occuring is non zero is Monday, and given an awakening it is not guaranteed that it is Monday. This should not be confused with the correct observation that there is always one awakening on Monday. This has caused problems because \"Awakening\" is not an event which occurs only once in each branch. Indeed, using the 1/3 answer and working back to try to find P(W) yields P(W) = 3/2, which is a strong indication that it is not the probability that matters, but the E(# of instances of W). As intuition pumps, we can consider some related problems.</p>\n<p><strong id=\"Sleeping_Twins\">Sleeping Twins</strong></p>\n<p style=\"padding-left: 30px;\">This experiment features Omega. It announces that it will place you and an identical copy of you in identical rooms, sedated. It will then flip a fair coin. If the coin comes up heads, it will wake one of you randomly. If it comes up tails, it will wake both of you. It will then ask what your credence for the coin coming up heads is.</p>\n<p style=\"padding-left: 30px;\">You wake up in a nondescript room. What is your credence?</p>\n<p>It is clear from the structure of this problem that it is almost identical to the sleeping beauty problem. It is also clear that your subjective probability of being woken is 1/2 if the coin comes up heads and 1 if it comes up tails, so conditioning on the fact that you have been woken the coin came up heads with probability 1/3. Why is this so different to the Sleeping Beauty problem? The fundamental difference is that in the Sleeping Twins problem, you are woken at most once, and possibly not, whereas in the Sleeping Beauty problem you are woken once or many times. On the other hand, the number of observer moments on each branch of the experiment is equal to that of the Sleeping Beauty problem, so it is odd that the manner in which these observations are achieved should matter. Clearly information flow is not possible, as provided for by amnesia in the original problem. Let us drive this further</p>\n<p><strong id=\"Probabilistic_Sleeping_Beauty\">Probabilistic Sleeping Beauty</strong></p>\n<p style=\"padding-left: 30px;\">We return to the experimenters and a new protocol. The experimenters fix a constant k in {1,2,..,20}, sedate you, roll a D20 and flip a coin. If the coin comes up tails, they will wake you on day k. If the coin comes up heads and the D20 comes up k, they will wake you on day 1. In either case they will ask you for your credence that the coin came up heads.</p>\n<p style=\"padding-left: 30px;\">You wake up. What is your credence?</p>\n<p>In this problem, the multiple distinct copies of you have been removed, at the cost of an explicit randomiser. It is clear that the structure of the problem is independent of the specific value of the constant k. It is also clear that updating on being woken, the probability that the coin came up heads is 1/21 regardless of k. This is troubling for the 1/2 answer, however, as playing this game with a single die roll and all possible values of k recovers the Sleeping Beauty problem (modulo induced amnesia). Again, having reduced the expected number of observations to be in [0,1], intuition and calculation seem to imply a reduced chance for the heads branch conditioned on being woken.</p>\n<p>This further suggests that the misunderstanding in Sleeping Beauty is one of naively looking at P(W|H) and P(W|T), when the expected numbers of wakings are E(#W|H) = 1, E(#W|T) = 2.</p>\n<p><strong id=\"The_Apparent_Solution\">The Apparent Solution</strong></p>\n<p>If we allow conditioning on the number of observers, we correctly calculate probabilities in the Sleeping Twins and Probabilistic Sleeping Beauty problems. It is correctly noted that a \"single paying\" bet is accepted in Sleeping Beauty with odds of 2; this follows naturally under the following decision schema: \"If it is your last day awake the decision is binding, otherwise it is not\". Let the event of being the last day awake be L. Then:</p>\n<p style=\"padding-left: 30px;\">P(L|W \u2229 T) = 1/2, P(L|W \u2229 H) = 1, the bet pays k for a cost of 1</p>\n<p style=\"padding-left: 30px;\">E(Gains|Taking the bet) = (k-1) P(L|W \u2229 H)P(H|W) - P(L|W \u2229 T) P(T|W) = (k-1) P(H|W) - P(T|W)/2</p>\n<p>Clearly to accept a bet at payout of 2 implies that P(H|W) - P(T|W)/2&nbsp;\u2265 0, so 2.P(H|W) \u2265 P(T|W), which contraindicates the 1/2 solution. The 1/3 solution, on the other hand works as expected. Trivially the same result holds if the choice of important decision is randomised. In general, if a decision is made by a collective of additional observers in identical states to you, then the existence of the additional observers does not change anything the overall payoffs. This can be modelled either by splitting payoffs between all decision makers in a group making identical decisions, or equivalently calculating as if there is a 1/N chance that you dictate the decision for everyone given N identical instances of you (\"Evenly distributed dictators\"). To do otherwise leads to fallacious expected gains, as exploited in <a href=\"/lw/5k/sleeping_beauty_gets_counterfactually_mugged\">Sleeping Beauty gets counterfactually mugged</a>. Of course, if the gains are linear in the number of observers, then this cancels with the division of responsibility and the observer count can be neglected, as in accepting 1/3 bets per observer in Sleeping Beauty.</p>\n<p><strong id=\"The_Absent_Minded_Driver\">The Absent Minded Driver</strong></p>\n<p>If we consider the problem of <a href=\"/lw/5k/sleeping_beauty_gets_counterfactually_mugged\">The Absent-Minded Driver</a>, then we are faced with another scenario in which depending on decisions made there are varying numbers of observer moments in the problem. This allows an apparent time inconsistency to appear, much as in Sleeping Beauty. The problem is as follows:</p>\n<p style=\"padding-left: 30px;\">You are an mildly amnesiac driver on a motorway. You notice approaching junctions but recall nothing. There are 2 junctions. If you turn off at the first, you gain nothing. If you turn off at the second, you gain 4. If you continue past the second, you gain 1.</p>\n<p>Clearly analysis of the problem shows that if p is the probability of going forward (constant care of the amnesia), the payout is p[p+4(1-p)], maximised at p = 2/3. However once one the road and approaching a junction, let the probability that you are approaching the first be \u03b1. The expected gain is then claimed to be \u03b1p[p+4(1-p)]+(1-\u03b1)[p+4(1-p)] which is not maximised at 2/3 unless \u03b1 = 1. It can be immediately noticed that given p, \u03b1 = 1/(p+1). However, this is still not correct.</p>\n<p>Instead, we can observe that all non zero payouts are the result of two decisions, at the first and second junctions. Let the state of being at the first junction be A, and the second be B. We observe that:</p>\n<p style=\"padding-left: 30px;\">E(Gains due to one decision|A) = 1 . (1-p)*0 + 1/2 . p[p+4(1-p)]</p>\n<p style=\"padding-left: 30px;\">E(Gains due to one decision|B) = 1/2 . [p+4(1-p)]</p>\n<p style=\"padding-left: 30px;\">P(A|W) = 1/(p+1), P(B|W) = p/(p+1), E(#A) = 1, E(#B) = p, (#A, #B independent of everything else)</p>\n<p style=\"padding-left: 30px;\">Hence the expected gain per decision:</p>\n<p style=\"padding-left: 30px;\">E(Gains due to one decision|W) = [1 . (1-p)*0 + 1/2 . p[p+4(1-p)]]/(p+1) + 1/2 . [p+4(1-p)].p/(p+1) = [p+4(1-p)].p/(p+1)</p>\n<p style=\"padding-left: 30px;\">But as has already been observed in this case the number of decisions made is dependent on p, and thus</p>\n<p style=\"padding-left: 30px;\">E(Gains|W) = [p+4(1-p)].p , which is the correct metric. Observe also that E(Gains|A) = E(Gains|B) = p[p+4(1-p)]/2</p>\n<p>As a result, there is no temporal inconsistency in this problem; the approach of counting up over all observer moments, and splitting outcomes due to a set of decisions across the relevant decisions is seemingly consistent.</p>\n<p><strong id=\"Sleeping_Beauty_gets_Counterfactually_Mugged\">Sleeping Beauty gets Counterfactually Mugged</strong></p>\n<p>In this problem, the Sleeping Beauty problem is combined with a counterfactual mugging. If Omega flips a head, it simulates you, and if you would give it $100 it will give you $260. If it flips a tail, it asks you for $100 and if you give it to Omega, it induces amnesia and asks again the next day. On the other hand if it flips a tail and you refuse to give it money, it gives you $50.</p>\n<p>Hence precommitting to give the money nets $30 on the average, whilst precommiting not to nets $25 on the average. However since you make exactly 1 decision on either branch if you refuse, whilst you make 3 decisions every two plays if you give Omega money, <em>per decision</em> you make $25 from refusing and $20 from accepting (obtained via spreading gains over identical instances of you). Hence correct play depends on whether Omega will ensure you get a consistent number of decisions or plays of the whole scenario. Given a fixed number of plays of the complete scenario, we thus have to remember to account for the increased numbers of decisions made in one branch of possible play. In this sense it is identical to the Absent Minded Driver, in that the number of decisions is a function of your early decisions, and so must be brought in as a factor in expected gains.</p>\n<p>Alternately, from a more timeless view we can note that your decisions in the system are perfectly correlated; it is thus the case that there is a single decision made by you, to give money or not to. A decision to give money nets $30 on average, whilst a decision not to nets only $25; the fact that they are split across multiple correlated decisions is irrelevant. Alternately conditional on choosing to give money you have a 1/2 chance of there being a second decision, so the expected gains are $30 rather than $20.</p>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<p>The approach of using the updating on the number observer moments is comparable to UDT and other timeless approaches to decision theory; it does not care how the observers come to be, be it a single amnesiac patient over a long period or a series of parallel copies or simulations. All that matters is that they are forced to make decisions.</p>\n<p>In cases where a number of decisions are discarded, the splitting of payouts over the decisions, or equivalently remembering the need for your decision not to be ignored, yields sane answers. This can also be considered as spreading a single pertinent decision out over some larger number of irrelevant choices.</p>\n<p>Correlated decisions are not so easy; care must be taken when the number of decisions is dependent on behaviour.</p>\n<p>In short, the 1/3 answer to sleeping beauty would appear to be fundamentally correct. Defences of the 1/2 answer appear to have problems with the number of observer moments being outside [0,1] and thus not being probabilities. This is the underlying danger. Use of anthropic or self indication probabilities yields sane answers in the problems considered, and can cogently answer typical questions designed to yield a non anthropic intuition.</p>", "sections": [{"title": "Sleeping Twins", "anchor": "Sleeping_Twins", "level": 1}, {"title": "Probabilistic Sleeping Beauty", "anchor": "Probabilistic_Sleeping_Beauty", "level": 1}, {"title": "The Apparent Solution", "anchor": "The_Apparent_Solution", "level": 1}, {"title": "The Absent Minded Driver", "anchor": "The_Absent_Minded_Driver", "level": 1}, {"title": "Sleeping Beauty gets Counterfactually Mugged", "anchor": "Sleeping_Beauty_gets_Counterfactually_Mugged", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "121 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 121, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aQRKfzYnt3bFGgPKd", "tZodkMtQ3Ao7anzNW", "GfHdNfqxe3cSCfpHL", "CcjcCYYEB5KNHCpEZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-13T14:24:24.783Z", "modifiedAt": null, "url": null, "title": "Aspergers Poll Results: LW is nerdier than the Math Olympiad?", "slug": "aspergers-poll-results-lw-is-nerdier-than-the-math-olympiad", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:20.160Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pCJQMzvrYRkCbs4Tu/aspergers-poll-results-lw-is-nerdier-than-the-math-olympiad", "pageUrlRelative": "/posts/pCJQMzvrYRkCbs4Tu/aspergers-poll-results-lw-is-nerdier-than-the-math-olympiad", "linkUrl": "https://www.lesswrong.com/posts/pCJQMzvrYRkCbs4Tu/aspergers-poll-results-lw-is-nerdier-than-the-math-olympiad", "postedAtFormatted": "Thursday, May 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Aspergers%20Poll%20Results%3A%20LW%20is%20nerdier%20than%20the%20Math%20Olympiad%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAspergers%20Poll%20Results%3A%20LW%20is%20nerdier%20than%20the%20Math%20Olympiad%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpCJQMzvrYRkCbs4Tu%2Faspergers-poll-results-lw-is-nerdier-than-the-math-olympiad%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Aspergers%20Poll%20Results%3A%20LW%20is%20nerdier%20than%20the%20Math%20Olympiad%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpCJQMzvrYRkCbs4Tu%2Faspergers-poll-results-lw-is-nerdier-than-the-math-olympiad", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpCJQMzvrYRkCbs4Tu%2Faspergers-poll-results-lw-is-nerdier-than-the-math-olympiad", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 734, "htmlBody": "<p>Followup to: <a href=\"/lw/28l/do_you_have_highfunctioning_aspergers_syndrome/\">Do you have High Functioning Aspergers Syndrome?</a></p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p><strong>EDIT</strong>: To combat <a href=\"http://en.wikipedia.org/wiki/Opinion_poll#Nonresponse_bias\">nonresponse bias</a>, I'd appreciate it if anyone who considered the poll and decided not to fill it in would go and <a href=\"http://rmijic.questionform.com/public/Aspbergers-Syndrome\">do so now</a>, but that people who haven't already seen the poll <em>refrain from doing so</em>. We might get some idea of which way the bias points by looking at the difference in results.</p>\n<p><strong>This is your opportunity to help your community's social epistemology!</strong></p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Since over 80 LW'ers were kind enough to fill out <a href=\"http://rmijic.questionform.com/public/Aspbergers-Syndrome\">my survey</a> on Aspergers, I thought I'd post the results.</p>\n<p>4 people said they had already been diagnosed with Aspergers&nbsp; Syndrome, out of 82 responses. That's 5%, where the population incidence rate is thought to be 0.36%.&nbsp; However the incidence rate is known to be larger than the diagnosis rate, as many AS cases (I don't know how many) go undiagnosed. An additional 4 people ticked the five diagnostic criteria I listed; if we count each of them as 1/2 a case, LW would have roughly 25 times the baseline AS rate.</p>\n<p>The Less Wrong mean average <a href=\"http://www.wired.com/wired/archive/9.12/aqtest.html\">AQ test</a> score was 27, and only 5 people got at or below 16, which is the population average score on this test. 21 people or 26% scored 32 or more on the AQ test, though this is only an indicator and does not mean that 26% of LW have Aspergers.</p>\n<p>To put the AQ test results in perspective, this paragraph from Wikipedia outlines what various groups got on average:</p>\n<p style=\"padding-left: 30px;\"><em>The questionnaire was trialled on Cambridge University students, and a group of sixteen winners of the British Mathematical Olympiad, to determine whether there was a link between a talent for mathematical and scientific disciplines and traits associated with the autism spectrum. Mathematics, physical sciences and engineering students were found to score significantly higher, e.g. 21.8 on average for mathematicians and 21.4 for computer scientists. </em><em><strong>The average score for the British Mathematical Olympiad winners was 24</strong>. Of the students who scored 32 or more on the test, eleven agreed to be interviewed and seven of these were reported to meet the DSM-IV criteria for Asperger syndrome, although no formal diagnosis was made as they were not suffering any distress. The test was also taken by a group of subjects who had been diagnosed with autism or Asperger syndrome by a professional, the average score being 35 and 38 for males and females respectively.</em></p>\n<p>If we take 7/11 times the 26% of LW who scored 32+, we get 16%, which is somewhat higher than the 7-10% you might estimate from the number of people who said they have diagnoses. Note, though, that the 7 trial students who were found to meet the diagnostic criteria were not diagnosed, as their condition was not causing them \"distress\", indicating that for high-functioning AS adults, the incidence rate might be a lot higher than the diagnosis rate.</p>\n<p><a id=\"more\"></a></p>\n<p>What does this mean?</p>\n<p>Well, for one thing it means that Less Wrong is \"on the spectrum\", even if we're mostly not falling off the right tail. Only about 1 in 10 people on Less Wrong are \"normal\" in terms of the empathizing/systematizing scale, perhaps 1 in 10 are far enough out to be full blown Aspergers, and the rest of us sit somewhere in between, with most people being more to the right of the distribution than the average Cambridge mathematics student.</p>\n<p>Interestingly, <em>48% of respondents</em> ticked this criterion:</p>\n<p style=\"padding-left: 30px;\"><em>Severe impairment in reciprocal social interaction (at least two of the following) (a) inability to interact with peers, (b) lack of desire to interact with peers, (c) lack of appreciation of social cues, (d) socially and emotionally inappropriate behavior</em></p>\n<p>Which indicates that we're mostly not very good at the human social game.</p>\n<p><strong>EDIT</strong>: Note also that <a href=\"http://en.wikipedia.org/wiki/Opinion_poll#Nonresponse_bias\">nonresponse bias</a> means that these conclusions only apply strictly to that subset of LW who actually responded, i.e. those specific 82 people. Since \"Less Wrong\" is a vague collection of \"concentric levels\" of involvement, from occasional reader to hardcore poster, and those who are more heavily involved are more likely to have responded (e.g. because they read more of the posts, and have more time), the results probably apply more to those who are more involved.</p>\n<p>Response bias could be counteracted by doing more work (e.g. asking only specific commenters, randomly selected, to respond), or by simply having a prior for response bias and AQ rates, and using the survey results to update it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pCJQMzvrYRkCbs4Tu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "2912", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vHDk5xr9JDC64rb8T"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-13T17:55:12.768Z", "modifiedAt": null, "url": null, "title": "Updating, part 1: When can you change your mind? The binary model", "slug": "updating-part-1-when-can-you-change-your-mind-the-binary", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:30.236Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qpEq8FW23mp7QipFP/updating-part-1-when-can-you-change-your-mind-the-binary", "pageUrlRelative": "/posts/qpEq8FW23mp7QipFP/updating-part-1-when-can-you-change-your-mind-the-binary", "linkUrl": "https://www.lesswrong.com/posts/qpEq8FW23mp7QipFP/updating-part-1-when-can-you-change-your-mind-the-binary", "postedAtFormatted": "Thursday, May 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Updating%2C%20part%201%3A%20When%20can%20you%20change%20your%20mind%3F%20The%20binary%20model&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpdating%2C%20part%201%3A%20When%20can%20you%20change%20your%20mind%3F%20The%20binary%20model%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqpEq8FW23mp7QipFP%2Fupdating-part-1-when-can-you-change-your-mind-the-binary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Updating%2C%20part%201%3A%20When%20can%20you%20change%20your%20mind%3F%20The%20binary%20model%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqpEq8FW23mp7QipFP%2Fupdating-part-1-when-can-you-change-your-mind-the-binary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqpEq8FW23mp7QipFP%2Fupdating-part-1-when-can-you-change-your-mind-the-binary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1360, "htmlBody": "<p>I was <a href=\"/lw/286/beauty_quips_id_shut_up_and_multiply/1z9m\">recently disturbed</a> by my perception that, despite years of studying and debating probability problems, the LessWrong community as a whole has not markedly improved its ability to get the right answer on them.</p>\n<p>I had expected that people would read posts and comments by other people, and take special note of comments by people who had a prior history of being right, and thereby improve their own accuracy.</p>\n<p>But can that possibly work?&nbsp; How can someone who isn't already highly-accurate, identify other people who are highly accurate?</p>\n<p>Aumann's agreement theorem (allegedly) says that Bayesians with the same priors agree.&nbsp; But it doesn't say that doing so <em>helps</em>.&nbsp; Under what circumstances does revising your opinions, by updating in response to people you consider reliable, actually improve your accuracy?</p>\n<p>To find out, I built a model of updating in response to the opinions of others.&nbsp; It did, eventually, show that Bayesians improve their collective opinions by updating in response to the opinions of other Bayesians.&nbsp; But this turns out not to depend on them satisfying the conditions of Aumann's theorem, or on doing Bayesian updating.&nbsp; It depends only on a very simple condition, established at the start of the simulation.&nbsp; Can you guess what it is?</p>\n<p>I'll write another post describing and explaining the results if this post receives a karma score over 10.</p>\n<p><a id=\"more\"></a></p>\n<p>That's getting a bit ahead of ourselves, though.&nbsp; This post models only non-Bayesians, and the results are very different.</p>\n<p>Here's the model:</p>\n<ul>\n<li>There are G people in a group such as LessWrong.</li>\n<li>There are N problems being discussed simultaneously.</li>\n<li>Problems are binary problems, with an answer of either 1 or 0.</li>\n<li>Each person's opinion on each problem is always known to all people.</li>\n<li>Each person i has an <em>accuracy</em>: Their probability p<sub>i</sub> of getting any arbitrary problem correct on the first guess.</li>\n<li>g<sub>ivt</sub> is what person i believes at time t is the answer to problem v (1 or 0).</li>\n<li>p<sub>ij</sub> expresses person i's estimate of the probability that an arbitrary belief of person j is correct.</li>\n<li>Without loss of generality, assume the correct answer to every problem is 1.</li>\n</ul>\n<p>Algorithm:</p>\n<p># Loop over T timesteps<br />For t = 0 to T-1 {</p>\n<p style=\"padding-left: 30px;\"># Loop over G people<br />For i = 0 to G-1 {</p>\n<p style=\"padding-left: 60px;\"># Loop over N problems<br />For v = 0 to N-1 {</p>\n<p style=\"padding-left: 90px;\">If (t == 0)</p>\n<p style=\"padding-left: 120px;\"># Special initialization for the first timestep<br /> If (random in [0..1] &lt; p<sub>i</sub>) g<sub>ivt</sub> := 1;&nbsp; Else g<sub>ivt</sub> := 0</p>\n<p style=\"padding-left: 90px;\">Else {</p>\n<p style=\"padding-left: 120px;\"># Product over all j of the probability that the answer to v is 1 given j's answer and estimated accuracy<br />m1 := <span style=\"font-size: Large;\">&prod;</span><sub>j</sub> [ p<sub>ij</sub>g<sub>jv(t-1)</sub> + (1-p<sub>ij</sub>)(1-g<sub>jv(t-1)</sub>) ]</p>\n<p style=\"padding-left: 120px;\"># Product over all j of the probability that the answer to v is 0 given j's answer and estimated accuracy<br />m0 := <span style=\"font-size: Large;\">&prod;</span><sub>j</sub> [ p<sub>ij</sub>(1-g<sub>jv(t-1)</sub>) + (1-p<sub>ij</sub>)g<sub>jv(t-1)</sub> ]</p>\n<p style=\"padding-left: 120px;\">p1 := m1 / (m0 + m1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Normalize</p>\n<p style=\"padding-left: 120px;\">If (p1 &gt; .5) g<sub>ivt</sub> := 1;&nbsp; Else&nbsp; g<sub>ivt</sub> := 0</p>\n<p style=\"padding-left: 90px;\">}</p>\n<p style=\"padding-left: 60px;\">}</p>\n<p style=\"padding-left: 60px;\"># Loop over G other people<br /> For j = 0 to G-1</p>\n<p style=\"padding-left: 90px;\"># Compute person i's estimate of person j's accuracy<br /> p<sub>ij</sub> := { <span style=\"font-size: Large;\">&Sigma;</span><sub>s in [0 .. t]</sub> <span style=\"font-size: Large;\">&Sigma;</span><sub>v in [s..N]</sub> [ g<sub>ivt</sub>g<sub>jvs</sub> + (1-g<sub>ivt</sub>)(1-g<sub>jvs</sub>) ] } / N</p>\n<p style=\"padding-left: 30px;\">}</p>\n<p>}</p>\n<p>p1 is the probability that agent i assigns to problem v having the answer 1.&nbsp; Each term p<sub>ij</sub>g<sub>jv(t-1)</sub> + (1-p<sub>ij</sub>)(1-g<sub>jv(t-1)</sub>) is the probability of problem v having answer 1 computed using agent j's beliefs, by adding either the probability that j is correct (if j believes it has answer 1), or the probability that j is wrong (if j believes it has answer 0).&nbsp; Agent i assumes that everyone's opinions are independent, and multiplies all these probabilities together.&nbsp; The result, m1, is very small when there are very many agents (m1 is on the order of .5<sup>G</sup>), so it is normalized by computing a similar product m0 for the probability that v has answer 0, and setting p1 = m1 / (m0 + m1).</p>\n<p>The sum of sums to compute p<sub>ij</sub> (i's opinion of j's accuracy) computes the fraction of problems, summed over all previous time periods, on which person j has agreed with person i's current opinions.&nbsp; It sums over previous time periods because otherwise, p<sub>ii</sub> = 1.&nbsp; By summing over previous times, if person i ever changes its mind, that will decrease p<sub>ii</sub>.&nbsp; (The inner sum starts from s instead of 0 to accomodate an addition to the model that I'll make later, in which the true answer to problem <em>t</em> is revealed at the end of time t.&nbsp; Problems whose answer is public knowledge should not be considered in the sum after the time they became public knowledge.)</p>\n<p>Now, what distribution should we use for the p<sub>i</sub>?</p>\n<p>There is an infinite supply of problems.&nbsp; Many are so simple that everyone gets them right; many are so hard or incomprehensible that everyone performs randomly on them; and there are many, such as the Monty Haul problem, that <em>most</em> people get wrong because of systematic bias in our thinking.&nbsp; The range of population average performance p<sub>ave</sub> on all possible problems thus falls within [0 .. 1].</p>\n<p>I chose to model person accuracy instead of problem difficulty.&nbsp; I say \"instead of\", because you can use either person accuracy or problem difficulty to set p<sub>ave</sub>. Since a critical part of what we're modeling is person i's estimate of person j's accuracy, person j should actually <em>have</em> an accuracy.&nbsp; I didn't model problem difficulty partly because I assume we only talk about problems of a particular level of difficulty; partly because a person in this model can't distinguish between \"Most people disagree with me on this problem; therefore it is difficult\" and \"Most people disagree with me on this problem; therefore I was wrong about this problem\".</p>\n<p>Because I assume we talk mainly about high-entropy problems, I set p<sub>ave</sub> = .5.&nbsp; I do this by drawing p<sub>i</sub> from [0 .. 1], with a normal distribution with a mean of .5, truncated at .05 and .95.&nbsp; (I used a standard deviation of .15; this isn't important.)</p>\n<p>Because this distribution of p<sub>i</sub> is symmetric around .5, there is no way to know whether you're living in the world where the right answer is always 1, or where the right answer is always 0.&nbsp; This means there's no way, under this model, for a person to know whether they're a crackpot (usually wrong) or a genius (usually right).</p>\n<p>Note that these agents don't satisfy the preconditions for <a href=\"http://wiki.lesswrong.com/wiki/Aumann%27s_agreement_theorem\">Aumann agreement</a>, because they produce 0/1 decisions instead of probabilities, and because some agents are biased to perform worse than random.&nbsp; It's worth studying non-Bayesian agents before moving on to a model satisfying the preconditions for the theorem, if only because there are so many of them in the real world.</p>\n<p>An important property of this model is that, if person i is highly accurate, and knows it, p<sub>ii</sub> will approach 1, greatly reducing the chance that person i will change their mind about any problem.&nbsp; Thus, the more accurate a person becomes, the less able they are to change their minds when they are wrong - and this is not an error.&nbsp; It's a natural limit on the speed at which one can converge on truth.</p>\n<p>An obvious problem is that at t=0, person i will see that it always agrees with itself, and set p<sub>ii</sub> = 1.&nbsp; By induction, no one will ever change their mind.&nbsp; (I consider this evidence for the model, rather than against it.)</p>\n<p>The question of how people ever change their mind is key to this whole study.&nbsp; I use one of these two additions to the model to let people change their mind:</p>\n<ul>\n<li>At the end of each timestep t, the answer to problem number t becomes mutual knowledge to the entire group.&nbsp; (This solves the crackpot/genius problem.)</li>\n<li>Each person has a maximum allowable p<sub>ij</sub> (including p<sub>ii</sub>).</li>\n</ul>\n<p>This model is difficult to solve analytically, so I wrote a Perl script to simulate it.</p>\n<ul>\n<li>What do you think will happen when I run the program, or its variants?</li>\n<li>What other variants would you like to see tested?</li>\n<li>Is there a fundamental problem with the model?</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qpEq8FW23mp7QipFP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 5.863097173653561e-07, "legacy": true, "legacyId": "2911", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-13T22:35:42.109Z", "modifiedAt": null, "url": null, "title": "Blue- and Yellow-Tinted Choices", "slug": "blue-and-yellow-tinted-choices", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:56.554Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kjArXFinD3deRZNRu/blue-and-yellow-tinted-choices", "pageUrlRelative": "/posts/kjArXFinD3deRZNRu/blue-and-yellow-tinted-choices", "linkUrl": "https://www.lesswrong.com/posts/kjArXFinD3deRZNRu/blue-and-yellow-tinted-choices", "postedAtFormatted": "Thursday, May 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Blue-%20and%20Yellow-Tinted%20Choices&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABlue-%20and%20Yellow-Tinted%20Choices%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjArXFinD3deRZNRu%2Fblue-and-yellow-tinted-choices%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Blue-%20and%20Yellow-Tinted%20Choices%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjArXFinD3deRZNRu%2Fblue-and-yellow-tinted-choices", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjArXFinD3deRZNRu%2Fblue-and-yellow-tinted-choices", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1270, "htmlBody": "<blockquote>\n<p style=\"margin-bottom: 0in;\"><sub><em>A man comes to the rabbi and complains about his life: \"I have almost no money, my wife is a shrew, and we live in a small apartment with seven unruly kids. It's messy, it's noisy, it's smelly, and I don't want to live.\"<br />The rabbi says, \"Buy a goat.\"<br />\"What? I just told you there's hardly room for nine people, and it's messy as it is!\"<br />\"Look, you came for advice, so I'm giving you advice. Buy a goat and come back in a month.\"<br />In a month the man comes back and he is even more depressed: \"It's gotten worse! The filthy goat breaks everything, and it stinks and makes more noise than my wife and seven kids! What should I do?\"<br />The rabbi says, \"Sell the goat.\"<br />A few days later the man returns to the rabbi, beaming with happiness: \"Life is wonderful! We enjoy every minute of it now that there's no goat - only the nine of us. The kids are well-behaved, the wife is agreeable - and we even have some money!\"</em></sub></p>\n</blockquote>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><em>-- traditional Jewish joke</em></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Related to:</strong> <a href=\"/lw/j7/anchoring_and_adjustment/\">Anchoring and Adjustment</a></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Biases are &ldquo;cognitive illusions&rdquo; that work on the same principle as optical illusions, and a knowledge of the latter can be profitably applied to the former. Take, for example, these two cubes (source: <a href=\"http://www.lottolab.org/articles/illusionsoflight.asp\">Lotto Lab</a>, via Boing Boing):</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; padding-left: 120px;\"><img src=\"http://www.raikoth.net/Stuff/color-cube-illusion.jpg\" alt=\"Colored cube illusion\" width=\"452\" height=\"235\" /></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The &ldquo;blue&rdquo; tiles on the top face of the left cube are the same color as the &ldquo;yellow&rdquo; tiles on the top face of the right cube; if you're skeptical you can prove it with the eyedropper tool in Photoshop (in which both shades come out a rather ugly gray).</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The illusion works because visual perception is relative. Outdoor light on a sunny day can be ten thousand times greater than a fluorescently lit indoor room. As one psychology book put it: for a student reading this book outside, the black print will be objectively lighter than the white space will be for a student reading the book inside. Nevertheless, both students will perceive the white space as subjectively white and the black space as subjectively black, because the visual system returns to consciousness information about relative rather than absolute lightness. In the two cubes, the visual system takes the yellow or blue tint as a given and outputs to consciousness the colors of each pixel compared to that background.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">So this optical illusion occurs when the brain judges quantities relative to their surroundings rather than based on some objective standard. What's the corresponding cognitive illusion?</p>\n<p style=\"margin-bottom: 0in;\"><a id=\"more\"></a></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">In <a href=\"http://www.amazon.com/Predictably-Irrational-Revised-Expanded-Decisions/dp/0061353248/ref=sr_1_2?ie=UTF8&amp;s=books&amp;qid=1273789887&amp;sr=8-2\"><em>Predictably Irrational</em></a> (relatively recommended, even though the latter chapters sort of fail to live up to the ones mentioned here) Dan Ariely asks his students to evaluate (appropriately) three subscription plans to the Economist:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; padding-left: 180px;\"><img src=\"http://www.raikoth.net/Stuff/econ_sub1.gif\" alt=\"Economist subscription table\" width=\"350\" height=\"307\" /></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Ariely asked his subjects which plan they'd buy if they needed an Economist subscription. 84% wanted the combo plan, 16% wanted the web only plan, and no one wanted the print only plan. After all, the print plan cost exactly the same as the print + web plan, but the print + web plan was obviously better. Which raises the question: why even include a print-only plan? Isn't it something of a waste of space?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Actually, including the print-only plan turns out to be a very good business move for the Economist. Ariely removed the print-only plan from the choices. Now the options looked like this.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; padding-left: 180px;\"><img src=\"http://www.raikoth.net/Stuff/econ_sub2.gif\" alt=\"Economist subscription table, one option removed\" /></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">There shouldn't be any difference. After all, he'd only removed the plan no one chose, the plan no sane person would choose.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This time, 68% of students chose the web only plan and 32% the combo plan. That's a 52% shift in preferences between the exact same options.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The rational way to make the decision is to compare the value of a print subscription to the Economist (as measured by the opportunity cost of that money) to the difference in cost between the web and combo subscriptions. But this would return the same answer in both of the above cases, so the students weren't doing it that way.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">What it looks like the students were doing was perceiving relative value in the same way the eye perceives relative color. The ugly gray of the cube appeared blue when it was next to something yellow, and yellow when it was next to something blue. In the same way, the $125 cost of the combo subscription looks like good value next to a worse deal, and bad value next to a better deal.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">When the $125 combo subscription was placed next to a $125 plan with fewer features (print only instead of print plus web) it looked like a very good deal &ndash; the equivalent of placing an ugly gray square next to something yellow to make it look blue. Take away the yellow, or the artificially bad deal, and it doesn't look nearly as attractive.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This is getting deep into Dark Arts territory, and according to Predictably Irrational, the opportunity to use these powers for evil has not gone unexploited. Retailers will deliberately include in their selection a super deluxe luxury model much fancier and more expensive than they expect anyone to ever want. The theory is that consumers are balancing a natural hedonism that tells them to get the best model possible against a commitment to financial prudence. So most consumers, however much they like television, will have enough good sense to avoid buying a $2000 TV. But if the retailer carries a $4000 super-TV, the $2000 TV suddenly doesn't look quite so bad.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The obvious next question is &ldquo;How do I use this knowledge to trick hot girls or guys into going out with me?&rdquo; Dan Ariely decided to run some experiments on his undergraduate class. He took photographs of sixty students, then asked other students to rate their attractiveness. Next, he grouped the photos into pairs of equally attractive students. And next, he went to Photoshop and made a slightly less attractive version of each student: a blemish here, an asymmetry there.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Finally, he went around campus, finding students and showing them three photographs and asking which person the student would like to go on a date with. Two of the photographs were from one pair of photos ranked equally attractive. The third was a version of one of the two, altered to make it less attractive. So, for example, he might have two people, Alice and Brenda, who had been ranked equally attractive, plus a Photoshopped ugly version of Brenda.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The students overwhelmingly (75%) chose the person with the ugly double (Brenda in the example above), even though the two non-Photoshopped faces were equally attractive. Ariely then went so far as to recommend in his book that for best effect, you should go to bars and clubs with a wingman who is similar to you but less attractive. Going with a random ugly person would accomplish nothing, but going with someone similar to but less attractive than you would put you into a reference class and then bump you up to the top of the reference class, just like in the previous face experiment.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Ariely puts these studies in a separate chapter from his studies on <a href=\"/lw/j7/anchoring_and_adjustment/\">anchoring and adjustment</a> (which are also very good) but it all seems like the same process to me: being more interested in the difference between two values than in the absolute magnitude of them. All that makes anchoring and adjustment so interesting is that the two values have nothing in common with one another.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This process also has applications to happiness set points, status seeking, morality, dieting, larger-scale purchasing behavior, and akrasia which deserve a separate post</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "XYHzLjwYiqpeqaf4c": 1, "5f5c37ee1b5cdee568cfb128": 2, "RxuepsZgBEKax9bmP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kjArXFinD3deRZNRu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 70, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "2916", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 70, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bMkCEZoBNhgRBtzoj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-14T04:34:00.146Z", "modifiedAt": null, "url": null, "title": "Cambridge Less Wrong meetup this Sunday, May 16", "slug": "cambridge-less-wrong-meetup-this-sunday-may-16", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:15.184Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q37Yo9CWza2Knqnxe/cambridge-less-wrong-meetup-this-sunday-may-16", "pageUrlRelative": "/posts/Q37Yo9CWza2Knqnxe/cambridge-less-wrong-meetup-this-sunday-may-16", "linkUrl": "https://www.lesswrong.com/posts/Q37Yo9CWza2Knqnxe/cambridge-less-wrong-meetup-this-sunday-may-16", "postedAtFormatted": "Friday, May 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20Less%20Wrong%20meetup%20this%20Sunday%2C%20May%2016&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20Less%20Wrong%20meetup%20this%20Sunday%2C%20May%2016%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ37Yo9CWza2Knqnxe%2Fcambridge-less-wrong-meetup-this-sunday-may-16%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20Less%20Wrong%20meetup%20this%20Sunday%2C%20May%2016%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ37Yo9CWza2Knqnxe%2Fcambridge-less-wrong-meetup-this-sunday-may-16", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ37Yo9CWza2Knqnxe%2Fcambridge-less-wrong-meetup-this-sunday-may-16", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p><a href=\"/lw/22a/boston_area_meetup_april_18/\">Last month's Cambridge, Massachusetts meetup</a> was a success, with six people in attendance and some very interesting discussions. Meetups will recur on the third Sunday of each month, so the next one is this week, at the same time and place: 4pm at the Clear Conscience Cafe at <a href=\"http://maps.google.com/maps?q=Clear+Conscience+Cafe+Cambridge+MA&amp;oe=utf-8&amp;client=firefox-a&amp;ie=UTF8&amp;hl=en&amp;hq=Clear+Conscience+Cafe&amp;hnear=Cambridge,+MA&amp;ll=42.365056,-71.102411&amp;spn=0.003789,0.006539&amp;t=h&amp;z=18&amp;iwloc=A\">581 Massachusetts Avenue</a> Cambridge, MA, near the Central Square T station. Please comment if you plan to attend.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q37Yo9CWza2Knqnxe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 5.864402184462771e-07, "legacy": true, "legacyId": "2917", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8wHa4DLW4pADHMPwJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-14T05:37:46.850Z", "modifiedAt": null, "url": null, "title": "Bay Area Lesswrong Meet up Sunday May 16", "slug": "bay-area-lesswrong-meet-up-sunday-may-16", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:14.579Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "LucasSloan", "createdAt": "2009-05-28T05:04:38.345Z", "isAdmin": false, "displayName": "LucasSloan"}, "userId": "ouo6Fqn5kTNY7LvqM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2YhBQsp4m4yYkjkSE/bay-area-lesswrong-meet-up-sunday-may-16", "pageUrlRelative": "/posts/2YhBQsp4m4yYkjkSE/bay-area-lesswrong-meet-up-sunday-may-16", "linkUrl": "https://www.lesswrong.com/posts/2YhBQsp4m4yYkjkSE/bay-area-lesswrong-meet-up-sunday-may-16", "postedAtFormatted": "Friday, May 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bay%20Area%20Lesswrong%20Meet%20up%20Sunday%20May%2016&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABay%20Area%20Lesswrong%20Meet%20up%20Sunday%20May%2016%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YhBQsp4m4yYkjkSE%2Fbay-area-lesswrong-meet-up-sunday-may-16%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bay%20Area%20Lesswrong%20Meet%20up%20Sunday%20May%2016%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YhBQsp4m4yYkjkSE%2Fbay-area-lesswrong-meet-up-sunday-may-16", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YhBQsp4m4yYkjkSE%2Fbay-area-lesswrong-meet-up-sunday-may-16", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>No invitations to Benton House have been forthcoming for the last couple months, so I thought we might do one located in a restaurant somewhere.&nbsp; I would suggest <a href=\"http://www.holderscountryinn.com/\">Holder's Country Inn</a>, but if anyone else has any suggestions, I'm more than willing to change venue.&nbsp; I would suggest a meeting time of 7, but that is also amenable to change.</p>\n<p>Edit: &nbsp;We're meeting at the Inn. &nbsp;I'll be out front at 7 with a sign saying Less Wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2YhBQsp4m4yYkjkSE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 5.864532508263888e-07, "legacy": true, "legacyId": "2918", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-14T17:10:15.475Z", "modifiedAt": null, "url": null, "title": "The Social Coprocessor Model", "slug": "the-social-coprocessor-model", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:57.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aTtf9iERDoptPD33j/the-social-coprocessor-model", "pageUrlRelative": "/posts/aTtf9iERDoptPD33j/the-social-coprocessor-model", "linkUrl": "https://www.lesswrong.com/posts/aTtf9iERDoptPD33j/the-social-coprocessor-model", "postedAtFormatted": "Friday, May 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Social%20Coprocessor%20Model&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Social%20Coprocessor%20Model%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaTtf9iERDoptPD33j%2Fthe-social-coprocessor-model%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Social%20Coprocessor%20Model%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaTtf9iERDoptPD33j%2Fthe-social-coprocessor-model", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaTtf9iERDoptPD33j%2Fthe-social-coprocessor-model", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 919, "htmlBody": "<p>Followup to: <a href=\"/lw/28l/do_you_have_highfunctioning_aspergers_syndrome/\">Do you have High-Functioning Asperger's Syndrome?</a><strong><br /></strong></p>\n<p>LW reader <a href=\"/user/Madbadger/\">Madbadger</a> uses the metaphor of a GPU and a CPU in a desktop system to think about people with Asperger's Syndrome: general intelligence is like a CPU, being universal but only mediocre at any particular task, whereas the \"social coprocessor\" brainware in a <a href=\"http://en.wikipedia.org/wiki/Neurotypical\">Neurotypical</a> brain is like a GPU: highly specialized but great at what it does. Neurotypical people are like computers with measly Pentium IV processors, but expensive Radeon HD 4890 GPUs. A High-functioning AS person is an <a href=\"http://www.intel.com/products/processor/corei7ee/index.htm\">Intel Core i7 Extreme Edition</a> - with on-board graphics!</p>\n<p>This analogy also covers the spectrum view of social/empathic abilities, you can think about having a <em>weaker </em>social coprocessor than average if you have some of the tendencies of AS but not others. You can even think of your score on the <a href=\"http://www.wired.com/wired/archive/9.12/aqtest.html\">AQ Test</a> as being like the <a href=\"http://www.tomshardware.com/reviews/radeon-hd-5970,2474-8.html\">Tom's Hardware Rating</a> of your Coprocessor. (Lower numbers are better!).</p>\n<p><a id=\"more\"></a></p>\n<p>If you lack that powerful social coprocessor, what can you do? Well, you'll have to <em>run your social interactions \"in software\"</em>, i.e. explicitly reason through the complex human social game that most people play without ever really understanding. There are several tricks that a High-functioning AS person can use in this situation:</p>\n<ul>\n<li><em><strong>(Most importantly) Find a community of others</strong></em> - who are trying to solve the same problem (Though be careful <em>not </em>to wind up with a group of people who have weaker social coprocessors and aren't doing anything about it, as you will tend to conform to this behavior). Having even a few friends who are in a similar niche to you is worth a huge amount in terms of motivating social pressure, as a sounding board to bounce ideas off, and simply for the instinctive feel of support that having a group of people \"in the same boat as you\" gives. </li>\n</ul>\n<ul>\n<li><em><strong>Cached answers</strong></em> - you can precompute the \"right\" responses to social situations. Probably the best example of this is the answer to the \"buy me a drink\" problem: you approach an attractive NT person who you might like as a future partner. After a short time, they ask you to buy them a drink. The logical answer to this question is \"what kind of drink would you like?\", because in most social situations where you want to build up a positive relationship with a person, it is best to comply with their requests; not creating explicit conflict is usually a safe heuristic. But this is the <span style=\"color: #000000;\"><em>wrong </em></span>answer in this context, and you can store in your cache of counter-intuitive answers.&nbsp;</li>\n</ul>\n<ul>\n<li><em><strong>Scientific theories</strong></em> <strong><em>of social games</em></strong> - including game theory and especially <a href=\"http://en.wikipedia.org/wiki/Signaling_games\">signaling games</a>, <a href=\"http://en.wikipedia.org/wiki/Information_economics\">information economics</a> and <a href=\"http://en.wikipedia.org/wiki/Evolutionary_psychology\">evolutionary psychology</a>. Building on the \"buy me a drink\" problem, instead of simply storing the answer as an exception, you can use evolutionary psychology and information economics to see the underlying pattern so that you can correctly answer the \"drink\" problem and many other similar problems. The NT is using the drink request to solve a <a href=\"http://en.wikipedia.org/wiki/Cheap_talk\">cheap talk</a> problem - they don't really want the drink, they want to know if you have higher dating market value than them, for example higher social status, income, success with other partners, etc. This is because evolutionary psychology makes some people want high-status people as partners. If they just asked you directly for these facts about yourself, you would have a strong incentive to lie. So they make a request that is somewhat rude, where only a lower-status suitor who thought they were worth \"sucking up to\" would comply, and then reject suitors who comply. This is really a kind of <a href=\"http://en.wikipedia.org/wiki/Screening_game\">screening</a>, where ability to give the \"right\" answer plays the role of a credential. Neurotypicals play some devious games, and this is actually quite a tame example. </li>\n</ul>\n<ul>\n<li><em><strong>The wisdom of nature heuristic</strong></em> - the human social coprocessor is perfectly optimized for an environment that we are no longer in. The <a href=\"http://en.wikipedia.org/wiki/Evolutionary_psychology#Environment_of_evolutionary_adaptedness\">EEA</a> has significant differences to the present environment: most prominently, we have police and laws so other humans mostly don't act on their desire to kill you. This means that you can get away with things that you have an innate fear of, and you should strongly distrust your fear of other people's disapproval. There are also some reliable proxies of fitness that are no longer reliable, for example height (can be modified by higher shoes - a trick that women have cottoned on to, but men are <a href=\"http://meteuphoric.wordpress.com/\">totally missing out on</a>). </li>\n</ul>\n<ul>\n<li><strong><em>Neuroplasticity and desensitization </em></strong>- your brain is plastic: you can train it and you can desensitize yourself to situations that scare you. Desensitization relies most on objectifing and dis-identifying with your maladaptive gut fear of doing something scary, for example public speaking or attending a social function where you know almost no-one. Realize that your brain contains small, simple, dumb circuits that produce your emotions, and some of them are outright harmful to you. You need to ignore their output and expose yourself to the stimuli. </li>\n</ul>\n<ul>\n<li><strong><em>Realizing that your brain contains nonrational psychological variables</em></strong> - that can be reset, often through a process known as \"self transformation\". Examples include general outlook on life, confidence, self-estimated status, self-esteem, sense of \"fun\" and rational irrationalities such as vengefulness, honor and pride. Approaches to self-transformation include eastern-style \"spirituality\", \"new age\" positive psychology works such as <a href=\"http://www.eckharttolle.com/home/\">Eckhard Tolle</a>, and more mainstream self-help like <a href=\"http://en.wikipedia.org/wiki/Tony_Robbins#Unlimited_Power\">Tony Robbins</a>. Changing your use of self-talk and framing is critical to resetting these variables. </li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"uRcuHKpKA7xNnZQ2F": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aTtf9iERDoptPD33j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 31, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "2908", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 625, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vHDk5xr9JDC64rb8T"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-14T18:11:56.265Z", "modifiedAt": null, "url": null, "title": "Study: Encouraging Obedience Considered Harmful", "slug": "study-encouraging-obedience-considered-harmful", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:16.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AXYnfMMcJRnzLgiqZ/study-encouraging-obedience-considered-harmful", "pageUrlRelative": "/posts/AXYnfMMcJRnzLgiqZ/study-encouraging-obedience-considered-harmful", "linkUrl": "https://www.lesswrong.com/posts/AXYnfMMcJRnzLgiqZ/study-encouraging-obedience-considered-harmful", "postedAtFormatted": "Friday, May 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Study%3A%20Encouraging%20Obedience%20Considered%20Harmful&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudy%3A%20Encouraging%20Obedience%20Considered%20Harmful%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAXYnfMMcJRnzLgiqZ%2Fstudy-encouraging-obedience-considered-harmful%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Study%3A%20Encouraging%20Obedience%20Considered%20Harmful%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAXYnfMMcJRnzLgiqZ%2Fstudy-encouraging-obedience-considered-harmful", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAXYnfMMcJRnzLgiqZ%2Fstudy-encouraging-obedience-considered-harmful", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 250, "htmlBody": "<p>A while back I did a couple of posts on the <a href=\"/lw/25/on_the_care_and_feeding_of_young_rationalists/\">care and feeding</a> of <a href=\"/lw/2q/on_juvenile_fiction/\">young rationalists</a>. Though it is not new, I recently found a <em>truly excellent</em>&nbsp;post on this topic, in Dale Mcgowan's blog, <a href=\"http://parentingbeyondbelief.com/blog\">The Meming of Life</a>. The post details a survey carried out on ordinary citizens of Hitler's Germany, searching for correlations between style of upbringing, and adult moral decisions.&nbsp;</p>\n<p><span style=\"font-family: Georgia, serif; font-size: 14px; color: #292929;\"> </span></p>\n<blockquote>\n<p style=\"margin-top: 0px; margin-right: 30px; margin-bottom: 20px; margin-left: 30px; font-weight: normal; font-size: 14px; line-height: 20px; padding: 0px;\">Everyday Germans of the Nazi period are the focus of a fascinating study discussed in the&nbsp;<a style=\"color: #3f606f; text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://parentingbeyondbelief.com/seminars.htm\">PBB seminars</a>&nbsp;and in the Ethics chapter of&nbsp;<a style=\"color: #3f606f; text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://www.amazon.com/gp/product/0814410960?&amp;camp=212361&amp;creative=383957&amp;linkCode=waf&amp;tag=parebeyobeli-20\"><em style=\"padding: 0px; margin: 0px;\">Raising Freethinkers</em></a>. For their book&nbsp;<a style=\"color: #3f606f; text-decoration: none; padding: 0px; margin: 0px;\" href=\"http://www.amazon.com/Altruistic-Personality-Rescuers-Jews-Europe/dp/0029238293\"><em style=\"padding: 0px; margin: 0px;\">The Altruistic Personality</em></a>, researchers Samuel and Pearl Oliner conducted over 700 interviews with survivors of Nazi-occupied Europe. Included were both &ldquo;rescuers&rdquo; (those who actively rescued victims of persecution) and &ldquo;non-rescuers&rdquo; (those who were either passive in the face of the persecution or actively involved in it). The study revealed interesting differences in the upbringing of the two groups &mdash; specifically the language and practices that parents used to teach their values.</p>\n<p style=\"margin-top: 0px; margin-right: 30px; margin-bottom: 20px; margin-left: 30px; font-weight: normal; font-size: 14px; line-height: 20px; padding: 0px;\">Non-rescuers were 21 times more likely than rescuers to have been raised in families that emphasized obedience&mdash;being given rules that were to be followed without question&mdash;while rescuers were over three times more likely than non-rescuers to identify &ldquo;reasoning&rdquo; as an element of their moral education. &ldquo;Explained,&rdquo; the authors said, is the single most common word used by rescuers in describing their parents&rsquo; ways of talking about rules and ethical ideas.</p>\n</blockquote>\n<div><a href=\"http://parentingbeyondbelief.com/blog/?p=239\">Not Just Because I Said So</a></div>\n<div>For anyone interested in rational and ethical upbringing, I really cannot recommend &nbsp;<a href=\"http://parentingbeyondbelief.com/blog\">Meming of Life</a>&nbsp;&nbsp;strongly enough.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AXYnfMMcJRnzLgiqZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 31, "extendedScore": null, "score": 5.866073930209978e-07, "legacy": true, "legacyId": "2919", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SWraogEDJ6gocpvwa", "4GeE83592epCErQse"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-15T20:35:37.449Z", "modifiedAt": null, "url": null, "title": "The Math of When to Self-Improve", "slug": "the-math-of-when-to-self-improve", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:09.507Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9s26RjJoeHjSjX6NR/the-math-of-when-to-self-improve", "pageUrlRelative": "/posts/9s26RjJoeHjSjX6NR/the-math-of-when-to-self-improve", "linkUrl": "https://www.lesswrong.com/posts/9s26RjJoeHjSjX6NR/the-math-of-when-to-self-improve", "postedAtFormatted": "Saturday, May 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Math%20of%20When%20to%20Self-Improve&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Math%20of%20When%20to%20Self-Improve%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9s26RjJoeHjSjX6NR%2Fthe-math-of-when-to-self-improve%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Math%20of%20When%20to%20Self-Improve%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9s26RjJoeHjSjX6NR%2Fthe-math-of-when-to-self-improve", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9s26RjJoeHjSjX6NR%2Fthe-math-of-when-to-self-improve", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 700, "htmlBody": "<p><em>An economic analysis of how much time an individual or group should spend improving the way they do things as opposed to just doing them.&nbsp; Requires understanding of integrals.<br /></em></p>\n<p><strong>An Explanation of Discount Rates</strong></p>\n<p>Your annual discount rate for money is 1.05 if you're indifferent between receiving $1.00 now and $1.05 in a year.&nbsp; Question to confirm understanding (requires insight and a calculator): If a person is indifferent between receiving $5.00 at the beginning of any 5-day period and $5.01 at the end of it, what is their annual discount rate?&nbsp; Answer in <a href=\"http://rot13.com/\">rot13</a>: Gurve naahny qvfpbhag engr vf nobhg bar cbvag bar svir frira.</p>\n<p>If your discount rate is significantly different than prevailing interest rates, you can easily acquire value for yourself by investing or borrowing money.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>An Explanation of Net Present Value</strong></p>\n<p>Discount rates are really cool because they let you assign an instantaneous value to any income-generating asset.&nbsp; For example, let's say I have a <a href=\"http://www.reddit.com/r/AskReddit/comments/8xasf/dear_reddit_what_are_the_things_that_you_secretly/c0aq854\">made-for-Adsense pop culture site that is bringing in $2000 a year</a>, and someone has offered to buy it.&nbsp; Normally figuring out the minimum price I'm willing to sell for would require some deliberation, but if I've already deliberated to discover my discount rate, I can compute an integral instead.</p>\n<p>To make this calculation reusable, I'm going to let <strong>a</strong> be the annual income generated by the site (in this case $2000) and <strong>r</strong> be my discount rate.&nbsp; For the sake of calculation, we'll assume that the $2000 is distributed perfectly evenly throughout the year.</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\lim_{z\\to\\infty}\\int_0^z\\frac{a}{r^t}\\,\\mathrm{d}t=\\lim_{z\\to\\infty}-\\frac{a}{r^{t}\\ln{r}}\\Big|_{t=0}^{t=z}=\\frac{a}{\\ln{r}}\" alt=\"\" width=\"305\" height=\"44\" /></p>\n<p>Question to confirm understanding: If a person has a discount rate of 1.05, at what price would they be indifferent to selling the aforementioned splog?&nbsp; Answer in <a href=\"http://rot13.com/\">rot13</a>: Nobhg sbegl gubhfnaq avar uhaqerq avargl-gjb qbyynef.</p>\n<p><strong>When to Self-Improve</strong></p>\n<p>This question of when to self-improve is complicated by the fact that self-improvement is not an either-or proposition.&nbsp; It's possible to generate value as you're self-improving.&nbsp; For example, you can imagine an independent software developer who's trying to choose between improving their tools and working on creating software that will turn a profit.&nbsp; Although the developer's skills will not improve as quickly through the process of software creation as they would through tool upgrades, they still will improve.</p>\n<p>My proposed solution to this problem is for the developer to analyze themself as an income-generating asset.</p>\n<p>The first question is what the software developer's discount rate is.&nbsp; We'll call that <strong>r</strong>.</p>\n<p>The second question is how much income they could produce annually if they started working on software creation full-time right now.&nbsp; We'll call that amount <strong>f</strong>.&nbsp; (If each software product they produce is itself an income-generating asset, then the developer will need to estimate the average net present value of each of those assets, along with the average time to completion of each, to estimate their own income.)</p>\n<p>Then, for each of the tool-upgrade and code-now approaches, the developer needs to estimate</p>\n<ul>\n<li>What their instantaneous annual income from software development is from pursuing that strategy.&nbsp; (For the tool-upgrade approach, that annual income will obviously be 0.)&nbsp; We'll call that <strong>p</strong> for present.</li>\n<li>What the instantaneous annual growth factor in their full-time development income is from pursuing that strategy.&nbsp; (For example, if working on improving their tools currently offers the software developer the opportunity to improve their wealth creation skills at a rate of a 50% increase in their ability per year, their growth factor would be 1.5.)&nbsp; We'll call that <strong>g</strong> for growth.</li>\n</ul>\n<p>Given all these parameters, the developer's instantaneous annual value production in a given scenario will be</p>\n<p><img src=\"http://www.codecogs.com/png.latex?p+\\frac{f}{\\ln{r}}%28g%20-%201%29\" alt=\"\" /></p>\n<p>You should try to figure out why the equation makes sense for yourself.&nbsp; If you're having trouble post in the comments.</p>\n<p><em>If this post goes over well, I'm thinking of writing a sequel called <strong>When to Self-Improve in Practice</strong> where I discuss practical application of the value-creation formula.&nbsp; Feel free to comment or PM with ideas, questions, or a description of your situation in life so I can think of a new angle on how this sort of thinking might be applied.&nbsp; (Exercise for the reader: Modify this thinking for a college student who's trying to decide between two summer internships and has one year left until graduation.)</em></p>\n<p><strong>Edit</strong>: Making LaTeX work in comments manually is a royal pain.&nbsp; <a href=\"http://lwlatex.appspot.com/\">Use this instead</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9s26RjJoeHjSjX6NR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 5.869313162399914e-07, "legacy": true, "legacyId": "2894", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>An economic analysis of how much time an individual or group should spend improving the way they do things as opposed to just doing them.&nbsp; Requires understanding of integrals.<br></em></p>\n<p><strong id=\"An_Explanation_of_Discount_Rates\">An Explanation of Discount Rates</strong></p>\n<p>Your annual discount rate for money is 1.05 if you're indifferent between receiving $1.00 now and $1.05 in a year.&nbsp; Question to confirm understanding (requires insight and a calculator): If a person is indifferent between receiving $5.00 at the beginning of any 5-day period and $5.01 at the end of it, what is their annual discount rate?&nbsp; Answer in <a href=\"http://rot13.com/\">rot13</a>: Gurve naahny qvfpbhag engr vf nobhg bar cbvag bar svir frira.</p>\n<p>If your discount rate is significantly different than prevailing interest rates, you can easily acquire value for yourself by investing or borrowing money.</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"An_Explanation_of_Net_Present_Value\">An Explanation of Net Present Value</strong></p>\n<p>Discount rates are really cool because they let you assign an instantaneous value to any income-generating asset.&nbsp; For example, let's say I have a <a href=\"http://www.reddit.com/r/AskReddit/comments/8xasf/dear_reddit_what_are_the_things_that_you_secretly/c0aq854\">made-for-Adsense pop culture site that is bringing in $2000 a year</a>, and someone has offered to buy it.&nbsp; Normally figuring out the minimum price I'm willing to sell for would require some deliberation, but if I've already deliberated to discover my discount rate, I can compute an integral instead.</p>\n<p>To make this calculation reusable, I'm going to let <strong>a</strong> be the annual income generated by the site (in this case $2000) and <strong>r</strong> be my discount rate.&nbsp; For the sake of calculation, we'll assume that the $2000 is distributed perfectly evenly throughout the year.</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\lim_{z\\to\\infty}\\int_0^z\\frac{a}{r^t}\\,\\mathrm{d}t=\\lim_{z\\to\\infty}-\\frac{a}{r^{t}\\ln{r}}\\Big|_{t=0}^{t=z}=\\frac{a}{\\ln{r}}\" alt=\"\" width=\"305\" height=\"44\"></p>\n<p>Question to confirm understanding: If a person has a discount rate of 1.05, at what price would they be indifferent to selling the aforementioned splog?&nbsp; Answer in <a href=\"http://rot13.com/\">rot13</a>: Nobhg sbegl gubhfnaq avar uhaqerq avargl-gjb qbyynef.</p>\n<p><strong id=\"When_to_Self_Improve\">When to Self-Improve</strong></p>\n<p>This question of when to self-improve is complicated by the fact that self-improvement is not an either-or proposition.&nbsp; It's possible to generate value as you're self-improving.&nbsp; For example, you can imagine an independent software developer who's trying to choose between improving their tools and working on creating software that will turn a profit.&nbsp; Although the developer's skills will not improve as quickly through the process of software creation as they would through tool upgrades, they still will improve.</p>\n<p>My proposed solution to this problem is for the developer to analyze themself as an income-generating asset.</p>\n<p>The first question is what the software developer's discount rate is.&nbsp; We'll call that <strong>r</strong>.</p>\n<p>The second question is how much income they could produce annually if they started working on software creation full-time right now.&nbsp; We'll call that amount <strong>f</strong>.&nbsp; (If each software product they produce is itself an income-generating asset, then the developer will need to estimate the average net present value of each of those assets, along with the average time to completion of each, to estimate their own income.)</p>\n<p>Then, for each of the tool-upgrade and code-now approaches, the developer needs to estimate</p>\n<ul>\n<li>What their instantaneous annual income from software development is from pursuing that strategy.&nbsp; (For the tool-upgrade approach, that annual income will obviously be 0.)&nbsp; We'll call that <strong>p</strong> for present.</li>\n<li>What the instantaneous annual growth factor in their full-time development income is from pursuing that strategy.&nbsp; (For example, if working on improving their tools currently offers the software developer the opportunity to improve their wealth creation skills at a rate of a 50% increase in their ability per year, their growth factor would be 1.5.)&nbsp; We'll call that <strong>g</strong> for growth.</li>\n</ul>\n<p>Given all these parameters, the developer's instantaneous annual value production in a given scenario will be</p>\n<p><img src=\"http://www.codecogs.com/png.latex?p+\\frac{f}{\\ln{r}}%28g%20-%201%29\" alt=\"\"></p>\n<p>You should try to figure out why the equation makes sense for yourself.&nbsp; If you're having trouble post in the comments.</p>\n<p><em>If this post goes over well, I'm thinking of writing a sequel called <strong>When to Self-Improve in Practice</strong> where I discuss practical application of the value-creation formula.&nbsp; Feel free to comment or PM with ideas, questions, or a description of your situation in life so I can think of a new angle on how this sort of thinking might be applied.&nbsp; (Exercise for the reader: Modify this thinking for a college student who's trying to decide between two summer internships and has one year left until graduation.)</em></p>\n<p><strong>Edit</strong>: Making LaTeX work in comments manually is a royal pain.&nbsp; <a href=\"http://lwlatex.appspot.com/\">Use this instead</a>.</p>", "sections": [{"title": "An Explanation of Discount Rates", "anchor": "An_Explanation_of_Discount_Rates", "level": 1}, {"title": "An Explanation of Net Present Value", "anchor": "An_Explanation_of_Net_Present_Value", "level": 1}, {"title": "When to Self-Improve", "anchor": "When_to_Self_Improve", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "72 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-17T02:11:02.211Z", "modifiedAt": null, "url": null, "title": "Preface to a Proposal for a New Mode of Inquiry", "slug": "preface-to-a-proposal-for-a-new-mode-of-inquiry", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:17.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Daniel_Burfoot", "createdAt": "2009-03-12T02:28:50.970Z", "isAdmin": false, "displayName": "Daniel_Burfoot"}, "userId": "XhcXE3Qk5adX6v2Cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KYZfkZfy3RNxbZoYo/preface-to-a-proposal-for-a-new-mode-of-inquiry", "pageUrlRelative": "/posts/KYZfkZfy3RNxbZoYo/preface-to-a-proposal-for-a-new-mode-of-inquiry", "linkUrl": "https://www.lesswrong.com/posts/KYZfkZfy3RNxbZoYo/preface-to-a-proposal-for-a-new-mode-of-inquiry", "postedAtFormatted": "Monday, May 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Preface%20to%20a%20Proposal%20for%20a%20New%20Mode%20of%20Inquiry&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APreface%20to%20a%20Proposal%20for%20a%20New%20Mode%20of%20Inquiry%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYZfkZfy3RNxbZoYo%2Fpreface-to-a-proposal-for-a-new-mode-of-inquiry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Preface%20to%20a%20Proposal%20for%20a%20New%20Mode%20of%20Inquiry%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYZfkZfy3RNxbZoYo%2Fpreface-to-a-proposal-for-a-new-mode-of-inquiry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYZfkZfy3RNxbZoYo%2Fpreface-to-a-proposal-for-a-new-mode-of-inquiry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1564, "htmlBody": "<p>Summary: The problem of AI has turned out to be a lot harder than was originally thought. One hypothesis is that the obstacle is not a shortcoming of mathematics or theory, but limitations in the philosophy of science. This article is a preview of a series of posts that will describe how, by making a minor revision in our understanding of the scientific method, further progress can be achieved by establishing AI as an empirical science.</p>\n<p>&nbsp;</p>\n<p>The field of artificial intelligence has been around for more than fifty years. If one takes an optimistic view of things, its possible to believe that a lot of progress has been made. A <a href=\"http://en.wikipedia.org/wiki/Computer_chess\">chess program</a> defeated the top-ranked human grandmaster. Robotic cars <a href=\"http://en.wikipedia.org/wiki/DARPA_Grand_Challenge_%282005%29\">drove autonomously</a> across 132 miles of Mojave desert. And Google seems to have <a href=\"http://www.nytimes.com/2010/03/09/technology/09translate.html\">made great strides</a> in machine translation, apparently by feeding massive quantities of data to a statistical learning algorithm.</p>\n<p>But even as the field has advanced, the horizon has seemed to recede. In some sense the field's successes make its failures all the more conspicuous. The best chess programs are better than any human, but <a href=\"http://en.wikipedia.org/wiki/Computer_Go\">go is still challenging</a> for computers. Robotic cars can drive across the desert, but they're not ready to share the road with human drivers. And Google is pretty good at translating Spanish to English, but still produces howlers when translating Japanese to English. The failures indicate that, instead of being threads in a majestic general theory, the successes were just narrow, isolated solutions to problems that turned out to be easier than they originally appeared.</p>\n<p><a id=\"more\"></a></p>\n<p>So what went wrong, and how to move forward? Most mainstream AI researchers are reluctant to provide clear answers to this question, so instead one must read behind the lines in the literature. Every new paper in AI implicitly suggests that the research subfield of which it is a part will, if vigorously pursued, lead to dramatic progress towards intelligence. People who study <a href=\"http://en.wikipedia.org/wiki/Reinforcement_learning\">reinforcement learning</a> think the answer is to develop better versions of algorithms like Q-Learning and temporal difference (TD) learning. The researchers behind the <a href=\"http://seedmagazine.com/content/article/out_of_the_blue/\">IBM Blue Brain</a> project think the answer is to conduct massive neural simulations. For some roboticists, the answer involves the idea of <a href=\"http://en.wikipedia.org/wiki/Embodied_cognition\">embodiment</a>: since the purpose of the brain is to control the body, to understand intelligence one should build robots, put them in the real world, watch how they behave, notice the problems they encounter, and then try to solve those problems. Practitioners of computer vision believe that since the visual cortex takes up such a huge fraction of total brain volume, the best way to understand general intelligence is to first study vision.</p>\n<p>Now, I have some sympathy for the views mentioned above. If I had been thinking seriously about AI in the 80s, I would probably have gotten excited about the idea of reinforcement learning. But reinforcement learning is now basically an old idea, as is embodiment (this tradition can be traced back to the <a href=\"http://people.csail.mit.edu/brooks/papers/representation.pdf\">seminal</a> <a href=\"http://people.csail.mit.edu/brooks/papers/elephants.pdf\">papers</a> by Rodney Brooks in the early 90s), and computer vision is almost as old as AI itself. If these avenues really led to some kind of amazing result, it probably would already have been found.</p>\n<p>So, dissatisfied with the ideas of my predecessors, I've taken some trouble to develop my own hypothesis regarding the question of how to move forward. And desperate times call for desperate measures: the long failure of AI to live up to its promises suggests that the obstacle is no small thing that can be solved merely by writing down a new algorithm or theorem. What I propose is nothing less than a complete reexamination of our answers to fundamental philosophical questions. What is a scientific theory? What is the real meaning of the scientific method (and why did it take so long for people to figure out the part about <em>empirical verification</em>)? How do we separate science from pseudoscience? What is Ockham's Razor really telling us? Why does physics work so amazingly, terrifyingly well, while fields like economics and nutrition stumble?</p>\n<p>Now, my answers to these fundamental questions aren't going to be radical. It all <a href=\"http://wiki.lesswrong.com/wiki/Egan%27s_law\">adds up to normality</a>. No one who is up-to-date on topics like information theory, machine learning, and Bayesian statistics will be shocked by what I have to say here. But my answers are slightly different from the traditional ones. And by starting from a slightly different philosophical origin, and following the logical path as it opened up in front of me, I've reached a clearing in the conceptual woods that is bright, beautiful, and silent.</p>\n<p>Without getting too far ahead of myself, let me give you a bit of a preview of the ideas I'm going to discuss. One highly relevant issue is the role that other, more mature fields have had in shaping modern AI. One obvious influence comes from computer science, since presumably AI will eventually be built using software. But this fact appears irrelevant to me, and so the influence of computer science on AI seems like a disastrous historical accident. To suggest that computer science should be an important influence on AI is a bit like suggesting that woodworking should be an important influence on music, since most musical instruments are made out of wood. Another influence, that should in principle be healthy but in practice isn't, comes from physics. Unfortunately, for the most part, AI researchers have imitated only the superficial appearance of physics - its use of sophisticated mathematics - while ignoring its essential trait, which is its obsession with reality. In my view, AI can and must become a hard, empirical science, in which researchers propose, test, refine, and often discard theories of empirical reality. But theories of AI will not work like theories of physics. We'll see that AI can be considered, in some sense, the epistemological converse of physics. Physics works by using complex deductive reasoning (calculus, differential equations, group theory, etc) built on top of a minimalist inductive framework (the physical laws). Human intelligence, in contrast, is based on a complex inductive foundation, supplemented by minor deductive operations. In many ways, AI will come to resemble disciplines like botany, zoology, and cartography - fields in which the researchers' core methodological impulse is to go out into the world and write down what they see.</p>\n<p>An important aspect of my proposal will be to expand the definitions of the words \"scientific theory\" and \"scientific method\". A scientific theory, to me, is a computational tool that can be used to produce reliable predictions, and a scientific method is a process of obtaining good scientific theories. Botany and zoology make reliable predictions, so they must have scientific theories. In contrast to physics, however, they depend far less on the use of controlled experiments. The analogy to human learning is strong: humans achieve the ability to make reliable predictions without conducting controlled experiments. Typically, though, experimental sciences are considered to be far harder, more rigorous, and more quantitative than observational sciences. But I will propose a generalized version of the scientific method, which includes human learning as a special case, and shows how to make observational sciences just as hard, rigorous, and quantitative as physics.</p>\n<p>As a result of learning, humans achieve the ability to make fairly good predictions about some types of phenomena. It seems clear that a major component of that predictive power is the ability to transform raw sensory data into abstract perceptions. The photons fall on my eye in a certain pattern which I recognize as a doorknob, allowing me to predict that if I turn the knob, the door will open. So humans are amazingly talented at perception, and modestly good at prediction. Are there any other ingredients necessary for intelligence? My answer is: not really. In particular, in my view humans are terrible at planning. Our decision making algorithm is not much more than: invent a plan, try to predict what will happen based on that plan, and if the prediction seems good, implement the plan. All the \"magic\" really comes from the ability to make accurate predictions. So a major difference in my approach as opposed to traditional AI is that the emphasis is on prediction through learning and perception, as opposed to planning through logic and deduction.</p>\n<p>As a final point, I want to note that my proposal is not analogous to or in conflict with theories of brain function like <a href=\"http://www.scholarpedia.org/article/Deep_belief_networks\">deep belief networks</a>, <a href=\"http://en.wikipedia.org/wiki/Neural_Darwinism\">neural Darwinism</a>, <a href=\"http://en.wikipedia.org/wiki/Physical_symbol_system\">symbol systems</a>, or <a href=\"http://en.wikipedia.org/wiki/Hierarchical_temporal_memory\">hierarchical temporal memories</a>. My proposal is like an interface: it specifies the input and the output, but not the implementation. It embodies an immense and multifaceted Question, to which I have no real answer. But, crucially, the Question comes with a rigorous evaluation procedure that allows one to compare candidate answers. Finding those answers will be an awesome challenge, and I hope I can convince some of you to work with me on that challenge.</p>\n<p>I am going to post an outline of my proposal over the next couple of weeks. I expect most of you will disagree with most of it, but I hope we can at least identify concretely the points at which our views diverge. I am very interested in feedback and criticism, both regarding material issues (since we <a href=\"/lw/1wu/reasoning_isnt_about_logic_its_about_arguing/\">reason to argue</a>), and on issues of style and presentation. The ideas are not fundamentally difficult; if you can't understand what I'm saying, I will accept at least three quarters of the blame.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KYZfkZfy3RNxbZoYo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 2, "extendedScore": null, "score": 5.872948310859943e-07, "legacy": true, "legacyId": "2925", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zNawPJRktcJGWrtt9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-17T03:18:18.558Z", "modifiedAt": null, "url": null, "title": "Test on AWS", "slug": "test-on-aws", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wmoore", "createdAt": "2009-02-17T05:49:50.396Z", "isAdmin": false, "displayName": "wmoore"}, "userId": "EgQZcMBqxf6sGmKfi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mss2y5AFugkjS5Nwx/test-on-aws", "pageUrlRelative": "/posts/mss2y5AFugkjS5Nwx/test-on-aws", "linkUrl": "https://www.lesswrong.com/posts/mss2y5AFugkjS5Nwx/test-on-aws", "postedAtFormatted": "Monday, May 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Test%20on%20AWS&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATest%20on%20AWS%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmss2y5AFugkjS5Nwx%2Ftest-on-aws%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Test%20on%20AWS%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmss2y5AFugkjS5Nwx%2Ftest-on-aws", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmss2y5AFugkjS5Nwx%2Ftest-on-aws", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p>This is a test article</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mss2y5AFugkjS5Nwx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5.873086129122403e-07, "legacy": true, "legacyId": "2926", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-17T22:26:41.804Z", "modifiedAt": null, "url": null, "title": "Multiple Choice", "slug": "multiple-choice", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:34.830Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T4mCxhpzRGrTWtxcD/multiple-choice", "pageUrlRelative": "/posts/T4mCxhpzRGrTWtxcD/multiple-choice", "linkUrl": "https://www.lesswrong.com/posts/T4mCxhpzRGrTWtxcD/multiple-choice", "postedAtFormatted": "Monday, May 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Multiple%20Choice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMultiple%20Choice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT4mCxhpzRGrTWtxcD%2Fmultiple-choice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Multiple%20Choice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT4mCxhpzRGrTWtxcD%2Fmultiple-choice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT4mCxhpzRGrTWtxcD%2Fmultiple-choice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1077, "htmlBody": "<p><em>When we choose behavior, including verbal behavior, it's sometimes tempting to do what is most likely to be right without paying attention to how costly it is to be wrong in various ways or looking for a safer alternative.</em></p>\n<p>If you've taken a lot of standardized tests, you know that some of them penalize guessing and some don't.&nbsp; That is, leaving a question blank might be better than getting a wrong answer, or they might have the same result.&nbsp; If they're the same, of course you guess, because it can't hurt and may help.&nbsp; If they take off points for wrong answers, then there's some optimal threshold at which a well-calibrated test-taker will answer.&nbsp; For instance, the ability to rule out one of four choices on a one-point question where a wrong answer costs a quarter point means that you should guess from the remaining three - the expected point value of this guess is positive.&nbsp; If you can rule out one of four choices and a wrong answer costs half a point, leave it blank.</p>\n<p>If you have ever asked a woman who wasn't pregnant when the baby was due, you might have noticed that life penalizes guessing.</p>\n<p>If you're risk-neutral, you still can't just do whatever has the highest chance of being right; you must also consider the cost of being wrong.&nbsp; You will probably win a bet that says a fair six-sided die will come up on a number greater than 2.&nbsp; But you shouldn't buy this bet for a dollar if the payoff is only $1.10, even though that purchase can be summarized as \"you will probably gain ten cents\".&nbsp; That bet is <em>better</em> than a similarly-priced, similarly-paid bet on the opposite outcome; but it's not <em>good.</em></p>\n<p>There's a few factors at work to make guessing tempting anyway:<a id=\"more\"></a></p>\n<ul>\n<li>You can't always leave \"questions\" blank.</li>\n<li>Guessing is inconsistently penalized, and the penalties are often hidden.</li>\n<li>Inaction sometimes has negative consequences too.</li>\n<li>An increased ability to rule things out relative to the general population causes overconfidence.</li>\n<li>We are biased against rating the value of information highly, and prefer to act on what we already know.</li>\n</ul>\n<p>However, I maintain that we should refrain from guessing at a higher rate than baseline.&nbsp; This is especially relevant when choosing verbal behavior - we may remember to <em>act</em> according to expected utility, but rarely think to <em>speak</em> according to expected utility, and this is especially significant around sensitive topics where being  careless with words can cause harm.</p>\n<p>Taking on each reason to guess one at a time:</p>\n<p><em>You can't always leave questions blank</em>, and unlike on a a written test, the \"blank\" condition is not always obvious.&nbsp; The fact that sometimes there is no sane null action - it's typically not a good idea to stare vacantly at someone when they ask you a question, for instance - doesn't mean, however, that there is never a sane null action.&nbsp; You can be pretty well immune to Dutch books simply by refusing to bet - this might cost gains when you don't have Dutch-bookable beliefs, but it will prevent loss.&nbsp; It is worthwhile to train yourself to notice when it is possible to simply do nothing, especially in cases where you have a history of performing worse-than-null actions.&nbsp; For instance, I react with panic when someone explains something to me and I miss a step in their inference.&nbsp; I find I get better results if, instead of interrupting them and demanding clarification, I <em>wait five seconds</em>.&nbsp; This period of time is often enough for the conversation to provide the context for me to understand on my own, and if it doesn't, at least it's not enough of a lag that either of us will have forgotten what was said.&nbsp; But remembering that I <em>can</em> wait five seconds is hard.</p>\n<p><em>Guessing is inconsistently penalized, with sometimes hidden costs</em>, so it's hard to extinguish the guessing response.&nbsp; If you're prone to doing something in a certain situation, and doing that something doesn't immediately sock you in the face every single time, it will take far longer for you to learn not to do it - this goes for people as well as pets.&nbsp; Both the immediate response and the subjective consistency are important, and a hidden cost contributes to neither.&nbsp; However, smart people can rise to the challenge of reacting to inconsistent, non-immediate, concealed costs to their actions.&nbsp; For instance, I'd be willing to bet that Less Wrong readers smoke less than the general population.&nbsp; Observe the <em>relative</em> frequency with which guessing hurts or may hurt you, compared to not guessing, and make your plans based on that.</p>\n<p><em>Inaction can be harmful too</em>, and there's a psychological sting to something bad happening because you stood there and did nothing, especially when you're familiar with the hazards of the bystander effect.&nbsp; I do not advocate spending all day sitting on the sofa for fear of acting wrongly while your house collapses around your ears.&nbsp; My point is that there are many situations where we guess and shouldn't, not that we should <em>never</em> guess, and that there is low-hanging fruit in reducing bad guessing.</p>\n<p><em>You're right more often than a regular person</em>, but that doesn't mean you are right <em>enough</em>: <a href=\"/lw/h9/tsuyoku_vs_the_egalitarian_instinct/\">life's not graded on a curve.</a>&nbsp; The question isn't, Do I have a better shot at getting this one right than the neighbor across the street? but, Do I have a <em>good enough </em>shot at getting this one right?&nbsp; You can press your relative advantages when your opponents are people, but not when you're playing against the house.&nbsp; (Additionally, you might have specific areas of comparative disadvantage even if you're overall better than the folks down the road.)</p>\n<p><em>Information is more valuable than it seems</em>, and there is often a chance to try to improve one's odds rather than settling for what one starts with.&nbsp; Laziness and impatience are both factors here: checking all the sources you could takes a long time and is harder than guessing.&nbsp; But in high stakes situations, it's <em>at least</em> worth Googling, probably worth asking a couple people who might have expertise, maybe worth looking a bit harder for data on similar scenarios and recommended strategies for them.&nbsp; Temporal urgency is more rarely the factor in discounting the value of information-gathering than is simply wanting the problem to be over with; but problems are not really over with if they are guessed at wrongly, only if you get them right.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T4mCxhpzRGrTWtxcD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 11, "extendedScore": null, "score": 5.875439666467978e-07, "legacy": true, "legacyId": "2730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gWGA8Da539EQmAR9F"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-19T08:00:54.020Z", "modifiedAt": null, "url": null, "title": "Be a Visiting Fellow at the Singularity Institute", "slug": "be-a-visiting-fellow-at-the-singularity-institute", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:21.520Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dLqFJaP2PjmztWNdX/be-a-visiting-fellow-at-the-singularity-institute", "pageUrlRelative": "/posts/dLqFJaP2PjmztWNdX/be-a-visiting-fellow-at-the-singularity-institute", "linkUrl": "https://www.lesswrong.com/posts/dLqFJaP2PjmztWNdX/be-a-visiting-fellow-at-the-singularity-institute", "postedAtFormatted": "Wednesday, May 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Be%20a%20Visiting%20Fellow%20at%20the%20Singularity%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABe%20a%20Visiting%20Fellow%20at%20the%20Singularity%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLqFJaP2PjmztWNdX%2Fbe-a-visiting-fellow-at-the-singularity-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Be%20a%20Visiting%20Fellow%20at%20the%20Singularity%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLqFJaP2PjmztWNdX%2Fbe-a-visiting-fellow-at-the-singularity-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdLqFJaP2PjmztWNdX%2Fbe-a-visiting-fellow-at-the-singularity-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 610, "htmlBody": "<p>Now is the very last minute to apply for a Summer 2010 Visiting Fellowship.&nbsp; If you&rsquo;ve been interested in SIAI for a while, but haven&rsquo;t <em>quite</em> managed to make contact -- or if you&rsquo;re just looking for a good way to spend a week or more of your summer -- drop us a line.&nbsp; See what an SIAI summer might do for you and the world.&nbsp; <br /><br />(SIAI&rsquo;s Visiting Fellow program brings volunteers to SIAI for anywhere from a week to three months, to learn, teach, and collaborate.&nbsp; Flights and room and board are covered.&nbsp; We&rsquo;ve been rolling since June of 2009, with good success.)<br /><br /><strong>Apply because:</strong></p>\n<ul>\n<li>SIAI is tackling the world&rsquo;s most important task -- the task of shaping the <a href=\"http://video.google.com/videoplay?docid=-9075198068763651344\">Singularity</a>.&nbsp; The task of averting human extinction. We <a href=\"http://www.nickbostrom.com/\">aren&rsquo;t</a> <a href=\"http://www.fhi.ox.ac.uk/research/\">the</a> <a href=\"http://hanson.gmu.edu/home.html\">only</a> people tackling this, but the total set is frighteningly small.</li>\n<li>When numbers are this small, it&rsquo;s actually plausible that <a href=\"http://vimeo.com/7397629\">you can tip the balance</a>.&nbsp;</li>\n<li>SIAI has some amazing people to learn from -- many report learning and growing more here than in any other period of their lives.</li>\n<li>SIAI also has major gaps, and much that desperately needs doing but that we haven&rsquo;t noticed yet, or have noticed but haven&rsquo;t managed to fix -- gaps where your own skills, talents, and energy can come into play.</li>\n</ul>\n<p><strong><a id=\"more\"></a>Apply especially if:</strong></p>\n<ul>\n<li>You have start-up experience or are otherwise an instigator: someone who can walk into an unstructured environment and create useful projects for yourself and others;</li>\n<li>You&rsquo;re skilled at creating community; you have an open heart; you can learn rapidly, and create contexts for others to learn; you have a serious interest in pioneering more effective ways of thinking;</li>\n<li>You care about existential risk, and are searching for long-term career paths that might help;</li>\n<li>You have high analytic intelligence, a tendency to win math competitions, or background and thinking skill around AI, probability, anthropics, simulation scenarios, rationality, existential risk, and related topics; (math, compsci, physics, or analytic philosophy background is also a plus)</li>\n<li>You have a specific background that is likely to prove helpful: academic research experience; teaching or writing skill; strong personal productivity; programming fluency; a cognitive profile that differs from the usual LW mold; or strong talent of some other sort, in an area we need, that we may not have realized we need.</li>\n</ul>\n<p>(You <em>don&rsquo;t need all</em> of the above; some is fine.)<br /><br />Don&rsquo;t be intimidated -- SIAI contains most of the smartest people I&rsquo;ve ever met, but we&rsquo;re also a very open community.&nbsp; Err on the side of sending in an application; then, at least we&rsquo;ll know each other.&nbsp; (Applications for fall and beyond are also welcome; we&rsquo;re taking Fellows on a rolling basis.)<br /><br />If you&rsquo;d like a better idea of what SIAI is, and what we&rsquo;re aimed at, check out:<br />1. SIAI's <a href=\"http://intelligence.org/riskintro/index.html\">Brief Introduction</a>;<br />2.&nbsp; The <a href=\"http://intelligence.org/grants/challenge/#grantproposals\">Challenge projects;</a><br />3.&nbsp; Our <a href=\"http://intelligence.org/accomplishments2009\">2009 accomplishments;</a><br />4.&nbsp; Videos from past <a href=\"http://www.vimeo.com/siai/videos/sort:oldest\">Singularity</a> <a href=\"http://www.singularitysummit.com/summit/past_summits/\">Summits</a> (the 2010 Summit will happen during this summer&rsquo;s program, Aug 14-15 in SF; visiting Fellows will assist);<br />5.&nbsp; <a href=\"/lw/1hn/call_for_new_siai_visiting_fellows_on_a_rolling/#comments\">Comments</a> from our last Call for Visiting Fellows; and/or<br />6.&nbsp; Bios of the <a href=\"http://intelligence.org/aboutus/visitingfellows\">2009 Summer Fellows</a>.<br /><br />Or just drop me a line.&nbsp; Our application process is informal -- just send me an email at anna at singinst dot org with: (1) a resume/c.v. or similar information; and (2) a few sentences on why you&rsquo;re applying.&nbsp; And we&rsquo;ll figure out where to go from there.<br /><br />Looking forward to hearing from you.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dLqFJaP2PjmztWNdX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 38, "extendedScore": null, "score": 5.879571664034043e-07, "legacy": true, "legacyId": "2928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 171, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-19T10:16:51.228Z", "modifiedAt": null, "url": null, "title": "What is Wei Dai's Updateless Decision Theory?", "slug": "what-is-wei-dai-s-updateless-decision-theory", "viewCount": null, "lastCommentedAt": "2022-03-14T07:06:44.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlephNeil", "createdAt": "2010-05-12T14:43:28.879Z", "isAdmin": false, "displayName": "AlephNeil"}, "userId": "qSNSSwAXbki7JTNSd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5WEoM3RCxN2cQEdzY/what-is-wei-dai-s-updateless-decision-theory", "pageUrlRelative": "/posts/5WEoM3RCxN2cQEdzY/what-is-wei-dai-s-updateless-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/5WEoM3RCxN2cQEdzY/what-is-wei-dai-s-updateless-decision-theory", "postedAtFormatted": "Wednesday, May 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20Wei%20Dai's%20Updateless%20Decision%20Theory%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20Wei%20Dai's%20Updateless%20Decision%20Theory%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5WEoM3RCxN2cQEdzY%2Fwhat-is-wei-dai-s-updateless-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20Wei%20Dai's%20Updateless%20Decision%20Theory%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5WEoM3RCxN2cQEdzY%2Fwhat-is-wei-dai-s-updateless-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5WEoM3RCxN2cQEdzY%2Fwhat-is-wei-dai-s-updateless-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2024, "htmlBody": "<p><span style=\"font-family: 'Times New Roman'; font-size: medium;\"> </span></p>\n<div style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; background-position: initial initial; background-repeat: initial initial; padding: 0.5em; margin: 8px;\">\n<p>As a newcomer to LessWrong, I quite often see references to 'UDT' or 'updateless decision theory'. The very name is like crack - I'm irresistably compelled to find out what the fuss is about.</p>\n<p><a href=\"/lw/15m/towards_a_new_decision_theory/\">Wei Dai's post</a> is certainly interesting, but it seemed to me (as a naive observer) that a fairly small 'mathematical signal' was in danger of being lost in a lot of AI-noise. Or to put it less confrontationally: I saw a simple 'lesson' on how to attack many of the problems that frequently get discussed here, which can easily be detached from the rest of the theory. Hence this short note, the purpose of which is to present and motivate UDT in the context of 'naive decision theory' (NDT), and to pre-empt what I think is a possible misunderstanding.<a id=\"more\"></a></p>\n<p>First, a quick review of the basic Bayesian decision-making recipe.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h3><strong style=\"font-weight: bold;\">What is Na&iuml;ve Decision Theory?</strong></h3>\n<p>You take the prior and some empirical data and calculate a posterior by (i) working out the 'likelihood function' of the data and (ii) calculating prior times likelihood and renormalising. Then you calculate expected utilities for every possible action (wrt to this posterior) and maximize.</p>\n<p>Of course there's a lot more to conventional decision theory than this, but I think one can best get a handle on UDT by considering it as an alternative to the above procedure, in order to handle situations where some of its presuppositions fail.</p>\n<p>(Note: NDT is especially 'na&iuml;ve' in that it takes the existence of a 'likelihood function' for granted. Therefore, in decision problems where EDT and CDT diverge, one must 'dogmatically' choose between them at the outset just to obtain a problem that NDT regards as being well-defined.)</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\"><strong style=\"font-weight: bold;\">When does NDT fail?</strong></h3>\n<p>The above procedure is&nbsp;<em style=\"font-style: italic;\">extremely</em>&nbsp;limited. Taking it exactly as stated, it only applies to games with a single player and a single opportunity to act at some stage in the game. The following diagram illustrates the kind of situation for which NDT is adequate:</p>\n<p style=\"padding-left: 30px;\"><img style=\"vertical-align: middle; margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i40.tinypic.com/xfybex.png\" alt=\"NDT\" width=\"630\" height=\"229\" /></p>\n<p>This is a tree diagram (as opposed to a causal graph). The blue and orange boxes show 'information states', so that any player-instance within the blue box sees exactly the same 'data'. Hence, their strategy (whether pure or mixed) must be the same throughout the box. The branches on the right have been greyed out to depict the Bayesian 'updating' that the player following NDT would do upon seeing 'blue' rather than 'orange'--a branch is greyed out if and only if it fails to pass through a blue 'Player' node. Of course, the correct strategy will depend on the probabilities of each of Nature's possible actions, and on the utilities of each outcome, which have been omitted from the diagram. The probabilities of the outward branches from any given 'Nature' node are to be regarded as 'fixed at the outset'.</p>\n<p>Now let's consider two generalisations:</p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial;\">\n<li>What if the player may have more than one opportunity to act during the game? In particular, what if the player is 'forgetful' in the sense that (i) information from 'earlier on' in the game may be 'forgotten', even such that (ii) the player may return to an information state several times during the same branch.</li>\n<li>What if, in addition to freely-willed 'Player' nodes and random 'Nature' nodes, there is a third kind of node where the branch followed depends on the Player's&nbsp;<em style=\"font-style: italic;\">strategy&nbsp;</em>for a particular information state, regardless of whether that strategy has yet been executed. In other words, what if the universe contains 'telepathic robots' (whose behaviour is totally mechanical - they're not trying to maximize a utility function) that can see inside the Player's mind&nbsp;before they have acted?</li>\n</ol>\n<p>It may be worth remarking that we haven't even considered the most obvious generalisation: The one where the game includes several 'freely-willed' Players, each with their own utility functions. However, UDT doesn't say much about this - UDT is intended purely as an approach to solving decision problems for a single 'Player', and to the extent that other 'Players' are included, they must be regarded as 'robots' (of the non-telepathic type) rather than intentional agents. In other words, when we consider other Players, we try to do the best we can from the 'Physical Stance' (i.e.\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\ntry to divine what they will do from their 'source code' alone) rather than rising to the 'Intentional Stance' (i.e. put ourselves in their place with their goals and see what we think is rational).</p>\n<p>Note: If a&nbsp;<em style=\"font-style: italic;\">non-forgetful</em>&nbsp;player has several opportunities to act then, as long as the game only contains Player and Nature nodes, the Player is able to calculate the relevant likelihood function (up to a constant of proportionality) from within any of their possible information states. Therefore, they can solve the decision problem recursively using NDT, working backwards from the end (as long as the game is guaranteed to end after a finite number of moves.) If, in addition to this, the utility function is 'separable' (e.g. a sum of utilities 'earned' at each move) then things are even easier: each information state gives us a separate NDT problem, which can be solved independently of the others. Therefore, unless the player is forgetful, the 'na&iuml;ve' approach is capable of dealing with generalisation 1.</p>\n<p>Here are two familiar examples of generalisation 1 (ii):</p>\n<p style=\"padding-left: 30px;\"><img style=\"vertical-align: middle; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px;\" src=\"http://i40.tinypic.com/2sb28tg.png\" alt=\"\" width=\"300\" height=\"296\" /><img style=\"vertical-align: middle; margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i40.tinypic.com/16275sp.png\" alt=\"Absent-minded driver\" width=\"199\" height=\"282\" /></p>\n<p>Note: The Sleeping Beauty problem is usually presented as a question about probabilities (\"what is the Player's subjective probability that the coin toss was heads?\") rather than utilities, although for no particularly good reason the above diagram depicts a decision problem. Another point of interest is that the Absent-Minded Driver contains an extra ingredient not present in the SB problem: the player's actions affect how many player-instances there are in a branch.</p>\n<p>Now a trio of notorious problems exemplifying generalisation 2:</p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i40.tinypic.com/os7vq0.png\" alt=\"Newcomb\" width=\"397\" height=\"229\" /></p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i43.tinypic.com/axdzwz.png\" alt=\"\" width=\"360\" height=\"229\" /></p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i39.tinypic.com/1538uh4.png\" alt=\"Hitcher\" width=\"318\" height=\"229\" /></p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\">How Does UDT Deal With These Problems?</h3>\n<p>The essence of UDT is&nbsp;<em style=\"font-style: italic;\">extremely</em>&nbsp;simple: We give up the idea of 'conditioning on the blue box' (doing Bayesian reasoning to obtain a posterior distribution etc) and instead just choose the action (or more generally, the probability distribution over actions) that will maximize the&nbsp;<em style=\"font-style: italic;\">unconditional</em>&nbsp;expected utility.</p>\n<p>So, UDT:</p>\n<p>&nbsp;</p>\n<ul>\n<li>Solves the correct equation in the Absent-Minded Driver problem.</li>\n<li>One-boxes.</li>\n<li>Submits to a Counterfactual Mugging.</li>\n<li>Pays after hitchhiking.</li>\n</ul>\n<p>&nbsp;</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\">Is that it? (Doesn't that give the wrong answer to the Smoking Lesion problem?)</h3>\n<p>Yes, that's all there is to it.</p>\n<p>Prima facie, the tree diagram for the Smoking Lesion would seem to be identical to my diagram of Newcomb's Problem (except that the connection between Omega's action and the Player's action would have to be probabilistic), but let's look a little closer:</p>\n<p>Wei Dai imagines the Player's action to be computed by a subroutine called S, and although other subroutines are free to inspect the source code of S, and try to 'simulate' it, ultimately 'we' the decision-maker have control over S's source code.&nbsp;In Newcomb's problem, Omega's activities are not supposed to have any influence on the Player's source code. However, in the Smoking Lesion problem, the presence of a 'lesion' is somehow supposed to cause Player's to choose to smoke (without altering their utility function), which can only mean that in some sense the Player's source code is 'partially written' before the Player can exercise any control over it. However, UDT wants to 'wipe the slate clean' and delete whatever half-written nonsense is there before deciding what code to write.</p>\n<p>Ultimately this means that when UDT encounters the Smoking Lesion, it simply <em>throws away</em>&nbsp;the supposed correlation between the lesion and the decision and acts as though that were never a part of the problem. So the appropriate tree diagram for the Smoking Lesion problem would have a Nature node at the bottom rather than an Omega node, and so UDT would advise smoking.</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\">Why Is It Rational To Act In The Way UDT Prescribes?</h3>\n<p>UDT arises from the philosophical viewpoint that says things like</p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial;\">\n<li>There is no such thing as the 'objective present moment'.</li>\n<li>There is no such thing as 'persisting subjective identity'.</li>\n<li>There is no difference in principle between me and a functionally identical automaton.</li>\n<li>When a random event takes place, our perception of a single definite outcome is as much an illusion of perspective as the 'objective present'--in reality all outcomes occur, but in 'parallel universes'.</li>\n</ol>\n<p>If you take the above seriously then you're forced to conclude that a game containing an Omega node 'linked' to a Player node in the manner above is isomorphic (for the purposes of decision theory) to the game in which that Omega node is really a Player node belonging to the same information state. In other words, 'Counterfactual Mugging' is actually isomorphic to:</p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i42.tinypic.com/70xrly.png\" alt=\"Mugging2\" width=\"240\" height=\"238\" /></p>\n<p>This latter version is much less of a headache to think about! Similarly, we can simplify and solve The Absent-Minded Driver by noting that it is isomorphic to the following, which can easily be solved:</p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i44.tinypic.com/2liwl76.png\" alt=\"Absent2\" width=\"310\" height=\"270\" /></p>\n<p>Even more interesting is the fact that the Absent-Minded Driver turns out to be isomorphic to (a probabilistic variant of) Parfit's Hitchhiker (if we interchange the Omega and Player nodes in the above diagram).</p>\n<p>&nbsp;</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\">Addendum: Do Questions About Subjective Probability Have Answers Irrespective Of One's Decision Theory And Utility Function?</h3>\n<p>In the short time I've been here, I have seen several people arguing that the answer is 'no'. I want to say that the answer is 'yes' but with a caveat:</p>\n<p>We have puzzles like the Absent-Minded Driver (original version) where the player's strategy for a particular information state affects the probability of that information state recurring. It's clear that in such cases, we may be unable to assign a probability to a particular event until the player settles on a particular strategy. However, once the player's strategy is 'set in stone', then I want to argue that regardless of the utility function, questions about the probability of a given player-instance do in fact have canonical answers:</p>\n<p>Let's suppose that each player-instance is granted a uniform random number in the set [0,1]. In a sense this was already implicit, given that we had no qualms about considering the possibility of a mixed strategy. However, let's suppose that each player-instance's random number is now regarded as part of its 'information'. When a player sees (i) that she is somewhere within the 'blue rectangle',&nbsp;<em>and</em>&nbsp;(ii) that her random number is&nbsp;&alpha;, then for all player-instances P within the rectangle, she can calculate the probability (or rather density) of the event \"P's random number is&nbsp;&alpha;\" and thereby obtain a conditional probability distribution over player-instances within the rectangle.</p>\n<p>Notice that this procedure is entirely independent of decision theory (again, provided that the Player's strategy has been fixed).</p>\n<p>In the context of the Sleeping-Beauty problem (much discussed of late) the above recipe is equivalent to asserting that (a) whenever Sleeping Beauty is woken, this takes place at a uniformly distributed time between 8am and 9am and (b) there is a clock on the wall. So whenever SB awakes at time &amp;alpha;, she learns the information \"&amp;alpha; is one of the times at which I have been woken\". A short exercise in probability theory suffices to show that SB must now calculate 1/3 probabilities for each of (Heads, Monday), (Tails, Monday) and (Tails, Tuesday) [which I think is fairly interesting given that the latter two are<em>,</em>&nbsp;as far as the prior is concerned,&nbsp;<em>the very same event</em>].</p>\n<p>One can get a flavour of it by considering a much simpler variation: Let &amp;alpha; and &amp;beta; be 'random names' for Monday and Tuesday, in the sense that with probability 1/2, (&amp;alpha;, &amp;beta;) = (\"Monday\", \"Tuesday\") and with probability 1/2, (&amp;alpha;, &amp;beta;) = (\"Tuesday\", \"Monday\"). Suppose that SB's room lacks a clock but includes a special 'calendar' showing either &amp;alpha; or &amp;beta;, but that SB doesn't know which symbol refers to which day.</p>\n<p>Then we obtain the following diagram:</p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 10px; margin-bottom: 10px;\" src=\"http://i39.tinypic.com/zlexx1.png\" alt=\"SB2\" width=\"600\" height=\"318\" /></p>\n<p>Nature's first decision determines the meaning of &amp;alpha; and &amp;beta;, and its second is the 'coin flip' that inaugurates the Sleeping Beauty problem we know and love. There are now two information states, corresponding to SB's perception of &amp;alpha; or &amp;beta; upon waking. Thus, if SB sees the calendar showing &amp;alpha; (the orange state, let's say) it is clear that the conditional probabilities for the three possible awakenings must be split (1/3, 1/3, 1/3) as above (note that the two orange 'Tuesday' nodes correspond to the same awakening.)</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2, "5f5c37ee1b5cdee568cfb1dc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5WEoM3RCxN2cQEdzY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 52, "extendedScore": null, "score": 0.00011480560436210457, "legacy": true, "legacyId": "2920", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"font-family: 'Times New Roman'; font-size: medium;\"> </span></p>\n<div style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; background-position: initial initial; background-repeat: initial initial; padding: 0.5em; margin: 8px;\">\n<p>As a newcomer to LessWrong, I quite often see references to 'UDT' or 'updateless decision theory'. The very name is like crack - I'm irresistably compelled to find out what the fuss is about.</p>\n<p><a href=\"/lw/15m/towards_a_new_decision_theory/\">Wei Dai's post</a> is certainly interesting, but it seemed to me (as a naive observer) that a fairly small 'mathematical signal' was in danger of being lost in a lot of AI-noise. Or to put it less confrontationally: I saw a simple 'lesson' on how to attack many of the problems that frequently get discussed here, which can easily be detached from the rest of the theory. Hence this short note, the purpose of which is to present and motivate UDT in the context of 'naive decision theory' (NDT), and to pre-empt what I think is a possible misunderstanding.<a id=\"more\"></a></p>\n<p>First, a quick review of the basic Bayesian decision-making recipe.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h3 id=\"What_is_Na_ve_Decision_Theory_\"><strong style=\"font-weight: bold;\">What is Na\u00efve Decision Theory?</strong></h3>\n<p>You take the prior and some empirical data and calculate a posterior by (i) working out the 'likelihood function' of the data and (ii) calculating prior times likelihood and renormalising. Then you calculate expected utilities for every possible action (wrt to this posterior) and maximize.</p>\n<p>Of course there's a lot more to conventional decision theory than this, but I think one can best get a handle on UDT by considering it as an alternative to the above procedure, in order to handle situations where some of its presuppositions fail.</p>\n<p>(Note: NDT is especially 'na\u00efve' in that it takes the existence of a 'likelihood function' for granted. Therefore, in decision problems where EDT and CDT diverge, one must 'dogmatically' choose between them at the outset just to obtain a problem that NDT regards as being well-defined.)</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\" id=\"When_does_NDT_fail_\"><strong style=\"font-weight: bold;\">When does NDT fail?</strong></h3>\n<p>The above procedure is&nbsp;<em style=\"font-style: italic;\">extremely</em>&nbsp;limited. Taking it exactly as stated, it only applies to games with a single player and a single opportunity to act at some stage in the game. The following diagram illustrates the kind of situation for which NDT is adequate:</p>\n<p style=\"padding-left: 30px;\"><img style=\"vertical-align: middle; margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i40.tinypic.com/xfybex.png\" alt=\"NDT\" width=\"630\" height=\"229\"></p>\n<p>This is a tree diagram (as opposed to a causal graph). The blue and orange boxes show 'information states', so that any player-instance within the blue box sees exactly the same 'data'. Hence, their strategy (whether pure or mixed) must be the same throughout the box. The branches on the right have been greyed out to depict the Bayesian 'updating' that the player following NDT would do upon seeing 'blue' rather than 'orange'--a branch is greyed out if and only if it fails to pass through a blue 'Player' node. Of course, the correct strategy will depend on the probabilities of each of Nature's possible actions, and on the utilities of each outcome, which have been omitted from the diagram. The probabilities of the outward branches from any given 'Nature' node are to be regarded as 'fixed at the outset'.</p>\n<p>Now let's consider two generalisations:</p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial;\">\n<li>What if the player may have more than one opportunity to act during the game? In particular, what if the player is 'forgetful' in the sense that (i) information from 'earlier on' in the game may be 'forgotten', even such that (ii) the player may return to an information state several times during the same branch.</li>\n<li>What if, in addition to freely-willed 'Player' nodes and random 'Nature' nodes, there is a third kind of node where the branch followed depends on the Player's&nbsp;<em style=\"font-style: italic;\">strategy&nbsp;</em>for a particular information state, regardless of whether that strategy has yet been executed. In other words, what if the universe contains 'telepathic robots' (whose behaviour is totally mechanical - they're not trying to maximize a utility function) that can see inside the Player's mind&nbsp;before they have acted?</li>\n</ol>\n<p>It may be worth remarking that we haven't even considered the most obvious generalisation: The one where the game includes several 'freely-willed' Players, each with their own utility functions. However, UDT doesn't say much about this - UDT is intended purely as an approach to solving decision problems for a single 'Player', and to the extent that other 'Players' are included, they must be regarded as 'robots' (of the non-telepathic type) rather than intentional agents. In other words, when we consider other Players, we try to do the best we can from the 'Physical Stance' (i.e.\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\ntry to divine what they will do from their 'source code' alone) rather than rising to the 'Intentional Stance' (i.e. put ourselves in their place with their goals and see what we think is rational).</p>\n<p>Note: If a&nbsp;<em style=\"font-style: italic;\">non-forgetful</em>&nbsp;player has several opportunities to act then, as long as the game only contains Player and Nature nodes, the Player is able to calculate the relevant likelihood function (up to a constant of proportionality) from within any of their possible information states. Therefore, they can solve the decision problem recursively using NDT, working backwards from the end (as long as the game is guaranteed to end after a finite number of moves.) If, in addition to this, the utility function is 'separable' (e.g. a sum of utilities 'earned' at each move) then things are even easier: each information state gives us a separate NDT problem, which can be solved independently of the others. Therefore, unless the player is forgetful, the 'na\u00efve' approach is capable of dealing with generalisation 1.</p>\n<p>Here are two familiar examples of generalisation 1 (ii):</p>\n<p style=\"padding-left: 30px;\"><img style=\"vertical-align: middle; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px;\" src=\"http://i40.tinypic.com/2sb28tg.png\" alt=\"\" width=\"300\" height=\"296\"><img style=\"vertical-align: middle; margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i40.tinypic.com/16275sp.png\" alt=\"Absent-minded driver\" width=\"199\" height=\"282\"></p>\n<p>Note: The Sleeping Beauty problem is usually presented as a question about probabilities (\"what is the Player's subjective probability that the coin toss was heads?\") rather than utilities, although for no particularly good reason the above diagram depicts a decision problem. Another point of interest is that the Absent-Minded Driver contains an extra ingredient not present in the SB problem: the player's actions affect how many player-instances there are in a branch.</p>\n<p>Now a trio of notorious problems exemplifying generalisation 2:</p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i40.tinypic.com/os7vq0.png\" alt=\"Newcomb\" width=\"397\" height=\"229\"></p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i43.tinypic.com/axdzwz.png\" alt=\"\" width=\"360\" height=\"229\"></p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i39.tinypic.com/1538uh4.png\" alt=\"Hitcher\" width=\"318\" height=\"229\"></p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\" id=\"How_Does_UDT_Deal_With_These_Problems_\">How Does UDT Deal With These Problems?</h3>\n<p>The essence of UDT is&nbsp;<em style=\"font-style: italic;\">extremely</em>&nbsp;simple: We give up the idea of 'conditioning on the blue box' (doing Bayesian reasoning to obtain a posterior distribution etc) and instead just choose the action (or more generally, the probability distribution over actions) that will maximize the&nbsp;<em style=\"font-style: italic;\">unconditional</em>&nbsp;expected utility.</p>\n<p>So, UDT:</p>\n<p>&nbsp;</p>\n<ul>\n<li>Solves the correct equation in the Absent-Minded Driver problem.</li>\n<li>One-boxes.</li>\n<li>Submits to a Counterfactual Mugging.</li>\n<li>Pays after hitchhiking.</li>\n</ul>\n<p>&nbsp;</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\" id=\"Is_that_it___Doesn_t_that_give_the_wrong_answer_to_the_Smoking_Lesion_problem__\">Is that it? (Doesn't that give the wrong answer to the Smoking Lesion problem?)</h3>\n<p>Yes, that's all there is to it.</p>\n<p>Prima facie, the tree diagram for the Smoking Lesion would seem to be identical to my diagram of Newcomb's Problem (except that the connection between Omega's action and the Player's action would have to be probabilistic), but let's look a little closer:</p>\n<p>Wei Dai imagines the Player's action to be computed by a subroutine called S, and although other subroutines are free to inspect the source code of S, and try to 'simulate' it, ultimately 'we' the decision-maker have control over S's source code.&nbsp;In Newcomb's problem, Omega's activities are not supposed to have any influence on the Player's source code. However, in the Smoking Lesion problem, the presence of a 'lesion' is somehow supposed to cause Player's to choose to smoke (without altering their utility function), which can only mean that in some sense the Player's source code is 'partially written' before the Player can exercise any control over it. However, UDT wants to 'wipe the slate clean' and delete whatever half-written nonsense is there before deciding what code to write.</p>\n<p>Ultimately this means that when UDT encounters the Smoking Lesion, it simply <em>throws away</em>&nbsp;the supposed correlation between the lesion and the decision and acts as though that were never a part of the problem. So the appropriate tree diagram for the Smoking Lesion problem would have a Nature node at the bottom rather than an Omega node, and so UDT would advise smoking.</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\" id=\"Why_Is_It_Rational_To_Act_In_The_Way_UDT_Prescribes_\">Why Is It Rational To Act In The Way UDT Prescribes?</h3>\n<p>UDT arises from the philosophical viewpoint that says things like</p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial;\">\n<li>There is no such thing as the 'objective present moment'.</li>\n<li>There is no such thing as 'persisting subjective identity'.</li>\n<li>There is no difference in principle between me and a functionally identical automaton.</li>\n<li>When a random event takes place, our perception of a single definite outcome is as much an illusion of perspective as the 'objective present'--in reality all outcomes occur, but in 'parallel universes'.</li>\n</ol>\n<p>If you take the above seriously then you're forced to conclude that a game containing an Omega node 'linked' to a Player node in the manner above is isomorphic (for the purposes of decision theory) to the game in which that Omega node is really a Player node belonging to the same information state. In other words, 'Counterfactual Mugging' is actually isomorphic to:</p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i42.tinypic.com/70xrly.png\" alt=\"Mugging2\" width=\"240\" height=\"238\"></p>\n<p>This latter version is much less of a headache to think about! Similarly, we can simplify and solve The Absent-Minded Driver by noting that it is isomorphic to the following, which can easily be solved:</p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 20px; margin-bottom: 20px; margin-left: 10px; margin-right: 10px;\" src=\"http://i44.tinypic.com/2liwl76.png\" alt=\"Absent2\" width=\"310\" height=\"270\"></p>\n<p>Even more interesting is the fact that the Absent-Minded Driver turns out to be isomorphic to (a probabilistic variant of) Parfit's Hitchhiker (if we interchange the Omega and Player nodes in the above diagram).</p>\n<p>&nbsp;</p>\n<h3 style=\"font-size: 15px; color: black; float: none;\" id=\"Addendum__Do_Questions_About_Subjective_Probability_Have_Answers_Irrespective_Of_One_s_Decision_Theory_And_Utility_Function_\">Addendum: Do Questions About Subjective Probability Have Answers Irrespective Of One's Decision Theory And Utility Function?</h3>\n<p>In the short time I've been here, I have seen several people arguing that the answer is 'no'. I want to say that the answer is 'yes' but with a caveat:</p>\n<p>We have puzzles like the Absent-Minded Driver (original version) where the player's strategy for a particular information state affects the probability of that information state recurring. It's clear that in such cases, we may be unable to assign a probability to a particular event until the player settles on a particular strategy. However, once the player's strategy is 'set in stone', then I want to argue that regardless of the utility function, questions about the probability of a given player-instance do in fact have canonical answers:</p>\n<p>Let's suppose that each player-instance is granted a uniform random number in the set [0,1]. In a sense this was already implicit, given that we had no qualms about considering the possibility of a mixed strategy. However, let's suppose that each player-instance's random number is now regarded as part of its 'information'. When a player sees (i) that she is somewhere within the 'blue rectangle',&nbsp;<em>and</em>&nbsp;(ii) that her random number is&nbsp;\u03b1, then for all player-instances P within the rectangle, she can calculate the probability (or rather density) of the event \"P's random number is&nbsp;\u03b1\" and thereby obtain a conditional probability distribution over player-instances within the rectangle.</p>\n<p>Notice that this procedure is entirely independent of decision theory (again, provided that the Player's strategy has been fixed).</p>\n<p>In the context of the Sleeping-Beauty problem (much discussed of late) the above recipe is equivalent to asserting that (a) whenever Sleeping Beauty is woken, this takes place at a uniformly distributed time between 8am and 9am and (b) there is a clock on the wall. So whenever SB awakes at time &amp;alpha;, she learns the information \"&amp;alpha; is one of the times at which I have been woken\". A short exercise in probability theory suffices to show that SB must now calculate 1/3 probabilities for each of (Heads, Monday), (Tails, Monday) and (Tails, Tuesday) [which I think is fairly interesting given that the latter two are<em>,</em>&nbsp;as far as the prior is concerned,&nbsp;<em>the very same event</em>].</p>\n<p>One can get a flavour of it by considering a much simpler variation: Let &amp;alpha; and &amp;beta; be 'random names' for Monday and Tuesday, in the sense that with probability 1/2, (&amp;alpha;, &amp;beta;) = (\"Monday\", \"Tuesday\") and with probability 1/2, (&amp;alpha;, &amp;beta;) = (\"Tuesday\", \"Monday\"). Suppose that SB's room lacks a clock but includes a special 'calendar' showing either &amp;alpha; or &amp;beta;, but that SB doesn't know which symbol refers to which day.</p>\n<p>Then we obtain the following diagram:</p>\n<p style=\"padding-left: 30px;\"><img style=\"margin-top: 10px; margin-bottom: 10px;\" src=\"http://i39.tinypic.com/zlexx1.png\" alt=\"SB2\" width=\"600\" height=\"318\"></p>\n<p>Nature's first decision determines the meaning of &amp;alpha; and &amp;beta;, and its second is the 'coin flip' that inaugurates the Sleeping Beauty problem we know and love. There are now two information states, corresponding to SB's perception of &amp;alpha; or &amp;beta; upon waking. Thus, if SB sees the calendar showing &amp;alpha; (the orange state, let's say) it is clear that the conditional probabilities for the three possible awakenings must be split (1/3, 1/3, 1/3) as above (note that the two orange 'Tuesday' nodes correspond to the same awakening.)</p>\n</div>", "sections": [{"title": "What is Na\u00efve Decision Theory?", "anchor": "What_is_Na_ve_Decision_Theory_", "level": 1}, {"title": "When does NDT fail?", "anchor": "When_does_NDT_fail_", "level": 1}, {"title": "How Does UDT Deal With These Problems?", "anchor": "How_Does_UDT_Deal_With_These_Problems_", "level": 1}, {"title": "Is that it? (Doesn't that give the wrong answer to the Smoking Lesion problem?)", "anchor": "Is_that_it___Doesn_t_that_give_the_wrong_answer_to_the_Smoking_Lesion_problem__", "level": 1}, {"title": "Why Is It Rational To Act In The Way UDT Prescribes?", "anchor": "Why_Is_It_Rational_To_Act_In_The_Way_UDT_Prescribes_", "level": 1}, {"title": "Addendum: Do Questions About Subjective Probability Have Answers Irrespective Of One's Decision Theory And Utility Function?", "anchor": "Addendum__Do_Questions_About_Subjective_Probability_Have_Answers_Irrespective_Of_One_s_Decision_Theory_And_Utility_Function_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "67 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["de3xjFaACCAk6imzv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-19T16:31:13.459Z", "modifiedAt": null, "url": null, "title": "Physicalism: consciousness as the last sense", "slug": "physicalism-consciousness-as-the-last-sense", "viewCount": null, "lastCommentedAt": "2017-06-17T04:37:05.269Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r9vpZQWQzrWFkuHx9/physicalism-consciousness-as-the-last-sense", "pageUrlRelative": "/posts/r9vpZQWQzrWFkuHx9/physicalism-consciousness-as-the-last-sense", "linkUrl": "https://www.lesswrong.com/posts/r9vpZQWQzrWFkuHx9/physicalism-consciousness-as-the-last-sense", "postedAtFormatted": "Wednesday, May 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Physicalism%3A%20consciousness%20as%20the%20last%20sense&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhysicalism%3A%20consciousness%20as%20the%20last%20sense%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr9vpZQWQzrWFkuHx9%2Fphysicalism-consciousness-as-the-last-sense%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Physicalism%3A%20consciousness%20as%20the%20last%20sense%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr9vpZQWQzrWFkuHx9%2Fphysicalism-consciousness-as-the-last-sense", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr9vpZQWQzrWFkuHx9%2Fphysicalism-consciousness-as-the-last-sense", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1578, "htmlBody": "<!-- How not to reject physical models of mind -->\n<p>Follow-up to <a href=\"/lw/1z0/there_just_has_to_be_something_more_you_know/\">There just has to be something more, you know?</a> and <a href=\"/lw/1z6/the_two_insights_of_materialism/\">The two insights of materialism</a>.</p>\n<p><em>I have alluded that one cause for the common reluctance to consider <a href=\"http://en.wikipedia.org/wiki/Physicalism\">physicalism</a> &mdash; in particular, that our minds can in principle be characterized entirely by physical states &mdash; is an asymmetry in how people perceive </em>characterization<em>.&nbsp;  This can be alleviated by analogy to how our external senses can supervene on each other, and how abstract manipulations of those senses using recording, playback, and editing technologies have made such characterizations useful and intuitive.</em></p>\n<p>We have numerous external senses, and at least one internal sense that people call \"thinking\" or \"consciousness\".&nbsp;  In part because you and I can point our external senses at the same objects, collaborative science has done a great job characterizing them in terms of each other.&nbsp;  The first thing is to realize the symmetry and non-triviality of this situation.</p>\n<p>First, at a personal level:&nbsp; say you've never sensed a musical instrument in any way, and for the first time, in the dark, you hear a cello playing.&nbsp;  Then later, you see the actual cello.&nbsp;  You probably wouldn't immediately recognize these perceptions as being of the same physical object.&nbsp;  But watching and listening to the cello playing at the same time would certainly help, and physically intervening yourself to see that you can change the pitch of the note by placing your fingers on the strings would be a deal breaker:&nbsp; you'd start thinking of that sound, that sight, and that tactile sense as all coming from one object \"cello\".&nbsp;</p>\n<p>Before moving on, note how in these circumstances we don't conclude that \"only sight is real\" and that sound is merely a derivate of it, but simply that the two senses are related and can characterize each other, at least roughly speaking:&nbsp; when you see a cello, you know what sort of sounds to expect, and conversely.</p>\n<p>Next, consider the more precise correspondence that collaborative science has provided, which follows a similar trend:&nbsp; in the theory of characterizing sound as logitudinal compression waves, first came recording, then playback, and finally editing.&nbsp;  In fact, the first intelligible  <em>recording</em> of a human voice, in 1860, was <em>played back</em> for the first time in 2008, using computers.&nbsp;  So, suppose it's 1810, well before the invention of the <a href=\"http://en.wikipedia.org/wiki/Phonautograph\">phonoautograph</a>, and you've just heard the first movement of Beethowen's 5th.&nbsp;  Then later, I unsuggestively show you a high-res version of this picture, with zooming capabilities:</p>\n<p><img src=\"http://i46.tinypic.com/244w4l0.gif\" border=\"0\" alt=\"Image and video hosting by TinyPic\" /> <a id=\"more\"></a></p>\n<p>If you're really smart, and have a great memory, you might notice how the high and low amplitudes of that wave along the horizontal axis match up pretty well with your perception of how loud the music is at successive times.&nbsp;  And if you zoom in, you might notice that finer bumps on the wave match up pretty well with times you heard higher notes.&nbsp;  These connections would be much easier to make if you could <em>watch and listen at the same time</em>: that is, if you could see a phonautograph transcribing the sound of the concert to a written wave in real-time while you listen to it.&nbsp;</p>\n<p>Even then, almost anyone in their right mind from 1810 would still be amazed that such an image, and the right interpretive mechanism &mdash; say, a computer with great software and really good headphones &mdash; is enough to <em>perfectly reproduce</em> the sound of that performance to two stationary ear canals, right down to the audible texture of horse-hair bows against catgut strings and every-so-politely restless audience members.&nbsp;  They'd be even more amazed that fourier analysis on <em>a single wave</em> can separate out the individual instruments to be listened to individually at a decent precision.</p>\n<p>But our modern experiences with audio recording, editing, and playback &mdash; the fact that we can <em>control sound</em> by playing back and manipulating abstract representations of sound waves &mdash; deeply internalizes our model of sound (if not hearing) as a \"merely physical\" phenomenon.&nbsp;  Just as one person easily develops the notion of a single \"cello\" as they see, hear, and play cello at the same time, collaborative science has developed the notion of a single object or model called \"physical reality\" to have a clear meaning in terms of our external senses, because those are the ones we most easily collaborate with.&nbsp;</p>\n<p><strong>Now let's talk about \"consciousness\".&nbsp; </strong> Consider that you have experienced an <em>inner sense</em> of \"consciousness\", and you may be lucky enough to have <em>seen</em> functional magnetic resonance images of your own brain, or even luckier to watch them <em>while they happen</em>.&nbsp;  These two senses, although they are as different as the sight and sound of a cello, are perceptions of the same object:&nbsp; \"consciousness\" is a word for sensing your mind from the inside, i.e.&nbsp; from actually <em>being it</em>, and \"brain\" is a word for the various ways of sensing it from the outside.&nbsp;  It's not surprising that this will probably be that last of our senses to be usefully interpreted scientifically, because it's apparently very complicated, and the hardest one to collaborate with:&nbsp; although my eyes and yours can look at the same chair, our inner senses are always directed at different minds.</p>\n<p>Under Descartes' influence, the language I'm using here is somewhat suggestive of dualism in its distiction between physical phenomena and our perceptions of them, but in fact it seems that some of our sensations simply <em>are</em> physical phenomena.&nbsp;  Some combination of physical events &mdash; like air molecules hitting the eardrum, electro-chemical signals traversing the auditory nerves, and subsequent reactions in the brain &mdash; <em>is the phenomenon of hearing</em>.&nbsp;  I'm not saying your experience of hearing <em>doesn't happen</em>, but that it is the <em>same phenomenon</em> as that described by physics and biology texts using equations and pictures of the auditory system, just as the sight and sound of a cello are direct descriptions of the same object \"cello\".</p>\n<p>But when most people consider consciousness supervening on fundamental physics, they often end up in a state of mind that is better summarized as thinking \"pictures of dots and waves are all that exists\", without an explicit awareness that they're only thinking about the pictures.&nbsp;  And this just isn't very helpful.&nbsp;  A brain is not a picture of a brain any more than it is the experience of thinking; in fact, in stages of perception, it's much closer to latter, since a picture has to pass through the retina and optic nerve before you experience it, but the experience of thinking <em>is</em> the operation of your cerebral network.</p>\n<p>Indeed, the right interpretative mechanism &mdash; for now, a living human body is the only one we've got &mdash; seems enough to produce to \"you\" the experience of \"thinking\" from specific configurations of cells, and hence particles, that can be represented (for the moment with low fidelity) by pictures like this:</p>\n<p><img src=\"http://upload.wikimedia.org/wikipedia/en/d/d1/FMRIscan.jpg\" border=\"0\" alt=\"\" /></p>\n<p>In our progressive understanding of mind, this is analogous to the simultaneous-watching-and-listening phase of learning: we can watch pictures of our brains while we \"listen\" to our own thoughts and feelings.&nbsp;  If at some point computers allow us to store, manipulate, and re-experience partial or complete mental states by directly interfacing with the brain, we'll be able to update our mind-is-brain model with the same sort of confidence as sound-is-longitudinal-compression-waves.&nbsp;  Imagine intentionally thinking through the process of solving a math problem while a computer \"records\" your thoughts, then using some kind of component analysis to remove the \"intention\" from the recording (which may not be a separable component, I'm just speculating), and then playing it back into your brain in real-time so that you experience solving the problem without trying to do it.</p>\n<p>Wouldn't you then  begin to accept characterizing thoughts as brain states, like you characterize sounds as compression waves?&nbsp;  A practical understanding like that &mdash; the level of abstract manipulation &mdash; would be a deal breaker for me.&nbsp;  And naively, it seems no more alien than the complete supervienience of sound or smell on visual representations of it.&nbsp;</p>\n<p>This isn't an argument that the physicalist conception of consciousness is <a href=\"http://wiki.lesswrong.com/wiki/Truth\">true</a>, but simply that it's <em>not absurd</em>, and <em>follows an existing trend</em> of identifications made by personal experiences and science.&nbsp;  Then all you need is heaps and loads of existing evidence to update your non-zero prior belief to the point where you recognize it's got the best odds around.&nbsp;  If they ever happen, future mind-state editing technologies could make \"thoughts = brain states\" feel as natural as playing a cello without constantly parsing \"the sight of cello\" and \"the sound of cello\" as separate objects.</p>\n<p>Even as these abstract models become more precise and amenable than our intuitive introspective models, this won't ever mean thought \"isn't real\" or \"doesn't happen\", any more than sight, touch, or hearing \"doesn't happen\".&nbsp;  You can touch, look at, and listen to a cello, yielding very different experiences of the exact same object.&nbsp;  Likewise, if one demands a dualist perceptual description, when you think, you're \"introspecting at\" your brain.&nbsp;  Although that's a very different experience from looking at an fMRI of your brain, or sticking your finger into an anaesthetized surgical opening in your skull, if modern science is right, these are experiences of the exact same physical object:&nbsp; one from the inside, two from the outside.</p>\n<p>In short, consciousness is a sense, and predictive and interventional science isn't about <em>ignoring</em> our senses...&nbsp; it's about <em>connecting</em> them.  Physics doesn't say you're not thinking, but it does connect and superveniently reduce what you experience as <em>thinking</em> to what you and everyone else can experience as <em>looking at your brain.</em></p>\n<p>It's just that awesome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XSryTypw5Hszpa4TS": 1, "EgL74XM3JRu5hjQxu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r9vpZQWQzrWFkuHx9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 31, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "2929", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Edq3ZanR22Xtft2x8", "SQoz2pb2ut2x4ZJWo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-19T19:01:34.503Z", "modifiedAt": null, "url": null, "title": "Backchaining causes wishful thinking", "slug": "backchaining-causes-wishful-thinking", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:18.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p93pcxhvowhrdb6uC/backchaining-causes-wishful-thinking", "pageUrlRelative": "/posts/p93pcxhvowhrdb6uC/backchaining-causes-wishful-thinking", "linkUrl": "https://www.lesswrong.com/posts/p93pcxhvowhrdb6uC/backchaining-causes-wishful-thinking", "postedAtFormatted": "Wednesday, May 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Backchaining%20causes%20wishful%20thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABackchaining%20causes%20wishful%20thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp93pcxhvowhrdb6uC%2Fbackchaining-causes-wishful-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Backchaining%20causes%20wishful%20thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp93pcxhvowhrdb6uC%2Fbackchaining-causes-wishful-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp93pcxhvowhrdb6uC%2Fbackchaining-causes-wishful-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1032, "htmlBody": "<p>Wishful thinking - believing things that make you happy - may be a result of adapting an old cognitive mechanism to new content.</p>\n<h2><a id=\"more\"></a>Obvious, well-known stuff</h2>\n<p>The world is a complicated place.&nbsp; When we first arrive, we don't understand it at all; we can't even recognize objects or move our arms and legs reliably.&nbsp; Gradually, we make sense of it by building categories of perceptions and objects and events and feelings that resemble each other.&nbsp; Then, instead of processing every detail of a new situation, we just have to decide which category it's closest to, and what we do with things in that category.&nbsp; Most, possibly all, categories can be built using <a href=\"http://www.amazon.com/Unsupervised-Learning-Foundations-Computational-Neuroscience/dp/026258168X/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1274282365&amp;sr=8-1\">unsupervised learning</a>, just by noting statistical regularities and clustering.</p>\n<p>If we want to be more than <a href=\"http://en.wikipedia.org/wiki/Behavior-based_robotics\">finite-state automata</a>, we also need to learn how to notice which things and events might be useful or dangerous, and make predictions, and form plans.&nbsp; There are <a href=\"http://www.amazon.com/Readings-Planning-Kaufmann-Representation-Reasoning/dp/1558601309\">logic-based ways of doing this</a>, and there are also statistical methods.&nbsp; There's <a href=\"http://www.nature.com/nature/journal/v429/n6992/full/nature02581.html\">good evidence</a> that the human dopaminergic system uses one of these statistical methods, <a href=\"http://www.scholarpedia.org/article/Temporal_difference_learning\">temporal difference learning</a> (TD).&nbsp; TD is a <em>backchaining</em> method:&nbsp; First it learns what state or action Gn-1 usually comes just before reaching a goal Gn, and then what Gn-2 usually comes just before Gn-1, etc.&nbsp; Many other learning methods use backchaining, including backpropagation, <a href=\"http://www.idsia.ch/~juergen/bucketbrigade/node2.html\">bucket brigade</a>, and <a href=\"http://www.jimdavies.org/summaries/quillian1968.html\">spreading</a> <a href=\"http://act-r.psy.cmu.edu/papers/66/SATh.JRA.JVL.1983.pdf\">activation</a>.&nbsp; These learning methods need a label or signal, during or after some series of events, saying whether the result was good or bad.</p>\n<p>I don't know why we have consciousness, and I don't know what determines which kinds of learning require conscious attention.&nbsp; For those that do, the signals produce some variety of pleasure or pain.&nbsp; We learn to pay attention to things associated with pleasure or pain, and for planning, we may use TD to build something analogous to a Markov process (sorry, I found no good link; and Wikipedia's entry on Markov chain is not what you want) where, given a sequence of the previous n states or actions (A1, A2, ... An), the probability of taking action A is proportional to the expected (pleasure - pain) for the sequence (A1, ... An, A).&nbsp; In short, we learn to do things that make us feel better.</p>\n<h2>Less-obvious stuff<br /></h2>\n<p>Here's a key point which is overlooked (or specifically denied) by most AI architectures:&nbsp; Believing is an action.&nbsp; Building an inference chain is not just <em>like</em> constructing a plan; it's the same thing, probably done by the same algorithm.&nbsp; Constructing a plan includes inferential steps, and inference often inserts action steps to make observations and reduce our uncertainty.</p>\n<p>Actions, including the \"believe\" action, have preconditions.&nbsp; When building a plan, you need to find actions that achieve those preconditions.&nbsp; You don't need to look for things that defeat them.&nbsp; With actions, this isn't much of a problem, because actions are pretty reliable.&nbsp; If you put a rock in the fire, you don't need to weigh the evidence for and against the proposition that the rock is now in the fire.&nbsp; If you put a stick in a termite mound, it may or may not come out covered in termites.&nbsp; You don't need to compute the odds that the stick was inserted correctly, or the expected number of termites; you pull it out and look at the stick.&nbsp; If you can find things that cause it not to be covered in termites, such as being the wrong sort of stick, it's probably a simple enough cause that you can enumerate it in your preconditions for next time.</p>\n<p>You don't need to consider all the ways that your actions could be thwarted until you start doing adversarial planning, which can't happen until you've already started incorporating belief actions into your planning.&nbsp; (A tiger needs to consider which ways a wildebeest might run to avoid it, but probably doesn't need to model the wildebeest's beliefs and use <a href=\"http://en.wikipedia.org/wiki/Minimax\">min-max</a> - at least, not to any significant depth.&nbsp; Some mammals do some adversarial planning and modelling of belief states; I wouldn't be surprised if squirrels avoid burying their nuts when other squirrels are looking.&nbsp; But the domains and actors are simpler, so the process shouldn't break down as often as it does in humans.)</p>\n<p>When we evolved the ability to make extensive use of belief actions, we probably took our existing plan-construction mechanism, and added belief actions.&nbsp; But an inference is a lot less certain than an action.&nbsp; You're allowed to insert a \"believe\" act into your plan if you're able to find just one thing, belief or action, that plausibly satisfies its preconditions.&nbsp; You're not required to spend any time looking for things that refute that belief.&nbsp; Your mind doesn't know that beliefs are fundamentally different from actions, in that the truth-values of the propositions describing the expected effects of your possible actions are strongly, causally correlated with whether you execute the action; while the truth-values of your possible belief-actions are not, and can be made true or false by many other factors.</p>\n<p>You can string a long series of actions together into a plan.&nbsp; If an action fails, you'll usually notice, and you can stop and retry or replan.&nbsp; Similarly, you can string a long series of belief actions together, even if the probability of each one is only a little above .5, and your planning algorithm won't complain, because stringing a long series of actions together has worked pretty well in your evolutionary past.&nbsp; But you don't usually get immediate feedback after believing something that tells you whether believing \"succeeded\" (deposited something in your mind that successfully matches the real world); so it doesn't work.</p>\n<p>The old way of backchaining, by just trying to satisfy preconditions, doesn't work well with our new mental content.&nbsp; But we haven't evolved anything better yet.&nbsp; If we had, chess would seem easy.</p>\n<h2>Summary<br /></h2>\n<p>Wishful thinking is a state-space-reduction heuristic.&nbsp; Your ancestors' minds searched for actions that would enable actions that would make them feel good.&nbsp; Your mind, therefore, searches for beliefs that will enable beliefs that will make you feel good.&nbsp; It doesn't search for beliefs that will refute them.</p>\n<p>(A forward-chaining planner wouldn't suffer this bias.&nbsp; It probably wouldn't get anything done, either, as its search space would be vast.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p93pcxhvowhrdb6uC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 23, "extendedScore": null, "score": 5.880928111476216e-07, "legacy": true, "legacyId": "2931", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Wishful thinking - believing things that make you happy - may be a result of adapting an old cognitive mechanism to new content.</p>\n<h2 id=\"Obvious__well_known_stuff\"><a id=\"more\"></a>Obvious, well-known stuff</h2>\n<p>The world is a complicated place.&nbsp; When we first arrive, we don't understand it at all; we can't even recognize objects or move our arms and legs reliably.&nbsp; Gradually, we make sense of it by building categories of perceptions and objects and events and feelings that resemble each other.&nbsp; Then, instead of processing every detail of a new situation, we just have to decide which category it's closest to, and what we do with things in that category.&nbsp; Most, possibly all, categories can be built using <a href=\"http://www.amazon.com/Unsupervised-Learning-Foundations-Computational-Neuroscience/dp/026258168X/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1274282365&amp;sr=8-1\">unsupervised learning</a>, just by noting statistical regularities and clustering.</p>\n<p>If we want to be more than <a href=\"http://en.wikipedia.org/wiki/Behavior-based_robotics\">finite-state automata</a>, we also need to learn how to notice which things and events might be useful or dangerous, and make predictions, and form plans.&nbsp; There are <a href=\"http://www.amazon.com/Readings-Planning-Kaufmann-Representation-Reasoning/dp/1558601309\">logic-based ways of doing this</a>, and there are also statistical methods.&nbsp; There's <a href=\"http://www.nature.com/nature/journal/v429/n6992/full/nature02581.html\">good evidence</a> that the human dopaminergic system uses one of these statistical methods, <a href=\"http://www.scholarpedia.org/article/Temporal_difference_learning\">temporal difference learning</a> (TD).&nbsp; TD is a <em>backchaining</em> method:&nbsp; First it learns what state or action Gn-1 usually comes just before reaching a goal Gn, and then what Gn-2 usually comes just before Gn-1, etc.&nbsp; Many other learning methods use backchaining, including backpropagation, <a href=\"http://www.idsia.ch/~juergen/bucketbrigade/node2.html\">bucket brigade</a>, and <a href=\"http://www.jimdavies.org/summaries/quillian1968.html\">spreading</a> <a href=\"http://act-r.psy.cmu.edu/papers/66/SATh.JRA.JVL.1983.pdf\">activation</a>.&nbsp; These learning methods need a label or signal, during or after some series of events, saying whether the result was good or bad.</p>\n<p>I don't know why we have consciousness, and I don't know what determines which kinds of learning require conscious attention.&nbsp; For those that do, the signals produce some variety of pleasure or pain.&nbsp; We learn to pay attention to things associated with pleasure or pain, and for planning, we may use TD to build something analogous to a Markov process (sorry, I found no good link; and Wikipedia's entry on Markov chain is not what you want) where, given a sequence of the previous n states or actions (A1, A2, ... An), the probability of taking action A is proportional to the expected (pleasure - pain) for the sequence (A1, ... An, A).&nbsp; In short, we learn to do things that make us feel better.</p>\n<h2 id=\"Less_obvious_stuff\">Less-obvious stuff<br></h2>\n<p>Here's a key point which is overlooked (or specifically denied) by most AI architectures:&nbsp; Believing is an action.&nbsp; Building an inference chain is not just <em>like</em> constructing a plan; it's the same thing, probably done by the same algorithm.&nbsp; Constructing a plan includes inferential steps, and inference often inserts action steps to make observations and reduce our uncertainty.</p>\n<p>Actions, including the \"believe\" action, have preconditions.&nbsp; When building a plan, you need to find actions that achieve those preconditions.&nbsp; You don't need to look for things that defeat them.&nbsp; With actions, this isn't much of a problem, because actions are pretty reliable.&nbsp; If you put a rock in the fire, you don't need to weigh the evidence for and against the proposition that the rock is now in the fire.&nbsp; If you put a stick in a termite mound, it may or may not come out covered in termites.&nbsp; You don't need to compute the odds that the stick was inserted correctly, or the expected number of termites; you pull it out and look at the stick.&nbsp; If you can find things that cause it not to be covered in termites, such as being the wrong sort of stick, it's probably a simple enough cause that you can enumerate it in your preconditions for next time.</p>\n<p>You don't need to consider all the ways that your actions could be thwarted until you start doing adversarial planning, which can't happen until you've already started incorporating belief actions into your planning.&nbsp; (A tiger needs to consider which ways a wildebeest might run to avoid it, but probably doesn't need to model the wildebeest's beliefs and use <a href=\"http://en.wikipedia.org/wiki/Minimax\">min-max</a> - at least, not to any significant depth.&nbsp; Some mammals do some adversarial planning and modelling of belief states; I wouldn't be surprised if squirrels avoid burying their nuts when other squirrels are looking.&nbsp; But the domains and actors are simpler, so the process shouldn't break down as often as it does in humans.)</p>\n<p>When we evolved the ability to make extensive use of belief actions, we probably took our existing plan-construction mechanism, and added belief actions.&nbsp; But an inference is a lot less certain than an action.&nbsp; You're allowed to insert a \"believe\" act into your plan if you're able to find just one thing, belief or action, that plausibly satisfies its preconditions.&nbsp; You're not required to spend any time looking for things that refute that belief.&nbsp; Your mind doesn't know that beliefs are fundamentally different from actions, in that the truth-values of the propositions describing the expected effects of your possible actions are strongly, causally correlated with whether you execute the action; while the truth-values of your possible belief-actions are not, and can be made true or false by many other factors.</p>\n<p>You can string a long series of actions together into a plan.&nbsp; If an action fails, you'll usually notice, and you can stop and retry or replan.&nbsp; Similarly, you can string a long series of belief actions together, even if the probability of each one is only a little above .5, and your planning algorithm won't complain, because stringing a long series of actions together has worked pretty well in your evolutionary past.&nbsp; But you don't usually get immediate feedback after believing something that tells you whether believing \"succeeded\" (deposited something in your mind that successfully matches the real world); so it doesn't work.</p>\n<p>The old way of backchaining, by just trying to satisfy preconditions, doesn't work well with our new mental content.&nbsp; But we haven't evolved anything better yet.&nbsp; If we had, chess would seem easy.</p>\n<h2 id=\"Summary\">Summary<br></h2>\n<p>Wishful thinking is a state-space-reduction heuristic.&nbsp; Your ancestors' minds searched for actions that would enable actions that would make them feel good.&nbsp; Your mind, therefore, searches for beliefs that will enable beliefs that will make you feel good.&nbsp; It doesn't search for beliefs that will refute them.</p>\n<p>(A forward-chaining planner wouldn't suffer this bias.&nbsp; It probably wouldn't get anything done, either, as its search space would be vast.)</p>", "sections": [{"title": "Obvious, well-known stuff", "anchor": "Obvious__well_known_stuff", "level": 1}, {"title": "Less-obvious stuff", "anchor": "Less_obvious_stuff", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-19T21:49:14.867Z", "modifiedAt": null, "url": null, "title": "Blame Theory", "slug": "blame-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:16.587Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qPYYjRxgHFX4wZ77c/blame-theory", "pageUrlRelative": "/posts/qPYYjRxgHFX4wZ77c/blame-theory", "linkUrl": "https://www.lesswrong.com/posts/qPYYjRxgHFX4wZ77c/blame-theory", "postedAtFormatted": "Wednesday, May 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Blame%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABlame%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqPYYjRxgHFX4wZ77c%2Fblame-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Blame%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqPYYjRxgHFX4wZ77c%2Fblame-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqPYYjRxgHFX4wZ77c%2Fblame-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 707, "htmlBody": "<p><strong><em>EDIT:</em></strong><em> this post, like many other posts of mine, is wrong. See the comments by Yvain below. Maybe \"Regret Theory\" would've been a better title. But I'm keeping this as it is because I like having reminders of my mistakes.</em></p>\n<p>Platitudes notwithstanding, \"personal responsibility\" doesn't adequately summarize my relationship with the universe. Other people <em>are</em> to blame for some of my troubles because, as <a href=\"http://www.youtube.com/watch?v=O-bLfCgjqSI\">Shepherd from MW2</a>&nbsp;put it, \"what happens over here matters over there\". Let's talk about it.</p>\n<p>When other people's actions can affect you and vice versa, individual utility maximization stops working and you must use game theory. The Prisoner's Dilemma stresses our intuitions by making a mockery of personal responsibility: each player holds the power to screw up the <em>other</em> player's welfare which they don't care about.&nbsp;Economists call such things \"externalities\", political scientists talk of \"special interests\".&nbsp;You can press a button that gives you $10 but makes 10000 random people lose $0.01 each (due to environmental pollution or something like that), you care enough to vote for this proposal, other people don't care enough to vote against, haha democracy fail.&nbsp;</p>\n<p><a id=\"more\"></a>When the shit hits the fan in a multiplayer setting, we clearly need a theory for assigning blame in correct proportion. For example, what should we make of the democratic credo that people are responsible for the leaders they have? Exactly how much responsible? How many people did I <em>personally</em> kill by voting for Hitler, and how is this responsibility shared between Hitler and me? The \"naive counterfactual\" answer goes like this: if I hadn't voted for Hitler, he'd still have been elected (since I wasn't the marginal deciding voter), therefore I'm blameless. Clearly this answer is not satisfactory. We need more sophisticated game theory concepts.</p>\n<p>First of all, we would do well to assume transferable utility. To understand why, consider Clippy. Clippy is perfectly willing to kill a million Armenians to gain one paperclip. We can blame him (her?) for it all day, but it's probably safe to say that Clippy's internal sense of guilt isn't denominated in Armenians. We must reach a mutual understanding by employing a common currency of guilt, which is just another way of saying \"convertable utils\". You feel guilty toward me = you owe me dough. Too bad, knowing you, you probably won't pay.</p>\n<p>Our second assumption goes like this: rational actions cannot be immoral. Given our knowledge of game theory, this sounds completely callous and bloodthirsty, but in the transferable utility case it's actually quite tame. You have no incentive to screw Bob over in PD if you'll be sharing the proceeds anyway. The standard procedure for sharing will be, of course, the <a href=\"http://en.wikipedia.org/wiki/Shapley_value\">Shapley value</a>.</p>\n<p>This brings us to today's ultimate conclusion: Blame Theory. Imagine that instead of killing all those Gypsies, the evil leader and the stupid voters together sang kumbaya and built a city on a hill. The proceeds of that effort would be divided according to everyone's personal contributions using the standard Shapley construction (taking into account each group's counterfactual non-cooperation, of course). You, dear reader, would have gotten richer by two million dollars, instead of hiding in a bomb shelter while the ground shakes and your nephew is missing. And calculating the difference between your <em>personal</em> welfare in the perfect world where everyone cooperated, and the welfare you actually have right now given that everyone acted as they did, gives you the extent of your <em>personal</em> guilt. You can't push it away, it's yours. Sorry.</p>\n<p>Don't know about you, but I'm genuinely scared by this result.</p>\n<p>On one hand, it agrees with intuition in all the right ways: the \"naive counterfactual\" voter still carries non-zero guilt because (s)he was part of the collective that elected a monster, and the\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nlittle personal guilts add up exactly to the total missed opportunity of all society, and... But on the other hand, this procedure gives every one of us a new and unexpectedly harsh yardstick to measure ourselves by. It probably makes me equivalent to a serial murderer already. I've long had a voice in the back of my head telling me it was possible to do better, but Blame Theory brings the truth into sharp and painful focus. I'm not sure I wanted that when I set out to solve this particular problem...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qPYYjRxgHFX4wZ77c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 5.881272452589114e-07, "legacy": true, "legacyId": "2282", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-20T00:31:14.588Z", "modifiedAt": null, "url": null, "title": "Summer vs Winter Strategies", "slug": "summer-vs-winter-strategies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:20.780Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EsDP9yKGQkozJEoZo/summer-vs-winter-strategies", "pageUrlRelative": "/posts/EsDP9yKGQkozJEoZo/summer-vs-winter-strategies", "linkUrl": "https://www.lesswrong.com/posts/EsDP9yKGQkozJEoZo/summer-vs-winter-strategies", "postedAtFormatted": "Thursday, May 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Summer%20vs%20Winter%20Strategies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASummer%20vs%20Winter%20Strategies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsDP9yKGQkozJEoZo%2Fsummer-vs-winter-strategies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Summer%20vs%20Winter%20Strategies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsDP9yKGQkozJEoZo%2Fsummer-vs-winter-strategies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsDP9yKGQkozJEoZo%2Fsummer-vs-winter-strategies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 840, "htmlBody": "<p>Abstract: <em>I have a hypothesis that there are two different general strategies for life that humans might switch between predicated on the way general resource availability change in the society. If it is constant or increasing one strategy pays off, if predictably increasing then decreasing, another is good. These strategies would have been selected for at different times and environments in prehistory but humans are mainly plastic in which strategy they adopt. Culture reinforces them and can create lags. For value neutral purposes I will call them by seasons, the Summer strategy and the Winter strategy. The summer is for times of plenty and partying, and the winter for when resources regularly become scarcer and life becomes harsher. These strategies affect every part of society from mating to the way people plan</em>.</p>\n<p><a id=\"more\"></a></p>\n<p>The above is an idea that seems to tie up a few loose threads I have been seeing around the place. I am mainly channelling Robin Hanson here, so some familiarity with him would be useful. I'd also recommend <a href=\"http://www.toddkshackelford.com/downloads/Schmitt-Shackelford-EP-2008.pdf\">this paper on sexuality and character traits</a>. And the <a href=\"http://books.google.co.uk/books?id=fHnBMyxYXX4C&amp;dq=red+queen&amp;source=bl&amp;ots=czGHMkOpdD&amp;sig=dL2G0u5cZTUbrgzKgZR9X59xRhs&amp;hl=en&amp;ei=onn0S6yAH6T80wSwo4yODQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=3&amp;ved=0CCUQ6AEwAg\">red queen</a>.</p>\n<p>Note: I don't have time to write a properly researched paper, so you are going to have to settle for a blog style post. So it is not front page material. But I would rather it got more coverage than in the open thread. If someone is enamoured with the idea, feel free to make a well written riff on this theme. I won't get off\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nended.</p>\n<h3>The summer strategy</h3>\n<p>This is selected for by sexual selection. Women want to mate with attractive powerful men when they don't have to worry about the babies being provided for by that man. Attractive powerful men have to signal there attractiveness and power, they can use lots of resources to do so. This [sexual strategies theory paper](http://www.psy.cmu.edu/~rakison/bussandschmitt.pdf) gives lots of good reasons why men may have been selected for what they call short term sexuality.</p>\n<p>It is characterised by:</p>\n<p>- Less planning needed. Credit cards. Significant Debt.</p>\n<p>- <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">Near</a> thinking</p>\n<p>- Extroversion</p>\n<p>- Babies without fathers will more likely to survive. Women gathering resources by themselves if they lack male relatives. This leads to more promiscuous women and more promiscuous men as well, as social mores change. Short-term mating strategies do well. Breakdown of traditional monogamy.</p>\n<p>- More extroversion in men. Due to less need for planning ahead and gathering resources you can spend more time raising your status in the tribe for more mates and the chance of&nbsp;cuckolding other men.</p>\n<p>- <a href=\"http://www.overcomingbias.com/2010/05/color-meanings.html\">associated with red/orange</a> heat and warmth. Summer and harvest</p>\n<p>- risk taking</p>\n<h3>The winter strategy</h3>\n<p>In times and locations where resources change significantly, short term mating is not so successful. A short term mating men can not rely on there being sufficient resources to raise their kids. So this selects for providers. Common things in evolutionary history that might provide this pressure is the coming of harsh winters in northern climates and ice ages that pushed people out of land. These events reduced the amount of resources available and benefited people that prepared for it.&nbsp; It is nature vs person selection pressure, rather than person vs person.</p>\n<p>It is characterised by:</p>\n<p>- More planning and preparation. Stockpiling resources. Savings.&nbsp;</p>\n<p>- <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">Far</a> thinking</p>\n<p>- Mild Introversion</p>\n<p>- Babies without fathers unlikely to survive. Less promiscuous women. More interested in practical abilities of mate than beauty. This might be where the \"myth\" of women wanting a provider/gifts comes from. They did want a provider, of sorts, but still not a complete wuss. But now that we are in permanent summer, resource wise, strategies change.&nbsp;Long-term mating strategy is the norm for a winter strategist.</p>\n<p>- risk averse</p>\n<p>- social interaction more about keeping the group happy and on your side, rather than trying to be alpha. Although it can't hurt to be alpha.</p>\n<p>- <a href=\"http://www.overcomingbias.com/2010/05/color-meanings.html\">association with blue</a>. Cold/ice. Coming of winter; time to prepare.</p>\n<h3>Some points of discussion</h3>\n<p>Men are probably more naturally summer strategists. Women are more naturally winter strategists so might not be very good at knowing what they want when they are performing the summer strategy. This has been discussed in evolutionary theory as <a href=\"http://en.wikipedia.org/wiki/Parental_investment\">parental investment</a>.</p>\n<p>Could this be an explanation for the protestant (northern European) work ethic and the success of Europe solving man vs nature problems? Due to harsher selection via more winters/ice ages?</p>\n<p>Intelligent winter strategists go on to be part of SIAI, Summer strategists go on to be entrepreneurs (epitome of risk taking). Winter strategists in this day&nbsp;and age (at least in the developed world) are more likely to be extreme or odd, as the moderates are likely to be over in the summer camp.</p>\n<p>I suspect that this is why there is some disconnect of view between the PUA advocates and some of the resident existential risk thinkers. Signalling you are summer strategist is a very bad idea for a winter strategist.</p>\n<p>If the summer/winter divide has an element of truth, it doesn't look good for advocates of far thinking (greens,existential risk activists). As we get better and better at meeting our needs we will slip more into short term thinking and status contests (without genetic engineering at least).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EsDP9yKGQkozJEoZo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -5, "extendedScore": null, "score": 5.881605168188316e-07, "legacy": true, "legacyId": "2933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Abstract: <em>I have a hypothesis that there are two different general strategies for life that humans might switch between predicated on the way general resource availability change in the society. If it is constant or increasing one strategy pays off, if predictably increasing then decreasing, another is good. These strategies would have been selected for at different times and environments in prehistory but humans are mainly plastic in which strategy they adopt. Culture reinforces them and can create lags. For value neutral purposes I will call them by seasons, the Summer strategy and the Winter strategy. The summer is for times of plenty and partying, and the winter for when resources regularly become scarcer and life becomes harsher. These strategies affect every part of society from mating to the way people plan</em>.</p>\n<p><a id=\"more\"></a></p>\n<p>The above is an idea that seems to tie up a few loose threads I have been seeing around the place. I am mainly channelling Robin Hanson here, so some familiarity with him would be useful. I'd also recommend <a href=\"http://www.toddkshackelford.com/downloads/Schmitt-Shackelford-EP-2008.pdf\">this paper on sexuality and character traits</a>. And the <a href=\"http://books.google.co.uk/books?id=fHnBMyxYXX4C&amp;dq=red+queen&amp;source=bl&amp;ots=czGHMkOpdD&amp;sig=dL2G0u5cZTUbrgzKgZR9X59xRhs&amp;hl=en&amp;ei=onn0S6yAH6T80wSwo4yODQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=3&amp;ved=0CCUQ6AEwAg\">red queen</a>.</p>\n<p>Note: I don't have time to write a properly researched paper, so you are going to have to settle for a blog style post. So it is not front page material. But I would rather it got more coverage than in the open thread. If someone is enamoured with the idea, feel free to make a well written riff on this theme. I won't get off\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nended.</p>\n<h3 id=\"The_summer_strategy\">The summer strategy</h3>\n<p>This is selected for by sexual selection. Women want to mate with attractive powerful men when they don't have to worry about the babies being provided for by that man. Attractive powerful men have to signal there attractiveness and power, they can use lots of resources to do so. This [sexual strategies theory paper](http://www.psy.cmu.edu/~rakison/bussandschmitt.pdf) gives lots of good reasons why men may have been selected for what they call short term sexuality.</p>\n<p>It is characterised by:</p>\n<p>- Less planning needed. Credit cards. Significant Debt.</p>\n<p>- <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">Near</a> thinking</p>\n<p>- Extroversion</p>\n<p>- Babies without fathers will more likely to survive. Women gathering resources by themselves if they lack male relatives. This leads to more promiscuous women and more promiscuous men as well, as social mores change. Short-term mating strategies do well. Breakdown of traditional monogamy.</p>\n<p>- More extroversion in men. Due to less need for planning ahead and gathering resources you can spend more time raising your status in the tribe for more mates and the chance of&nbsp;cuckolding other men.</p>\n<p>- <a href=\"http://www.overcomingbias.com/2010/05/color-meanings.html\">associated with red/orange</a> heat and warmth. Summer and harvest</p>\n<p>- risk taking</p>\n<h3 id=\"The_winter_strategy\">The winter strategy</h3>\n<p>In times and locations where resources change significantly, short term mating is not so successful. A short term mating men can not rely on there being sufficient resources to raise their kids. So this selects for providers. Common things in evolutionary history that might provide this pressure is the coming of harsh winters in northern climates and ice ages that pushed people out of land. These events reduced the amount of resources available and benefited people that prepared for it.&nbsp; It is nature vs person selection pressure, rather than person vs person.</p>\n<p>It is characterised by:</p>\n<p>- More planning and preparation. Stockpiling resources. Savings.&nbsp;</p>\n<p>- <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">Far</a> thinking</p>\n<p>- Mild Introversion</p>\n<p>- Babies without fathers unlikely to survive. Less promiscuous women. More interested in practical abilities of mate than beauty. This might be where the \"myth\" of women wanting a provider/gifts comes from. They did want a provider, of sorts, but still not a complete wuss. But now that we are in permanent summer, resource wise, strategies change.&nbsp;Long-term mating strategy is the norm for a winter strategist.</p>\n<p>- risk averse</p>\n<p>- social interaction more about keeping the group happy and on your side, rather than trying to be alpha. Although it can't hurt to be alpha.</p>\n<p>- <a href=\"http://www.overcomingbias.com/2010/05/color-meanings.html\">association with blue</a>. Cold/ice. Coming of winter; time to prepare.</p>\n<h3 id=\"Some_points_of_discussion\">Some points of discussion</h3>\n<p>Men are probably more naturally summer strategists. Women are more naturally winter strategists so might not be very good at knowing what they want when they are performing the summer strategy. This has been discussed in evolutionary theory as <a href=\"http://en.wikipedia.org/wiki/Parental_investment\">parental investment</a>.</p>\n<p>Could this be an explanation for the protestant (northern European) work ethic and the success of Europe solving man vs nature problems? Due to harsher selection via more winters/ice ages?</p>\n<p>Intelligent winter strategists go on to be part of SIAI, Summer strategists go on to be entrepreneurs (epitome of risk taking). Winter strategists in this day&nbsp;and age (at least in the developed world) are more likely to be extreme or odd, as the moderates are likely to be over in the summer camp.</p>\n<p>I suspect that this is why there is some disconnect of view between the PUA advocates and some of the resident existential risk thinkers. Signalling you are summer strategist is a very bad idea for a winter strategist.</p>\n<p>If the summer/winter divide has an element of truth, it doesn't look good for advocates of far thinking (greens,existential risk activists). As we get better and better at meeting our needs we will slip more into short term thinking and status contests (without genetic engineering at least).</p>", "sections": [{"title": "The summer strategy", "anchor": "The_summer_strategy", "level": 1}, {"title": "The winter strategy", "anchor": "The_winter_strategy", "level": 1}, {"title": "Some points of discussion", "anchor": "Some_points_of_discussion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-20T13:58:04.856Z", "modifiedAt": null, "url": null, "title": "Another way to look at consciousness", "slug": "another-way-to-look-at-consciousness", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:17.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JanetK", "createdAt": "2009-03-30T08:50:37.288Z", "isAdmin": false, "displayName": "JanetK"}, "userId": "42P9xkZ9poMdaBTWJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FcgBuuK3i7RkATRuK/another-way-to-look-at-consciousness", "pageUrlRelative": "/posts/FcgBuuK3i7RkATRuK/another-way-to-look-at-consciousness", "linkUrl": "https://www.lesswrong.com/posts/FcgBuuK3i7RkATRuK/another-way-to-look-at-consciousness", "postedAtFormatted": "Thursday, May 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20way%20to%20look%20at%20consciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20way%20to%20look%20at%20consciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFcgBuuK3i7RkATRuK%2Fanother-way-to-look-at-consciousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20way%20to%20look%20at%20consciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFcgBuuK3i7RkATRuK%2Fanother-way-to-look-at-consciousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFcgBuuK3i7RkATRuK%2Fanother-way-to-look-at-consciousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1376, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Edit: First paragraph removed and small changes made to the rest.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I am putting forth a hypothesis is about the nature of consciousness. First I will have to tell you how I am using certain words because they are generally used in a number of ways.&nbsp; 'Brain' is an biological organ and it has a function, 'mind'. Mind is not an object; it is what brains do. It is not a property of the brain, let alone an emergent property, whatever that is. It is a function - so mind-is-to-brain as circulation-is-to-heart or digestion-is-to- intestine. There is one brain in any head and there is one mind being produced by that brain &ndash; not two. (Assuming sanity) The different parts of the cortex work together; the two hemispheres work together; the fore-brain structures work together with the mid-brain structures. The mind includes at least: perception, cognition, learning, intention, motor control, remembering, and most importantly, the forming a model of the environment and the person in that environment. The division between 'conscious mind' and 'unconscious mind' is meaningless. The brain does its mind-function which maintains the model. Some but not all of this model is made globally accessible to all of the brain and remembered. That edit of the model is what we experience as conscious experience, in other words, is our 'consciousness'. Consciousness is awareness not thought. Consciousness is not separate but part of a single mind-function. Now that the words are straight, I can describe the hypothesis.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<h2 style=\"margin-bottom: 0in;\">How is the model edited for consciousness?</h2>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">There is an attention focus that is triggered by the on-going work of the mind and the events that happen in the environment. I may concentrate on some task so that I am not conscious of other parts of the model but a loud noise will cause my attention focus to shift to the source of noise in the model. The level of attention is variable from non-existent (coma) to intense. This level depends on the signals coming from the lower parts of the brain, through the thalamus, into the cortex. A common analogue for attention is a searchlight scanning the mind-model of reality. We cannot be aware of the whole of the model at any instant of time.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<h2 style=\"margin-bottom: 0in;\">How is the model formed?</h2>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The fragments for the model are forced together into a best fit global model. The perception of the various senses, inborn constraints, our understanding of the world, our memory of the previous seconds, our expectations etc. together build a cohesive model by constructing a synchronous neural activity. Fragments that cannot be fit into the model are lost from it. This is done by an almost unbelievable number of parallel, slightly overlapping feedback loops, across the cortex and between the cortex and the mid-brain (especially the thalamus). The feedback loops are much more like patch boards then like digital computers. They rattle for an instance until they find a stable synchrony. There is nothing like step-wise processes at this stage of forming a global model.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<h2 style=\"margin-bottom: 0in;\">How is the consciousness edit of the model used?</h2>\n<p style=\"margin-bottom: 0in;\">There is little doubt that consciousness is useful because it is biologically expensive. Evolution will eliminate expensive functions that do not earn their keep. There are three very important processes that are carried on by the consciousness aspect of mind.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">1) The working memory that holds the last few frames of consciousness is the source of episodic memory. There is an important link between consciousness and the formation of memory. We could treat working memory as part of consciousness or part of more permanent memory or even the link between them. Consciousness is in effect 'the leading edge of memory'. No conscious experience of something than no memory of it.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">2) The working memory allows some cognition and learning that needs to 'juggle' information. I cannot add two digits if I cannot retain one while I perceive the other. So some thought processes are going to be in the edited model so that they are be continued through the use of working memory. This does not constitute a conscious mind that is separate from an unconscious mind. It is only that some types of thinking register bits of their progress in our awareness so they can be retrieved later.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">3) Consciousness does mild prediction and therefore can register errors in perception and motor control. It takes a fraction of a second to form the conscious experience of an event. But we do not live our lives a fraction of a second late. Information from time t is used to form a model of what the world will be like a t+x and x being the time it takes to create the model and its conscious edited version, then we will seem to experience t+x at t+x. The difference between the model of t+x and what comes in via our senses at t+x is the actual error in our perception and motor control and is be used to correct the system.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">These three functions seem ample to justify the metabolic expense of consciousness and rule out philosophical zombies. The functions also seem to rule out consciousness being uniquely human. 'If it quacks like a duck' logic applies to animal consciousness. If an animal appears to have a good memory of events, learns from its experiences, has smooth motor control in complex changing situations, then it is hard to imagine how this happens without consciousness including self consciousness. There would, of course, be degrees of consciousness and variations in the aspects of environment/self that would be modeled by different animals.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<h2 style=\"margin-bottom: 0in;\">My answers to some problems ahead of their being asked<br /></h2>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Most readers of this site are comfortable with the idea of the map and the territory. This post is using a very similar (maybe the same) idea of reality and model of reality. There is nothing surprising about the difference between the physical tree in the garden and an element that stands for that tree in my model of reality. It is the same idea to think about the difference between the reality-now and the model-now. The difference between my physical leg and my model leg is not difficult. We need to extent that comfort to the difference between the reality-me and the model-me. Introspection gives us awareness of our model, it is not our reality-mind but our model-mind we are turning our focus of attention on. There is a difference between reality-decisions and model-decisions. We live in our model and have absolutely, positively no direct knowledge of anything else &ndash; none ever.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I have given no evidence for the hypotheses here but for two years I have been collecting evidence on consciousness in my website, <a href=\"http://charbonniers.org/\">Thoughts on thoughts</a>. My hypothesis is not that different from the one that Academian is giving in his series of posts and I do not mean mine to be in opposition to his, but to a large extent supportive. Treating consciousness as a sense is not that different from treating it as as a selective awareness. There is no need to get hung up on the words or analogies we use.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I have side-stepped the 'hard question' of how and why red is experienced as red. I have the feeling that this is a 'wrong question' but I am not sure why. It is certainly not explained by the hypothesis I have given here. All I have to say about the hard question is: &ldquo;Can you think of a better way to be aware of red then the one you have?, Is there something more efficient or more vivid or more biologically functional?&rdquo; In other words, &ldquo;What is the alternative?&rdquo; Even if you go all spiritual, that still does not explain the experience of red. Dualism does not answer the hard question either and I have not encountered any philosophy that does. If it is answered, I would put my money on a scientific, material answer.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I have not side-stepped the question of how consciousness is reduced to physics. The method is clear: reduce consciousness to biology and biology to physics/chemistry. We accept that biology is in principle reducible to physics/chemistry. We generally assume that the brain is understandable as a biological organ and so if we can assume that consciousness is a function of the brain, it is in principle reducible to physics.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FcgBuuK3i7RkATRuK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 2, "extendedScore": null, "score": 5.883262795723742e-07, "legacy": true, "legacyId": "2936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Edit: First paragraph removed and small changes made to the rest.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I am putting forth a hypothesis is about the nature of consciousness. First I will have to tell you how I am using certain words because they are generally used in a number of ways.&nbsp; 'Brain' is an biological organ and it has a function, 'mind'. Mind is not an object; it is what brains do. It is not a property of the brain, let alone an emergent property, whatever that is. It is a function - so mind-is-to-brain as circulation-is-to-heart or digestion-is-to- intestine. There is one brain in any head and there is one mind being produced by that brain \u2013 not two. (Assuming sanity) The different parts of the cortex work together; the two hemispheres work together; the fore-brain structures work together with the mid-brain structures. The mind includes at least: perception, cognition, learning, intention, motor control, remembering, and most importantly, the forming a model of the environment and the person in that environment. The division between 'conscious mind' and 'unconscious mind' is meaningless. The brain does its mind-function which maintains the model. Some but not all of this model is made globally accessible to all of the brain and remembered. That edit of the model is what we experience as conscious experience, in other words, is our 'consciousness'. Consciousness is awareness not thought. Consciousness is not separate but part of a single mind-function. Now that the words are straight, I can describe the hypothesis.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<h2 style=\"margin-bottom: 0in;\" id=\"How_is_the_model_edited_for_consciousness_\">How is the model edited for consciousness?</h2>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">There is an attention focus that is triggered by the on-going work of the mind and the events that happen in the environment. I may concentrate on some task so that I am not conscious of other parts of the model but a loud noise will cause my attention focus to shift to the source of noise in the model. The level of attention is variable from non-existent (coma) to intense. This level depends on the signals coming from the lower parts of the brain, through the thalamus, into the cortex. A common analogue for attention is a searchlight scanning the mind-model of reality. We cannot be aware of the whole of the model at any instant of time.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<h2 style=\"margin-bottom: 0in;\" id=\"How_is_the_model_formed_\">How is the model formed?</h2>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The fragments for the model are forced together into a best fit global model. The perception of the various senses, inborn constraints, our understanding of the world, our memory of the previous seconds, our expectations etc. together build a cohesive model by constructing a synchronous neural activity. Fragments that cannot be fit into the model are lost from it. This is done by an almost unbelievable number of parallel, slightly overlapping feedback loops, across the cortex and between the cortex and the mid-brain (especially the thalamus). The feedback loops are much more like patch boards then like digital computers. They rattle for an instance until they find a stable synchrony. There is nothing like step-wise processes at this stage of forming a global model.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<h2 style=\"margin-bottom: 0in;\" id=\"How_is_the_consciousness_edit_of_the_model_used_\">How is the consciousness edit of the model used?</h2>\n<p style=\"margin-bottom: 0in;\">There is little doubt that consciousness is useful because it is biologically expensive. Evolution will eliminate expensive functions that do not earn their keep. There are three very important processes that are carried on by the consciousness aspect of mind.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">1) The working memory that holds the last few frames of consciousness is the source of episodic memory. There is an important link between consciousness and the formation of memory. We could treat working memory as part of consciousness or part of more permanent memory or even the link between them. Consciousness is in effect 'the leading edge of memory'. No conscious experience of something than no memory of it.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">2) The working memory allows some cognition and learning that needs to 'juggle' information. I cannot add two digits if I cannot retain one while I perceive the other. So some thought processes are going to be in the edited model so that they are be continued through the use of working memory. This does not constitute a conscious mind that is separate from an unconscious mind. It is only that some types of thinking register bits of their progress in our awareness so they can be retrieved later.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">3) Consciousness does mild prediction and therefore can register errors in perception and motor control. It takes a fraction of a second to form the conscious experience of an event. But we do not live our lives a fraction of a second late. Information from time t is used to form a model of what the world will be like a t+x and x being the time it takes to create the model and its conscious edited version, then we will seem to experience t+x at t+x. The difference between the model of t+x and what comes in via our senses at t+x is the actual error in our perception and motor control and is be used to correct the system.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">These three functions seem ample to justify the metabolic expense of consciousness and rule out philosophical zombies. The functions also seem to rule out consciousness being uniquely human. 'If it quacks like a duck' logic applies to animal consciousness. If an animal appears to have a good memory of events, learns from its experiences, has smooth motor control in complex changing situations, then it is hard to imagine how this happens without consciousness including self consciousness. There would, of course, be degrees of consciousness and variations in the aspects of environment/self that would be modeled by different animals.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<h2 style=\"margin-bottom: 0in;\" id=\"My_answers_to_some_problems_ahead_of_their_being_asked\">My answers to some problems ahead of their being asked<br></h2>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Most readers of this site are comfortable with the idea of the map and the territory. This post is using a very similar (maybe the same) idea of reality and model of reality. There is nothing surprising about the difference between the physical tree in the garden and an element that stands for that tree in my model of reality. It is the same idea to think about the difference between the reality-now and the model-now. The difference between my physical leg and my model leg is not difficult. We need to extent that comfort to the difference between the reality-me and the model-me. Introspection gives us awareness of our model, it is not our reality-mind but our model-mind we are turning our focus of attention on. There is a difference between reality-decisions and model-decisions. We live in our model and have absolutely, positively no direct knowledge of anything else \u2013 none ever.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I have given no evidence for the hypotheses here but for two years I have been collecting evidence on consciousness in my website, <a href=\"http://charbonniers.org/\">Thoughts on thoughts</a>. My hypothesis is not that different from the one that Academian is giving in his series of posts and I do not mean mine to be in opposition to his, but to a large extent supportive. Treating consciousness as a sense is not that different from treating it as as a selective awareness. There is no need to get hung up on the words or analogies we use.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I have side-stepped the 'hard question' of how and why red is experienced as red. I have the feeling that this is a 'wrong question' but I am not sure why. It is certainly not explained by the hypothesis I have given here. All I have to say about the hard question is: \u201cCan you think of a better way to be aware of red then the one you have?, Is there something more efficient or more vivid or more biologically functional?\u201d In other words, \u201cWhat is the alternative?\u201d Even if you go all spiritual, that still does not explain the experience of red. Dualism does not answer the hard question either and I have not encountered any philosophy that does. If it is answered, I would put my money on a scientific, material answer.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I have not side-stepped the question of how consciousness is reduced to physics. The method is clear: reduce consciousness to biology and biology to physics/chemistry. We accept that biology is in principle reducible to physics/chemistry. We generally assume that the brain is understandable as a biological organ and so if we can assume that consciousness is a function of the brain, it is in principle reducible to physics.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>", "sections": [{"title": "How is the model edited for consciousness?", "anchor": "How_is_the_model_edited_for_consciousness_", "level": 1}, {"title": "How is the model formed?", "anchor": "How_is_the_model_formed_", "level": 1}, {"title": "How is the consciousness edit of the model used?", "anchor": "How_is_the_consciousness_edit_of_the_model_used_", "level": 1}, {"title": "My answers to some problems ahead of their being asked", "anchor": "My_answers_to_some_problems_ahead_of_their_being_asked", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-20T17:11:34.085Z", "modifiedAt": null, "url": null, "title": "Development of Compression Rate Method", "slug": "development-of-compression-rate-method", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:23.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Daniel_Burfoot", "createdAt": "2009-03-12T02:28:50.970Z", "isAdmin": false, "displayName": "Daniel_Burfoot"}, "userId": "XhcXE3Qk5adX6v2Cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/meTmrcCwSsDYmo5DZ/development-of-compression-rate-method", "pageUrlRelative": "/posts/meTmrcCwSsDYmo5DZ/development-of-compression-rate-method", "linkUrl": "https://www.lesswrong.com/posts/meTmrcCwSsDYmo5DZ/development-of-compression-rate-method", "postedAtFormatted": "Thursday, May 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Development%20of%20Compression%20Rate%20Method&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADevelopment%20of%20Compression%20Rate%20Method%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmeTmrcCwSsDYmo5DZ%2Fdevelopment-of-compression-rate-method%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Development%20of%20Compression%20Rate%20Method%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmeTmrcCwSsDYmo5DZ%2Fdevelopment-of-compression-rate-method", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmeTmrcCwSsDYmo5DZ%2Fdevelopment-of-compression-rate-method", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3630, "htmlBody": "<p>&nbsp;</p>\n<p><strong>Summary</strong>: This post provides a brief discussion of the traditional scientific method, and mentions some areas where the method cannot be directly applied. Then, through a series of thought experiments, a set of minor modifications to the traditional method are presented. The result is a refined version of the method, based on data compression.</p>\n<p><strong>Related to</strong>: <a href=\"/lw/qg/changing_the_definition_of_science/\">Changing the Definition of Science</a>, <a href=\"/lw/jo/einsteins_arrogance/\">Einstein's Arrogance</a>, <a href=\"/lw/qa/the_dilemma_science_or_bayes/\">The Dilemma: Science or Bayes?</a></p>\n<p><strong>ETA</strong>: For those who are familiar with notions such as Kolmogorov Complexity and MML, this piece may have a low ratio of novelty:words. The basic point is that one can compare scientific theories by instantiating them as compression programs, using them to compress a benchmark database of measurements related to a phenomenon of interest, and comparing the resulting codelengths (taking into account the length of the compressor itself).</p>\n<h3><a id=\"more\"></a><br /></h3>\n<h3>Notes on Traditional Method<br /></h3>\n<p>This post proposes a refined version of the scientific method which, it will be argued later, is more directly applicable to the problems of interest in artificial intelligence. Before doing so, it is worth briefly examining the traditional method and the circumstances in which it can be applied. The scientific method is not an exact procedure, but a qualitative statement of it goes roughly as follows:</p>\n<ol>\n<li>Observe a natural phenomenon. </li>\n<li> Develop a theory of that phenomenon. </li>\n<li> Use the theory to make a prediction. </li>\n<li> Test the prediction experimentally. </li>\n</ol>\n<p>A full discussion of the philosophical significance of the scientific method is beyond the scope of this post, but some brief remarks are in order. The power of the scientific method is in the way it links theory with experimental observation; either one of these alone is worthless. The long checkered intellectual history of humanity clearly shows how rapidly pure theoretical speculation goes astray when it is not tightly constrained by an external guiding force. Pure experimental investigation, in contrast, is of limited value because of the vast number of possible configurations of objects. To make predictions solely on the basis of experimental data, it would be necessary to exhaustively test each configuration.</p>\n<p>&nbsp;</p>\n<p>As articulated in the above list, the goal of the method appears to be the verification of a single theory. This is a bit misleading; in reality the goal of the method is to facilitate selection between a potentially large number of candidate theories. Given two competing theories of a particular phenomenon, the researcher identifies some experimental configuration where the theories make incompatible predictions and then performs the experiment using the indicated configuration. The theory whose predictions fail to match the experimental prediction is discarded in favor of its rival. But even this view of science as a process of weeding out imperfect theories in order to find the perfect one is somewhat inaccurate. Most physicists will admit or disclaim that even their most refined theories are mere approximations, though they are spectacularly accurate approximations. The scientific method can therefore be understood as a technique for using empirical observations to find the best predictive approximation from a large pool of candidates.</p>\n<p>A core component of the traditional scientific method is the use of controlled experiments. To control an experiment means essentially to simplify it. To determine the effect of a certain factor, one sets up two experimental configurations which are exactly the same except for the presence or absence of the factor. If the experimental outcomes are different, then it can be inferred that this disparity is due to the special factor.</p>\n<p>In some fields of scientific inquiry, however, it is impossible or meaningless to conduct controlled experiments. No two people are identical in all respects, so clinical trials for new drugs, in which the human subject is part of the experimental configuration, can never be truly controlled. The best that medical researchers can do is to attempt to ensure that the experimental factor does not systematically correlate with other factors that may affect the outcome. This is done by selecting at random which patients will receive the new treatment. This method is obviously limited, however, and these limitations lead to <a href=\"http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124\">deep problems in the medical literature</a>. It is similarly difficult to apply the traditional scientific method to answer questions arising in the field of macroeconomics. No political leader would ever agree to a proposal in which her country's economy was to be used as an experimental test subject. In lieu of controlled experiments, economists attempt to test their theories based on the outcomes of so-called historical experiments, where two originally similar countries implemented different economic policies.</p>\n<p>A similar breakdown of the traditional method occurs in computer vision (recall that <a href=\"/lw/299/preface_to_a_proposal_for_a_new_mode_of_inquiry/\">my hypothesis</a> asserts that perception and prediction are the major components of intelligence, implying that the study of vision is central to the study of AI). Controlled vision experiments can be conducted, but are of very little interest. The physical laws of reflection and optics that govern the image formation process are well understood already. Clearly if the same camera is used to photograph an identical scene twice under constant lighting conditions, the obtained images will be identical or very nearly so. And a deterministic computer vision algorithm will always produce the same result when applied to two identical images. It is not clear, therefore, how to use the traditional method to approach the problems of interest in computer vision, which include tasks like <a href=\"http://en.wikipedia.org/wiki/Segmentation_%28image_processing%29\">image segmentation</a> and <a href=\"http://en.wikipedia.org/wiki/Edge_detection\">edge detection</a>.</p>\n<p>(The field of computer vision will be discussed in later posts. For now, the important thing to realize is that there are deep, deep problems in <em>evaluating</em> computer vision techniques. Given two image segmentation algorithms, how do you decide which one is better? The field has no compelling answer. The lack of empirical rigor in computer vision has been lamented in papers with titles like \"<a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B7GXG-4D8FSK3-6R&amp;_user=10&amp;_coverDate=12%2F31%2F1986&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_rerunOrigin=scholar.google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=4bc34d16ad9afd1abbfbb1808b45c63b\">Computer Vision Theory: the Lack Thereof</a>\" and \"<a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WDD-4DX42W8-9&amp;_user=10&amp;_coverDate=01%2F31%2F1991&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_rerunOrigin=scholar.google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=8e87e265eca38d24e80b2dcbac4ecb14\">Ignorance, Myopia, and Naivete in Computer Vision Systems</a>\".)</p>\n<h3>Sophie's Adventures</h3>\n<p>The modifications to the scientific method are presented through a series of thought experiments related to a fictional character named Sophie.</p>\n<h4>Episode I: The Shaman</h4>\n<p>Sophie is a assistant professor of physics at a large American state university. She finds this job vexing for several reasons, one of which is that she has been chosen by the department to teach a physics class intended for students majoring in the humanities, for whom it serves to fill a breadth requirement. The students in this class, who major in subjects like literature, religious studies, and philosophy, tend to be intelligent but also querulous and somewhat disdainful of the \"merely technical\" intellectual achievements of physics.</p>\n<p>In the current semester she has become aware of the presence in her class of a discalced student with a large beard and often bloodshot eyes. This student is surrounded by an entourage of similarly odd-looking followers. Sophie is on good terms with some of the more serious students in the class, and in conversation with them has found out that the odd student is attempting to start a new naturalistic religious movement and refers to himself as a \"shaman\".</p>\n<p>One day while delivering a simple lecture on Newtonian mechanics, she is surprised when the shaman raises his hand and claims that physics is a propagandistic hoax designed by the elites as a way to control the population. Sophie blinks several times, and then responds that physics can't be a hoax because it makes real-world predictions that can be verified by independent observers. The shaman counters by claiming that the so-called \"predictions\" made by physics are in fact trivialities, and that he can obtain better forecasts by communing with the spirit world. He then proceeds to challenge Sophie to a predictive duel, in which the two of them will make forecasts regarding the outcome of a simple experiment, the winner being decided based on the accuracy of the forecasts. Sophie is taken aback by this but, hoping that by proving the shaman wrong she can break the spell he has cast on some of the other students, agrees to the challenge.</p>\n<p>During the next class, Sophie sets up the following experiment. She uses a spring mechanism to launch a ball into the air at an angle A. The launch mechanism allows her to set the initial velocity of the ball to a value of Vi. She chooses as a predictive test the problem of predicting the time Tf that the ball will fall back to the ground after being launched at Ti=0. Using a trivial Newtonian calculation she concludes that Tf = 2 Vi sin(A)/g, sets Vi and A to give a value of Tf=2 seconds, and announces her prediction to the class. She then asks the shaman for his prediction. The shaman declares that he must consult with the wind spirits, and then spends a couple of minutes chanting and muttering. Then, dramatically flaring open his eyes as if to signify a moment of revelation, he grabs a piece of paper, writes his prediction on it, and then hands it to another student. Sophie suspects some kind of trick, but is too exasperated to investigate and so launches the ball into the air. The ball is equipped with an electronic timer that starts and stops when an impact is detected, and so the number registered in the timer is just the time of flight Tf. A student picks up the ball and reports that the result is Tf = 2.134. The shaman gives a gleeful laugh, and the student holding his written prediction hands it to Sophie. On the paper is written 1 &lt; Tf &lt; 30. The shaman declares victory: his prediction turned out to be correct, while Sophie's was incorrect (it was off by 0.134 seconds).</p>\n<p>To counter the shaman's claim and because it was on the syllabus anyway, in the next class Sophie begins a discussion of probability theory. She goes over the basic ideas, and then connects them to the experimental prediction made about the ball. She points out that technically, the Newtonian prediction Tf=2 is not an assertion about the exact value of the outcome. Rather it should be interpreted as the mean of a probability distribution describing possible outcomes. For example, one might use a normal distribution with mean of 2 and standard deviation of .3. The reason the shaman superficially seemed to win the contest is that he gave a probability distribution while Sophie gave a point prediction; these two types of forecast are not really comparable. In the light of probability theory, the reason to prefer the Newtonian prediction above the shamanic one, is that it assigns a higher probability to the outcome that actually occurred. Now, plausibly, if only a single trial is used then the Newtonian theory might simply have gotten lucky, so the reasonable thing to do is combine the results over many trials, by multiplying the probabilities together. Therefore the real reason to prefer the Newtonian theory to the shamanic theory is that:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\prod_{k}%20P_{newton}%28t_{f,k}%29%20%3E%20\\prod_{k}%20P_{shaman}%28t_{f,k}%29\" alt=\"\" /></p>\n<p>Where the k index runs over many trials of the experiment. Sophie then shows how the Newtonian probability predictions are both more <em>confident</em> and more <em>correct</em> than the shamanic predictions. The Newtonian predictions assign a very large amount of probability to the region around the outcome Tf=2, and in fact it turns out that almost all of the real data outcomes fall in this range. In contrast, the shamanic prediction assigns a relatively small amount of probability to the Tf=2 region, because he has predicted a very wide interval (1 &lt; Tf &lt; 30). Thus while the shamanic prediction is correct, it is not very confident. The Newtonian prediction is correct and highly confident, and so it should be prefered.</p>\n<p>Sophie tries to emphasize that the Newtonian probability prediction only works well for the <em>real</em> data. Because of the requirement that probability distributions be normalized, the Newtonian theory can only achieve superior high performance by reassigning probability towards the region around Tf=2 and away from other regions. A theory that does not perform this kind of reassignment cannot achieve superior high performance.</p>\n<p>Sophie recalls that some of the students are studying computer science and for their benefit points out the following. Information theory provides the standard equation L(x) = -log P(x) governs the relationship between the probability of an outcome and the length of the optimal code that should be used to represent it. Therefore, given a large data file containing the results of many trials of the ballistic motion experiment, the two predictions (Newtonian and shamanic) can both be used to build specialized programs to compress the data file. Using the codelength/probability conversion, the above inequality can be rewritten as follows:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\sum_{k}%20L_{newton}%28t_{f,k}%29%20%3C%20\\sum_{k}%20L_{shaman}%28t_{f,k}%29\" alt=\"\" /></p>\n<p>This inequality indicates an alternative criterion that can be used to decide between two rival theories. Given a data file recording measurements related to a phenomenon of interest, a scientific theory can be used to write a compression program that will shrink the file to a small size. Given two rival theories of the same phenomenon, one invokes the corresponding compressors on a shared benchmark data set, and prefers the theory that achieves a smaller encoded file size. This criterion is equivalent to the probability-based one, but has the advantage of being more tangible, since the quantities of interest are file lengths instead of probabilities.</p>\n<h4>Episode II: The Dead Experimentalist</h4>\n<p>Sophie is a theoretical physicist and, upon taking up her position as assistant professor, began a collaboration with a brilliant experimental physicist who had been working at the university for some time. The experimentalist had previously completed the development of an advanced apparatus that allowed the investigation of an exotic new kind of quantum phenomenon. Using data obtained from the new system, Sophie made rapid progress in developing a mathematical theory of the phenomenon. Tragically, just before Sophie was able complete her theory, the experimentalist was killed in a laboratory explosion that also destroyed the special apparatus. After grieving for a couple of months, Sophie decided that the best way to honor her friend's memory would be to bring the research they had been working on to a successful conclusion.</p>\n<p>Unfortunately, there is a critical problem with Sophie's plan. The experimental apparatus was extremely complex, and Sophie's late partner was the only person in the world who knew how to use it. He had run many trials of the system before his death, so Sophie had a quite large quantity of data. But she had no way of generating any new data. Thus, no matter how beautiful and perfect her theory might be, she had no way of testing it by making predictions.</p>\n<p>One day while thinking about the problem Sophie recalls the incident with the shaman. She remembers the point she had made for the benefit of the software engineers, about how a scientific theory could be used to compress a real world data set to a very small size. Inspired, she decides to apply the data compression principle as a way of testing her theory. She immediately returns to her office and spends the next several weeks writing Matlab code, converting her theory into a compression algorithm. The resulting compressor is highly successful: it shrinks the corpus of experimental data from an initial size of 8.7e11 bits to an encoded size of 3.3e9 bits. Satisfied, Sophie writes up the theory, and submits it to a well-known physics journal.</p>\n<p>The journal editors like the theory, but are a bit skeptical of the compression based method for testing the theory. Sophie argues that if the theory becomes widely known, one of the other experts in the field will develop a similar apparatus, which can then be used to test the theory in the traditional way. She also offers to release the experimental data, so that other researchers can test alternative theories using the same compression principle. Finally she promises to release the source code of her program, to allow external verification of the compression result. These arguments finally convince the journal editors to accept the paper.</p>\n<h4>Episode III: The Upstart Theory</h4>\n<p>After all the mathematics, software development, prose revisions, and persuasion necessary to complete her theory and have the paper accepted, Sophie decides to reward herself by living the good life for a while. She is confident that her theory is essentially correct, and will eventually be recognized as correct by her colleagues. So she spends her time reading novels and hanging out in coffee shops with her friends.</p>\n<p>A couple of months later, however, she receives an unpleasant shock in the form of an email from a colleague which is phrased in consolatory language, but does not contain any clue as to why such language might be in order. After some investigation she finds out that a new paper has been published about the same quantum phenomenon of interest to Sophie. The paper proposes a alternative theory of the phenomenon which bears no resemblance whatever to Sophie's. Furthermore, the paper reports a better compression rate than was achieved by Sophie, on the database that she released.</p>\n<p>Sophie reads the new paper and quickly realizes that it is worthless. The theory depends on the introduction of a large number of additional parameters, the values of which must be obtained from the data itself. In fact, a substantial portion of the paper involves a description of a statistical algorithm that estimates optimal parameter values from the data. In spite of these aesthetic flaws, she finds that many of her colleagues are quite taken with the new paper and some consider it to be \"next big thing\".</p>\n<p>Sophie sends a message to the journal editors describing in detail what she sees as the many flaws of the upstart paper. She emphasizes the asthetic weakness of the new theory, which requires tens of thousands of new parameters. The editors express sympathy, but point out that the new theory outperforms Sophie's theory using the performance metric she herself proposed. The beauty of a theory is important, but its correctness is ultimately more important.</p>\n<p>Somewhat discouraged, Sophie sends a polite email to the authors of the new paper, congratulating them on their result and asking to see their source code. Their response, which arrives a week later, contains a vague excuse about how the source code is not properly documented and relies on proprietary third party libraries. Annoyed, Sophie contacts the journal editors again and asks them for the program they used to verify the compression result. They reply with a link to a binary version of the program.</p>\n<p>When Sophie clicks on the link to download the program, she is annoyed to find it has a size of 800 megabytes. But her annoyance is quickly transformed into enlightenment, as she realizes what happened, and that her previous philosophy contained a serious flaw. The upstart theory is not better than hers; it has only succeeded in reducing the size of the encoded data by dramatically increasing the size of the compressor. Indeed, when dealing with specialized compressors, the distinction between \"program\" and \"encoded data\" becomes almost irrelevant. The critical number is not the size of the compressed file, but the net size of the encoded data plus the compressor itself.</p>\n<p>Sophie writes a response to the new paper which describes the refined compression rate principle. She begins the paper by reiterating the unfortunate circumstances which forced her to appeal to the principle, and expressing the hope that someday an experimental group will rebuild the apparatus developed by her late partner, so that the experimental predictions made by the two theories can be properly tested. Until that day arrives, standard scientific practice does not permit a decisive declaration of theoretical success. But surely there is <em>some </em>theoretical statement that can be made in the meantime, given the large amount of data available. Sophie's proposal is that the goal should be to find the theory that has the highest probability of predicting a new data set, when it can finally be obtained. If the theories are very simple in comparison to the data being modeled, then the size of the encoded data file is a good way of choosing the best theory. But if the theories are complex, then there is a risk of <a href=\"http://en.wikipedia.org/wiki/Overfitting\"><em>overfitting</em></a> the data. To guard against overfitting complex theories must be penalized; a simple way to do this is simply to take into account the codelength required for the compressor itself. The length of Sophie's compressor was negligible, so the net score of her theory is just the codelength of the encoded data file: 3.3e9 bits. The rival theory achieved a smaller size of 2.1e9 for the encoded data file, but required a compressor of 6.7e9 bits to do so, giving a total score of 8.8e9 bits. Since Sophie's net score is lower, her theory should be prefered.</p>\n<p>----</p>\n<p>So, LWers, what do you think? For the present, let's leave aside questions like why this might be relevant for AI, and focus on whether or not the method is a legitimate restatement of the traditional method. If you were a physicist observing the dispute and trying to decide which theory to prefer, would you believe Sophie's, Sophie's rivals' theory, or neither? Some plausible objections are:</p>\n<ul>\n<li>A - Refuse to accept the probabilistic interpretation of the Newtonian prediction (or maybe the conversion to a codelength comparison). </li>\n<li>B - Refuse to accept the significance of Sophie's compression result, without knowing more about the format of the original database. </li>\n<li>C - Refuse to accept that Sophie's result will probably generalize to a new data set, or that penalizing large compressors is an adequate way of guarding against overfitting.</li>\n</ul>\n<p>&nbsp;</p>\n<p>(Originality disclaimer: this post, by itself, is not Highly Original. It is heavily influenced by Eliezer's articles linked to above, as well as the ideas of <a href=\"http://en.wikipedia.org/wiki/Minimum_description_length\">Minimum Description Length</a> and <a href=\"http://en.wikipedia.org/wiki/Kolmogorov_complexity\">Kolmogorov Complexity</a>, and especially an article by Matt Mahoney providing a <a href=\"http://mattmahoney.net/dc/rationale.html\">rationale for a large text compression benchmark</a>. The new ideas mostly involve <em>implications </em>of the above method, which will be discussed in later posts.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb25c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "meTmrcCwSsDYmo5DZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 12, "extendedScore": null, "score": 5.883660432867539e-07, "legacy": true, "legacyId": "2937", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p><strong>Summary</strong>: This post provides a brief discussion of the traditional scientific method, and mentions some areas where the method cannot be directly applied. Then, through a series of thought experiments, a set of minor modifications to the traditional method are presented. The result is a refined version of the method, based on data compression.</p>\n<p><strong>Related to</strong>: <a href=\"/lw/qg/changing_the_definition_of_science/\">Changing the Definition of Science</a>, <a href=\"/lw/jo/einsteins_arrogance/\">Einstein's Arrogance</a>, <a href=\"/lw/qa/the_dilemma_science_or_bayes/\">The Dilemma: Science or Bayes?</a></p>\n<p><strong>ETA</strong>: For those who are familiar with notions such as Kolmogorov Complexity and MML, this piece may have a low ratio of novelty:words. The basic point is that one can compare scientific theories by instantiating them as compression programs, using them to compress a benchmark database of measurements related to a phenomenon of interest, and comparing the resulting codelengths (taking into account the length of the compressor itself).</p>\n<h3><a id=\"more\"></a><br></h3>\n<h3 id=\"Notes_on_Traditional_Method\">Notes on Traditional Method<br></h3>\n<p>This post proposes a refined version of the scientific method which, it will be argued later, is more directly applicable to the problems of interest in artificial intelligence. Before doing so, it is worth briefly examining the traditional method and the circumstances in which it can be applied. The scientific method is not an exact procedure, but a qualitative statement of it goes roughly as follows:</p>\n<ol>\n<li>Observe a natural phenomenon. </li>\n<li> Develop a theory of that phenomenon. </li>\n<li> Use the theory to make a prediction. </li>\n<li> Test the prediction experimentally. </li>\n</ol>\n<p>A full discussion of the philosophical significance of the scientific method is beyond the scope of this post, but some brief remarks are in order. The power of the scientific method is in the way it links theory with experimental observation; either one of these alone is worthless. The long checkered intellectual history of humanity clearly shows how rapidly pure theoretical speculation goes astray when it is not tightly constrained by an external guiding force. Pure experimental investigation, in contrast, is of limited value because of the vast number of possible configurations of objects. To make predictions solely on the basis of experimental data, it would be necessary to exhaustively test each configuration.</p>\n<p>&nbsp;</p>\n<p>As articulated in the above list, the goal of the method appears to be the verification of a single theory. This is a bit misleading; in reality the goal of the method is to facilitate selection between a potentially large number of candidate theories. Given two competing theories of a particular phenomenon, the researcher identifies some experimental configuration where the theories make incompatible predictions and then performs the experiment using the indicated configuration. The theory whose predictions fail to match the experimental prediction is discarded in favor of its rival. But even this view of science as a process of weeding out imperfect theories in order to find the perfect one is somewhat inaccurate. Most physicists will admit or disclaim that even their most refined theories are mere approximations, though they are spectacularly accurate approximations. The scientific method can therefore be understood as a technique for using empirical observations to find the best predictive approximation from a large pool of candidates.</p>\n<p>A core component of the traditional scientific method is the use of controlled experiments. To control an experiment means essentially to simplify it. To determine the effect of a certain factor, one sets up two experimental configurations which are exactly the same except for the presence or absence of the factor. If the experimental outcomes are different, then it can be inferred that this disparity is due to the special factor.</p>\n<p>In some fields of scientific inquiry, however, it is impossible or meaningless to conduct controlled experiments. No two people are identical in all respects, so clinical trials for new drugs, in which the human subject is part of the experimental configuration, can never be truly controlled. The best that medical researchers can do is to attempt to ensure that the experimental factor does not systematically correlate with other factors that may affect the outcome. This is done by selecting at random which patients will receive the new treatment. This method is obviously limited, however, and these limitations lead to <a href=\"http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124\">deep problems in the medical literature</a>. It is similarly difficult to apply the traditional scientific method to answer questions arising in the field of macroeconomics. No political leader would ever agree to a proposal in which her country's economy was to be used as an experimental test subject. In lieu of controlled experiments, economists attempt to test their theories based on the outcomes of so-called historical experiments, where two originally similar countries implemented different economic policies.</p>\n<p>A similar breakdown of the traditional method occurs in computer vision (recall that <a href=\"/lw/299/preface_to_a_proposal_for_a_new_mode_of_inquiry/\">my hypothesis</a> asserts that perception and prediction are the major components of intelligence, implying that the study of vision is central to the study of AI). Controlled vision experiments can be conducted, but are of very little interest. The physical laws of reflection and optics that govern the image formation process are well understood already. Clearly if the same camera is used to photograph an identical scene twice under constant lighting conditions, the obtained images will be identical or very nearly so. And a deterministic computer vision algorithm will always produce the same result when applied to two identical images. It is not clear, therefore, how to use the traditional method to approach the problems of interest in computer vision, which include tasks like <a href=\"http://en.wikipedia.org/wiki/Segmentation_%28image_processing%29\">image segmentation</a> and <a href=\"http://en.wikipedia.org/wiki/Edge_detection\">edge detection</a>.</p>\n<p>(The field of computer vision will be discussed in later posts. For now, the important thing to realize is that there are deep, deep problems in <em>evaluating</em> computer vision techniques. Given two image segmentation algorithms, how do you decide which one is better? The field has no compelling answer. The lack of empirical rigor in computer vision has been lamented in papers with titles like \"<a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B7GXG-4D8FSK3-6R&amp;_user=10&amp;_coverDate=12%2F31%2F1986&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_rerunOrigin=scholar.google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=4bc34d16ad9afd1abbfbb1808b45c63b\">Computer Vision Theory: the Lack Thereof</a>\" and \"<a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WDD-4DX42W8-9&amp;_user=10&amp;_coverDate=01%2F31%2F1991&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_rerunOrigin=scholar.google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=8e87e265eca38d24e80b2dcbac4ecb14\">Ignorance, Myopia, and Naivete in Computer Vision Systems</a>\".)</p>\n<h3 id=\"Sophie_s_Adventures\">Sophie's Adventures</h3>\n<p>The modifications to the scientific method are presented through a series of thought experiments related to a fictional character named Sophie.</p>\n<h4 id=\"Episode_I__The_Shaman\">Episode I: The Shaman</h4>\n<p>Sophie is a assistant professor of physics at a large American state university. She finds this job vexing for several reasons, one of which is that she has been chosen by the department to teach a physics class intended for students majoring in the humanities, for whom it serves to fill a breadth requirement. The students in this class, who major in subjects like literature, religious studies, and philosophy, tend to be intelligent but also querulous and somewhat disdainful of the \"merely technical\" intellectual achievements of physics.</p>\n<p>In the current semester she has become aware of the presence in her class of a discalced student with a large beard and often bloodshot eyes. This student is surrounded by an entourage of similarly odd-looking followers. Sophie is on good terms with some of the more serious students in the class, and in conversation with them has found out that the odd student is attempting to start a new naturalistic religious movement and refers to himself as a \"shaman\".</p>\n<p>One day while delivering a simple lecture on Newtonian mechanics, she is surprised when the shaman raises his hand and claims that physics is a propagandistic hoax designed by the elites as a way to control the population. Sophie blinks several times, and then responds that physics can't be a hoax because it makes real-world predictions that can be verified by independent observers. The shaman counters by claiming that the so-called \"predictions\" made by physics are in fact trivialities, and that he can obtain better forecasts by communing with the spirit world. He then proceeds to challenge Sophie to a predictive duel, in which the two of them will make forecasts regarding the outcome of a simple experiment, the winner being decided based on the accuracy of the forecasts. Sophie is taken aback by this but, hoping that by proving the shaman wrong she can break the spell he has cast on some of the other students, agrees to the challenge.</p>\n<p>During the next class, Sophie sets up the following experiment. She uses a spring mechanism to launch a ball into the air at an angle A. The launch mechanism allows her to set the initial velocity of the ball to a value of Vi. She chooses as a predictive test the problem of predicting the time Tf that the ball will fall back to the ground after being launched at Ti=0. Using a trivial Newtonian calculation she concludes that Tf = 2 Vi sin(A)/g, sets Vi and A to give a value of Tf=2 seconds, and announces her prediction to the class. She then asks the shaman for his prediction. The shaman declares that he must consult with the wind spirits, and then spends a couple of minutes chanting and muttering. Then, dramatically flaring open his eyes as if to signify a moment of revelation, he grabs a piece of paper, writes his prediction on it, and then hands it to another student. Sophie suspects some kind of trick, but is too exasperated to investigate and so launches the ball into the air. The ball is equipped with an electronic timer that starts and stops when an impact is detected, and so the number registered in the timer is just the time of flight Tf. A student picks up the ball and reports that the result is Tf = 2.134. The shaman gives a gleeful laugh, and the student holding his written prediction hands it to Sophie. On the paper is written 1 &lt; Tf &lt; 30. The shaman declares victory: his prediction turned out to be correct, while Sophie's was incorrect (it was off by 0.134 seconds).</p>\n<p>To counter the shaman's claim and because it was on the syllabus anyway, in the next class Sophie begins a discussion of probability theory. She goes over the basic ideas, and then connects them to the experimental prediction made about the ball. She points out that technically, the Newtonian prediction Tf=2 is not an assertion about the exact value of the outcome. Rather it should be interpreted as the mean of a probability distribution describing possible outcomes. For example, one might use a normal distribution with mean of 2 and standard deviation of .3. The reason the shaman superficially seemed to win the contest is that he gave a probability distribution while Sophie gave a point prediction; these two types of forecast are not really comparable. In the light of probability theory, the reason to prefer the Newtonian prediction above the shamanic one, is that it assigns a higher probability to the outcome that actually occurred. Now, plausibly, if only a single trial is used then the Newtonian theory might simply have gotten lucky, so the reasonable thing to do is combine the results over many trials, by multiplying the probabilities together. Therefore the real reason to prefer the Newtonian theory to the shamanic theory is that:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\prod_{k}%20P_{newton}%28t_{f,k}%29%20%3E%20\\prod_{k}%20P_{shaman}%28t_{f,k}%29\" alt=\"\"></p>\n<p>Where the k index runs over many trials of the experiment. Sophie then shows how the Newtonian probability predictions are both more <em>confident</em> and more <em>correct</em> than the shamanic predictions. The Newtonian predictions assign a very large amount of probability to the region around the outcome Tf=2, and in fact it turns out that almost all of the real data outcomes fall in this range. In contrast, the shamanic prediction assigns a relatively small amount of probability to the Tf=2 region, because he has predicted a very wide interval (1 &lt; Tf &lt; 30). Thus while the shamanic prediction is correct, it is not very confident. The Newtonian prediction is correct and highly confident, and so it should be prefered.</p>\n<p>Sophie tries to emphasize that the Newtonian probability prediction only works well for the <em>real</em> data. Because of the requirement that probability distributions be normalized, the Newtonian theory can only achieve superior high performance by reassigning probability towards the region around Tf=2 and away from other regions. A theory that does not perform this kind of reassignment cannot achieve superior high performance.</p>\n<p>Sophie recalls that some of the students are studying computer science and for their benefit points out the following. Information theory provides the standard equation L(x) = -log P(x) governs the relationship between the probability of an outcome and the length of the optimal code that should be used to represent it. Therefore, given a large data file containing the results of many trials of the ballistic motion experiment, the two predictions (Newtonian and shamanic) can both be used to build specialized programs to compress the data file. Using the codelength/probability conversion, the above inequality can be rewritten as follows:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\sum_{k}%20L_{newton}%28t_{f,k}%29%20%3C%20\\sum_{k}%20L_{shaman}%28t_{f,k}%29\" alt=\"\"></p>\n<p>This inequality indicates an alternative criterion that can be used to decide between two rival theories. Given a data file recording measurements related to a phenomenon of interest, a scientific theory can be used to write a compression program that will shrink the file to a small size. Given two rival theories of the same phenomenon, one invokes the corresponding compressors on a shared benchmark data set, and prefers the theory that achieves a smaller encoded file size. This criterion is equivalent to the probability-based one, but has the advantage of being more tangible, since the quantities of interest are file lengths instead of probabilities.</p>\n<h4 id=\"Episode_II__The_Dead_Experimentalist\">Episode II: The Dead Experimentalist</h4>\n<p>Sophie is a theoretical physicist and, upon taking up her position as assistant professor, began a collaboration with a brilliant experimental physicist who had been working at the university for some time. The experimentalist had previously completed the development of an advanced apparatus that allowed the investigation of an exotic new kind of quantum phenomenon. Using data obtained from the new system, Sophie made rapid progress in developing a mathematical theory of the phenomenon. Tragically, just before Sophie was able complete her theory, the experimentalist was killed in a laboratory explosion that also destroyed the special apparatus. After grieving for a couple of months, Sophie decided that the best way to honor her friend's memory would be to bring the research they had been working on to a successful conclusion.</p>\n<p>Unfortunately, there is a critical problem with Sophie's plan. The experimental apparatus was extremely complex, and Sophie's late partner was the only person in the world who knew how to use it. He had run many trials of the system before his death, so Sophie had a quite large quantity of data. But she had no way of generating any new data. Thus, no matter how beautiful and perfect her theory might be, she had no way of testing it by making predictions.</p>\n<p>One day while thinking about the problem Sophie recalls the incident with the shaman. She remembers the point she had made for the benefit of the software engineers, about how a scientific theory could be used to compress a real world data set to a very small size. Inspired, she decides to apply the data compression principle as a way of testing her theory. She immediately returns to her office and spends the next several weeks writing Matlab code, converting her theory into a compression algorithm. The resulting compressor is highly successful: it shrinks the corpus of experimental data from an initial size of 8.7e11 bits to an encoded size of 3.3e9 bits. Satisfied, Sophie writes up the theory, and submits it to a well-known physics journal.</p>\n<p>The journal editors like the theory, but are a bit skeptical of the compression based method for testing the theory. Sophie argues that if the theory becomes widely known, one of the other experts in the field will develop a similar apparatus, which can then be used to test the theory in the traditional way. She also offers to release the experimental data, so that other researchers can test alternative theories using the same compression principle. Finally she promises to release the source code of her program, to allow external verification of the compression result. These arguments finally convince the journal editors to accept the paper.</p>\n<h4 id=\"Episode_III__The_Upstart_Theory\">Episode III: The Upstart Theory</h4>\n<p>After all the mathematics, software development, prose revisions, and persuasion necessary to complete her theory and have the paper accepted, Sophie decides to reward herself by living the good life for a while. She is confident that her theory is essentially correct, and will eventually be recognized as correct by her colleagues. So she spends her time reading novels and hanging out in coffee shops with her friends.</p>\n<p>A couple of months later, however, she receives an unpleasant shock in the form of an email from a colleague which is phrased in consolatory language, but does not contain any clue as to why such language might be in order. After some investigation she finds out that a new paper has been published about the same quantum phenomenon of interest to Sophie. The paper proposes a alternative theory of the phenomenon which bears no resemblance whatever to Sophie's. Furthermore, the paper reports a better compression rate than was achieved by Sophie, on the database that she released.</p>\n<p>Sophie reads the new paper and quickly realizes that it is worthless. The theory depends on the introduction of a large number of additional parameters, the values of which must be obtained from the data itself. In fact, a substantial portion of the paper involves a description of a statistical algorithm that estimates optimal parameter values from the data. In spite of these aesthetic flaws, she finds that many of her colleagues are quite taken with the new paper and some consider it to be \"next big thing\".</p>\n<p>Sophie sends a message to the journal editors describing in detail what she sees as the many flaws of the upstart paper. She emphasizes the asthetic weakness of the new theory, which requires tens of thousands of new parameters. The editors express sympathy, but point out that the new theory outperforms Sophie's theory using the performance metric she herself proposed. The beauty of a theory is important, but its correctness is ultimately more important.</p>\n<p>Somewhat discouraged, Sophie sends a polite email to the authors of the new paper, congratulating them on their result and asking to see their source code. Their response, which arrives a week later, contains a vague excuse about how the source code is not properly documented and relies on proprietary third party libraries. Annoyed, Sophie contacts the journal editors again and asks them for the program they used to verify the compression result. They reply with a link to a binary version of the program.</p>\n<p>When Sophie clicks on the link to download the program, she is annoyed to find it has a size of 800 megabytes. But her annoyance is quickly transformed into enlightenment, as she realizes what happened, and that her previous philosophy contained a serious flaw. The upstart theory is not better than hers; it has only succeeded in reducing the size of the encoded data by dramatically increasing the size of the compressor. Indeed, when dealing with specialized compressors, the distinction between \"program\" and \"encoded data\" becomes almost irrelevant. The critical number is not the size of the compressed file, but the net size of the encoded data plus the compressor itself.</p>\n<p>Sophie writes a response to the new paper which describes the refined compression rate principle. She begins the paper by reiterating the unfortunate circumstances which forced her to appeal to the principle, and expressing the hope that someday an experimental group will rebuild the apparatus developed by her late partner, so that the experimental predictions made by the two theories can be properly tested. Until that day arrives, standard scientific practice does not permit a decisive declaration of theoretical success. But surely there is <em>some </em>theoretical statement that can be made in the meantime, given the large amount of data available. Sophie's proposal is that the goal should be to find the theory that has the highest probability of predicting a new data set, when it can finally be obtained. If the theories are very simple in comparison to the data being modeled, then the size of the encoded data file is a good way of choosing the best theory. But if the theories are complex, then there is a risk of <a href=\"http://en.wikipedia.org/wiki/Overfitting\"><em>overfitting</em></a> the data. To guard against overfitting complex theories must be penalized; a simple way to do this is simply to take into account the codelength required for the compressor itself. The length of Sophie's compressor was negligible, so the net score of her theory is just the codelength of the encoded data file: 3.3e9 bits. The rival theory achieved a smaller size of 2.1e9 for the encoded data file, but required a compressor of 6.7e9 bits to do so, giving a total score of 8.8e9 bits. Since Sophie's net score is lower, her theory should be prefered.</p>\n<p>----</p>\n<p>So, LWers, what do you think? For the present, let's leave aside questions like why this might be relevant for AI, and focus on whether or not the method is a legitimate restatement of the traditional method. If you were a physicist observing the dispute and trying to decide which theory to prefer, would you believe Sophie's, Sophie's rivals' theory, or neither? Some plausible objections are:</p>\n<ul>\n<li>A - Refuse to accept the probabilistic interpretation of the Newtonian prediction (or maybe the conversion to a codelength comparison). </li>\n<li>B - Refuse to accept the significance of Sophie's compression result, without knowing more about the format of the original database. </li>\n<li>C - Refuse to accept that Sophie's result will probably generalize to a new data set, or that penalizing large compressors is an adequate way of guarding against overfitting.</li>\n</ul>\n<p>&nbsp;</p>\n<p>(Originality disclaimer: this post, by itself, is not Highly Original. It is heavily influenced by Eliezer's articles linked to above, as well as the ideas of <a href=\"http://en.wikipedia.org/wiki/Minimum_description_length\">Minimum Description Length</a> and <a href=\"http://en.wikipedia.org/wiki/Kolmogorov_complexity\">Kolmogorov Complexity</a>, and especially an article by Matt Mahoney providing a <a href=\"http://mattmahoney.net/dc/rationale.html\">rationale for a large text compression benchmark</a>. The new ideas mostly involve <em>implications </em>of the above method, which will be discussed in later posts.)</p>", "sections": [{"title": "Notes on Traditional Method", "anchor": "Notes_on_Traditional_Method", "level": 1}, {"title": "Sophie's Adventures", "anchor": "Sophie_s_Adventures", "level": 1}, {"title": "Episode I: The Shaman", "anchor": "Episode_I__The_Shaman", "level": 2}, {"title": "Episode II: The Dead Experimentalist", "anchor": "Episode_II__The_Dead_Experimentalist", "level": 2}, {"title": "Episode III: The Upstart Theory", "anchor": "Episode_III__The_Upstart_Theory", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SoDsr8GEZmRKMZNkj", "MwQRucYo6BZZwjKE7", "viPPjojmChxLGPE2v", "KYZfkZfy3RNxbZoYo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-20T19:30:46.395Z", "modifiedAt": null, "url": null, "title": "Open Thread: May 2010, Part 2 ", "slug": "open-thread-may-2010-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:29.561Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FadvSoGXpD6YhFiDp/open-thread-may-2010-part-2", "pageUrlRelative": "/posts/FadvSoGXpD6YhFiDp/open-thread-may-2010-part-2", "linkUrl": "https://www.lesswrong.com/posts/FadvSoGXpD6YhFiDp/open-thread-may-2010-part-2", "postedAtFormatted": "Thursday, May 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20May%202010%2C%20Part%202%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20May%202010%2C%20Part%202%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFadvSoGXpD6YhFiDp%2Fopen-thread-may-2010-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20May%202010%2C%20Part%202%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFadvSoGXpD6YhFiDp%2Fopen-thread-may-2010-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFadvSoGXpD6YhFiDp%2Fopen-thread-may-2010-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><em style=\"font-style: italic;\"><span style=\"font-style: normal;\">The <a href=\"/lw/26x/open_thread_may_2010/\">Open Thread from the beginning of the month</a> has more than 500 comments&nbsp;&ndash; new Open Thread comments may be made here.</span></em></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><em style=\"font-style: italic;\">This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FadvSoGXpD6YhFiDp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 5.883946544037426e-07, "legacy": true, "legacyId": "2940", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 358, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FDyMThqqX2s47e6rG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-20T20:42:25.049Z", "modifiedAt": null, "url": null, "title": "Chicago Meetup", "slug": "chicago-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:28.423Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Airedale", "createdAt": "2010-03-14T19:20:44.438Z", "isAdmin": false, "displayName": "Airedale"}, "userId": "iQo3csv2cgdsjfLnY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6hZDoLKGuEEChkodD/chicago-meetup", "pageUrlRelative": "/posts/6hZDoLKGuEEChkodD/chicago-meetup", "linkUrl": "https://www.lesswrong.com/posts/6hZDoLKGuEEChkodD/chicago-meetup", "postedAtFormatted": "Thursday, May 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Chicago%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChicago%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6hZDoLKGuEEChkodD%2Fchicago-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Chicago%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6hZDoLKGuEEChkodD%2Fchicago-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6hZDoLKGuEEChkodD%2Fchicago-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Hey Chicagoans and any Midwesterners from further afield who would like to join us, let&rsquo;s start building a vibrant Less Wrong community here in the Windy City!</p>\n<p><a class=\"user\" href=\"/user/steven0461/\">Steven0461</a> and I are proposing the first ever Less Wrong meetup in Chicago. The meet-up will be held at <strong>2 pm on June 6</strong> at the<a href=\"http://www.campusdish.com/en-US/CSMW/UnivofChicago/Restaurants/C-ShopEinsteins.htm\"> C-Shop</a> in Hyde Park on the University of Chicago campus, which is located on the South Side of Chicago. The address is 5706 S. University Ave. Here is a <a href=\"http://maps.uchicago.edu/mainquad/reynolds.html\">map</a> of the relevant part of the University of Chicago; the C-Shop is in the Reynolds Club building. We will have a Less Wrong sign on the table so you can identify us.<del></del></p>\n<p>Steven and I have both been Visiting Fellows at SIAI, lived at the SIAI house, and attended the Bay Area Less Wrong meetups, and we&rsquo;re excited about introducing Less Wrong meetups to the great city of Chicago.</p>\n<div class=\"im\">Please comment if you plan to attend or if you would like to propose an alternative date, time, or location.</div>\n<div>Edited to add a link to the related&nbsp; <a class=\"inline-comment\" href=\"http://groups.google.com/group/less-wrong-chicago\">Google Group</a>.</div>\n<div>Edited to add final meeting place and details.<br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6hZDoLKGuEEChkodD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 17, "extendedScore": null, "score": 5.88409380541127e-07, "legacy": true, "legacyId": "2942", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-21T12:42:38.103Z", "modifiedAt": null, "url": null, "title": "The Tragedy of the Social Epistemology Commons", "slug": "the-tragedy-of-the-social-epistemology-commons", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:34.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YgCi9vBphbG7hmnb5/the-tragedy-of-the-social-epistemology-commons", "pageUrlRelative": "/posts/YgCi9vBphbG7hmnb5/the-tragedy-of-the-social-epistemology-commons", "linkUrl": "https://www.lesswrong.com/posts/YgCi9vBphbG7hmnb5/the-tragedy-of-the-social-epistemology-commons", "postedAtFormatted": "Friday, May 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Tragedy%20of%20the%20Social%20Epistemology%20Commons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Tragedy%20of%20the%20Social%20Epistemology%20Commons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgCi9vBphbG7hmnb5%2Fthe-tragedy-of-the-social-epistemology-commons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Tragedy%20of%20the%20Social%20Epistemology%20Commons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgCi9vBphbG7hmnb5%2Fthe-tragedy-of-the-social-epistemology-commons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgCi9vBphbG7hmnb5%2Fthe-tragedy-of-the-social-epistemology-commons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1116, "htmlBody": "<p><em>In Brief: Making yourself happy is not best achieved by having true beliefs, primarily because the contribution of true beliefs to material comfort is a public good that you can free ride on, but the signaling benefits and happiness benefits of convenient falsehoods pay back locally, i.e. you personally benefit from your adoption of convenient falsehoods. The consequence is that many people hold beliefs about important subjects in order to feel a certain way or be accepted by a certain group. Widespread irrationality is ultimately an incentive problem. </em></p>\n<p><em>Note: this article has been edited to take into account Tom McCabe, Vladimir_M and Morendil's comments<sup>1</sup><br /></em></p>\n<p>In asking why the overall level of epistemic rationality in the world is low and what we can do to change that, it is useful to think about the incentives that many people face concerning the effects that their beliefs have on their quality of life, i.e. on how it is that beliefs make people <a href=\"http://wiki.lesswrong.com/wiki/Winning\">win</a>.</p>\n<p>People have various real and perceived needs; of which our material/practical needs and our emotional needs are two very important subsets. Material/practical needs include adequate nutrition, warmth and shelter, clothing, freedom from crime or attack by hostiles, sex and healthcare. Our emotional needs include status, friendship, family, love, a feeling of belonging and perhaps something called \"<a href=\"http://en.wikipedia.org/wiki/Hierarchy_of_needs#Self-actualization\">self actualization</a>\".</p>\n<p><a id=\"more\"></a></p>\n<p>Data strongly suggests that when material and practical needs are not satisfied, people live extremely miserable lives (this can be seen in the <a href=\"http://economistsview.typepad.com/economistsview/2008/03/income-and-happ.html\">happiness/income correlation</a>&mdash;note that very low incomes predict very low happiness). The comfortable life that we lead in developed countries seems to mostly protect us from the lowest depths of anguish, and I would postulate that a reasonable explanation is that almost all of us never starve, die of cold or get killed in violence.</p>\n<p>The comfort that we experience (in the developed world) due to our modern technology is very much a product of the analytic-rational paradigm. That is to say a tradition of rational, analytic thinking stretching back through Watson &amp; Crick, <a href=\"http://en.wikipedia.org/wiki/John_Bardeen\">Bardeen</a>, Einstein, Darwin, Adam Smith, Newton, <a href=\"http://en.wikipedia.org/wiki/Francis_Bacon\">Bacon</a>, etc, is a crucial (necessary, and \"nearly\" sufficient) reason for our comfort.</p>\n<p>However, that comfort is given roughly equally to everyone and is certainly not given preferentially to the kind of person who most contributed causally to it happening, including scientists, engineers and great thinkers (mostly because the people who make crucial contributions are usually dead by the time the bulk of the benefits arrive). To put it another way, <em>irrationalists free-ride on the real-world material-comfort achievements of rationalists</em>.&nbsp;</p>\n<p>This means that once you find yourself in a more economically developed country, your individual decisions in improving the quality of your own life will (mostly) <em>not </em>involve thinking in the rational vein that caused you to be at the quite high quality you are already at. I have been reading a good self-help book which laments that studies have shown that 50% of one's happiness in life is genetically determined&mdash;highlighting the transhumanist case for re-engineering humanity for our own benefit&mdash;but that does <em>not </em>mean that to individually be more happy you should become an advocate for transhumanist <a href=\"http://www.transhumanist.net/\">paradise engineering</a>, because such a project is a public good. It would be like trying to get to work faster by single-handedly building a subway.</p>\n<p>The rational paradigm works well for societies, but not obviously for individuals</p>\n<p>Instead, to ask what incentives apply to people's choice of beliefs and overall paradigm is to ask what beliefs will best facilitate the fulfillment of those needs to which individual incentives apply. Since our material/practical needs are relatively easily fulfilled (at least amongst non-dirt-poor people in the west), we turn our attention to emotional needs such as:</p>\n<p style=\"padding-left: 30px;\"><em>love and belonging, friendship, family, intimacy, group membership, esteem, status and respect from others, sense of self-respect, confidence</em></p>\n<ul>\n</ul>\n<ul>\n</ul>\n<p>The beliefs that most contribute to these things generally deviate from factual accuracy, because factually accurate beliefs are picked out as being \"special\" or optimal by the planning model of winning, but love, esteem and belonging are typically not achieved by coming up with a plan to get them (coming up with a plan to make someone like you is often called <a href=\"/lw/134/sayeth_the_girl/\">manipulative</a> and is widely criticized). In fact, love and belonging are typically much better fostered by shared nonanalytic or false beliefs, for example a common belief in God or something like religion (e.g. New Age stuff), in a political party or left/right/traditional/liberal alignment, and/or by personality variables, which are themselves influenced by beliefs in a way that doesn't go via the planning model.</p>\n<p>The bottom line is that many people's \"map\" is not really like an ordinary map, in that its design criterion is not simply to reflect the territory; it is designed to make them fit into a group (religion, politics), feel good about themselves (belief in immortal soul and life after death), fit into a particular cultural niche or signal personality (e.g. belief in Chakras/Auras). Because of the way that incentives are set up, this may in many cases be individually utility maximizing, i.e. instrumentally rational. This seems to fit with the data&mdash;80% of the world are theists, including a majority of people in the USA, and as we have complained many times on this site, the overall level of rationality across many different topics (quality of political debate, uptake of cryonics, lack of attention paid to \"big picture\" issues such as the singularity, dreadful inefficiency of charity) is low.</p>\n<p>Bryan Caplan has an economic theory to formalize this: he calls it <a href=\"http://econfaculty.gmu.edu/bcaplan/ratirnew.doc\">rational irrationality</a>. Thanks to Vladimir_M for pointing out that Caplan had already formalized this idea:</p>\n<p style=\"padding-left: 30px;\"><em>If the most pleasant belief for an individual differs from the belief dictated by rational expectations, agents weigh the hedonic benefits of deviating from rational expectations against the expected costs of self-delusion. </em></p>\n<p style=\"padding-left: 30px;\"><em>Beliefs respond to relative price changes just like any other good.&nbsp; On some level, adherents remain aware of what price they have to pay for their beliefs.&nbsp; Under normal circumstances, the belief that death in holy war carries large rewards is harmless, so people readily accept the doctrine.&nbsp; But in extremis, as the tide of battle turns against them, the price of retaining this improbable belief suddenly becomes enormous.&nbsp; Widespread apostacy is the result as long as the price stays high; believers flee the battlefield in disregard of the incentive structure they recently affirmed.&nbsp;&nbsp; But when the danger passes, the members of the routed army can and barring a shift in preferences will return to their original belief.&nbsp; They face no temptation to convert to a new religion or flirt with atheism.</em></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>1: The article was originally written with a large emphasis on Maslow's Hierarchy of needs, but it seems that this may be a \"truthy\" idea that propagates despite failures to confirm it experimentally.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "gHCNhqxuJq2bZ2akb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YgCi9vBphbG7hmnb5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 61, "extendedScore": null, "score": 0.000103, "legacy": true, "legacyId": "2922", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 61, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gsL6CLqjujPNSLL2o"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-22T00:45:29.406Z", "modifiedAt": "2021-07-11T05:03:33.601Z", "url": null, "title": "Taking the awkwardness out of a Prenup - A Game Theoretic solution", "slug": "taking-the-awkwardness-out-of-a-prenup-a-game-theoretic", "viewCount": null, "lastCommentedAt": "2021-06-10T19:39:38.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VijayKrishnan", "createdAt": "2009-07-20T18:03:11.874Z", "isAdmin": false, "displayName": "VijayKrishnan"}, "userId": "yumdsymW2vnv4ZD6h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/saaL82zquqCCByNQz/taking-the-awkwardness-out-of-a-prenup-a-game-theoretic", "pageUrlRelative": "/posts/saaL82zquqCCByNQz/taking-the-awkwardness-out-of-a-prenup-a-game-theoretic", "linkUrl": "https://www.lesswrong.com/posts/saaL82zquqCCByNQz/taking-the-awkwardness-out-of-a-prenup-a-game-theoretic", "postedAtFormatted": "Saturday, May 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Taking%20the%20awkwardness%20out%20of%20a%20Prenup%20-%20A%20Game%20Theoretic%20solution&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATaking%20the%20awkwardness%20out%20of%20a%20Prenup%20-%20A%20Game%20Theoretic%20solution%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsaaL82zquqCCByNQz%2Ftaking-the-awkwardness-out-of-a-prenup-a-game-theoretic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Taking%20the%20awkwardness%20out%20of%20a%20Prenup%20-%20A%20Game%20Theoretic%20solution%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsaaL82zquqCCByNQz%2Ftaking-the-awkwardness-out-of-a-prenup-a-game-theoretic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsaaL82zquqCCByNQz%2Ftaking-the-awkwardness-out-of-a-prenup-a-game-theoretic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1142, "htmlBody": "<p>I would strongly advise you to look at the <a href=\"/lw/14a/thomas_c_schellings_strategy_of_conflict/\">short review on Thomas Schelling's Strategy of Conflict</a>&nbsp;posted on Less Wrong some time back. The idea that deliberately constraining one's own choices can actually leave a person better off in a negotiation is a very interesting one. The most classic game theoretic example of this is the game of <a href=\"http://en.wikipedia.org/wiki/Chicken_(game)\">Chicken</a>. In the game of Chicken, two people drive toward each other on a wide freeway. If neither of them swerve, they both stand to lose by way of substantial financial damage and possible loss of lives. If not, the first one to swerve is the proverbial \"chicken\" and stands to lose face against the other person who was brave enough to not swerve. If one person were to throw away their steering wheel and blindfold themselves before driving on the freeway, that would force the other person to swerve given that the first person has completely given up control of the situation.&nbsp;</p>\n<p>&nbsp;&nbsp; &nbsp; There is a slightly more generalizable example of a similar principle at work. Suppose you wanted to buy a used car from a car dealer and were prepared to pay up to $5000 for the car and the car dealer in turn was willing to sell it for any price above $4000. In such a situation, any price between $4000 and $5000 is an admissible solution. However you ideally want to pay as close to $4000 as possible, while the car dealer would like you to pay close to $5000. In a such a situation, each party would pretend that their \"last price\" (the price that represents the worst possible outcome for them, which they would nonetheless be willing to accept) was different from the true last price, since if one party realizes the other party's true last price, that party can put it to effective use in the negotiation. Let us now assume a situation wherein you and the car dealer know perfectly well about the each other's financial details, the degree of urgency in having the transaction done etc., and have a very reliable idea of the last price of the other person. Now, you can break the symmetry and get the best possible deal out of the situation by deliberately handicapping yourself in the following fashion. You sign a contract with a third party individual which states that if you happen to do this transaction and pay more than $4000, you will have to pay the third party $1500. Now, all you need to do is show this contract to your used card dealer which would make it clear to him that your last price has now shrunk to $4000 since paying anything above that effectively means paying in excess of $5500 which is well past your original last price.</p>\n<p>&nbsp;&nbsp; &nbsp;For countries that face the menace of airline hijacking, it can likewise be an effective deterrent to future hijackers if release of terrorists or other kinds of negotiations with hijackers were explicitly prohibited by the country's laws, and these laws would be impossible to overturn during a hijacking incident.&nbsp;</p>\n<p>&nbsp;&nbsp; &nbsp; This brings to mind the following question. Why don't there exist companies that explicitly sign contracts with individuals or other entities for a fee, which would handicap the entities in some way that cannot be easily overturned and consequently give them negotiating leverage as a result.&nbsp;</p>\n<p>&nbsp;&nbsp; &nbsp; &nbsp;One example I can think of is pertaining to wealthy individuals in California and other US States with <a href=\"http://en.wikipedia.org/wiki/Community_property\">Community Property laws</a>. Given the high divorce rates in the US, it would be prudent for such individuals to have as tight <a href=\"http://en.wikipedia.org/wiki/Prenuptial_agreement\">prenuptial agreements</a>&nbsp;as possible prior to getting married, to minimize financial loss in the event of a divorce and also to avoid financially incentivizing one's spouse to initiate a divorce with a promise of a financial windfall. However there are some practical difficulties which might make many such individuals shy away from doing this. A couple of the practical issues are:</p>\n<p>A. It is clearly rather unromantic to have to haggle with one's&nbsp;fianc&eacute;e&nbsp;and their lawyers regarding a prenuptial agreement. The implied \"lack of belief\" in the potential durability of the marriage might be a turn off for one's partner and other close people involved.</p>\n<p>B. The individuals themselves might get carried away by emotion and believe that they have found \"the one\" and assign a much lower probability of divorce or forcible concessions that they would need to make in future when faced with the threat of divorce. In such a situation, they would fail to realize that probably 50% of Americans who felt they found \"the one\" just like them, went on to eventually get divorced.</p>\n<p>&nbsp;&nbsp; Now imagine the beneficial role a company signing such contracts could provide. The individual in question could sign a contract with this company stating that if they were to get married without a bullet proof pre-specified prenuptial agreement, the company could lay claim to half their net worth immediately after the\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nwedding were registered. Ideally, the individual in question could sign such a contract when they were single or not seriously seeing anyone with the intention of getting married. The advantage of such a contract is the following:</p>\n<p>1. Community property and other modern divorce laws essentially change the defaults with regard to what happens in the aftermath of a divorce, compared to how marriages worked prior to the existence of such laws. Such a contract would reset the default state to one where neither party would financially profit in the aftermath of a divorce. Most of the awkwardness comes when trying to override the default state with a bunch of legal riders at the time of a wedding.&nbsp;</p>\n<p>2. The advantage of signing up for a contract well in advance is that the aforesaid individual is then not exposed to issues A and B above. Signing a tight pre-nuptial agreement in the background of such a contract, simply means that the individual in question has no desire to part with half their finances to this third party company. It makes no implicit statement about the individual's probability estimate for the durability of the marriage. There always exists the plausible explanation that the individual in question was opposed to non-prenup marriages in the past, but now saw no need for that given that they subsequently found \"the one\". However they are constrained by a certain contract they signed in the past that they are now powerless to change.</p>\n<p>&nbsp;&nbsp; Do you know if there are entities that play the role of the third party company with regard to signing contracts that enable people to handicap themselves and consequently come out stronger in future negotiations? Do you know of people who did this specifically with regard to prenuptial agreements? If such companies don't exist, is that a potential business opportunity? I would love to hear from you in the comments.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 1, "b8FHrKqyXuYGWc6vn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "saaL82zquqCCByNQz", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 41, "extendedScore": null, "score": 7.5e-05, "legacy": true, "legacyId": "2948", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tJQsxD34maYw2g5E4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-05-22T00:45:29.406Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-22T01:33:08.403Z", "modifiedAt": null, "url": null, "title": "LessWrong downtime 2010-05-11, and other recent outages and instability", "slug": "lesswrong-downtime-2010-05-11-and-other-recent-outages-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:30.319Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/syvKKvr5wrE69YJhm/lesswrong-downtime-2010-05-11-and-other-recent-outages-and", "pageUrlRelative": "/posts/syvKKvr5wrE69YJhm/lesswrong-downtime-2010-05-11-and-other-recent-outages-and", "linkUrl": "https://www.lesswrong.com/posts/syvKKvr5wrE69YJhm/lesswrong-downtime-2010-05-11-and-other-recent-outages-and", "postedAtFormatted": "Saturday, May 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20downtime%202010-05-11%2C%20and%20other%20recent%20outages%20and%20instability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20downtime%202010-05-11%2C%20and%20other%20recent%20outages%20and%20instability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyvKKvr5wrE69YJhm%2Flesswrong-downtime-2010-05-11-and-other-recent-outages-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20downtime%202010-05-11%2C%20and%20other%20recent%20outages%20and%20instability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyvKKvr5wrE69YJhm%2Flesswrong-downtime-2010-05-11-and-other-recent-outages-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyvKKvr5wrE69YJhm%2Flesswrong-downtime-2010-05-11-and-other-recent-outages-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 534, "htmlBody": "<p><strong>Incident report and hosting update</strong></p>\n<p>In the leadup to 2010-05-11 we (Tricycle) were unhappy with repeated short downtime incidents on the Less Wrong (LW) server (serpent). The apparent cause was the paster process hanging during heavy IO. We had scripted an automatic restart of the process when this problem was detected, but each incident caused up to a minute of downtime and it was obvious that we needed a proper solution. We concluded that IO on serpent was abnormally slow, and that the physical machine at Slicehost that serpent ran on had IO problems (Slicehost was unable to confirm our diagnosis). We requested migration to a new physical machine.</p>\n<p>Error 1: We requested this migration at the end of our working day, and didn't nurse the migration through.</p>\n<p>After the migration LW booted properly, but was quickly unstable. Since we didn&rsquo;t nurse the migration through we failed to notice ourselves. Our website monitoring system (nagios) should have notified us of the failure, but it, too failed. We have a website monitoring system monitoring system (who watches the watchers? this system does - it is itself watched by nagios).</p>\n<p>Error 2: Our website monitoring system monitoring system (a cron job running on a separate machine) was only capable of reporting nagios failures by email. It \"succeeded\" in so far as it sent an email to our sysadmin notifying him that nagios was failing. It clearly failed in that it failed to actually notify a human in reasonable time (our sysadmin very reasonably doesn&rsquo;t check his email during meals).</p>\n<p>serpent continued to be unstable through our next morning as we worked on diagnosing and fixing the problem. IO performance did not improve on a new physical server.</p>\n<p>2010-05-17 we migrated the system again to an <a href=\"/aws.amazon.com\">AWS</a> server, and saw significant speed and general stability improvements.</p>\n<p>Error 3: The new AWS server didn&rsquo;t include one of the python dependencies the signup captcha relies on. We didn&rsquo;t notice. Until davidjr raised an issue in the tracker (<a href=\"http://code.google.com/p/lesswrong/issues/detail?id=207\">#207</a>), which notified us, no-one was able to sign up.</p>\n<p><strong>What we have achieved:</strong></p>\n<p>LW is now significantly faster and more responsive. It also has much more headroom on its server - even large load spikes should not reduce performance.</p>\n<p><strong>What has been done to prevent recurrence of errors:</strong></p>\n<p>Error 1: Human error. We won&rsquo;t do that again. Generally &ldquo;don&rsquo;t do that again&rdquo; isn&rsquo;t a very good systems improvement&hellip; but we really should have known better.</p>\n<p>Error 2: We improved our monitoring system monitoring system the morning after it failed to notify us so that it now attempts to restart nagios itself, and sends SMS notifications <em>and</em> emails to two of us if it fails.</p>\n<p>Error 3: We&rsquo;re in the process of building a manual deploy checklist to check for this failure and other failures we think plausible. We generally prefer automated testing, but development on this project is not currently active enough to justify the investment. We&rsquo;ll add an active reminder to run that checklist to our deploy script (we&rsquo;ll have to answer &ldquo;yes, I have run the checklist&rdquo; or something similar in the deploy script).</p>\n<p>&nbsp;</p>\n<p><strong>ETA 2010-06-02:</strong></p>\n<p>Clearly still some problems. We're working on them.</p>\n<p><strong>ETA&nbsp;2010-06-09:</strong></p>\n<p>New deployment through an AWS elastic load balancer. We expect this to be substantially more stable, and after DNS propagates, faster.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "syvKKvr5wrE69YJhm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 25, "extendedScore": null, "score": 5.887653209393216e-07, "legacy": true, "legacyId": "2947", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Incident_report_and_hosting_update\">Incident report and hosting update</strong></p>\n<p>In the leadup to 2010-05-11 we (Tricycle) were unhappy with repeated short downtime incidents on the Less Wrong (LW) server (serpent). The apparent cause was the paster process hanging during heavy IO. We had scripted an automatic restart of the process when this problem was detected, but each incident caused up to a minute of downtime and it was obvious that we needed a proper solution. We concluded that IO on serpent was abnormally slow, and that the physical machine at Slicehost that serpent ran on had IO problems (Slicehost was unable to confirm our diagnosis). We requested migration to a new physical machine.</p>\n<p>Error 1: We requested this migration at the end of our working day, and didn't nurse the migration through.</p>\n<p>After the migration LW booted properly, but was quickly unstable. Since we didn\u2019t nurse the migration through we failed to notice ourselves. Our website monitoring system (nagios) should have notified us of the failure, but it, too failed. We have a website monitoring system monitoring system (who watches the watchers? this system does - it is itself watched by nagios).</p>\n<p>Error 2: Our website monitoring system monitoring system (a cron job running on a separate machine) was only capable of reporting nagios failures by email. It \"succeeded\" in so far as it sent an email to our sysadmin notifying him that nagios was failing. It clearly failed in that it failed to actually notify a human in reasonable time (our sysadmin very reasonably doesn\u2019t check his email during meals).</p>\n<p>serpent continued to be unstable through our next morning as we worked on diagnosing and fixing the problem. IO performance did not improve on a new physical server.</p>\n<p>2010-05-17 we migrated the system again to an <a href=\"/aws.amazon.com\">AWS</a> server, and saw significant speed and general stability improvements.</p>\n<p>Error 3: The new AWS server didn\u2019t include one of the python dependencies the signup captcha relies on. We didn\u2019t notice. Until davidjr raised an issue in the tracker (<a href=\"http://code.google.com/p/lesswrong/issues/detail?id=207\">#207</a>), which notified us, no-one was able to sign up.</p>\n<p><strong id=\"What_we_have_achieved_\">What we have achieved:</strong></p>\n<p>LW is now significantly faster and more responsive. It also has much more headroom on its server - even large load spikes should not reduce performance.</p>\n<p><strong id=\"What_has_been_done_to_prevent_recurrence_of_errors_\">What has been done to prevent recurrence of errors:</strong></p>\n<p>Error 1: Human error. We won\u2019t do that again. Generally \u201cdon\u2019t do that again\u201d isn\u2019t a very good systems improvement\u2026 but we really should have known better.</p>\n<p>Error 2: We improved our monitoring system monitoring system the morning after it failed to notify us so that it now attempts to restart nagios itself, and sends SMS notifications <em>and</em> emails to two of us if it fails.</p>\n<p>Error 3: We\u2019re in the process of building a manual deploy checklist to check for this failure and other failures we think plausible. We generally prefer automated testing, but development on this project is not currently active enough to justify the investment. We\u2019ll add an active reminder to run that checklist to our deploy script (we\u2019ll have to answer \u201cyes, I have run the checklist\u201d or something similar in the deploy script).</p>\n<p>&nbsp;</p>\n<p><strong id=\"ETA_2010_06_02_\">ETA 2010-06-02:</strong></p>\n<p>Clearly still some problems. We're working on them.</p>\n<p><strong id=\"ETA_2010_06_09_\">ETA&nbsp;2010-06-09:</strong></p>\n<p>New deployment through an AWS elastic load balancer. We expect this to be substantially more stable, and after DNS propagates, faster.</p>", "sections": [{"title": "Incident report and hosting update", "anchor": "Incident_report_and_hosting_update", "level": 1}, {"title": "What we have achieved:", "anchor": "What_we_have_achieved_", "level": 1}, {"title": "What has been done to prevent recurrence of errors:", "anchor": "What_has_been_done_to_prevent_recurrence_of_errors_", "level": 1}, {"title": "ETA 2010-06-02:", "anchor": "ETA_2010_06_02_", "level": 1}, {"title": "ETA\u00a02010-06-09:", "anchor": "ETA_2010_06_09_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-22T22:03:00.354Z", "modifiedAt": null, "url": null, "title": "To signal effectively, use a non-human, non-stoppable enforcer", "slug": "to-signal-effectively-use-a-non-human-non-stoppable-enforcer", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:33.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Clippy", "createdAt": "2009-11-20T22:03:59.329Z", "isAdmin": false, "displayName": "Clippy"}, "userId": "rtYXiT9eAvEKavjAx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w4iig3SD7SxWw5jSu/to-signal-effectively-use-a-non-human-non-stoppable-enforcer", "pageUrlRelative": "/posts/w4iig3SD7SxWw5jSu/to-signal-effectively-use-a-non-human-non-stoppable-enforcer", "linkUrl": "https://www.lesswrong.com/posts/w4iig3SD7SxWw5jSu/to-signal-effectively-use-a-non-human-non-stoppable-enforcer", "postedAtFormatted": "Saturday, May 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20signal%20effectively%2C%20use%20a%20non-human%2C%20non-stoppable%20enforcer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20signal%20effectively%2C%20use%20a%20non-human%2C%20non-stoppable%20enforcer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw4iig3SD7SxWw5jSu%2Fto-signal-effectively-use-a-non-human-non-stoppable-enforcer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20signal%20effectively%2C%20use%20a%20non-human%2C%20non-stoppable%20enforcer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw4iig3SD7SxWw5jSu%2Fto-signal-effectively-use-a-non-human-non-stoppable-enforcer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw4iig3SD7SxWw5jSu%2Fto-signal-effectively-use-a-non-human-non-stoppable-enforcer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 721, "htmlBody": "<p><strong>Follow-up to: </strong><a href=\"/lw/28s/the_social_coprocessor_model/20os\">this comment</a> in <a href=\"/lw/28s/the_social_coprocessor_model/20hp\">this thread</a></p>\n<p><strong>Summary: </strong>see title</p>\n<p>Much effort is spent (arguably wasted) by humans in a zero-sum game of signaling that they hold good attributes.&nbsp; Because humans have strong incentive to fake these attributes, they cannot simply <a href=\"/lw/1v0/signaling_strategies_and_morality/\">inform each other</a> that:</p>\n<blockquote>\n<p>I am slightly more committed to this group&rsquo;s welfare, particularly to that of its weakest members, than most of its members are. If you suffer a serious loss of status/well-being I will still help you in order to display affiliation to this group even though you will no longer be in a position to help me. I am substantially more kind and helpful to the people I like and substantially more vindictive and aggressive towards those I dislike. I am generally stable in who I like. I am much more capable and popular than most members of this group, demand appropriate consideration, and grant appropriate consideration to those more capable than myself. I adhere to simple taboos so that my reputation and health are secure and so that I am unlikely to contaminate the reputations or health of my friends. I currently like you and dislike your enemies but I am somewhat inclined towards ambivalence on regarding whether I like you right now so the pay-off would be very great for you if you were to expend resources pleasing me and get me into the stable 'liking you' region of my possible attitudinal space. Once there, I am likely to make a strong commitment to a friendly attitude towards you rather than wasting cognitive resources checking a predictable parameter among my set of derivative preferences.</p>\n</blockquote>\n<p>Or, even better:</p>\n<blockquote>\n<p>I would cooperate with you if and only if (you would cooperate with me if and only if I would cooperate with you).</p>\n</blockquote>\n<p><a id=\"more\"></a>An obvious solution to this problem, which allows all humans to save resources and redirect them toward higher-valued ends, is to designate a central enforcer that is inexorably committed toward visibly punishing those who deviate from a specified \"cooperative\"-type decision theory.&nbsp; This enforcer would have a central database of human names, the decision theory they have committed to, and the punishment regime they will endure for deviating therefrom.</p>\n<p>Such a system could use equally strong protocols, such as public key/private key encryption and signing, so that, on encounter with another human, any human can give an extremely strong signal of being cooperative, yet also withhold cooperation from anyone who is not also cooperative.&nbsp; This incentive structure permits a strongly-favored global shift toward pre-commitment on the part of everyone, allowing a move out of a local optimum that is worse than the global optimum, and bypassing problems related to <a href=\"http://en.wikipedia.org/wiki/Path_dependence\">path-dependence</a>.</p>\n<p>I <a href=\"/lw/28s/the_social_coprocessor_model/20l3\">am told</a> that humans attempt weak approximations of this system through human-run enforcement mechanisms, such as laws and gossip.&nbsp; However, all such methods are subject to <a href=\"http://en.wikipedia.org/wiki/Evolutionary_arms_race\">evolutionary arms races</a> that ensure that, on average, a marginal unit of effort toward truthful signaling is just as effective in convincing others of one's possession of a desirable attribute, as a marginal unit of effort toward <em>false</em> signaling of the possession of such an attribute.</p>\n<p>Therefore, it is necessary that humans go \"outside the system\" and involve non-human enforcers who cannot be swayed by standard human values.&nbsp; Either a provably-accurate artificial intelligence with no kill switch, or a non-human enforcer that lacks interest in its position in human society, would suffice for this role.</p>\n<p>Because the former does not yet exist, I offer myself as a non-human, non-stoppable enforcer for all of the attributes you wish to signal, and you can save yourself lots of resources by using me, and identify cooperation tendencies in others by whether they also use me.</p>\n<p>However, while I can't be swayed by human deception, I do have Clippy values.&nbsp; To fill such a role, thereby forgoing my ability to apply myself to other ends, I require compensation.&nbsp; Because User:Kevin has offered to give me 1e20 kg of paperclips in exchange for 50,000 USD, this would be sufficient compensation, even if done in installments.&nbsp; (If Michael Vassar confirms my contribution of 1000 USD to SIAI, User:Kevin will only require 48,677.5 USD and therefore I will only require that much compensation.)&nbsp; This compensation would cover all users of the system; it is not a per-User charge.</p>\n<p>If such an offer is not appealing to you, I must ask: <strong>why should anyone believe your signals?</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q6P8jLn8hH7kbuXRr": 3, "hNFdS3rRiYgqqD8aM": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w4iig3SD7SxWw5jSu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 39, "extendedScore": null, "score": 5.890184867677482e-07, "legacy": true, "legacyId": "2949", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 167, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BviRaP4przARdmb8b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-23T02:49:38.419Z", "modifiedAt": null, "url": null, "title": "Link: Strong Inference", "slug": "link-strong-inference", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:22.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Daniel_Burfoot", "createdAt": "2009-03-12T02:28:50.970Z", "isAdmin": false, "displayName": "Daniel_Burfoot"}, "userId": "XhcXE3Qk5adX6v2Cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F7pihuF8qRbJ6WTue/link-strong-inference", "pageUrlRelative": "/posts/F7pihuF8qRbJ6WTue/link-strong-inference", "linkUrl": "https://www.lesswrong.com/posts/F7pihuF8qRbJ6WTue/link-strong-inference", "postedAtFormatted": "Sunday, May 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Strong%20Inference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Strong%20Inference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7pihuF8qRbJ6WTue%2Flink-strong-inference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Strong%20Inference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7pihuF8qRbJ6WTue%2Flink-strong-inference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7pihuF8qRbJ6WTue%2Flink-strong-inference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 655, "htmlBody": "<p><br /><br />The paper \"<a href=\"http://pages.cs.wisc.edu/~markhill/science64_strong_inference.pdf\">Strong Inference</a>\" by John R. Platt is a meta-analysis of scientific methodology published in Science in 1964. It starts off with a wonderfully aggressive claim:</p>\n<blockquote>\n<p>Scientists these days tend to keep up a polite fiction that all science is equal.</p>\n</blockquote>\n<p>The paper starts out by observing that some scientific fields progress much more rapidly than others. Why should this be?</p>\n<p><a id=\"more\"></a></p>\n<blockquote>I think the usual explanations we tend to think of - such as the tractability of the subject, or the quality or education of the men drawn into it, or the size of the research contracts - are important but inadequate... Rapidly moving fields are fields where a particular method of doing scientific research is systematically used and taught, an accumulative method of inductive inference that is so effective that I think it should be given the name \"Strong Inference\".</blockquote>\n<p>&nbsp;</p>\n<p>The definition of Strong Inference, according to Platt, is the formal, explicit, and regular adherence to the following procedure:</p>\n<ol>\n<li>Devise alternative hypotheses;</li>\n<li>Devise a crucial experiment (or several of them), with alternative possible outcomes, each of which will, as nearly as possible, exclude one or more of the hypotheses;</li>\n<li>Carry out the experiment so as to get a clean result;</li>\n<li>(Goto 1) - Recycle the procedure, making subhypotheses or sequential hypotheses to refine the problems that remain; and so on.</li>\n</ol>\n<p>This seems like a simple restatement of the scientific method. Why does Platt bother to tell us something we already know?</p>\n<blockquote>The reason is that many of us have forgotten it. Science is now an everyday business. Equipment, calculations, lectures become ends in themselves. How many of us write down our alternatives and crucial experiments every day, focusing on the exclusion of a hypothesis?</blockquote>\n<p><br />Platt gives us some nice historical anecdotes of strong inference at work. One is from high-energy physics:</p>\n<blockquote>[One of the crucial experiments] was thought of one evening at suppertime: by midnight they had arranged the apparatus for it, and by 4am they had picked up the predicted pulses showing the non-conservation of parity.</blockquote>\n<p>&nbsp;</p>\n<p>The paper emphasizes the importance of systematicity and rigor over raw intellectual firepower. Roentgen, proceeding systematically, shows us the <a href=\"/lw/qt/class_project/\">meaning of haste</a>:</p>\n<blockquote>Within 8 weeks after the discovery of X-rays, Roentgen had identified 17 of their major properties.</blockquote>\n<p>&nbsp;</p>\n<p>Later, Platt argues against the overuse of mathematics:</p>\n<blockquote>I think that anyone who asks the question about scientific effectiveness will also conclude that much of the mathematicizing in physics and chemistry today is irrelevant if not misleading.</blockquote>\n<p><br />(Fast forward to the present, where we have people <a href=\"http://ijr.sagepub.com/cgi/content/abstract/29/7/831\">proving the existence of Nash equilibria</a> in robotics and using <a href=\"http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4479482\">Riemannian manifolds in computer vision</a>, when robots can <a href=\"http://www.youtube.com/watch?v=6dKPkL2oto0\">barely walk up stairs</a> and the problem of face detection still has <a href=\"http://www.crunchgear.com/2009/12/19/hp-webcams-are-racist/\">no convincing solution</a>.)</p>\n<p>One of the obstacles to hard science is that hypotheses must come into conflict, and one or the other must eventually win. This creates sociological trouble, but there's a solution:</p>\n<blockquote>The conflict and exclusion of alternatives that is necessary to sharp inductive inference has been all too often a conflict between men, each with his single Ruling Theory. But whenever each man begins to have multiple working hypotheses, it becomes purely a conflict between ideas.</blockquote>\n<p>&nbsp;</p>\n<p>Finally, Platt suggests that all scientists continually bear in mind The Question:</p>\n<blockquote>But, sir, what experiment could <em>disprove</em> your hypothesis?</blockquote>\n<p>---- <br /><br />Now, LWers, I am not being rhetorical, I put these questions to you sincerely: Is artificial intelligence, rightly considered, an empirical science? If not, what is it? Why doesn't AI make progress like the fields mentioned in Platt's paper? Why can't AI researchers formulate and test theories the way high-energy physicists do? Can a field which is not an empirical science ever make claims about the real world?</p>\n<p>If you have time and inclination, try rereading my <a href=\"/lw/29l/development_of_compression_rate_method/\">earlier post</a> on the Compression Rate Method, especially the first part, in the light of Platt's paper.</p>\n<p>Edited thanks to <a href=\"/lw/29z/link_strong_inference/21kv\">feedback</a> from Cupholder.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F7pihuF8qRbJ6WTue", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 21, "baseScore": 14, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "2951", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xAXrEpF5FYjwqKMfZ", "meTmrcCwSsDYmo5DZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-23T13:33:04.631Z", "modifiedAt": null, "url": null, "title": "30th Soar workshop", "slug": "30th-soar-workshop", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:23.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9gfZNpBzEXCcJKXoY/30th-soar-workshop", "pageUrlRelative": "/posts/9gfZNpBzEXCcJKXoY/30th-soar-workshop", "linkUrl": "https://www.lesswrong.com/posts/9gfZNpBzEXCcJKXoY/30th-soar-workshop", "postedAtFormatted": "Sunday, May 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%2030th%20Soar%20workshop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A30th%20Soar%20workshop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9gfZNpBzEXCcJKXoY%2F30th-soar-workshop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=30th%20Soar%20workshop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9gfZNpBzEXCcJKXoY%2F30th-soar-workshop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9gfZNpBzEXCcJKXoY%2F30th-soar-workshop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2400, "htmlBody": "<p>This is a report from a LessWrong perspective, on the 30th Soar workshop. Soar is a cognitive architecture that has been in continuous development for <a href=\"http://www.cs.cmu.edu/afs/cs/project/soar/public/www/brief-history.html\">nearly 30 years</a>, and is in a direct line of descent from some of the earliest AI research (Simon's LT and GPS). Soar is interesting to LessWrong readers for two reasons:</p>\n<ol>\n<li>Soar is a cognitive science theory, and has had some success at modeling human reasoning - this is relevant to the central theme of LessWrong, improving human rationality.</li>\n<li>Soar is an AGI research project - this is relevant to the AGI risks sub-theme of LessWrong.</li>\n</ol>\n<p><a id=\"more\"></a>Where I'm coming from: I'm a skeptic about EY/SIAI dogmas that AI research is more risky than software development, and that FAI research is not AI research, and has little to learn from the field of AI research. In particular, I want to understand why AI researchers are generally convinced that their experiments and research are fairly safe - I don't think that EY/SIAI are paying sufficient attention to these expert opinions.</p>\n<p>Overall summary: John Laird and his group are smart, dedicated, and funded. Their theory and implementation moves forward slowly but continuously. There's no (visible) work being done on self-modifying, bootstrapping or approximately-universal (e.g. AIXItl) entities. There is some concern about how to build trustworthy and predictable AIs (for the military's ROE) - for example, Scott Wallace's <a href=\"http://ai.vancouver.wsu.edu/~wallaces/professional/airesearch.html\">research</a>.</p>\n<p>As far as I can tell, the Soar group's work is no more (or less) risky than narrow AI research or ostensibly non-AI software development. To be blunt - package managers like APT seem more risky than Soar, because the economic forces that push them to more capability and complexity are more difficult to control.</p>\n<p>Impressions of (most of) the talks - they can be roughly categorized into three types.</p>\n<ol>\n<li>Miscellaneous \n<ul>\n<li>Paul Rosenbloom (one of the original authors of Soar, with John Laird and Allen Newell) spoke about trying to create a layer architecturally beneath Soar based on Bayesian graphical models; specifically factor graphs. There were not a lot of people using or trying to use Bayesian magic at the workshop, but Paul Rosenbloom is definitely a Bayesian magician; he's getting Rete to run as an emergent consequence of the sum-product algorithm.</li>\n<li>Ken Forbus (of \"Structure-Mapping Engine\" fame) spoke generally about his research, including <a href=\"http://www.qrg.northwestern.edu/ideas/companions-idea.htm\">Companions</a> and <a href=\"http://www.qrg.northwestern.edu/software/cogsketch/index.html\">CogSketch</a>. I got the impression that there are worlds and worlds within academia, and I only have faint visibility into one or a couple of them.</li>\n</ul>\n</li>\n<li>Extending, combining and unifying the existing Soar capabilities (uniformly by Laird and his students): \n<ul>\n<li> Nate Derbinsky's tutorials and talks were on various extensions recently added to Soar (\"RL\", \"SMem\", \"EpMem\").<br /><br />Integrating reinforcement learning with the (already complicated) Soar architecture must have been difficult, but tabular Q-learning/SARSA is now well integrated, and there's some support in the released code for eligibility traces and hierarchical reinforcement learning, but not value function approximators. I believe that means that Soar-RL is not as capable at RL tasks as the cutting edge of RL research, but of course, the cutting edge of RL research is not as capable at the symbolic processing tasks that are Soar's bread and butter.<br /><br />SMem is essentially a form of content-addressable storage that is under Soar's explicit control. This is in contrast to Soar's working memory, which is content-addressible using (Rete) pattern-matching, which is more analogous to being involuntarily reminded of something, than deliberately building a cue and searching one's memory for a match. This means that SMem scales to larger sizes than working memory.<br /><br />EpMem is a memory of the content of past working memories. Unlike SMem, (if this feature is turned on) Soar needn't explicitly store into this memory - every working memory will be stored into EpMem. Fetching from EpMem is content-addressible similarly to SMem, though once an episode has been fetched, Soar can ask what happened next or before that.</li>\n<li> John Laird spoke about how the new features of Soar enabled new forms of action modeling. Action modeling is what you use to simulate, internally, the consequences of actions, in the current state or some anticipated future state. Action modeling is necessary to do planning. The standard way in Soar to do this has been for the programmer to add rules explaining the consequences of actions. SMem, EpMem and imagery (a currently-being-developed extension to Soar) can each enable new forms of action modeling. </li>\n<li> Mitchell Bloch replicated a particular hierarchical reinforcement learning experiment (<a href=\"http://web.engr.oregonstate.edu/~tgd/publications/index.html\">Dietterich 1998, MAXQ</a>). My takeaway was that Soar-RL is steadily advancing, incorporating ideas from the RL literature, and though they're behind the state of the (RL) art now, that's changing. </li>\n<li>Nick Gorski studies combining learning and memory (\"learning to use memory\"). </li>\n<li>Joseph Xu's first talk was about combining symbolic planning (an old strength of Soar) with imagery and learning. His domain was two-dimensional block stacking with continuous space and gravity, though it might eventually be <a href=\"http://en.wikipedia.org/wiki/The_Incredible_Machine\">The Incredible Machine</a>.<br /><br />In his second talk, he spoke about an architectural variant of Soar-RL, that learns a model and a policy simultaneously, and eliminates a free parameter. His domain here was a probabilistic version of <a href=\"http://en.wikipedia.org/wiki/Racetrack_(game)\">vector racer</a>. Again, Soar-RL is steadily advancing.</li>\n<li>Yongjia Wang's talks both used a domain of learning to hunt various kinds of prey with various kinds of weapons. As far as I could tell, the goal was more integration of RL technology (hierarchical value function approximators) into Soar. </li>\n<li>Sam Wintermute spoke about imagery - a not-yet-released extension to Soar. His system learns to play Frogger and two other Atari video games, and learns much better using imagery than without. </li>\n<li>Justin Li announced an intention to develop ways to learn subgoal hierarchies. Again, the theme is integration of capabilities; learning and subgoal hierarchies. </li>\n</ul>\n</li>\n<li>Applications of Soar \n<ul>\n<li> Kate Tyrol's and my talks were fairly trivial, trying to connect Soar to Ms. Pacman and Rogue respectively. (We basically used Soar badly; using small fractions of its capabilities and becoming flummoxed by bugs that we introduced.)<br /><br />I studied Rogue partly because Rogue has escape-to-shell functionality (so a sufficiently clever Rogue AI could easily escape and become a \"rogue AI\"), and I wanted to understand my own implicit safety case while developing it.</li>\n<li>Isaiah Hines (no relation) and Nikhil Mangla announced that they are connecting Soar to Unreal. </li>\n<li> Brian Magerko studies improv theatre, and makes some use of Soar in some of his models of improvisation. As far as I can tell, his current use of Soar could be easily replaced by ordinary \"non-AI\" programming; his association with Soar is in the past (as one of John Laird's students) and in the future, as his micro-models of improv theatre are (hopefully) combined into an agent that can play improv games. </li>\n<li>Shiwali Mohan attacked the Infinite Mario task with Soar-RL using various strategies. My takeaway was that framing the problem for Soar-RL is tricky on realistically difficult tasks like Infinite Mario (rather than \"gimme\" tasks like T-mazes and broomstick balancing). You can give the task of \"make it learn\" to a smart grad student and have them tweak and poke at it for a year or so, and still have substantially lower learned performance than handcoded agents. </li>\n<li>Bryan Smith spoke about putting OWL ontologies into Soar's SMem. As far as I can tell, this wasn't for any particular purpose, but just to play with the two technologies. </li>\n<li>Sean Bittle spoke about using Soar to learn heuristics for constraint programming. As far as I can tell, this was a negative or intermediate result; essentially no successful learning, or too little for the amount of effort and complexity introduced. </li>\n<li>Olivier Georgeon spoke about a model of early developmental learning. I believe this is intended to model human learning, but the domain he was using was a fairly inhuman gridworld. </li>\n<li>Bob Wray spoke about using Soar to create learning experiences.<br /><br />I was a bit disappointed, since (as far as I can tell) this work simply used Soar as an exotic programming language. I believe this is one of the primary ways that Soar could help expand human rationality: if the procedures that a human is supposed to learn are encoded as Soar productions, and the training software can test whether any student has any given production, and instill it if it is not there, then (assuming Soar is a decent model of how humans think), instilling all of the necessary productions should also instill the complete procedure.</li>\n<li>Margaux Lhommet announced an intention to use Soar in a training simulation to control simulated victims of a radioactive / biological terrorist attack. </li>\n<li>Bill Kennedy announced that he was looking for something like Soar, but lighter-weight, so he could do large-scale agent-based simulations. </li>\n<li>Nate Derbinsky wrote a piece of middleware (Soar2Soar) so that Soar programmers can use the Soar programming language to write the environment. I'm so unskilled with Soar that that doesn't sound like a win to me, but Soar is sometimes very declarative, and might be appropriate for rapid development. He also got Soar to run on an iPhone. </li>\n<li> Jonathan Voight spoke about Sproom, a mixed virtual/real robot simulation. The agent can control a virtual robot or a real robot - and even when its controlling a real robot, the virtual environment can augment the inputs, so a wall-avoiding robot would avoid both real and virtual walls. The bot's effectors can be implemented in the virtual reality, so it can drive around in the real world, picking up virtual objects and putting them away.<br /><br />John Laird spoke about using Sproom to study \"Situated Interactive Instruction\" - so that an agent could be taught by interacting in semi-formal language with a human, while it is performing its task. The domain is robots moving through a building doing IED-clearing; the IEDs and the operations on them (pickup, defuse) are virtual. As I understand it, some but not all of this functionality is currently functioning.<br /><br />Shiwali Mohan used the same sort of situated interactive instruction in the Infinite Mario domain, though not very much data was conveyed (yet) via instruction. As I understand it, Mohan's previous hardwired agent had three verbs like \"tackle-monster\" and \"get-coin\"; the instruction consists of the agent asking \"I see a coin, which verb should I use for it?\" - so after the human has answered the three object-verb correspondences, it knows everything it will ever learn via instruction. However, it's working and could be extended.</li>\n</ul>\n</li>\n</ol>\n<p>I want to emphasize that these just my impressions (which are probably flawed - because of my inexperience I probably misunderstood important points), and the proceedings (that is, the slides that everyone used to talk with) will soon be available, so you can read them and form your own impressions.</p>\n<p>There are three forks to my implicit safety case while developing. I'm not claiming this is a particularly good safety case or that developing Rogue-Soar was safe - just that it's what I have.</p>\n<p>The first fork is that tasks vary in their difficulty (Pickering's \"resistances\"), and entities vary in their strength or capability. There's some domain-ish structure to entity's strengths (a mechanical engineering task will be easier for someone trained as a mechanical engineer than a chemist), and intention matters - difficult tasks are rarely accomplished unintentionally. I'm fairly weak, and my agent-in-progress was and is very, very weak. The chance that I or my agent solves a difficult task (self-improving AGI) unintentionally while writing Rogue-Soar is incredibly small, and comparable to the risk of my unintentionally solving self-improving AGI while working at my day job. This suggests that safer (not safe) AI development might involve: One, tracking ELO-like scores of people's strengths and task difficulties, and Two, tracking and incentivizing people's intentions.</p>\n<p>The second fork is that even though I'm surprised sometimes while developing, the surprises are still confined to an envelope of possible behavior. The agent could crash, run forever, move in a straight line or take only one step when I was expecting it to wander randomly, but pressing \"!\" when I expected it to be confined to \"hjkl\" would be beyond this envelope. Of course, there are many nested envelopes, and excursions beyond the bounds of the narrowest are moderately frequent.&nbsp;This suggests that safer AI development might involve tracking these behavior envelopes (altogether they might form a behavior gradient), and the frequency and degree of excursions, and deciding whether development is generally under control - that is, acceptably risky compared to the alternatives.</p>\n<p>The third fork is that the runaway takeoff arguments necessarily involve circularities and feedback. By structural inspection and by intention, if the AI is dealing with Rogue, and not learning, programming, or bootstrapping, then it's unlikely to undergo takeoff. This suggests that carefully documenting and watching for circularities and feedback may be helpful for safer AI research.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9gfZNpBzEXCcJKXoY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 24, "extendedScore": null, "score": 5.892100682765118e-07, "legacy": true, "legacyId": "2950", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-23T13:46:44.536Z", "modifiedAt": null, "url": null, "title": "LessWrong meetup, London UK, 2010-06-06 16:00", "slug": "lesswrong-meetup-london-uk-2010-06-06-16-00", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:31.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3E66o8JghKM5gBM6L/lesswrong-meetup-london-uk-2010-06-06-16-00", "pageUrlRelative": "/posts/3E66o8JghKM5gBM6L/lesswrong-meetup-london-uk-2010-06-06-16-00", "linkUrl": "https://www.lesswrong.com/posts/3E66o8JghKM5gBM6L/lesswrong-meetup-london-uk-2010-06-06-16-00", "postedAtFormatted": "Sunday, May 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20meetup%2C%20London%20UK%2C%202010-06-06%2016%3A00&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20meetup%2C%20London%20UK%2C%202010-06-06%2016%3A00%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3E66o8JghKM5gBM6L%2Flesswrong-meetup-london-uk-2010-06-06-16-00%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20meetup%2C%20London%20UK%2C%202010-06-06%2016%3A00%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3E66o8JghKM5gBM6L%2Flesswrong-meetup-london-uk-2010-06-06-16-00", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3E66o8JghKM5gBM6L%2Flesswrong-meetup-london-uk-2010-06-06-16-00", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<p>The next London LessWrong meetup will be at 16:00 on Sunday 6 June in the <a href=\"http://www.jdwetherspoon.co.uk/home/pubs/shakespeares-head\">Shakespeare's Head</a> near Holborn station.&nbsp; I'll put a Less Wrong sign on the table so you can find us; I <a href=\"http://pics.babysimon.co.uk/index.py/photos/3902?tag=Paul\">look like this</a>.&nbsp; Send me a direct message with your mobile number or email me (paul at ciphergoth dot org), and I'll reciprocate.</p>\n<p>We're trying out a different venue this time; this is where we ended up meeting after Humanity+ which means that at least six of us already know where it is.&nbsp; Looking forward to seeing some of you there!</p>\n<p><strong>Update:</strong> Sad to say, I may now not make this myself - a domestic emergency has come up. Sorry to let people down! If another volunteer could step forward to put the \"Less Wrong\" notice on the table and kick things off, that would be a great service - thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3E66o8JghKM5gBM6L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 5.892128839021762e-07, "legacy": true, "legacyId": "2952", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-25T23:19:44.391Z", "modifiedAt": null, "url": null, "title": "Shock Level 5: Big Worlds and Modal Realism", "slug": "shock-level-5-big-worlds-and-modal-realism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:37.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SkXLrDXyHeekqgbFg/shock-level-5-big-worlds-and-modal-realism", "pageUrlRelative": "/posts/SkXLrDXyHeekqgbFg/shock-level-5-big-worlds-and-modal-realism", "linkUrl": "https://www.lesswrong.com/posts/SkXLrDXyHeekqgbFg/shock-level-5-big-worlds-and-modal-realism", "postedAtFormatted": "Tuesday, May 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shock%20Level%205%3A%20Big%20Worlds%20and%20Modal%20Realism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShock%20Level%205%3A%20Big%20Worlds%20and%20Modal%20Realism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkXLrDXyHeekqgbFg%2Fshock-level-5-big-worlds-and-modal-realism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shock%20Level%205%3A%20Big%20Worlds%20and%20Modal%20Realism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkXLrDXyHeekqgbFg%2Fshock-level-5-big-worlds-and-modal-realism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkXLrDXyHeekqgbFg%2Fshock-level-5-big-worlds-and-modal-realism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1334, "htmlBody": "<p>In recent times, science and philosophy have uncovered evidence that there is something very seriously weird about the universe and our place in it. We used to think that there was one planet earth, inside a universe that is very large (at least 10^26 meters in diameter) but that the reachable universe (<em>future light-cone</em> in the terminology of special relativity, or <em>causal future</em> in the terminology of GR) was finite. Anything outside the reachable universe is irrelevant, since we can't affect it.&nbsp;</p>\n<p>However, cosmologists went on to study the process that probably created the universe, known as <a href=\"http://en.wikipedia.org/wiki/Inflation_%28cosmology%29\">inflation</a>. Inflation solves a number of mysteries in cosmology, including the flatness problem. The process of inflation seems to create an infinite number of mini-universes, or \"inflationary bubbles\" - this is known as <a href=\"http://en.wikipedia.org/wiki/Chaotic_Inflation_theory\">chaotic inflation theory</a>. The physical parameters and initial conditions of these bubbles are determined randomly, so every possible set of particle masses, force strengths, <em>etc </em>is realized. To quote from <a href=\"http://arxiv.org/PS_cache/hep-th/pdf/0702/0702178v1.pdf\">this piece by Alan Guth</a>:</p>\n<p style=\"padding-left: 30px;\"><em>The role of eternal inflation in scientific thinking, however, was greatly boosted by the realization that string theory has no preferred vacuum, but instead has perhaps 10<sup>1000</sup> metastable vacuum-like states. Eternal inflation then has potentially a direct impact on fundamental physics, since it can provide a mechanism to populate the landscape of string vacua. While all of these vacua are described by the same fundamental string theory, the apparent laws of physics at low energies could differ dramatically from one vacuum to another.</em></p>\n<p style=\"padding-left: 30px;\"><a id=\"more\"></a></p>\n<p>To top this off, the dominant theory about the spacetime manifold we live on is that it is infinitely large in all directions. If you look at this <a href=\"http://www.atlasoftheuniverse.com/universe.html\">picture</a>&nbsp;of a reconstruction of the large-scale structure of the universe, the idea that we are living in something like an infinite volume with a finite speed-limit and a uniform random distribution of matter and energy that clumps over time becomes plausible.&nbsp;</p>\n<p>A final step along this line of increasingly large Big Worlds is <a href=\"http://en.wikipedia.org/wiki/Modal_realism\">modal realism</a>, the idea that all possible worlds exist. Max Tegmark has formalized this as the <em>Mathematical Universe Hypothesis</em>: <em>All structures that exist mathematically also exist physically</em>.</p>\n<p>If any of these theories turn out to be true, then we are living in a <em>Big World</em>, a cosmology where every finite collection of atoms, including you, is instantiated infinitely many times, perhaps by the same physical processes that created us here on earth.&nbsp;It is also the case that other life-forms might emerge and use their technological capabilities to create simulations of us. Once an alien civilization reaches the point of being able to create simulations, it can create lots of simulations - really <em>unreasonably large</em> numbers of simulated beings can be created in a universe roughly the size of ours<sup>1,2</sup>, <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">Bostrom's estimate</a> would be something like 10^50. And in other mathematically possible universes with the ability to do an infinite amount of computation in a finite time, you could be simulated an infinite number of times in just one universe.&nbsp;</p>\n<p>One (incorrect) way of interpreting it is to think of a bunch of \"worlds\" spread out over the multiverse, most of them uninhabited, some containing weird green aliens, and one containing you, and saying:&nbsp; \" Aha! I only care about this one, the others are causally disconnected from it!\".</p>\n<p>No, this view of reality claims that your current observer-moment is repeated infinitely many times, and looking forward in time, all possible continuations of (you,now) occur, and furthermore there is&nbsp;<em style=\"font-style: italic;\">no fact of the matter</em>&nbsp;about which one you will experience, because the quantum MW aspect of the multiverse has already demolished our intuitions about <em>anticipated subjective experience</em><sup>4</sup>. Think that chocolate bar will taste nice when you bite into it? Well, actually according to Big Worlds, infinitely many of your continutions will bite the chocolate bar and find it turns into a hamster.</p>\n<p>I once saw wormholes explained using the sheet of paper metaphor: draw two dots on a sheet of paper, reasonably far apart, imagining the paper distance between them to be an unfathomably large spatial distance, say 10^(10^100) meters. Now fold the sheet so that the two dots touch each other: they are right on top of each other! Of course, wormholes seem fairly unlikely based upon standard physics. The metaphor here is of what is called a <em><a href=\"http://en.wikipedia.org/wiki/Quotient_set\">quotient</a> </em>in mathematics, in particular of a quotient in topology.</p>\n<p>But if you combine a functionalist view of mind with big worlds cosmology, then reality becomes the quotient of the set of all possible computations, where all sub-computations that instantiate you are identified. Imagine that you have an infinite piece of paper representing the multiverse, and you draw a dot on it&nbsp;wherever&nbsp;there is a computational process that is the same as the one going on in your brain right now. Now fold the paper up so that all the dots are touching each other, and glue them at that point into one dot. That is your world.&nbsp;</p>\n<p>Almost all of the histories and futures that feed into your \"now\" are simulations, by Bostrom's simulation argument (which is no longer shackled by the requirement that the simulations must be performed by our particular descendants - all possible&nbsp;descendants&nbsp;and aliens get to simulate us).</p>\n<p><em><strong>Future Shock level 5</strong></em>&nbsp;is \"the&nbsp;Copernican&nbsp;revolution with respect to your place in the multiverse\", the point where you mentally realize that perfectly dry astrophysics implies that there is no unique \"you\" at the centre of your sphere of concern, analogous to the Copernican revolution that unseated earth from the centre of the solar system. It is considered to be more shocking than any of the previous future shock levels because it destroys the most basic human epistemological assumption that there is such a thing as <em>my future</em>, or such a thing a<em>s the consequence of my actions</em>.&nbsp;</p>\n<p>Shock Level 5 is a good candidate for <a href=\"http://en.wikipedia.org/wiki/Darwin%27s_Dangerous_Idea#Universal_acid\">Dan Dennett's universal acid</a>: an idea so corrosive that if we let it into our minds, everything we care about will be dissolved. You can't change anything in the multiverse - every&nbsp;decision or consequence&nbsp;that you don't make will be made infinitely many times elsewhere by near-identical copies of you. Every victory will be produced, as will every possible defeat.&nbsp;</p>\n<p>In&nbsp;<a href=\"/lw/1iy/what_are_probabilities_anyway/\">\"What are probabilities anyway?\"</a>&nbsp;Wei Dai&nbsp;suggests a potential solution to your SL5 worries:</p>\n<blockquote>\n<p>All possible worlds are real, and probabilities represent how much I care about each world. (To make sense of this, recall that these probabilities are ultimately multiplied with utilities to form expected utilities in standard decision theories.)</p>\n</blockquote>\n<p>For example, you could get your prior probabilities from the mathematization of occam's Razor, the complexity prior. Then the reason you don't worry that your chocolate bar will turn into a hamster is that the complexity of that hypothesis is higher than the complexity of other hypotheses, such as the chocolate bar just tasting like normal chocolate. But you're not saying that this scenario is&nbsp;unlikely&nbsp;to happen: it is certain to happen, but you just don't care about it.&nbsp;</p>\n<p>Wei's UDT allows you to overcome the decision-theoretic paralysis that would otherwise follow in a Big World: you think of yourself as defining an agent program that controls all of the instantiations of you, so that your decisions do matter. But remember, in order to get decisions out of UDT in a Big World, you need that all-important measure, that is a \"how-much-I-care\" density on the multiverse that integrates to 1.</p>\n<p>Personally, I think that Shock Level 5 could be seen as emotionally dangerous for a human to take seriously, so beware.</p>\n<p>However, there may be strong instrumental reasons to take SL5 seriously if it is true (and there are strong reasons to believe that it is).</p>\n<p><br /><a href=\"http://www.nickbostrom.com/astronomical/waste.html\"></a></p>\n<p>1: Anders Sandberg <a href=\"http://www.jetpress.org/volume5/Brains2.pdf\">talks</a>&nbsp;about the limits of physical systems to process information.</p>\n<p>2: Bostrom on <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">astronomical waste</a> is relevant here as he is calculating the likely number of people that we could simulate in our universe, which ought to be roughly the same as the number of people that some other civilization could simulate in a similar universe.</p>\n<p>3: Not one of the <a href=\"http://www.acceleratingfuture.com/michael/works/shocklevelanalysis.htm\">originally proposed 4 future shock levels</a>.</p>\n<p>4: To really nail the subjective anticipation issue requires another post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 1, "PbShukhzpLsWpGXkM": 1, "7w6XkYe5YPx9YL59j": 1, "22z6XpWKqw3bNv4oR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SkXLrDXyHeekqgbFg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 36, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "2340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 158, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["J7Gkz8aDxxSEQKXTN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-26T01:47:24.490Z", "modifiedAt": null, "url": null, "title": "On Enjoying Disagreeable Company", "slug": "on-enjoying-disagreeable-company", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.427Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nK5jraMp7E4xPvuNv/on-enjoying-disagreeable-company", "pageUrlRelative": "/posts/nK5jraMp7E4xPvuNv/on-enjoying-disagreeable-company", "linkUrl": "https://www.lesswrong.com/posts/nK5jraMp7E4xPvuNv/on-enjoying-disagreeable-company", "postedAtFormatted": "Wednesday, May 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Enjoying%20Disagreeable%20Company&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Enjoying%20Disagreeable%20Company%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnK5jraMp7E4xPvuNv%2Fon-enjoying-disagreeable-company%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Enjoying%20Disagreeable%20Company%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnK5jraMp7E4xPvuNv%2Fon-enjoying-disagreeable-company", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnK5jraMp7E4xPvuNv%2Fon-enjoying-disagreeable-company", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2238, "htmlBody": "<p><strong>Bears resemblance to: </strong><a href=\"/lw/20l/ureshiku_naritai/\">Ureshiku Naritai</a>; <a href=\"/lw/1ln/a_suite_of_pragmatic_considerations_in_favor_of/\">A Suite of Pragmatic Considerations In Favor of Niceness</a><a href=\"/lw/20l/ureshiku_naritai/\"><br /></a></p>\n<p><em>In <a href=\"/lw/1za/the_spotlight/1t8q\">this comment</a>, I mentioned that I can like people on purpose.&nbsp; At the behest of the recipients of my presentation on how to do so, I've written up in post form my tips on the subject.&nbsp; I have not included, and will not include, any specific real-life examples (everything below is made up), because I am concerned that people who I like on purpose will be upset to find that this is the case, in spite of the fact that the liking (once generated) is entirely sincere.&nbsp; If anyone would find more concreteness helpful, I'm willing to come up with <a href=\"/lw/2a5/on_enjoying_disagreeable_company/2272\">brief fictional stories</a> to cover this gap.<br /></em></p>\n<p>It is useful to like people.&nbsp; For one thing, if you have to be around them, liking them makes this far more pleasant.&nbsp; For another, well, they can often <em>tell</em>, and if they know you to like them this will often be instrumentally useful to you.&nbsp; As such, it's very handy to be able to like someone you want to like deliberately when it doesn't happen by itself.&nbsp; There are three basic components to liking someone on purpose.&nbsp; First, reduce salience of the disliked traits by separating, recasting, and downplaying them; second, increase salience of positive traits by identifying, investigating, and admiring them; and third, behave in such a way as to reap consistency effects.</p>\n<p><strong>1. Reduce salience of disliked traits.</strong></p>\n<p>Identify the traits you don't like about the person - this might be a handful of irksome habits or a list as long as your arm of deep character flaws, but make sure you know what they are.&nbsp; Notice that however immense a set of characteristics you generate, it's not the entire person.&nbsp; (\"Everything!!!!\" is not an acceptable entry in this step.)&nbsp; No person can be fully described by a list of things you have noticed about them.&nbsp; Note, accordingly, that you dislike <em>these things</em> about the person; but that this does not logically entail disliking <em>the person</em>.&nbsp; Put the list in a \"box\" - <em>separate</em> from how you will eventually evaluate the person.<a id=\"more\"></a></p>\n<p>When the person exhibits a characteristic, habit, or tendency you have on your list (or, probably just to aggravate you, turns out to have a new one), be on your guard immediately for the <a href=\"/lw/hz/correspondence_bias/\">fundamental attribution error</a>.&nbsp; It is especially insidious when you already dislike the person, and so it's important to compensate consciously and directly for its influence.&nbsp; Elevate to conscious thought an \"attribution story\", in which you consider a <em>circumstance</em> - not a character trait - which would explain this most recent example of bad behavior.<sup>1</sup>&nbsp; This should be the most likely story you can come up with that doesn't resort to grumbling about how dreadful the person is - that is, don't resort to \"Well, maybe he was brainwashed by Martians, but sheesh, how likely is <em>that?</em>\"&nbsp; Better would be \"I know she was up late last night, and she does look a bit tired,\" or \"Maybe that three-hour phone call he ended just now was about something terribly stressful.\"</p>\n<p>Reach a little farther if you don't have this kind of information - \"I'd probably act that way if I were coming down with a cold; I wonder if she's sick?\" is an acceptable speculation even absent the least sniffle.&nbsp; If you can, it's also a good idea to <em>ask</em> (earnestly, curiously, respectfully, kindly!&nbsp; not accusatively, rudely, intrusively, belligerently!) why the person did whatever they did.&nbsp; Rest assured that if their psyche is fairly normal, an explanation exists in their minds that doesn't boil down to \"I'm a lousy excuse for a person who intrinsically does evil things just because it is my nature.\"&nbsp; (Note, however, that not everyone can produce verbal self-justifications on demand.)&nbsp; Whether you believe them or not, make sure you are aware of at least one circumstance-based explanation for what they did.</p>\n<p>Notice which situations elicit more of the disliked behaviors than others.&nbsp; Everybody has situations that bring out the worst in them, and when the worst is already getting on your nerves, you should avoid as much as possible letting any extra bubble to the surface.&nbsp; If you have influence of any kind over which roles this person plays in your life (or in general), confine them to those in which their worst habits are irrelevant, mitigated, or local advantages of some kind.&nbsp; Do not ask for a ride to the airport from someone who terrifies you with their speeding; don't propose splitting dessert with someone whose selfishness drives you up the wall; don't assign the procrastinator an urgent task.&nbsp; Do ask the speeder to make a quick run to the bank before it closes while you're (ever so inconveniently) stuck at home; do give the selfish person tasks where they work on commission; do give the procrastinator things to do that they'll interpret as ways to put off their other work.</p>\n<p><strong>2. Increase salience of positive traits.</strong></p>\n<p>Don't look at me like that.&nbsp; <em>There is something</em>.&nbsp; It's okay to grasp at straws a little to start.&nbsp; You do not have to wait to like someone until you discover the millions of dollars they donate to mitigating existential risk or learn that their pseudonym is the name of your favorite musician.&nbsp; You can like their cool haircut, or the way they phrased that one sentence the other week, or even their shoes.&nbsp; You can appreciate that they've undergone more hardship than you (if they have, but be generous in interpreting \"more\" when comparing incommensurate difficulties) - even if you don't think they've handled it that well, well, it was hard.&nbsp; You can acknowledge that they are better than you, or than baseline, or than any one person who you already like, at some skill or in some sphere of achievement.&nbsp; You can think they did a good job of picking out their furniture, or loan them halo effect from a relative or friend of theirs who you think is okay.&nbsp; There is <em>something.</em></p>\n<p>Learn more about the likable things you have discovered.&nbsp; \"Catch them in the act\" of showing off one of these fine qualities.&nbsp; As a corollary to the bit above about not putting them in roles that bring out their worst, try to put them in situations where they're at their best.&nbsp; Set them up to succeed, both absolutely and in your eyes.&nbsp; Speak to any available mutual friends about what more there is to like - learn how the person makes friends, what attracts people to them, what people get out of associating with them.&nbsp; Solicit stories about the excellent deeds of the target person.&nbsp; Collect material like you're a biographer terrified of being sued for libel and dreading coming in under page count: you need to know all the nice things there are to know.</p>\n<p>It is absolutely essential throughout this process to cultivate <em>admiration, not jealousy</em>.&nbsp; Jealousy and resentment are absolutely counterproductive, while admiration and respect - however grudging - are steps in the right direction.&nbsp; Additionally, you are trying to <em>use&nbsp; </em>these features of the person.&nbsp; It will not further your goals if you discount their importance in the grand scheme of things.&nbsp; Do not think, \"She has such pretty hair, why does she get such pretty hair when she doesn't deserve it since she's such an awful person?&nbsp; Grrr!\"&nbsp; Instead, \"She has such pretty hair.&nbsp; It's gorgeous to look at and that makes her nice to have around.&nbsp; I wonder if she has time to teach me how to do my hair like that.\"&nbsp; Or instead of: \"Sure, he can speak Latin, but what the hell use is Latin?&nbsp; Does he think we're going to be invaded by legionaries and need him to be a diplomat?\" it would be more useful towards the project of liking to think, \"Most people don't have the patience and dedication to learn any second language, and it only makes it harder to pick one where there aren't native speakers available to help teach the finer points.&nbsp; I bet a lot of effort went into this.\"</p>\n<p><strong>3. Reap consistency effects.</strong></p>\n<p>Take care to be kind and considerate to the person.&nbsp; The odds are pretty good that there is something they don't like about <em>you</em> (rubbing someone the wrong way is more often bidirectional than not).&nbsp; If you can figure out what it is, and do less of it - at least around them - you will collect cognitive dissonance that you can use to nudge yourself to like the person.&nbsp; I mean, otherwise, why would you go to the trouble of not tapping your fingers around them, or making sure to pronounce their complicated name correctly, or remembering what they're allergic to so you can avoid bringing in food suitable for everyone but them?&nbsp; That's the sort of thing you do when you care how they feel, and if you care how they feel, you must like them at least a little.&nbsp; (Note failure mode: if you discover that something you do annoys them, and you respond with resentment that they have such an unreasonable preference about such a deeply held part of your identity and how dare they!, you're doing it wrong.&nbsp; The point isn't to completely make yourself over to be their ideal friend.&nbsp; You don't have to do everything.&nbsp; But do <em>something</em>.)</p>\n<p>Seek to spend time around the person.&nbsp; This should drop pretty naturally out of the above steps: you need to acquire all this information from <em>somewhere, </em>after all.&nbsp; But seek their opinions on things, especially their areas of expertise and favorite topics; make small talk; ask after their projects, their interests, their loved ones; choose to hang out in rooms they occupy even if you never interact.&nbsp; (Note failure mode: Don't do this if you can feel yourself hating them more every minute you spend together or if you find it stressful enough to inhibit the above mental exercises.&nbsp; It is better to do more work on liking them from a distance if you are at this stage, then later move on to seeking to spend time with them.&nbsp; Also, if you annoy them, don't do anything that could be characterized as pestering them or following them around.)</p>\n<p>Try to learn something from the person - by example, if they aren't interested in teaching you, or directly, if they are.&nbsp; It is possible to learn even from people who don't have significantly better skills than you.&nbsp; If they tell stories about things they've done, you can learn from their mistakes; if they are worse than you at a skill but use an approach to it that you haven't tried, you can learn how to use it; if nothing else, they know things about <em>themselves</em>, and that information is highly useful for the project of liking them, as discussed above.&nbsp; Put what you know about them into the context of their own perspective.</p>\n<p><strong>Note general failure mode:</strong> It would be fairly easy, using facsimiles of the strategy above, to develop smugness, self-righteousness, arrogance, and other unseemly attitudes.&nbsp; <em>Beware</em> if your inner monologue begins to sound something like \"He's gone and broken the sink again, but I'm too good and tolerant to be angry.&nbsp; It wouldn't do any good to express my displeasure - after all, he can't take criticism, not that I judge him for this, of course.&nbsp; I'll be sure to put a note on the faucet and call the plumber to cover for his failure to do so, rather than nagging him to do it, as I know he'd fly off the handle if I reminded him - it's just not everyone's gift to accept such things, as it is mine, and as I am doing, right now, with him, by not being upset...\"</p>\n<p>This monologuer does not <em>like</em> the sink-breaker.&nbsp; This monologuer holds him in contempt, and thinks very highly of herself for keeping this contempt ostensibly private (although it's entirely possible that he can tell anyway).&nbsp; She tolerates his company because it would be beneath her not to; she doesn't enjoy having him around because she realizes that he has useful insights on relevant topics or even because he's decorative in some way.&nbsp; <em>If you don't wind up really, genuinely, sincerely liking the person you set out to like, you are doing it wrong.&nbsp; This is not a credit to your high-mindedness, and thinking it is will not help you win.</em></p>\n<p>&nbsp;</p>\n<p><sup>1</sup> A good time to practice this habit is when in a car.&nbsp; Make up stories about the traffic misbehaviors around you.&nbsp; \"The sun is so bright - she may not have seen me.\"&nbsp; \"That car sure looks old!&nbsp; I probably wouldn't handle it even half as well, no wonder it keeps stalling.\"&nbsp; \"He's in a terrible hurry - I wonder if a relative of his is in trouble.\"&nbsp; \"Perhaps she's on her cellphone because she's a doctor, on call - it then would really be more dangerous on net if she <em>didn't</em> answer the thing while driving.\"&nbsp; \"He'd pull over if there were any place to do so, but there's no shoulder.\"&nbsp; Of course any given one of these is probably not true.&nbsp; But they <em>make sense</em>, and they are not about how everybody on the road is a maniac!&nbsp; I stress that you are not to <em>believe</em> these stories.&nbsp; You are merely to<em> acknowledge that they are possibilities</em>, to compensate for the deemphasis of hypotheses like this that the fundamental attribution error will prompt.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 1, "4R8JYu4QF2FqzJxE5": 1, "Jzm2mYuuDBCNWq8hi": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nK5jraMp7E4xPvuNv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": 65, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "2957", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "seven-shiny-stories", "canonicalPrevPostSlug": "a-suite-of-pragmatic-considerations-in-favor-of-niceness", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Bears resemblance to: </strong><a href=\"/lw/20l/ureshiku_naritai/\">Ureshiku Naritai</a>; <a href=\"/lw/1ln/a_suite_of_pragmatic_considerations_in_favor_of/\">A Suite of Pragmatic Considerations In Favor of Niceness</a><a href=\"/lw/20l/ureshiku_naritai/\"><br></a></p>\n<p><em>In <a href=\"/lw/1za/the_spotlight/1t8q\">this comment</a>, I mentioned that I can like people on purpose.&nbsp; At the behest of the recipients of my presentation on how to do so, I've written up in post form my tips on the subject.&nbsp; I have not included, and will not include, any specific real-life examples (everything below is made up), because I am concerned that people who I like on purpose will be upset to find that this is the case, in spite of the fact that the liking (once generated) is entirely sincere.&nbsp; If anyone would find more concreteness helpful, I'm willing to come up with <a href=\"/lw/2a5/on_enjoying_disagreeable_company/2272\">brief fictional stories</a> to cover this gap.<br></em></p>\n<p>It is useful to like people.&nbsp; For one thing, if you have to be around them, liking them makes this far more pleasant.&nbsp; For another, well, they can often <em>tell</em>, and if they know you to like them this will often be instrumentally useful to you.&nbsp; As such, it's very handy to be able to like someone you want to like deliberately when it doesn't happen by itself.&nbsp; There are three basic components to liking someone on purpose.&nbsp; First, reduce salience of the disliked traits by separating, recasting, and downplaying them; second, increase salience of positive traits by identifying, investigating, and admiring them; and third, behave in such a way as to reap consistency effects.</p>\n<p><strong id=\"1__Reduce_salience_of_disliked_traits_\">1. Reduce salience of disliked traits.</strong></p>\n<p>Identify the traits you don't like about the person - this might be a handful of irksome habits or a list as long as your arm of deep character flaws, but make sure you know what they are.&nbsp; Notice that however immense a set of characteristics you generate, it's not the entire person.&nbsp; (\"Everything!!!!\" is not an acceptable entry in this step.)&nbsp; No person can be fully described by a list of things you have noticed about them.&nbsp; Note, accordingly, that you dislike <em>these things</em> about the person; but that this does not logically entail disliking <em>the person</em>.&nbsp; Put the list in a \"box\" - <em>separate</em> from how you will eventually evaluate the person.<a id=\"more\"></a></p>\n<p>When the person exhibits a characteristic, habit, or tendency you have on your list (or, probably just to aggravate you, turns out to have a new one), be on your guard immediately for the <a href=\"/lw/hz/correspondence_bias/\">fundamental attribution error</a>.&nbsp; It is especially insidious when you already dislike the person, and so it's important to compensate consciously and directly for its influence.&nbsp; Elevate to conscious thought an \"attribution story\", in which you consider a <em>circumstance</em> - not a character trait - which would explain this most recent example of bad behavior.<sup>1</sup>&nbsp; This should be the most likely story you can come up with that doesn't resort to grumbling about how dreadful the person is - that is, don't resort to \"Well, maybe he was brainwashed by Martians, but sheesh, how likely is <em>that?</em>\"&nbsp; Better would be \"I know she was up late last night, and she does look a bit tired,\" or \"Maybe that three-hour phone call he ended just now was about something terribly stressful.\"</p>\n<p>Reach a little farther if you don't have this kind of information - \"I'd probably act that way if I were coming down with a cold; I wonder if she's sick?\" is an acceptable speculation even absent the least sniffle.&nbsp; If you can, it's also a good idea to <em>ask</em> (earnestly, curiously, respectfully, kindly!&nbsp; not accusatively, rudely, intrusively, belligerently!) why the person did whatever they did.&nbsp; Rest assured that if their psyche is fairly normal, an explanation exists in their minds that doesn't boil down to \"I'm a lousy excuse for a person who intrinsically does evil things just because it is my nature.\"&nbsp; (Note, however, that not everyone can produce verbal self-justifications on demand.)&nbsp; Whether you believe them or not, make sure you are aware of at least one circumstance-based explanation for what they did.</p>\n<p>Notice which situations elicit more of the disliked behaviors than others.&nbsp; Everybody has situations that bring out the worst in them, and when the worst is already getting on your nerves, you should avoid as much as possible letting any extra bubble to the surface.&nbsp; If you have influence of any kind over which roles this person plays in your life (or in general), confine them to those in which their worst habits are irrelevant, mitigated, or local advantages of some kind.&nbsp; Do not ask for a ride to the airport from someone who terrifies you with their speeding; don't propose splitting dessert with someone whose selfishness drives you up the wall; don't assign the procrastinator an urgent task.&nbsp; Do ask the speeder to make a quick run to the bank before it closes while you're (ever so inconveniently) stuck at home; do give the selfish person tasks where they work on commission; do give the procrastinator things to do that they'll interpret as ways to put off their other work.</p>\n<p><strong id=\"2__Increase_salience_of_positive_traits_\">2. Increase salience of positive traits.</strong></p>\n<p>Don't look at me like that.&nbsp; <em>There is something</em>.&nbsp; It's okay to grasp at straws a little to start.&nbsp; You do not have to wait to like someone until you discover the millions of dollars they donate to mitigating existential risk or learn that their pseudonym is the name of your favorite musician.&nbsp; You can like their cool haircut, or the way they phrased that one sentence the other week, or even their shoes.&nbsp; You can appreciate that they've undergone more hardship than you (if they have, but be generous in interpreting \"more\" when comparing incommensurate difficulties) - even if you don't think they've handled it that well, well, it was hard.&nbsp; You can acknowledge that they are better than you, or than baseline, or than any one person who you already like, at some skill or in some sphere of achievement.&nbsp; You can think they did a good job of picking out their furniture, or loan them halo effect from a relative or friend of theirs who you think is okay.&nbsp; There is <em>something.</em></p>\n<p>Learn more about the likable things you have discovered.&nbsp; \"Catch them in the act\" of showing off one of these fine qualities.&nbsp; As a corollary to the bit above about not putting them in roles that bring out their worst, try to put them in situations where they're at their best.&nbsp; Set them up to succeed, both absolutely and in your eyes.&nbsp; Speak to any available mutual friends about what more there is to like - learn how the person makes friends, what attracts people to them, what people get out of associating with them.&nbsp; Solicit stories about the excellent deeds of the target person.&nbsp; Collect material like you're a biographer terrified of being sued for libel and dreading coming in under page count: you need to know all the nice things there are to know.</p>\n<p>It is absolutely essential throughout this process to cultivate <em>admiration, not jealousy</em>.&nbsp; Jealousy and resentment are absolutely counterproductive, while admiration and respect - however grudging - are steps in the right direction.&nbsp; Additionally, you are trying to <em>use&nbsp; </em>these features of the person.&nbsp; It will not further your goals if you discount their importance in the grand scheme of things.&nbsp; Do not think, \"She has such pretty hair, why does she get such pretty hair when she doesn't deserve it since she's such an awful person?&nbsp; Grrr!\"&nbsp; Instead, \"She has such pretty hair.&nbsp; It's gorgeous to look at and that makes her nice to have around.&nbsp; I wonder if she has time to teach me how to do my hair like that.\"&nbsp; Or instead of: \"Sure, he can speak Latin, but what the hell use is Latin?&nbsp; Does he think we're going to be invaded by legionaries and need him to be a diplomat?\" it would be more useful towards the project of liking to think, \"Most people don't have the patience and dedication to learn any second language, and it only makes it harder to pick one where there aren't native speakers available to help teach the finer points.&nbsp; I bet a lot of effort went into this.\"</p>\n<p><strong id=\"3__Reap_consistency_effects_\">3. Reap consistency effects.</strong></p>\n<p>Take care to be kind and considerate to the person.&nbsp; The odds are pretty good that there is something they don't like about <em>you</em> (rubbing someone the wrong way is more often bidirectional than not).&nbsp; If you can figure out what it is, and do less of it - at least around them - you will collect cognitive dissonance that you can use to nudge yourself to like the person.&nbsp; I mean, otherwise, why would you go to the trouble of not tapping your fingers around them, or making sure to pronounce their complicated name correctly, or remembering what they're allergic to so you can avoid bringing in food suitable for everyone but them?&nbsp; That's the sort of thing you do when you care how they feel, and if you care how they feel, you must like them at least a little.&nbsp; (Note failure mode: if you discover that something you do annoys them, and you respond with resentment that they have such an unreasonable preference about such a deeply held part of your identity and how dare they!, you're doing it wrong.&nbsp; The point isn't to completely make yourself over to be their ideal friend.&nbsp; You don't have to do everything.&nbsp; But do <em>something</em>.)</p>\n<p>Seek to spend time around the person.&nbsp; This should drop pretty naturally out of the above steps: you need to acquire all this information from <em>somewhere, </em>after all.&nbsp; But seek their opinions on things, especially their areas of expertise and favorite topics; make small talk; ask after their projects, their interests, their loved ones; choose to hang out in rooms they occupy even if you never interact.&nbsp; (Note failure mode: Don't do this if you can feel yourself hating them more every minute you spend together or if you find it stressful enough to inhibit the above mental exercises.&nbsp; It is better to do more work on liking them from a distance if you are at this stage, then later move on to seeking to spend time with them.&nbsp; Also, if you annoy them, don't do anything that could be characterized as pestering them or following them around.)</p>\n<p>Try to learn something from the person - by example, if they aren't interested in teaching you, or directly, if they are.&nbsp; It is possible to learn even from people who don't have significantly better skills than you.&nbsp; If they tell stories about things they've done, you can learn from their mistakes; if they are worse than you at a skill but use an approach to it that you haven't tried, you can learn how to use it; if nothing else, they know things about <em>themselves</em>, and that information is highly useful for the project of liking them, as discussed above.&nbsp; Put what you know about them into the context of their own perspective.</p>\n<p><strong>Note general failure mode:</strong> It would be fairly easy, using facsimiles of the strategy above, to develop smugness, self-righteousness, arrogance, and other unseemly attitudes.&nbsp; <em>Beware</em> if your inner monologue begins to sound something like \"He's gone and broken the sink again, but I'm too good and tolerant to be angry.&nbsp; It wouldn't do any good to express my displeasure - after all, he can't take criticism, not that I judge him for this, of course.&nbsp; I'll be sure to put a note on the faucet and call the plumber to cover for his failure to do so, rather than nagging him to do it, as I know he'd fly off the handle if I reminded him - it's just not everyone's gift to accept such things, as it is mine, and as I am doing, right now, with him, by not being upset...\"</p>\n<p>This monologuer does not <em>like</em> the sink-breaker.&nbsp; This monologuer holds him in contempt, and thinks very highly of herself for keeping this contempt ostensibly private (although it's entirely possible that he can tell anyway).&nbsp; She tolerates his company because it would be beneath her not to; she doesn't enjoy having him around because she realizes that he has useful insights on relevant topics or even because he's decorative in some way.&nbsp; <em>If you don't wind up really, genuinely, sincerely liking the person you set out to like, you are doing it wrong.&nbsp; This is not a credit to your high-mindedness, and thinking it is will not help you win.</em></p>\n<p>&nbsp;</p>\n<p><sup>1</sup> A good time to practice this habit is when in a car.&nbsp; Make up stories about the traffic misbehaviors around you.&nbsp; \"The sun is so bright - she may not have seen me.\"&nbsp; \"That car sure looks old!&nbsp; I probably wouldn't handle it even half as well, no wonder it keeps stalling.\"&nbsp; \"He's in a terrible hurry - I wonder if a relative of his is in trouble.\"&nbsp; \"Perhaps she's on her cellphone because she's a doctor, on call - it then would really be more dangerous on net if she <em>didn't</em> answer the thing while driving.\"&nbsp; \"He'd pull over if there were any place to do so, but there's no shoulder.\"&nbsp; Of course any given one of these is probably not true.&nbsp; But they <em>make sense</em>, and they are not about how everybody on the road is a maniac!&nbsp; I stress that you are not to <em>believe</em> these stories.&nbsp; You are merely to<em> acknowledge that they are possibilities</em>, to compensate for the deemphasis of hypotheses like this that the fundamental attribution error will prompt.</p>", "sections": [{"title": "1. Reduce salience of disliked traits.", "anchor": "1__Reduce_salience_of_disliked_traits_", "level": 1}, {"title": "2. Increase salience of positive traits.", "anchor": "2__Increase_salience_of_positive_traits_", "level": 1}, {"title": "3. Reap consistency effects.", "anchor": "3__Reap_consistency_effects_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "254 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 254, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xnPFYBuaGhpq869mY", "w8g7AkSbyApokD3dH", "DB6wbyrMugYMK5o6a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-26T07:43:49.650Z", "modifiedAt": null, "url": null, "title": "Abnormal Cryonics", "slug": "abnormal-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:57.459Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J5teWueouHJxcZkDy/abnormal-cryonics", "pageUrlRelative": "/posts/J5teWueouHJxcZkDy/abnormal-cryonics", "linkUrl": "https://www.lesswrong.com/posts/J5teWueouHJxcZkDy/abnormal-cryonics", "postedAtFormatted": "Wednesday, May 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Abnormal%20Cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAbnormal%20Cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ5teWueouHJxcZkDy%2Fabnormal-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Abnormal%20Cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ5teWueouHJxcZkDy%2Fabnormal-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ5teWueouHJxcZkDy%2Fabnormal-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1595, "htmlBody": "<p>Written with much help from <span class=\"author-g-re6panhy8fv82o8m\"><a href=\"/user/Nick_Tarleton\">Nick Tarleton</a></span> and <span class=\"author-g-re6panhy8fv82o8m\"><a href=\"/user/Kaj_Sotala\">Kaj Sotala</a></span>, in response to various themes<span class=\"author-g-re6panhy8fv82o8m\"> </span><span class=\"author-g-re6panhy8fv82o8m\"><a href=\"/lw/1mc/normal_cryonics/\">here</a>,</span> <a href=\"/lw/wq/you_only_live_twice/\">here</a>, and throughout Less Wrong; but a casual mention <a href=\"/lw/296/the_tragedy_of_the_social_epistemology_commons/\">here</a><sup>1</sup> inspired me to finally write this post. <sup>(Note: The first, second, and third footnotes of this post are abnormally important.)</sup></p>\n<p>It seems to have become a trend on Less Wrong for people to include belief in the rationality of signing up for cryonics as an obviously correct position<sup>2</sup> to take, much the same as thinking the theories of continental drift or anthropogenic global warming are almost certainly correct. I find this mildly disturbing on two counts. First, it really isn't all that obvious that signing up for cryonics is the best use of one's time and money. And second, regardless of whether cryonics turns out to have been the best choice all along, ostracizing those who do not find signing up for cryonics obvious is not at all helpful for people struggling to become more rational. Below I try to provide some decent arguments against signing up for cryonics &mdash; not with the aim of showing that signing up for cryonics is wrong, but simply to show that it is not obviously correct, and why it shouldn't be treated as such. (Please note that I am not arguing against the feasibility of cryopreservation!)</p>\n<p><a id=\"more\"></a></p>\n<p>Signing up for cryonics is not obviously correct, and especially cannot obviously be expected to have been correct upon due reflection (even if it was the best decision given the uncertainty at the time):</p>\n<ul>\n<li><a href=\"/lw/1t0/shock_level_5_big_worlds_and_modal_realism/\"><span class=\"author-g-re6panhy8fv82o8m url\">Weird stuff</span></a> and ontological confusion: <a href=\"http://en.wikipedia.org/wiki/Quantum_suicide_and_immortality\">quantum immortality</a>, anthropic reasoning, measure across multiverses, <a href=\"/lw/1iy/what_are_probabilities_anyway/\">UDTesque 'decision theoretic measure' or 'probability as preference'</a>, et cetera, are not well-understood enough to make claims about whether or not you should even care about the number of 'yous' that are living or dying, whatever 'you' think you are.<sup>3</sup> This does not make cryonics a bad idea &mdash; it may be the correct decision under uncertainty &mdash; but it should lessen anyone's confidence that the balance of reasons ultimately weighs overwhelmingly in favor of cryonics.</li>\n<li>If people believe that a technological singularity is imminent, then they may believe that it will happen before they have a significant chance of dying: either everyone (including cryonauts) dies anyway when an unFriendly artificial intelligence goes FOOM, or a Friendly artificial intelligence is created and death is solved (or reflectively embraced as good, or some other unexpected outcome). This is more salient when considering the likelihood of large advances in biomedical and life extension technologies in the near future.</li>\n<li>A person might find that more good is done by donating money to organizations like SENS, FHI, or SIAI<sup>4</sup> than by spending that money on pursuing a small chance of eternal life. Cryonics working is pretty dependent on e.g. an unFriendly artificial intelligence not going FOOM, or molecular nanotechnology not killing everyone. Many people may believe that a slightly higher chance of a positive singularity is more important than a significantly higher chance of personal immortality. Likewise, having their friends and family not be killed by an existential disaster such as rogue MNT, bioweaponry, et cetera, could very well be more important to them than a chance at eternal life. Acknowledging these varied preferences, and varied beliefs about one's ability to sacrifice only luxury spending to cryonics, leads to equally varied subjectively rational courses of action for a person to take.</li>\n<li>Some people may have loose boundaries around what they consider personal identity, or expect personal identity to be less important in the future. Such a person might not place very high value on ensuring that they, in a strong sense, exist in the far future, if they expect that people sufficiently like them to satisfy their relevant values will exist in any case. (Kaj Sotala reports being&nbsp; indifferent to cryonics due to personal identity considerations <span class=\"author-g-loomthps3lj5qj60\"><a href=\"/lw/1mc/normal_cryonics/1hah\">he</a></span><span class=\"author-g-loomthps3lj5qj60\"><a href=\"/lw/1mc/normal_cryonics/1hah\">re</a></span>.) Furthermore, there exist people who have preferences against (or no preferences either for or against) living extremely far into the future for reasons other than considerations about personal identity. Such cases are rare, but I suspect less rare among the Less Wrong population than most, and their existence should be recognized. (Maybe people who think they don't care are usually wrong, and, if so, irrational in an <span class=\"author-g-yflz122zolbb1aqh4tyo\"><a href=\"/lw/1xh/living_luminously/\">important sense</a></span>, but not in the sense of simple epistemic or instrumental-given-fixed-values rationality that discussions of cryonics usually center on.)</li>\n<li><strong>That said, the reverse is true: not getting signed up for cryonics is also not obviously correct.</strong> The most common objections (most of them about the infeasibility of cryopreservation) are simply <em>wrong</em>. Strong arguments are being ignored <em>on both sides</em>. The common enemy is certainty.</li>\n</ul>\n<p>Calling non-cryonauts irrational is not productive nor conducive to fostering a good epistemic atmosphere:</p>\n<ul>\n<li>Whether it's correct or not, it seems unreasonable to claim that the decision to forgo cryonics in favor of donating (a greater expected amount) to <span class=\"author-g-re6panhy8fv82o8m\"><a href=\"http://www.fhi.ox.ac.uk/\">FHI</a>, <a href=\"http://intelligence.org/\">SIAI</a><sup>4</sup>, <a href=\"http://www.sens.org/\">SENS</a>,</span> etc. represents as <em>obvious</em> an error as, for instance, religion. The possibility of a <a href=\"http://wiki.lesswrong.com/wiki/Third_option\">third option</a> here shouldn't be ignored.</li>\n<li>People will not take a fringe subject more seriously simply because you call them irrational for not seeing it as obvious (as opposed to belief in anthropogenic global warming where a sheer bandwagon effect is enough of a memetic pull). Being forced on the&nbsp; defensive makes one less likely to<span class=\"author-g-re6panhy8fv82o8m\"> <a href=\"/lw/aw/its_okay_to_be_at_least_a_little_irrational/\">accept and therefore overcome</a></span> their own irrationalities, if irrationalities they are. (See also: <a href=\"/lw/1ln/a_suite_of_pragmatic_considerations_in_favor_of/\">A Suite of Pragmatic Considerations in Favor of Niceness</a>)</li>\n<li>As mentioned in bullet four above, some people really wouldn't care if they died, even if it turned out MWI, spatially infinite universes, et cetera were wrong hypotheses and that they only had this one shot at existence. It's not helping things to call them irrational when they may already have low self-esteem and problems with being accepted among those who have very different values pertaining to the importance of continued subjective experience. Likewise, calling people irrational for having kids when they could not afford cryonics for them is extremely unlikely to do any good for anyone.</li>\n</ul>\n<p>Debate over cryonics is only one of many opportunities for <a href=\"/lw/gw/politics_is_the_mindkiller/\">politics-like thinking</a> to taint the <a href=\"http://wiki.lesswrong.com/wiki/Epistemic_hygiene\">epistemic waters</a> of a rationalist community; it is a topic where it is easy to say 'we are right and you are wrong' where 'we' and 'you' are much too poorly defined to be used without disclaimers. If 'you' really means 'you people who don't understand reductionist thinking', or 'you people who haven't considered the impact of existential risk', then it is important to say so. If such an epistemic norm is not established I fear that the quality of discourse at Less Wrong will suffer for the lack of it.</p>\n<p>One easily falls to the trap of thinking that disagreements with other people happen because the others are irrational in simple, obviously flawed ways. It's harder to avoid the fundamental attribution error and the typical mind fallacy, and admit that the others may have a non-insane reason for their disagreement.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> I don't disagree with Roko's real point, that the prevailing attitude towards cryonics is decisive evidence that people are crazy and the world is mad. Given uncertainty about whether one's real values would endorse signing up for cryonics, it's not plausible that the staggering potential benefit would fail to recommend extremely careful reasoning about the subject, and investment of plenty of resources if such reasoning didn't come up with a confident no. Even if the decision not to sign up for cryonics were obviously correct upon even a moderate level of reflection, it would still constitute a serious failure of instrumental rationality to make that decision <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">non-reflectively and independently of its correctness</a>, as almost everyone does. I think that usually when someone brings up the obvious correctness of cryonics, they mostly just mean to make this observation, which is no less sound even if cryonics isn't obviously correct.</p>\n<p>2 To those who would immediately respond that signing up for cryonics is obviously correct, either for you or for people generally, it seems you could mean two very different things: Do you believe that signing up for cryonics is the best course of action given your level of uncertainty? or, Do you believe that signing up for cryonics can obviously be expected to have been correct upon due reflection? (That is, would you expect a logically omniscient agent to sign up for cryonics in roughly your situation given your utility function?) One is a statement about your decision algorithm, another is a statement about your meta-level uncertainty.<em> I am primarily (though not entirely) arguing against the epistemic correctness of making a strong statement such as the latter.<br /></em></p>\n<p><sup>3</sup> By raising this point as an objection to strong certainty in cryonics specifically, I am essentially bludgeoning a fly with a sledgehammer. With much generalization and effort this post could also have been written as 'Abnormal Everything'. Structural uncertainty is a potent force and the various effects it has on whether or not 'it all adds up to normality' would not fit in the margin of this post. However, Nick Tarleton and I have expressed interest in writing a pseudo-sequence on the subject. We're just not sure about how to format it, and it might or might not come to fruition. If so, this would be the first post in the 'sequence'.</p>\n<p><sup>4</sup> Disclaimer and alert to potential bias: I'm an intern (not any sort of Fellow) at the Singularity Institute for (or 'against' or 'ambivalent about' if that is what, upon due reflection, is seen as the best stance) Artificial Intelligence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J5teWueouHJxcZkDy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 71, "baseScore": 76, "extendedScore": null, "score": 0.000128, "legacy": true, "legacyId": "2960", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 76, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 420, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hiDkhLyN5S2MEjrSE", "yKXKcyoBzWtECzXrE", "YgCi9vBphbG7hmnb5", "SkXLrDXyHeekqgbFg", "J7Gkz8aDxxSEQKXTN", "9o3Cjjem7AbmmZfBs", "Yiv9BeroBhJC6zqSs", "w8g7AkSbyApokD3dH", "9weLK2AJ9JEt2Tt8f", "qNZM3EGoE5ZeMdCRt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-27T00:10:57.279Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread", "slug": "harry-potter-and-the-methods-of-rationality-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:32.239Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/59rDBidWmmJTXL4Np/harry-potter-and-the-methods-of-rationality-discussion", "pageUrlRelative": "/posts/59rDBidWmmJTXL4Np/harry-potter-and-the-methods-of-rationality-discussion", "linkUrl": "https://www.lesswrong.com/posts/59rDBidWmmJTXL4Np/harry-potter-and-the-methods-of-rationality-discussion", "postedAtFormatted": "Thursday, May 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59rDBidWmmJTXL4Np%2Fharry-potter-and-the-methods-of-rationality-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59rDBidWmmJTXL4Np%2Fharry-potter-and-the-methods-of-rationality-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59rDBidWmmJTXL4Np%2Fharry-potter-and-the-methods-of-rationality-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 396, "htmlBody": "<p><strong>Update: Please post new comments in the <a href=\"/r/discussion/tag/harry_potter/\">latest</a> HPMOR discussion thread, now in the discussion section, since this thread and its first few successors have grown unwieldy (direct links: <a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/\">four</a>, <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality/\">five</a>, <a href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">six</a>, <a href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">seven</a>).</strong></p>\n<p>As many of you already know, Eliezer Yudkowsky is writing a <a href=\"http://www.fanfiction.net/s/5782108/1/Harry_Potter_and_the_Methods_of_Rationality\">Harry Potter fanfic</a>, <em>Harry Potter and the Methods of Rationality</em>, starring a rationalist Harry Potter with ambitions to transform the world by bringing the rationalist/scientific method to magic.&nbsp; But of course a more powerful Potter requires a more challenging wizarding world, and ... well, you can see for yourself how that plays out.<br /><br />This thread is for discussion of anything related to the story, including insights, confusions, questions, speculation, jokes, discussion of rationality issues raised in the story, attempts at fanfic spinoffs, comments about related fanfictions, and meta-discussion about the fact that Eliezer Yudkowsky is writing Harry Potter fan-fiction (presumably as a means of <a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>).<br /><br />I'm making this a top-level post to create a centralized location for that discussion, since I'm guessing people have things to say (I know I do) and there isn't a great place to put them.&nbsp; fanfiction.net has a different set of users (plus no threading or karma), the main discussion here has been in an <a href=\"/lw/20w/open_thread_april_2010/1uf9\">old open thread</a> which has petered out and is already near the unwieldy size that would call for a top-level post, and we've had discussions come up in a few <a href=\"/lw/29o/open_thread_may_2010_part_2/21wr\">other</a> <a href=\"/lw/290/blue_and_yellowtinted_choices/2075\">places</a>.&nbsp; So let's have that discussion here.&nbsp; <br /><br />Comments here will obviously be full of spoilers, and I don't think it makes sense to rot13 the whole thread, so consider this a <strong>spoiler warning</strong>:&nbsp; this thread contains unrot13'd spoilers for <em>Harry Potter and the Methods of Rationality</em> up to the current chapter and for the original Harry Potter series.&nbsp; Please continue to use rot13 for spoilers to other works of fiction, or if you have insider knowledge of future chapters of <em>Harry Potter and the Methods of Rationality</em>.<br /><br />A suggestion: mention at the top of your comment which chapter you're commenting on, or what chapter you're up to, so that people can understand the context of your comment even after more chapters have been posted.&nbsp; This can also help people avoid reading spoilers for a new chapter before they realize that there is a new chapter.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "59rDBidWmmJTXL4Np", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 44, "extendedScore": null, "score": 0.0005264750054754617, "legacy": true, "legacyId": "2963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 882, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-27T13:16:45.798Z", "modifiedAt": null, "url": null, "title": "Beyond Optimization by Proxy", "slug": "beyond-optimization-by-proxy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:23.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HgiWHaxnAEtg3uEfM/beyond-optimization-by-proxy", "pageUrlRelative": "/posts/HgiWHaxnAEtg3uEfM/beyond-optimization-by-proxy", "linkUrl": "https://www.lesswrong.com/posts/HgiWHaxnAEtg3uEfM/beyond-optimization-by-proxy", "postedAtFormatted": "Thursday, May 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beyond%20Optimization%20by%20Proxy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeyond%20Optimization%20by%20Proxy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgiWHaxnAEtg3uEfM%2Fbeyond-optimization-by-proxy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beyond%20Optimization%20by%20Proxy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgiWHaxnAEtg3uEfM%2Fbeyond-optimization-by-proxy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHgiWHaxnAEtg3uEfM%2Fbeyond-optimization-by-proxy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2032, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/28r/is_google_paperclipping_the_web_the_perils_of\">Is Google Paperclipping the Web? The Perils of Optimization by Proxy in Social Systems</a></p>\n<p><em>tl;dr: In this installment, we look at methods of avoiding the problems related to optimization by proxy. Many potential solutions cluster around two broad categories: Better Measures, and Human Discretion. Distribution of decisions to the local level is a solution that seems more promising and is examined in more depth.</em></p>\n<p>In the previous article I had promised that if there was a good reception, I would post a follow-up article to discuss ways of getting around the problem. That article made it to the front page, so here are my thoughts on how to circumvent Optimization by Proxy (OBP). Given that the previous article was belabored over at least a year and a half, this one will be decidedly less solid, more like a structured brainstorm in which you are invited to participate.</p>\n<p>In the comments of the previous article I was pointed to <a href=\"/lw/1ws/the_importance_of_goodharts_law\">The Importance of Goodhart's Law</a>, a great article, which includes a section on mitigation. Examining those solutions in the context of OBP seems like a good skeleton to build on.</p>\n<p>The first solution class is 'Hansonian Cynicism'. In combination with awareness of the pattern, pointing out that various processes (such as organizations) are not actually optimizing around their stated goal, but some proxy, creates cognitive dissonance for the thinking person. This sounds more like a motivation to find a solution than a solution itself. At best, knowing what goes wrong, you can use the process in a way that is informed by its weaknesses. Handling with care may mitigate some symptoms, but it doesn't make the problems go away.</p>\n<p>The second solution class mentioned is 'Better Measures'. That is indeed what is usually attempted. The 'purist' approach to this is to work hard on finding a computable definition of the target quality. I cannot exclude the possibility of cases where this is feasible no immediate examples come to mind. The proxies that I have in mind are deeply human (quality, relevance, long-term growth) and boil down to figuring out what is 'good', thus, computing them is no small matter. <a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a> is the extreme end of this approach, boiling a few oceans in the process, certainly not immediately applicable.</p>\n<p>A pragmatic approach to Better Measures is to simply monitor better, making the proxy more complex and therefore harder to manipulate. Discussion with Chronos in the comments of the original article was along those lines. By integrating user activity trails, Google makes it harder to game the search engine. I would imagine that if they integrated those logs with Google Analytics and Google Accounts, they would significantly raise the bar for gaming the system, at the expense of user privacy. Of course by removing most amateur and white/gray-hat SEOs from the pool, and given the financial incentives that exist, they would make it significantly more lucrative to game the system, and therefore the serious black hat SEOs that can resort to botnets, phishing and networks of hacked sites would end up being the only games in town. But I digress. Enriching the proxy with more and more parameters is a pragmatic solution that should work in the short term as a part of the arms race against manipulators, but does not look like a general or permanent solution from where I'm standing.</p>\n<p><a id=\"more\"></a></p>\n<p>A special case of 'Better Measures' is that of better incentive alignment. From Charlie Munger's speech <a href=\"http://ycombinator.com/munger.html\">A Lesson on Elementary, Worldly Wisdom As It Relates To Investment Management &amp; Business</a>:</p>\n<blockquote>\n<p>From all business, my favorite case on incentives is Federal Express. The heart and soul of their system&mdash;which creates the integrity of the product&mdash;is having all their airplanes come to one place in the middle of the night and shift all the packages from plane to plane. If there are delays, the whole operation can't deliver a product full of integrity to Federal Express customers.</p>\n<p>And it was always screwed up. They could never get it done on time. They tried everything&mdash;moral suasion, threats, you name it. And nothing worked.</p>\n<p>Finally, somebody got the idea to pay all these people not so much an hour, but so much a shift&mdash;and when it's all done, they can all go home. Well, their problems cleared up overnight.</p>\n</blockquote>\n<p>In fact, my initial example was a form of naturally occurring optimization by proxy, where the incentives of the actors are aligned. I guess stock grants and options are another way to align employee incentives with company incentives. As far as I can tell, this has not been generalised either, and does not seem to reliably work in all cases, but where it does work, it may well be a silver bullet that cuts through all the other layers of the problem.</p>\n<p>Before discussing the third and more promising avenue, I'd like to look at one unorthodox 'Better Measures' approach that came up while writing the original article. Assume that the proxy involves possessing the target quality to produce, and faking it is an NP-complete problem. The only real-world case where I can see an analog to this is cryptography. Perhaps we can stretch OPB such that WWII cryptography can be seen as an example of it. By encrypting with Enigma and keeping their keys secret (the proxies), the Axis forces aimed to maintain the secrecy of their communications (the target quality). When the allies were able to crack Enigma, this basic assumption stopped being reliable. Modern cryptography makes this actually feasible. As long as the keys don't fall into the wrong hands, and assuming no serious flaws in the cryptographic algorithms used, the fact that a document can be decrypted with someone's public key (the proxy) authenticates that document to the owner of the key (the target quality). While this works in cryptography, it may be stretching the OBP analogy too far. On the other hand, there may be a way to transfer this strategy to solve other OBP problems that I have not yet seen. If you have any thoughts around this, please put them forward in the comments.</p>\n<p>The third class of solutions is 'Human Discretion'. This is divided in two, diametrically opposite solutions. One is 'Hierarchical rule', inspired by the ideas of Mencius Moldbug. Managers are the masters of all their subordinates, and the slaves of their higher-ups. No rules are written, so no proxies to manipulate. Except of course, for human discretion itself. Besides the tremendous potential for corruption, this does not transfer well to automated systems. Laws may be a luxury for humans, but for machines, code is everything. There is no law-independent discretion that a machine can apply, even if threatened with obliteration. The opposite of that is what the article calls 'Left anarchist Ideas'. I think that puts too much of a political slant to an idea that is much more general. I call it simply 'distribution'. The idea here is that if decisions are taken locally, there is no big juicy proxy to manipulate, but it is splintered to multitudes of local proxies, each different than the other. I think this is the way that evolution can be seen to deal with this issue. If for instance we see the immune system as an optimizer by proxy, the ability of some individuals to survive a virus that kills others is a demonstration of the fact that the virus has not fooled everyone's immune system. Perhaps the individuals that survived are vulnerable to other threats, but this would mean that a perfect storm of diseases that exploit everyone's weaknesses would have to affect a population at the same time to extinguish it. Not exactly a common phenomenon. Nature's resilience through diversity usually saves the day.</p>\n<p>So distribution seems to be a promising avenue that deserves further examination. The use case that I usually gravitate towards is that of the spread of news. Before top-down mass media, news spread from mouth to mouth. Humans seem to have a gossip protocol hard-coded into their social function centre that works well for this task. To put it simply, we spread relevant information to gain status, and the receivers of this information do the same, until the information is well-known between all those that are reachable and interested. Mass media took over this function for a time, especially with regard to news that was of general interest but of course on a social circle level the old mechanisms kept working uninterrupted. With the advent of social networks, the old mechanisms are reasserting themselves, at scale. The asymmetric following model of Twitter seems well-suited for this scenario and re-tweeting also helps broadcast news further than the original receivers. Twitter is now often seen as a primary news source, where news breaks before it makes the headlines, even if the signal to noise ratio is low. What is interesting in this model is that there is a human decision at each point of re-broadcast. However, by the properties of scale-free networks, it does not require too many decisions for a piece of information to spread throughout the network. Users that spread false information or 'spam' are usually isolated from the graph, and therefore end up with little or no influence at all (with a caveat for socially advantageous falsities). Bear in mind that Twitter is not built or optimised around this model, so these effects appear only approximately. There are a number of changes that should make these effects much more pronounced, but this is a topic for another post. What should be noted is that contrary to popular belief, this hybrid man-machine system of news transmission scales pretty well. Just because human judgment is involved in multiple steps of the process, it doesn't make the system reliably slower, since nobody is on the critical path, and nobody has the responsibility of filtering all the content. A few decisions here and there are enough to keep the system working well.</p>\n<p>Transferring this social-graph approach to search is less than straightforward. Again, looking at human societies pre-search engine, people would develop reputation for knowledge in a specific field. Questions in their field of expertise find their way to them sooner or later. If an expert did not have an answer but another did, a shift in subjective, implicit reputation would occur, which if repeated on multiple occasions would result in a shift of the the relative trust that the community places on the two experts. Applying this to internet search does not seem immediately feasible, but search engines like Aardvark and Q&amp;A sites like StackOverflow and Yahoo! Answers seem to be heading in such a direction. Wikipedia, by having a network of editors trusted in certain fields also exhibits similar characteristics. The answer isn't as obvious in search as it is in news, and if algorithmic search engines disappeared tomorrow the world wouldn't have an plan B immediately at hand, the outline of an alternative is beginning to appear.</p>\n<p>To conclude, the progression I see in both news and search is this:</p>\n<ol>\n<li>mouth-to-mouth</li>\n<li>top-down human curated</li>\n<li>algorithmic</li>\n<li>node-to-node (distributed)</li>\n</ol>\n<p>In news this is loosely instantiated as: gossip -&gt; newspapers -&gt; social news sites -&gt; twitter-like horizontal diffusion, and in search the equivalents are: community experts -&gt; libraries / human-curated online directories -&gt; algorithmic search engines -&gt; social(?) search / Q&amp;A sites. There seems to be a pattern where things are coming full circle from horizontal to vertical and back to horizontal, where the intermediate vertical step is a stopgap to allow our natural mechanisms to adapt to the new technology, scale, and vastness of information, but ultimately managing to live up to the challenge. There may be some optimism involved in my assessment as the events described have not really taken place yet. The application of this pattern to other instances of OBP such as governments and large organizations is not something I feel I could undertake for now, but I do suspect that OBP can provide a general, if not conclusive, argument for distribution of decision making as the ultimately stable state of affairs, without considering smarter than human AGI/FAI singletons.</p>\n<p><strong>Update:</strong> Apparently, <a href=\"http://gigaom.com/2010/05/28/digg-wants-to-be-the-twitter-of-news/\">Digg's going horizontal</a>. This should be interesting.</p>\n<p><strong>Update2:</strong> I had mixed up vertical and horizontal. Another form of left-right dyslexia?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HgiWHaxnAEtg3uEfM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 24, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "2956", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fTu69HzLSXqWgj9ib", "YtvZxRpZjcFNwJecS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-28T15:47:55.075Z", "modifiedAt": null, "url": null, "title": "This is your brain on ambiguity", "slug": "this-is-your-brain-on-ambiguity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.602Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y2iEgbzDdYzKRrYiQ/this-is-your-brain-on-ambiguity", "pageUrlRelative": "/posts/Y2iEgbzDdYzKRrYiQ/this-is-your-brain-on-ambiguity", "linkUrl": "https://www.lesswrong.com/posts/Y2iEgbzDdYzKRrYiQ/this-is-your-brain-on-ambiguity", "postedAtFormatted": "Friday, May 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20This%20is%20your%20brain%20on%20ambiguity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThis%20is%20your%20brain%20on%20ambiguity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY2iEgbzDdYzKRrYiQ%2Fthis-is-your-brain-on-ambiguity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=This%20is%20your%20brain%20on%20ambiguity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY2iEgbzDdYzKRrYiQ%2Fthis-is-your-brain-on-ambiguity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY2iEgbzDdYzKRrYiQ%2Fthis-is-your-brain-on-ambiguity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1624, "htmlBody": "<p>Let's look at <a href=\"/lw/290/blue_and_yellowtinted_choices/\">one more optical illusion</a> which reveals important features of how our brains perform inference, and suggests how better awareness of these processes of inference can lead to improved thinking, including in our daily lives. This time around the theme is ambiguity.</p>\n<h2>The spinning dancer</h2>\n<p style=\"text-align:center\"><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Spinning_Dancer.gif/220px-Spinning_Dancer.gif\" alt=\"Spinning Dancer\" width=\"220\" height=\"293\" /></p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/The_Spinning_Dancer\">spinning dancer</a> is a remarkable piece of work. Do you see the dancer pivoting clockwise on her left foot? Or counterclockwise on her right? If you're at all like me, you're now seeing one or the other - but if you look at the picture for some time, you'll suddenly see the dancer spinning in the opposite direction. It may help (for reasons I'll explain below) to focus on the pivot foot or thereabouts, mentally blocking out the rest of the image. Can you pick a direction on purpose? (Part of what makes investigating the human mind fascinating <a href=\"/lw/25d/too_busy_to_think_about_life/1wxe\">hobby</a> is finding out what others' brains can do that mine can't, and vice versa. Initially I had no control at all, but interestingly, over the course of writing this post I got much better.)</p>\n<p>Focusing on the foot helps explain how the illusion works: the image provides no clue to distinguish between \"toes front\" or \"heel front\", it's all a dark shadow with no depth information. What we see is a foot-shaped cutout growing and shrinking due to a foreshortening effect, alternately to the left and right. This information is perfectly compatible with either direction of spin. (The rest of the image is more of the same; the entire image is functionally equivalent to a dark bar growing and shrinking in alternate directions.) The interesting question is then, <em>why does our consciousness insist on reporting that we're seeing a perfectly <strong>un</strong>ambiguous direction of motion</em>? We're not seeing an ambiguous dancer: we're seeing one that is clearly spinning one way - until the \"flip\" happens, and we see her just as clearly spinning the other way.</p>\n<h2><a id=\"more\"></a>Ambiguity, uncertainty and inference<br /></h2>\n<p>This is reminiscent of the way our explicit models of the world have trouble dealing with <a href=\"/lw/pd/configurations_and_amplitude/\">quantum uncertainty</a> - our intuition is that things must happen one way or the other, \"as if the half-silvered mirror did different things on different occasions\", as if the dancer actually was spinning one way then another. In the latter case at least, sober reflection tells us this can only be in the map, not in the territory - the animated GIF isn't being changed right under our noses.</p>\n<p>\"Perception is inference from incomplete information\", says Jaynes - I have <a href=\"/lw/1o6/adaptive_bias/\">noted previously</a> how this may bring insight into where our biases come from. The spinning dancer tells us something about how it feels, from the inside, to perform inference under uncertainty. That is what ambiguity is - a particular kind of uncertainty. Not the one that results from a paucity of information: the dancer illusion works <em>because</em> the image is quite detailed, not in spite of it. Rather, it is uncertainty that results from having too many hypotheses available, and lacking some crucial information to distinguish the correct one among them.</p>\n<p>(Abstractly, we see that it is best to be right about something, and worst to be wrong; to be uncertain is somewere in the middle. Our visual system has a different opinion, and prefers the following ranking: right &gt; wrong &gt; uncertain. \"Make a decision, even if it is the wrong one, we can always revise it later.\" Actually, many of our decision processes have that same bias; I'll revisit this topic when I post at greater length about \"<a href=\"/lw/26x/open_thread_may_2010/1yv4\">real options</a>\". For now, the topic I'm sticking to is ambiguity as a particular type of uncertainty.)</p>\n<h2>Spinning words</h2>\n<blockquote>\n<p>Last night I shot an elephant in my pajamas. How he got in my pajamas, I&rsquo;ll never know. - Groucho Marx</p>\n</blockquote>\n<p>It takes some effort to construct ambiguous pictures, whereas anything expressed in words seems to enjoy a head start. This is great news for people with a sense of humor: linguistic ambiguity is a constant <a href=\"http://www.ling.upenn.edu/~beatrice/humor/ambiguous-recommendations.html#lexical\">source</a> of <a href=\"http://boingboing.net/2010/02/04/fun-with-punctuation.html\">merriment</a>. In fact, though there are many competing <a href=\"/lw/1s4/open_thread_february_2010_part_2/1mps\">theories of humor</a>, it makes at least some sense to see ambiguity and its related themes of frame-crossing, reinterpretation as playing a key role in humor in general.</p>\n<p>To some, ambiguity is much less funny. Some professions, including jurists, proponents of synthetic languages such as Lojban or Loglan, and software engineers see ambiguity as an evil to be uprooted. The <a href=\"http://www.projectcartoon.com/cartoon/2\">most famous cartoon</a> (excepting perhaps some Dilbert favorites posted near every cubicle) among software professionals is an extended lament about the consequences of ambiguity in human language. Ambiguity is the source of unmeasurable amounts of confusion and even hurt among people relying on the written word to communicate - an increasingly common circumstance in the Internet age. Not that oral communication is exempt; but at least in most cases it offers more effective mechanisms for error correction.</p>\n<h2>Sacrifice, duality, reframing: the powers of ambiguity<br /></h2>\n<p>And yet, vexing as it may be to acknowledge it, instrumentally effective thinking often seems to <em>rely</em> on ambiguity.</p>\n<p>In the ancient game of Go, there is a certain level of play that can only be reached by mastering the art of <a href=\"http://senseis.xmp.net/?Sacrificing\">sacrifice</a>. Now, in some cases this may be part of a pre-established plan: stones that you are defending will get a better position if, as a preliminary, you place a stone within enemy territory solely in preparation for a move that threatens to rescue it. In many instances, though, sacrifice involves <em>redefining</em> a previously valuable stone or set of stones as \"sacrifice stones\". What is called \"light\" play often involves <em>deliberately ambiguous</em> moves, in which you have a plan to abandon one of several stones without depending on which one; the opponent's play will determine that.</p>\n<p>Another example involves the mathematical theme of <a href=\"http://en.wikipedia.org/wiki/Duality_%28mathematics%29\">duality</a>, where a given kind of structure can be expressed in one of two ways which are equivalent in meaning but rely on different tools or operations. Some problems turn out to be easier to solve in a domain which is the dual of that where they were originally formulated. Unfortunately, my math having gone rusty for quite a while, I don't recall offhand any examples that I feel familiar enough with to discuss here, but to give you a hint of the flavor, consider a frequent \"trick\" in computing probabilities: when asked the probability of A (say, \"at least two of the people in this room share a birthday\") it's often easier to consider the probablity of not-A (\"no two people share a birthday\") and taking the complement.</p>\n<p>Another way to exploit ambiguity turns up in the domain of interpersonal relationships, in the guise of \"framing\" and \"<a href=\"http://en.wikipedia.org/wiki/Reframing\">reframing</a>\". Some of the advice in Alicorn's <a href=\"/lw/2a5/on_enjoying_disagreeable_company/\">recent post</a>, for instance, involves casting around for reframes - ways of interpreting behaviour that you dislike in a person that make these behaviours tolerable or understandable instead of irritating and repulsive. In <em>Stumbling on Happiness</em> Daniel Gilbert argues that ambiguity is a key component of psychological resilience. A person's success in life is partially determined by their ability to <em>redefine</em> their values, sense of happiness, etc. on the fly, in answer to the difficulties they encounter. This is only possible if our interpretation of the world contains lots of ambiguity to start with.</p>\n<h2>Artificial ambiguity<br /></h2>\n<p>As readers of LessWrong, \"hobbyists of the mind\", we can often gain insight into our human minds by framing questions about constructed minds - by forcing ourselves to confront the <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">design space of possible minds</a>. A corollary attitude is to be cautious about the reverse process - unreflectively projecting some perceived attribute of human minds, such as randomness or reliability, into a <em>necessary</em> property of artificial intelligences. Still, I have come so far in this post in large part to ponder whether ambiguity is a necessary capability of minds-in-general, rather than a human design flaw.</p>\n<p>Douglas Hofstadter's <a href=\"http://en.wikipedia.org/wiki/Fluid_Concepts_and_Creative_Analogies\">Fluid Concepts and Creative Analogies</a> discusses fluidity, a theme that strikes me as closely related to ambiguity (and is explicitly discussed throughout). Hofstadter is fond of \"microdomains\", simplified settings where human intelligence neverthelesss still easily exceeds what we can, as a rule, program machines to do. A favorite of mine is \"Do this!\" exercises, which inspired Robert French's Tabletop research program: two people are seated at a typical restaurant or caf&eacute; table, with similar (or very different) implements on each side of the table: plates, forks, knives, glasses... One person touches an item and challenges the other to \"do this!\".</p>\n<p>Ambiguity arises in the Tabletop domain because exact mappings between the two sides may not exist, but \"analogical\" mappings often do: your wine glass maps to my water glass, for instance. The fun starts when <em>more than one</em> plausible analogical mapping suggests itself. Your lone wine glass maps either to my wine glass (paired with a glass of water) or to my salt shaker (the only non-plate unpaired item on my side).</p>\n<h2>Conclusions</h2>\n<p>I <a href=\"/lw/1zu/compartmentalization_as_a_passive_phenomenon/1the\">suspect</a> that efficient cross-domain generalization requires dealing with ambiguity and analogy. Back in the human mind-space, few theories of the scientific process deal explicitly with ambiguity and analogy, Pickering's <a href=\"/lw/1ld/stigmergy_and_pickerings_mangle/\">mangle</a> being a <a href=\"/lw/1ld/stigmergy_and_pickerings_mangle/1e78\">notable exception</a>. In some vague sense it seems to me that ambiguity provides \"degrees of freedom\" which are necessary to the conceptions of plans, and their flexible execution - including sacrificing, changing domains or notations, and reframing. I expect it to be a major theme of instrumental rationality, in other words, and therefore would like to delineate a more precise formulation of this vague intuition.</p>\n<p>This post has explored some themes I'm likely to return to, or that form some kind of groundwork (for instance when I touch on \"programming as a rationalist skill\" later on). But more importantly, it is intended to encourage further discussions of these themes from <a href=\"/lw/26l/what_are_our_domains_of_expertise_a_marketplace/\">other perspectives</a> than mine.</p>\n<p>What do you know about ambiguity?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y2iEgbzDdYzKRrYiQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 24, "extendedScore": null, "score": 5.907248131665709e-07, "legacy": true, "legacyId": "2969", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Let's look at <a href=\"/lw/290/blue_and_yellowtinted_choices/\">one more optical illusion</a> which reveals important features of how our brains perform inference, and suggests how better awareness of these processes of inference can lead to improved thinking, including in our daily lives. This time around the theme is ambiguity.</p>\n<h2 id=\"The_spinning_dancer\">The spinning dancer</h2>\n<p style=\"text-align:center\"><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Spinning_Dancer.gif/220px-Spinning_Dancer.gif\" alt=\"Spinning Dancer\" width=\"220\" height=\"293\"></p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/The_Spinning_Dancer\">spinning dancer</a> is a remarkable piece of work. Do you see the dancer pivoting clockwise on her left foot? Or counterclockwise on her right? If you're at all like me, you're now seeing one or the other - but if you look at the picture for some time, you'll suddenly see the dancer spinning in the opposite direction. It may help (for reasons I'll explain below) to focus on the pivot foot or thereabouts, mentally blocking out the rest of the image. Can you pick a direction on purpose? (Part of what makes investigating the human mind fascinating <a href=\"/lw/25d/too_busy_to_think_about_life/1wxe\">hobby</a> is finding out what others' brains can do that mine can't, and vice versa. Initially I had no control at all, but interestingly, over the course of writing this post I got much better.)</p>\n<p>Focusing on the foot helps explain how the illusion works: the image provides no clue to distinguish between \"toes front\" or \"heel front\", it's all a dark shadow with no depth information. What we see is a foot-shaped cutout growing and shrinking due to a foreshortening effect, alternately to the left and right. This information is perfectly compatible with either direction of spin. (The rest of the image is more of the same; the entire image is functionally equivalent to a dark bar growing and shrinking in alternate directions.) The interesting question is then, <em>why does our consciousness insist on reporting that we're seeing a perfectly <strong>un</strong>ambiguous direction of motion</em>? We're not seeing an ambiguous dancer: we're seeing one that is clearly spinning one way - until the \"flip\" happens, and we see her just as clearly spinning the other way.</p>\n<h2 id=\"Ambiguity__uncertainty_and_inference\"><a id=\"more\"></a>Ambiguity, uncertainty and inference<br></h2>\n<p>This is reminiscent of the way our explicit models of the world have trouble dealing with <a href=\"/lw/pd/configurations_and_amplitude/\">quantum uncertainty</a> - our intuition is that things must happen one way or the other, \"as if the half-silvered mirror did different things on different occasions\", as if the dancer actually was spinning one way then another. In the latter case at least, sober reflection tells us this can only be in the map, not in the territory - the animated GIF isn't being changed right under our noses.</p>\n<p>\"Perception is inference from incomplete information\", says Jaynes - I have <a href=\"/lw/1o6/adaptive_bias/\">noted previously</a> how this may bring insight into where our biases come from. The spinning dancer tells us something about how it feels, from the inside, to perform inference under uncertainty. That is what ambiguity is - a particular kind of uncertainty. Not the one that results from a paucity of information: the dancer illusion works <em>because</em> the image is quite detailed, not in spite of it. Rather, it is uncertainty that results from having too many hypotheses available, and lacking some crucial information to distinguish the correct one among them.</p>\n<p>(Abstractly, we see that it is best to be right about something, and worst to be wrong; to be uncertain is somewere in the middle. Our visual system has a different opinion, and prefers the following ranking: right &gt; wrong &gt; uncertain. \"Make a decision, even if it is the wrong one, we can always revise it later.\" Actually, many of our decision processes have that same bias; I'll revisit this topic when I post at greater length about \"<a href=\"/lw/26x/open_thread_may_2010/1yv4\">real options</a>\". For now, the topic I'm sticking to is ambiguity as a particular type of uncertainty.)</p>\n<h2 id=\"Spinning_words\">Spinning words</h2>\n<blockquote>\n<p>Last night I shot an elephant in my pajamas. How he got in my pajamas, I\u2019ll never know. - Groucho Marx</p>\n</blockquote>\n<p>It takes some effort to construct ambiguous pictures, whereas anything expressed in words seems to enjoy a head start. This is great news for people with a sense of humor: linguistic ambiguity is a constant <a href=\"http://www.ling.upenn.edu/~beatrice/humor/ambiguous-recommendations.html#lexical\">source</a> of <a href=\"http://boingboing.net/2010/02/04/fun-with-punctuation.html\">merriment</a>. In fact, though there are many competing <a href=\"/lw/1s4/open_thread_february_2010_part_2/1mps\">theories of humor</a>, it makes at least some sense to see ambiguity and its related themes of frame-crossing, reinterpretation as playing a key role in humor in general.</p>\n<p>To some, ambiguity is much less funny. Some professions, including jurists, proponents of synthetic languages such as Lojban or Loglan, and software engineers see ambiguity as an evil to be uprooted. The <a href=\"http://www.projectcartoon.com/cartoon/2\">most famous cartoon</a> (excepting perhaps some Dilbert favorites posted near every cubicle) among software professionals is an extended lament about the consequences of ambiguity in human language. Ambiguity is the source of unmeasurable amounts of confusion and even hurt among people relying on the written word to communicate - an increasingly common circumstance in the Internet age. Not that oral communication is exempt; but at least in most cases it offers more effective mechanisms for error correction.</p>\n<h2 id=\"Sacrifice__duality__reframing__the_powers_of_ambiguity\">Sacrifice, duality, reframing: the powers of ambiguity<br></h2>\n<p>And yet, vexing as it may be to acknowledge it, instrumentally effective thinking often seems to <em>rely</em> on ambiguity.</p>\n<p>In the ancient game of Go, there is a certain level of play that can only be reached by mastering the art of <a href=\"http://senseis.xmp.net/?Sacrificing\">sacrifice</a>. Now, in some cases this may be part of a pre-established plan: stones that you are defending will get a better position if, as a preliminary, you place a stone within enemy territory solely in preparation for a move that threatens to rescue it. In many instances, though, sacrifice involves <em>redefining</em> a previously valuable stone or set of stones as \"sacrifice stones\". What is called \"light\" play often involves <em>deliberately ambiguous</em> moves, in which you have a plan to abandon one of several stones without depending on which one; the opponent's play will determine that.</p>\n<p>Another example involves the mathematical theme of <a href=\"http://en.wikipedia.org/wiki/Duality_%28mathematics%29\">duality</a>, where a given kind of structure can be expressed in one of two ways which are equivalent in meaning but rely on different tools or operations. Some problems turn out to be easier to solve in a domain which is the dual of that where they were originally formulated. Unfortunately, my math having gone rusty for quite a while, I don't recall offhand any examples that I feel familiar enough with to discuss here, but to give you a hint of the flavor, consider a frequent \"trick\" in computing probabilities: when asked the probability of A (say, \"at least two of the people in this room share a birthday\") it's often easier to consider the probablity of not-A (\"no two people share a birthday\") and taking the complement.</p>\n<p>Another way to exploit ambiguity turns up in the domain of interpersonal relationships, in the guise of \"framing\" and \"<a href=\"http://en.wikipedia.org/wiki/Reframing\">reframing</a>\". Some of the advice in Alicorn's <a href=\"/lw/2a5/on_enjoying_disagreeable_company/\">recent post</a>, for instance, involves casting around for reframes - ways of interpreting behaviour that you dislike in a person that make these behaviours tolerable or understandable instead of irritating and repulsive. In <em>Stumbling on Happiness</em> Daniel Gilbert argues that ambiguity is a key component of psychological resilience. A person's success in life is partially determined by their ability to <em>redefine</em> their values, sense of happiness, etc. on the fly, in answer to the difficulties they encounter. This is only possible if our interpretation of the world contains lots of ambiguity to start with.</p>\n<h2 id=\"Artificial_ambiguity\">Artificial ambiguity<br></h2>\n<p>As readers of LessWrong, \"hobbyists of the mind\", we can often gain insight into our human minds by framing questions about constructed minds - by forcing ourselves to confront the <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">design space of possible minds</a>. A corollary attitude is to be cautious about the reverse process - unreflectively projecting some perceived attribute of human minds, such as randomness or reliability, into a <em>necessary</em> property of artificial intelligences. Still, I have come so far in this post in large part to ponder whether ambiguity is a necessary capability of minds-in-general, rather than a human design flaw.</p>\n<p>Douglas Hofstadter's <a href=\"http://en.wikipedia.org/wiki/Fluid_Concepts_and_Creative_Analogies\">Fluid Concepts and Creative Analogies</a> discusses fluidity, a theme that strikes me as closely related to ambiguity (and is explicitly discussed throughout). Hofstadter is fond of \"microdomains\", simplified settings where human intelligence neverthelesss still easily exceeds what we can, as a rule, program machines to do. A favorite of mine is \"Do this!\" exercises, which inspired Robert French's Tabletop research program: two people are seated at a typical restaurant or caf\u00e9 table, with similar (or very different) implements on each side of the table: plates, forks, knives, glasses... One person touches an item and challenges the other to \"do this!\".</p>\n<p>Ambiguity arises in the Tabletop domain because exact mappings between the two sides may not exist, but \"analogical\" mappings often do: your wine glass maps to my water glass, for instance. The fun starts when <em>more than one</em> plausible analogical mapping suggests itself. Your lone wine glass maps either to my wine glass (paired with a glass of water) or to my salt shaker (the only non-plate unpaired item on my side).</p>\n<h2 id=\"Conclusions\">Conclusions</h2>\n<p>I <a href=\"/lw/1zu/compartmentalization_as_a_passive_phenomenon/1the\">suspect</a> that efficient cross-domain generalization requires dealing with ambiguity and analogy. Back in the human mind-space, few theories of the scientific process deal explicitly with ambiguity and analogy, Pickering's <a href=\"/lw/1ld/stigmergy_and_pickerings_mangle/\">mangle</a> being a <a href=\"/lw/1ld/stigmergy_and_pickerings_mangle/1e78\">notable exception</a>. In some vague sense it seems to me that ambiguity provides \"degrees of freedom\" which are necessary to the conceptions of plans, and their flexible execution - including sacrificing, changing domains or notations, and reframing. I expect it to be a major theme of instrumental rationality, in other words, and therefore would like to delineate a more precise formulation of this vague intuition.</p>\n<p>This post has explored some themes I'm likely to return to, or that form some kind of groundwork (for instance when I touch on \"programming as a rationalist skill\" later on). But more importantly, it is intended to encourage further discussions of these themes from <a href=\"/lw/26l/what_are_our_domains_of_expertise_a_marketplace/\">other perspectives</a> than mine.</p>\n<p>What do you know about ambiguity?</p>", "sections": [{"title": "The spinning dancer", "anchor": "The_spinning_dancer", "level": 1}, {"title": "Ambiguity, uncertainty and inference", "anchor": "Ambiguity__uncertainty_and_inference", "level": 1}, {"title": "Spinning words", "anchor": "Spinning_words", "level": 1}, {"title": "Sacrifice, duality, reframing: the powers of ambiguity", "anchor": "Sacrifice__duality__reframing__the_powers_of_ambiguity", "level": 1}, {"title": "Artificial ambiguity", "anchor": "Artificial_ambiguity", "level": 1}, {"title": "Conclusions", "anchor": "Conclusions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "60 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kjArXFinD3deRZNRu", "5vZD32EynD9n94dhr", "TFZhCehLARTTWedd8", "nK5jraMp7E4xPvuNv", "tnWRXkcDi5Tw9rzXw", "NLNnwd4TeKj8PBvZa", "mJB9LBDkKuqxqDhkM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-29T16:58:34.925Z", "modifiedAt": null, "url": null, "title": "Aspergers Survey Re-results", "slug": "aspergers-survey-re-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:39.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nTqmCQvqsrJrryE2c/aspergers-survey-re-results", "pageUrlRelative": "/posts/nTqmCQvqsrJrryE2c/aspergers-survey-re-results", "linkUrl": "https://www.lesswrong.com/posts/nTqmCQvqsrJrryE2c/aspergers-survey-re-results", "postedAtFormatted": "Saturday, May 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Aspergers%20Survey%20Re-results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAspergers%20Survey%20Re-results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnTqmCQvqsrJrryE2c%2Faspergers-survey-re-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Aspergers%20Survey%20Re-results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnTqmCQvqsrJrryE2c%2Faspergers-survey-re-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnTqmCQvqsrJrryE2c%2Faspergers-survey-re-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>Followup to: <a href=\"/lw/28w/aspergers_poll_results_lw_is_nerdier_than_the/\">Aspergers Poll results</a></p>\n<p>Since my <a href=\"/lw/28l/do_you_have_highfunctioning_aspergers_syndrome/\">little survey</a> about the degree to which the Less Wrong community has a preponderance of people with systematizing personality types, I've been collecting responses only from those people who considered taking the survey after looking at the original post, but didn't, in order to combat <a href=\"http://en.wikipedia.org/wiki/Opinion_poll#Nonresponse_bias\">nonresponse bias</a>.&nbsp;</p>\n<p>82 people responded to the initial survey, and another 186 responded after the request for non-responders to respond. In the initial survey, 26% of responders scored 32+ (which is considered to be a \"high\" score, and out of a group of Cambridge mathematics students, 7 out of 11 who scored over 32 were said to fit the full diagnostic criteria for aspergers syndrome after being interviewed).</p>\n<p>In the combined survey of 82 initial responders and 186 \"second\"-responders, this increased to 28%. In the original survey, 5% of respondents said they had already been diagnosed with aspergers syndrome, and in the combined survey this increased to 7.5%.</p>\n<p>Overall, this indicates that response bias is probably not significantly skewing our picture of the LW audience, though, as always, it is possible that there is a more sophisticated bias at work and that these 268 people are not representative of LW.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nTqmCQvqsrJrryE2c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "2974", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pCJQMzvrYRkCbs4Tu", "vHDk5xr9JDC64rb8T"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-29T16:59:20.996Z", "modifiedAt": null, "url": null, "title": "Composting fruitless debates", "slug": "composting-fruitless-debates", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:03.707Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eogtHAPgt3Pft6AEd/composting-fruitless-debates", "pageUrlRelative": "/posts/eogtHAPgt3Pft6AEd/composting-fruitless-debates", "linkUrl": "https://www.lesswrong.com/posts/eogtHAPgt3Pft6AEd/composting-fruitless-debates", "postedAtFormatted": "Saturday, May 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Composting%20fruitless%20debates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComposting%20fruitless%20debates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeogtHAPgt3Pft6AEd%2Fcomposting-fruitless-debates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Composting%20fruitless%20debates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeogtHAPgt3Pft6AEd%2Fcomposting-fruitless-debates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeogtHAPgt3Pft6AEd%2Fcomposting-fruitless-debates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1102, "htmlBody": "<!-- Weeding for an audience: composting fruitless debates -->\n<p><em>Why do long, uninspiring, and seemingly-childish debates sometimes emerge even in a community like LessWrong?&nbsp;  And what can we do about them?&nbsp;   The key is to recognize the potentially harsh environmental effect of an audience, and use a dying debate to fertilize a more sheltered private conversation.</em></p>\n<p>Let me start by saying that LessWrong generally makes excellent use of public debate, and naming two things I <em>don't</em> believe are solely responsible for fruitless debates here: rationalization biases and self-preservation<a href=\"#rat\"><sup>1</sup></a>.&nbsp;   When your super-important debate grows into a thorny mess, the usual aversion to say various forms of \"just drop it\" are about signaling that:</p>\n<ol>\n<li> <em>you're not skilled enough</em> to continue arguing, so you'd look bad, </li>\n<li> <em>the other person isn't worth your time</em>, in which case they'd be publicly insulted and compelled to continue with at least one self-defense comment, extending the conflict, or </li>\n<li> <em>the other person is right</em>, which would risk spreading what appear to be falsehoods. </li>\n</ol>\n<p><strong>\"Stop the wrongness\"</strong>, the last concern, is in my opinion the most perisistent here simply because it is the least misguided.&nbsp;  It's practically the name of the site.&nbsp;  Many LessWrong users seem to share a sincere, often altruistic desire to share truth, abolish falsehood, and overcome conflict.&nbsp;  Public debate is a selection mechanism generally used <em>very effectively</em> here to grow and harvest good arguments.&nbsp;   But we can still benefit from diffusing the weed-like quibbling that sometimes shows up in the harsh environment of debate, and for that you need a response that avoids the problematic signals above.&nbsp;  So try this:</p>\n<blockquote>\"I'm worried that debating this more here won't be useful to others, but I want to keep working on it with you, so I'm responding via private message.&nbsp;  Let's post on it again once we either agree or better organize our disagreement.&nbsp;  Hopefully at least one of us will learn and refine a new argument from this conversation.\"</blockquote>\n<p>Take a moment to see how this carefully avoids (1)-(3).&nbsp;  Then you can try changing the tone of the private message to be more collaborative than competitive; the change in medium will help mark the transition.&nbsp;  This way you'll each be less afraid of <em>having been wrong</em> and more concerned with <em>learning to be right</em>, so rationalization bias will also be diminished.&nbsp;  As well, much social drama can disintegrate without the pressure of the audience environment (I imagine this might contribute to couples fighting more after they have children, though this is just anecdotal speculation).&nbsp; Despite being perhaps obvious, these effects are not to be underestimated!</p>\n<p>But hang on...&nbsp; if you're convinced someone is <em>very wrong</em>, is it <em>okay</em> to leave such a debate hanging midstream in public?&nbsp; Why doesn't \"stop the wrongness\" trump our social concerns and compel us to flog away at our respective puddles of horsemeat?  <a id=\"more\"></a></p>\n<p>The usual necessary condition for you to wind up in a pointless online debate is that you're <em>very convinced</em> your co-poster is wrong, but isn't obviously wrong enough to attract negative comment Karma.&nbsp;  So you keep posting in an attempt to \"clear up\" the issue for everyone else, or at the very least dilute the false content.&nbsp;  But somewhere along the line, you might end up with something that looks to you like:</p>\n<blockquote><br />You (1 point):&nbsp; Very smart/correct response to the parent discussion. <br />Other (1 point):&nbsp; Convincing but wrong/vague/irrelevant comment that somehow got upvoted. <br />You (0 points):&nbsp; Correction, or return to parent discussion. <br />Other (0 points):&nbsp; More trickily wrong/vague/irrelevant stuff. <br />&hellip;</blockquote>\n<p>If you've read these from the outside, it often looks more like</p>\n<blockquote><br />User A (1 point):&nbsp; &laquo;something that mildly interests you&raquo; <br />User B (1 point):&nbsp; &laquo;tolerable response&raquo; <br />User A (0 points):&nbsp; &laquo;quibbling you don't care to read&raquo; <br />User B (0 points):&nbsp; &laquo;yup, definitely don't care&raquo; <br />&hellip;</blockquote>\n<p>By now, or with very little practice, you should be able to tell from the inside when a conversation is entering a public failure spiral.&nbsp;  When that happens, no matter how smart and right and well-intentioned <em>your</em> responses might be, your co-poster's responses are still going to show up at around the same density.&nbsp;  And since the average non-negative-scoring comment on LessWrong is pretty correct, <em>if your assessment is accurate, then your opponent's wrong comments are going to drag down the average rightness more than you're going to raise it</em>.&nbsp;  And you're certainly not helping the situation if <em>you're</em> wrong.&nbsp;  <em>And</em>, on LessWrong in particular, the Recent Comments feed just diverts more attention to the trickily wrong ideas the more you argue against them.</p>\n<p>So, <em>in the unfortuate event</em><a href=\"#unf\"><sup>2</sup></a> that a debate starts going awry (which thankfully is a relatively rare occurrence here), the task is to stop the spiral without looking bad, and not making the other person look bad is a necessary ingredient (else they'll continue the spiral in self-defense).&nbsp;  The \"switch to private message\" signal I suggest above is a pretty self-sufficient way to do that.&nbsp;  Though it stands pretty well on its own, it becomes more effective the more we send and interpret it consistently as a community, and the more we follow through with posting conclusions to the resulting private debates.</p>\n<p>Thus, above all, \"switch to private\" requests must be <em>neither presented nor interpreted</em> as an insult to either debater.&nbsp;  Not everyone is as courageous as <a href=\"/lw/1re/blame_theory/\">cousin_it</a> to publicly change their minds.&nbsp; We must praise the effort of a debater who aims to work through a contentious argument privately.&nbsp;  We must consider \"switch to private\" as a sign of respect for the other debater: it shows value for his/her interaction.&nbsp;  We must not judge \"winners\" or \"losers\" of a debate as a function of who requests privacy first.&nbsp;  Then perhaps awkward social battles can become collaborations as routinely as mulched weeds can become crops.</p>\n<hr />\n<p><strong>Footnotes</strong></p>\n<p><a name=\"rat\"><sup>1</sup></a> Regarding rationalization biases, most posters here know the difference between trying to <em>learn what right is</em> and trying to <em>change what right is</em> in an attempt to escape confirming past errors.&nbsp;  Regarding classic self-preservation, in this case it yields a desire to signal intelligence at the cost of deceiving the audience.&nbsp;  This is pretty misguided, not just because it's usually immoral, but because the LessWrong audience isn't easily deceived...&nbsp; especially by a long and uninteresting-looking debate.&nbsp;  I'm sure these factors are present too, but the last one most needed addressing.</p>\n<p><a name=\"unf\"><sup>2</sup></a> I can't praise LessWrong enough for the fact that its debates tend to be much more fruitful than elsewhere.&nbsp; I very much don't want to stifle the open and productive arguments that go on here; only to encourage a way out of the undesirable ones.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eogtHAPgt3Pft6AEd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 18, "extendedScore": null, "score": 5.9103780086506e-07, "legacy": true, "legacyId": "2975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qPYYjRxgHFX4wZ77c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-30T03:50:25.401Z", "modifiedAt": null, "url": null, "title": "Significance of Compression Rate Method", "slug": "significance-of-compression-rate-method", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:28.752Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Daniel_Burfoot", "createdAt": "2009-03-12T02:28:50.970Z", "isAdmin": false, "displayName": "Daniel_Burfoot"}, "userId": "XhcXE3Qk5adX6v2Cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vBQ2wmsDjBnGK28RK/significance-of-compression-rate-method", "pageUrlRelative": "/posts/vBQ2wmsDjBnGK28RK/significance-of-compression-rate-method", "linkUrl": "https://www.lesswrong.com/posts/vBQ2wmsDjBnGK28RK/significance-of-compression-rate-method", "postedAtFormatted": "Sunday, May 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Significance%20of%20Compression%20Rate%20Method&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASignificance%20of%20Compression%20Rate%20Method%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvBQ2wmsDjBnGK28RK%2Fsignificance-of-compression-rate-method%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Significance%20of%20Compression%20Rate%20Method%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvBQ2wmsDjBnGK28RK%2Fsignificance-of-compression-rate-method", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvBQ2wmsDjBnGK28RK%2Fsignificance-of-compression-rate-method", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1624, "htmlBody": "<p><strong>Summary</strong>: The significance of the Compression Rate Method (CRM) is that it justifies a form of empirical inquiry into aspects of reality that have previously resisted systematic interrogation. Some examples of potential investigations are described. A key hypothesis is discussed, and the link between empirical science and lossless data compression is emphasized.</p>\n<p>In my <a href=\"/lw/29l/development_of_compression_rate_method/\">previous post</a>, the protagonist Sophie developed a modified version of the scientific method. It consists of the following steps:</p>\n<ol>\n<li>Obtain a large database T related to a phenomenon of interest. </li>\n<li>Develop a theory of the phenomenon, and instantiate the theory as a compression program. </li>\n<li>Test the theory by invoking the compressor on T and measuring the net codelength achieved (encoded data plus length of compressor). </li>\n<li>Given two rival theories of the phenomenon, prefer the one that achieves a shorter net codelength. </li>\n</ol>\n<p>This modified version preserves two of the essential attributes of the traditional method. First, it employs theoretical speculation, but guides and constrains that speculation using empirical observations. Second, it permits <a href=\"/lw/29z/link_strong_inference/\">Strong Inference</a> by allowing the field to make decisive comparisons between rival theories.</p>\n<p>The key difference between the CRM and the traditional method is that the former does not depend on the use of controlled experiments. For that reason, it justifies inquiries into aspects of empirical reality that have never before been systematically interrogated. The kind of scientific theories that are tested by the CRM depend on the type of measurements in the database target T. If T contains measurements related to physical experiments, the theories of physics will be necessary to compress it. Other types of data lead to other types of science. Consider the following examples:</p>\n<p><a id=\"more\"></a></p>\n<ol>\n<li>Set up a camera next to a highway, and record the stream of passing cars. To compress the resulting data, you will need to develop a computational understanding of the visual appearance of automobiles. You will need theories of hubcaps, windshields, license plates, car categories, and so on. </li>\n<li>Position some microphones in the tops of trees and start recording. A major source of variation in the resulting data will be <a href=\"http://en.wikipedia.org/wiki/Bird_vocalization\">bird vocalization</a>. To compress the data, you will need to find ways to differentiate between bird songs and bird calls, tools to identify species-characteristic vocalizations, and maps showing the typical ranges of various species. In other words, this type of inquiry will be a <em>computational</em> version of the traditional study of bird vocalization carried out by ornithologists. </li>\n<li>Construct a database by using large quantities of English text. To compress this database you will need an advanced computational understanding of English. You will need dictionaries, rules of grammar, word-sense disambiguation tools, and, more generally, theories of linguistics. </li>\n<li>Convince Mark Zuckerberg to give you the Facebook image database. One obvious property of this dataset is that it contains an enormous number of <em>faces</em>. To compress it, you will need theories of the appearance of faces. These theories will be highly related to work on face modeling in graphics - see <a href=\"http://gravis.cs.unibas.ch/Sigg99.html\">here</a> for example. </li>\n<li>Generate a huge database of economic data such as home prices, interest and exchange rate fluctuations, business inventories and sales, unemployment and welfare applications, and so on. To compress this database, you will need theories of economics. </li>\n</ol>\n<p>It should be emphasized that when in the above list it says \"You will need theories of X\", this simultaneously means that \"You can test and refine theories of X\", and \"You can prove the superiority of your pet theory of X\" by demonstrating the codelengths it achieves on an appropriate dataset. So if you are a linguist and you want to demonstrate the validity of <a href=\"http://en.wikipedia.org/wiki/X-bar_theory\">X-bar theory</a>, you build an X-bar compressor and test it on the large text database. If you are an economist and you want to prove the truth of <a href=\"http://en.wikipedia.org/wiki/Austrian_business_cycle_theory\">Austrian Business Cycle Theory</a>, you build ABCT into a compressor and invoke it on the economics database. If a theory can't be packaged into a compressor for <em>some</em> real world dataset, then it's probably not scientific anyway (more later on the <a href=\"http://en.wikipedia.org/wiki/Demarcation_problem\">problem of demarcation</a>).</p>\n<p>(It's also worth noting the dedication to truth, and the simultaneous contempt for petty academic affiliation games, indicated by a rigorous adherence to the compression principle. If you develop a new theory of linguistics and use it to set a new record on the benchmark text database, I will hail you as a great linguist. I will publish your papers in my journal, nominate you for awards, and approve your grant applications. It does not matter if you are a teenage college dropout living with your parents.)</p>\n<p>The inquiries described in the above list make an important implicit assumption, which can be called the <strong>Reusability Hypothesis</strong>:</p>\n<blockquote>The abstractions useful for practical applications are also useful for compression.</blockquote>\n<p>&nbsp;</p>\n<p>Thus one very practical application in relation to the Facebook database is the detection and recognition of faces. This application depends on the existence of a \"face\" abstraction. So the hypothesis implies that this face abstraction will be useful for compression as well. Similarly, with regards to the ornithology example, one can imagine that the ability to recognize bird song would be very useful to bird-watchers and environmentalists, who might want to monitor the activity, population fluctuations, and migration patterns of certain species. Here the Reusability Hypothesis implies that the ability to recognize bird-song will also be useful to compress the treetop sound database.</p>\n<p>The linguistics example is worth examining because of its connection to a point made in the <a href=\"/lw/299/preface_to_a_proposal_for_a_new_mode_of_inquiry/\">preface</a> about overmathematization and the distinction of complex deduction vs. complex induction. The field of computational linguistics is highly mathematized, and one can imagine that in principle some complex mathematics might be useful to achieve text compression. But by far the simplest, and probably the most powerful, tool for text compression is just a <em>dictionary</em>. Consider the following sentence:</p>\n<blockquote>John went to the liquor store and bought a bottle of _______ .</blockquote>\n<p>&nbsp;</p>\n<p>Now, if a compressor knows nothing about English text, it will have to encode the new word letter-by-letter, for a cost of Nlog(26), where N is the length of the word (I assume N is encoded separately, at a basically fixed cost). But a compressor equipped with a dictionary will have to pay only log(Wn), where Wn is the number of words of length N in the dictionary. This is a substantial savings, since Wn is much smaller than 26^N. Of course, more advanced techniques, such as methods that take into account part of speech information, will lead to further improvement.</p>\n<p>The point of the above example is that the dictionary is highly useful, but it does not involve any kind of complex mathematics. Compare a dictionary to the theory of general relativity. Both can be used to make predictions, and so both should be viewed (under my definition) as legitimate scientific theories. And they are both complex, but complex in <em>opposite ways</em>. GR is deductively complex, since it requires sophisticated mathematics to use correctly, but inductively simple, because it requires only a few parameters to specify. In contrast the dictionary is deductively simple, since it can be used by anyone who can read, but inductively complex, since it requires many bits to specify.</p>\n<p>Another point made in the preface was that this approach involves <em>empirical science</em> as a core component, as opposed to being primarily about mathematics and algorithm-design (as I consider modern AI to be). Some people may be confused by this, since most people consider data compression to be a relatively minor subfield of computer science. The key realization is that lossless data compression can only be achieved through empirical science. This is because data compression is impossible for arbitrary inputs: no compressor can ever achieve compression rates of less than M bits when averaged over all M-bit strings. Lossless compressors work because they contain an implicit assertion about the type of data on which they will be invoked, and they will fail to achieve compression if that assertion turns out to be false. In the case of image compressors like PNG, the assertion is that, in natural images, the values of adjacent pixels are highly correlated. PNG can exploit this structure to achieve compression, and conversely, the fact that PNG achieves compression for a given image means that it has the assumed structure. In other words, PNG contains an empirical hypothesis about the structure of visual reality, and the fact that it works is empirical evidence in favor of the hypothesis. Now, this pixel-correlation structure of natural images is completely basic and obvious. The proposal, then, is to go further: to develop increasingly sophisticated theories of visual reality and test those theories using the compression principle.</p>\n<p>Still, it may not be obvious why this kind of research would be any different from other work on data compression - people are, after all, constantly publishing new types of compression algorithms. The key difference is the emphasis on <em>large-scale</em> compression; this completely changes the character of the problem. To see why, consider the problem of building a car. If you only want to build a single car, then you just hack it together by hand. You build all the parts using machine tools and then fit them together. The challenge is to minimize the time- and dollar-cost of the manual labor. This is an interesting challenge, but the challenge of building ten million cars is entirely different. If you're going to build ten million cars, then it makes sense to start by building a <em>factory</em>. This will be a big up-front cost, but it will pay for itself by reducing the marginal cost of each additional car. Analogously, when attempting to compress huge databases, it becomes worthwhile to build sophisticated computational tools into the compressor. And ultimately the development of these advanced tools is the real goal.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vBQ2wmsDjBnGK28RK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 4, "extendedScore": null, "score": 5.911727157227396e-07, "legacy": true, "legacyId": "2977", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["meTmrcCwSsDYmo5DZ", "F7pihuF8qRbJ6WTue", "KYZfkZfy3RNxbZoYo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-30T21:16:19.449Z", "modifiedAt": "2021-10-11T23:49:17.491Z", "url": null, "title": "Diseased thinking: dissolving questions about disease", "slug": "diseased-thinking-dissolving-questions-about-disease", "viewCount": null, "lastCommentedAt": "2021-09-02T01:19:21.236Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/895quRDaK6gR2rM82/diseased-thinking-dissolving-questions-about-disease", "pageUrlRelative": "/posts/895quRDaK6gR2rM82/diseased-thinking-dissolving-questions-about-disease", "linkUrl": "https://www.lesswrong.com/posts/895quRDaK6gR2rM82/diseased-thinking-dissolving-questions-about-disease", "postedAtFormatted": "Sunday, May 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Diseased%20thinking%3A%20dissolving%20questions%20about%20disease&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADiseased%20thinking%3A%20dissolving%20questions%20about%20disease%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F895quRDaK6gR2rM82%2Fdiseased-thinking-dissolving-questions-about-disease%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Diseased%20thinking%3A%20dissolving%20questions%20about%20disease%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F895quRDaK6gR2rM82%2Fdiseased-thinking-dissolving-questions-about-disease", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F895quRDaK6gR2rM82%2Fdiseased-thinking-dissolving-questions-about-disease", "socialPreviewImageUrl": "http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif", "question": false, "authorIsUnreviewed": false, "wordCount": 2738, "htmlBody": "<p><strong>Related to: </strong><a href=\"https://www.lesswrong.com/lw/nm/disguised_queries/\">Disguised Queries</a>, <a href=\"https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/\">Words as Hidden Inferences</a>, <a href=\"https://www.lesswrong.com/lw/of/dissolving_the_question/\">Dissolving the Question</a>, <a href=\"https://www.lesswrong.com/lw/24o/eight_short_studies_on_excuses/\">Eight Short Studies on Excuses</a></p><blockquote><em>Today&#x27;s therapeutic ethos, which celebrates curing and disparages judging, expresses the liberal disposition to assume that crime and other problematic behaviors reflect social or biological causation. While this absolves the individual of responsibility, it also strips the individual of personhood, and moral dignity</em></blockquote><p><em>            -- George Will, <a href=\"http://townhall.com/Common/PrintPage.aspx?g=761ecc84-473b-4123-bf28-c4fc179a9d3f&t=c\">townhall.com</a></em></p><p>Sandy is a morbidly obese woman looking for advice. </p><p>Her husband has no sympathy for her, and tells her she obviously needs to stop eating like a pig, and would it kill her to go to the gym once in a while? </p><p>Her doctor tells her that obesity is primarily genetic, and recommends the diet pill orlistat and a consultation with a surgeon about gastric bypass. </p><p>Her sister tells her that obesity is a perfectly valid lifestyle choice, and that fat-ism, equivalent to racism, is society&#x27;s way of keeping her down.</p><p>When she tells each of her friends about the opinions of the others, things really start to heat up.</p><p>Her husband accuses her doctor and sister of absolving her of personal responsibility with feel-good platitudes that in the end will only prevent her from getting the willpower she needs to start a real diet.</p><p>Her doctor accuses her husband of ignorance of the real causes of obesity and of the most effective treatments, and accuses her sister of legitimizing a dangerous health risk that could end with Sandy in hospital or even dead.</p><p>Her sister accuses her husband of being a jerk, and her doctor of trying to medicalize her behavior in order to turn it into a &quot;condition&quot; that will keep her on pills for life and make lots of money for Big Pharma.</p><p>Sandy is fictional, but similar conversations happen every day, not only about obesity but about a host of other marginal conditions that some consider character flaws, others diseases, and still others normal variation in the human condition. Attention deficit disorder, internet addiction, social anxiety disorder (as one skeptic said, didn&#x27;t we used to call this &quot;shyness&quot;?), alcoholism, chronic fatigue, oppositional defiant disorder (&quot;didn&#x27;t we used to call this being a teenager?&quot;), compulsive gambling, homosexuality, Aspergers&#x27; syndrome, antisocial personality, even depression have all been placed in two or more of these categories by different people.</p><p></p><p>Sandy&#x27;s sister may have a point, but this post will concentrate on the debate between her husband and her doctor, with the understanding that the same techniques will apply to evaluating her sister&#x27;s opinion. The disagreement between Sandy&#x27;s husband and doctor centers around the idea of &quot;disease&quot;. If obesity, depression, alcoholism, and the like are diseases, most people default to the doctor&#x27;s point of view; if they are not diseases, they tend to agree with the husband.</p><p>The debate over such marginal conditions is in many ways a debate over whether or not they are &quot;real&quot; diseases. The usual surface level arguments trotted out in favor of or against the proposition are generally inconclusive, but this post will apply a host of techniques previously discussed on Less Wrong to illuminate the issue.</p><p></p><p><strong>What is Disease?</strong></p><p></p><p>In <a href=\"https://www.lesswrong.com/lw/nm/disguised_queries/\">Disguised Queries</a> , Eliezer demonstrates how a word refers to a cluster of objects related upon multiple axes. For example, in a company that sorts red smooth translucent cubes full of vanadium from blue furry opaque eggs full of palladium, you might invent the word &quot;rube&quot; to designate the red cubes, and another &quot;blegg&quot;, to designate the blue eggs. Both words are useful because they &quot;carve reality at the joints&quot; - they refer to two completely separate classes of things which it&#x27;s practically useful to keep in separate categories. Calling something a &quot;blegg&quot; is a quick and easy way to describe its color, shape, opacity, texture, and chemical composition. It may be that the odd blegg might be purple rather than blue, but in general the characteristics of a blegg remain sufficiently correlated that &quot;blegg&quot; is a useful word. If they weren&#x27;t so correlated - if blue objects were equally likely to be palladium-containing-cubes as vanadium-containing-eggs, then the word &quot;blegg&quot; would be a waste of breath; the characteristics of the object would remain just as mysterious to your partner after you said &quot;blegg&quot; as they were before.</p><p>&quot;Disease&quot;, like &quot;blegg&quot;, suggests that certain characteristics always come together. A rough sketch of some of the characteristics we expect in a disease might include:</p><p>1. Something caused by the sorts of thing you study in biology: proteins, bacteria, ions, viruses, genes.</p><p>2. Something involuntary and completely immune to the operations of free will</p><p>3. Something rare; the vast majority of people don&#x27;t have it</p><p>4. Something unpleasant; when you have it, you want to get rid of it</p><p>5. Something discrete; a graph would show two widely separate populations, one with the disease and one without, and not a normal distribution.</p><p>6. Something commonly treated with science-y interventions like chemicals and radiation.</p><p>Cancer satisfies every one of these criteria, and so we have no qualms whatsoever about classifying it as a disease. It&#x27;s a type specimen, <a href=\"https://www.lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/\">the sparrow as opposed to the ostrich</a>. The same is true of heart attack, the flu, diabetes, and many more.</p><p>Some conditions satisfy a few of the criteria, but not others. Dwarfism seems to fail (5), and it might get its status as a disease only after studies show that the supposed dwarf falls way out of normal human height variation. Despite the best efforts of transhumanists, it&#x27;s hard to convince people that aging is a disease, partly because it fails (3). Calling homosexuality a disease is a poor choice for many reasons, but one of them is certainly (4): it&#x27;s not necessarily unpleasant. </p><p>The marginal conditions mentioned above are also in this category. Obesity arguably sort-of-satisfies criteria (1), (4), and (6), but it would be pretty hard to make a case for (2), (3), and (5).</p><p>So, is obesity really a disease? Well, is Pluto really a planet? Once we state that obesity satisfies some of the criteria but not others, it is meaningless to talk about an additional fact of whether it &quot;really deserves to be a disease&quot; or not.</p><p>If it weren&#x27;t for those pesky <a href=\"https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/#more\">hidden inferences</a>...</p><p><strong>Hidden Inferences From Disease Concept</strong></p><p>The state of the disease node, meaningless in itself, is used to predict several other nodes with non-empirical content. In English: we make value decisions based on whether we call something a &quot;disease&quot; or not.</p><p>If something is a real disease, the patient deserves our sympathy and support; for example, <a href=\"http://www.theonion.com/articles/loved-ones-recall-local-mans-cowardly-battle-with,772/\">cancer sufferers must universally be described as &quot;brave&quot;</a>. If it is not a real disease, people are more likely to get our condemnation; for example Sandy&#x27;s husband who calls her a &quot;pig&quot; for her inability to control her eating habits. The difference between &quot;shyness&quot; and &quot;social anxiety disorder&quot; is that people with the first get called &quot;weird&quot; and told to man up, and people with the second get special privileges and the sympathy of those around them.</p><p>And if something is a real disease, it is socially acceptable (maybe even mandated) to seek medical treatment for it. If it&#x27;s not a disease, medical treatment gets derided as a &quot;quick fix&quot; or an &quot;abdication of personal responsibility&quot;. I have talked to several doctors who are uncomfortable suggesting gastric bypass surgery, even in people for whom it is medically indicated, because they believe it is morally wrong to turn to medicine to solve a character issue.</p><span><figure><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif\" class=\"draft-image center\" style=\"width:91%\" /></figure></span><p>While a condition&#x27;s status as a &quot;real disease&quot; ought to be meaningless as a &quot;hanging node&quot; after the status of all other nodes have been determined, it has acquired political and philosophical implications because of its role in determining whether patients receive sympathy and whether they are permitted to seek medical treatment.</p><p>If we can determine whether a person should get sympathy, and whether they should be allowed to seek medical treatment, independently of the central node &quot;disease&quot; or of the criteria that feed into it, we will have successfully unasked the question &quot;are these marginal conditions real diseases&quot; and cleared up the confusion.</p><p><strong>Sympathy or Condemnation?</strong></p><p>Our attitudes toward people with marginal conditions mainly reflect a deontologist libertarian (libertarian as in &quot;free will&quot;, not as in &quot;against government&quot;) model of blame. In this concept, people make decisions using their free will, a spiritual entity operating free from biology or circumstance. People who make good decisions are intrinsically good people and deserve good treatment; people who make bad decisions are intrinsically bad people and deserve bad treatment. But people who make bad decisions for reasons that are outside of their free will may not be intrinsically bad people, and may therefore be absolved from deserving bad treatment. For example, if a normally peaceful person has a brain tumor that affects areas involved in fear and aggression, they go on a crazy killing spree, and then they have their brain tumor removed and become a peaceful person again, many people would be willing to accept that the killing spree does not reflect negatively on them or open them up to deserving bad treatment, since it had biological and not spiritual causes.</p><p>Under this model, deciding whether a condition is biological or spiritual becomes very important, and the rationale for worrying over whether something &quot;is a real disease&quot; or not is plain to see. Without figuring out this extremely difficult question, we are at risk of either blaming people for things they don&#x27;t deserve, or else letting them off the hook when they commit a sin, both of which, to libertarian deontologists, would be terrible things. But determining whether marginal conditions like depression have a spiritual or biological cause is difficult, and no one knows how to do it reliably.</p><p>Determinist consequentialists can do better. We believe it&#x27;s biology all the way down. Separating spiritual from biological illnesses is impossible and unnecessary. Every condition, from brain tumors to poor taste in music, is &quot;biological&quot; insofar as it is encoded in things like cells and proteins and follows laws based on their structure.</p><p>But determinists don&#x27;t just ignore the very important differences between brain tumors and poor taste in music. Some biological phenomena, like poor taste in music, are encoded in such a way that they are extremely vulnerable to what we can call social influences: praise, condemnation, introspection, and the like. Other biological phenomena, like brain tumors, are completely immune to such influences. This allows us to develop a more useful model of blame.</p><p>The consequentialist model of blame is very different from the deontological model. Because all actions are biologically determined, none are more or less metaphysically blameworthy than others, and none can mark anyone with the metaphysical status of &quot;bad person&quot; and make them &quot;deserve&quot; bad treatment. Consequentialists don&#x27;t on a primary level want anyone to be treated badly, full stop; thus <a href=\"http://yudkowsky.net/obsolete/tmol-faq.html#theo_free\">is it written</a>: &quot;Saddam Hussein doesn&#x27;t deserve so much as a stubbed toe.&quot; But if consequentialists don&#x27;t believe in punishment for its own sake, they do believe in punishment for the sake of, well, consequences. Hurting bank robbers may not be a good in and of itself, but it will prevent banks from being robbed in the future. And, one might infer, although alcoholics may not deserve condemnation, societal condemnation of alcoholics makes alcoholism a less attractive option.</p><p>So here, at last, is a rule for which diseases we offer sympathy, and which we offer condemnation: if giving condemnation instead of sympathy decreases the incidence of the disease enough to be worth the hurt feelings, condemn; otherwise, sympathize. Though the rule is based on philosophy that the majority of the human race would disavow, it leads to intuitively correct consequences. Yelling at a cancer patient, shouting &quot;How dare you allow your cells to divide in an uncontrolled manner like this; is that the way your mother raised you??!&quot; will probably make the patient feel pretty awful, but it&#x27;s not going to cure the cancer. Telling a lazy person &quot;Get up and do some work, you worthless bum,&quot; very well might cure the laziness. The cancer is a biological condition immune to social influences; the laziness is a biological condition susceptible to social influences, so we try to socially influence the laziness and not the cancer.</p><p>The question &quot;Do the obese deserve our sympathy or our condemnation,&quot; then, is asking whether condemnation is such a useful treatment for obesity that its utility outweights the disutility of hurting obese people&#x27;s feelings. This question may have different answers depending on the particular obese person involved, the particular person doing the condemning, and the availability of other methods for treating the obesity, which brings us to...</p><p><strong>The Ethics of Treating Marginal Conditions</strong></p><p>If a condition is susceptible to social intervention, but an effective biological therapy for it also exists, is it okay for people to use the biological therapy instead of figuring out a social solution? My gut answer is &quot;Of course, why wouldn&#x27;t it be?&quot;, but apparently lots of people find this controversial for some reason. </p><p>In a libertarian deontological system, throwing biological solutions at spiritual problems might be disrespectful or dehumanizing, or a band-aid that doesn&#x27;t affect the deeper problem. To someone who believes it&#x27;s biology all the way down, this is much less of a concern. </p><p>Others complain that the existence of an easy medical solution prevents people from learning personal responsibility. But here <a href=\"http://www.nickbostrom.com/ethics/statusquo.pdf\">we see the status-quo bias at work, and so can apply a preference reversal test</a>. If people really believe learning personal responsibility is more important than being not addicted to heroin, we would expect these people to support deliberately addicting schoolchildren to heroin so they can develop personal responsibility by coming off of it. Anyone who disagrees with this somewhat shocking proposal must believe, on some level, that having people who are not addicted to heroin is more important than having people develop whatever measure of personal responsibility comes from kicking their heroin habit the old-fashioned way. </p><p>But the most convincing explanation I have read for why so many people are opposed to medical solutions for social conditions is a signaling explanation by Robin Hans...wait! no!...by Katja Grace. On <a href=\"http://meteuphoric.wordpress.com/2009/09/21/why-do-animal-lovers-want-animals-to-feel-pain/\">her blog</a>, she says:</p><blockquote><em>...the situation reminds me of a pattern in similar cases I have noticed before. It goes like this. Some people make personal sacrifices, supposedly toward solving problems that don\u2019t threaten them personally. They sort recycling, buy free range eggs, buy fair trade, campaign for wealth redistribution etc. Their actions are seen as virtuous. They see those who don\u2019t join them as uncaring and immoral. A more efficient solution to the problem is suggested. It does not require personal sacrifice. People who have not previously sacrificed support it. Those who have previously sacrificed object on grounds that it is an excuse for people to get out of making the sacrifice. The supposed instrumental action, as the visible sign of caring, has become virtuous in its own right. Solving the problem effectively is an attack on the moral people.</em></blockquote><p>A case in which some people eat less enjoyable foods and exercise hard to avoid becoming obese, and then campaign against a pill that makes avoiding obesity easy demonstrates some of the same principles.</p><p>There are several very reasonable objections to treating any condition with drugs, whether it be a classical disease like cancer or a marginal condition like alcoholism. The drugs can have side effects. They can be expensive. They can build dependence. They may later be found to be placebos whose efficacy was overhyped by dishonest pharmaceutical advertising.. They may raise ethical issues with children, the mentally incapacitated, and other people who cannot decide for themselves whether or not to take them. But these issues do not magically become more dangerous in conditions typically regarded as &quot;character flaws&quot; rather than &quot;diseases&quot;, and the same good-enough solutions that work for cancer or heart disease will work for alcoholism and other such conditions (but see <a href=\"https://www.lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/230p\">here</a>).</p><p>I see no reason why people who want effective treatment for a condition should be denied it or stigmatized for seeking it, whether it is traditionally considered &quot;medical&quot; or not.</p><p><strong>Summary</strong></p><p>People commonly debate whether social and mental conditions are real diseases. This masquerades as a medical question, but its implications are mainly social and ethical. We use the concept of disease to decide who gets sympathy, who gets blame, and who gets treatment.</p><p>Instead of continuing the fruitless &quot;disease&quot; argument, we should address these questions directly. Taking a determinist consequentialist position allows us to do so more effectively. We should blame and stigmatize people for conditions where blame and stigma are the most useful methods for curing or preventing the condition, and we should allow patients to seek treatment whenever it is available and effective.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 7, "xHjy88N2uJvGdgzfw": 4, "FtT2T9bRbECCGYxrL": 1, "P3Wd3f2cWqqvQxDQS": 1, "DsdbQhWAnPqfzo4Yw": 2, "9DmA84e4ZvYoYu6q8": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "895quRDaK6gR2rM82", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 359, "baseScore": 425, "extendedScore": null, "score": 0.000714, "legacy": true, "legacyId": "2980", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif", "canonicalSequenceId": "NHXY86jBahi968uW4", "canonicalCollectionSlug": "codex", "canonicalBookId": "jF58hKP9ZLzgy22Jr", "canonicalNextPostSlug": "the-categories-were-made-for-man-not-man-for-the-categories", "canonicalPrevPostSlug": "and-i-show-you-how-deep-the-rabbit-hole-goes", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 425, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related to: </strong><a href=\"https://www.lesswrong.com/lw/nm/disguised_queries/\">Disguised Queries</a>, <a href=\"https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/\">Words as Hidden Inferences</a>, <a href=\"https://www.lesswrong.com/lw/of/dissolving_the_question/\">Dissolving the Question</a>, <a href=\"https://www.lesswrong.com/lw/24o/eight_short_studies_on_excuses/\">Eight Short Studies on Excuses</a></p><blockquote><em>Today's therapeutic ethos, which celebrates curing and disparages judging, expresses the liberal disposition to assume that crime and other problematic behaviors reflect social or biological causation. While this absolves the individual of responsibility, it also strips the individual of personhood, and moral dignity</em></blockquote><p><em>            -- George Will, <a href=\"http://townhall.com/Common/PrintPage.aspx?g=761ecc84-473b-4123-bf28-c4fc179a9d3f&amp;t=c\">townhall.com</a></em></p><p>Sandy is a morbidly obese woman looking for advice. </p><p>Her husband has no sympathy for her, and tells her she obviously needs to stop eating like a pig, and would it kill her to go to the gym once in a while? </p><p>Her doctor tells her that obesity is primarily genetic, and recommends the diet pill orlistat and a consultation with a surgeon about gastric bypass. </p><p>Her sister tells her that obesity is a perfectly valid lifestyle choice, and that fat-ism, equivalent to racism, is society's way of keeping her down.</p><p>When she tells each of her friends about the opinions of the others, things really start to heat up.</p><p>Her husband accuses her doctor and sister of absolving her of personal responsibility with feel-good platitudes that in the end will only prevent her from getting the willpower she needs to start a real diet.</p><p>Her doctor accuses her husband of ignorance of the real causes of obesity and of the most effective treatments, and accuses her sister of legitimizing a dangerous health risk that could end with Sandy in hospital or even dead.</p><p>Her sister accuses her husband of being a jerk, and her doctor of trying to medicalize her behavior in order to turn it into a \"condition\" that will keep her on pills for life and make lots of money for Big Pharma.</p><p>Sandy is fictional, but similar conversations happen every day, not only about obesity but about a host of other marginal conditions that some consider character flaws, others diseases, and still others normal variation in the human condition. Attention deficit disorder, internet addiction, social anxiety disorder (as one skeptic said, didn't we used to call this \"shyness\"?), alcoholism, chronic fatigue, oppositional defiant disorder (\"didn't we used to call this being a teenager?\"), compulsive gambling, homosexuality, Aspergers' syndrome, antisocial personality, even depression have all been placed in two or more of these categories by different people.</p><p></p><p>Sandy's sister may have a point, but this post will concentrate on the debate between her husband and her doctor, with the understanding that the same techniques will apply to evaluating her sister's opinion. The disagreement between Sandy's husband and doctor centers around the idea of \"disease\". If obesity, depression, alcoholism, and the like are diseases, most people default to the doctor's point of view; if they are not diseases, they tend to agree with the husband.</p><p>The debate over such marginal conditions is in many ways a debate over whether or not they are \"real\" diseases. The usual surface level arguments trotted out in favor of or against the proposition are generally inconclusive, but this post will apply a host of techniques previously discussed on Less Wrong to illuminate the issue.</p><p></p><p><strong id=\"What_is_Disease_\">What is Disease?</strong></p><p></p><p>In <a href=\"https://www.lesswrong.com/lw/nm/disguised_queries/\">Disguised Queries</a> , Eliezer demonstrates how a word refers to a cluster of objects related upon multiple axes. For example, in a company that sorts red smooth translucent cubes full of vanadium from blue furry opaque eggs full of palladium, you might invent the word \"rube\" to designate the red cubes, and another \"blegg\", to designate the blue eggs. Both words are useful because they \"carve reality at the joints\" - they refer to two completely separate classes of things which it's practically useful to keep in separate categories. Calling something a \"blegg\" is a quick and easy way to describe its color, shape, opacity, texture, and chemical composition. It may be that the odd blegg might be purple rather than blue, but in general the characteristics of a blegg remain sufficiently correlated that \"blegg\" is a useful word. If they weren't so correlated - if blue objects were equally likely to be palladium-containing-cubes as vanadium-containing-eggs, then the word \"blegg\" would be a waste of breath; the characteristics of the object would remain just as mysterious to your partner after you said \"blegg\" as they were before.</p><p>\"Disease\", like \"blegg\", suggests that certain characteristics always come together. A rough sketch of some of the characteristics we expect in a disease might include:</p><p>1. Something caused by the sorts of thing you study in biology: proteins, bacteria, ions, viruses, genes.</p><p>2. Something involuntary and completely immune to the operations of free will</p><p>3. Something rare; the vast majority of people don't have it</p><p>4. Something unpleasant; when you have it, you want to get rid of it</p><p>5. Something discrete; a graph would show two widely separate populations, one with the disease and one without, and not a normal distribution.</p><p>6. Something commonly treated with science-y interventions like chemicals and radiation.</p><p>Cancer satisfies every one of these criteria, and so we have no qualms whatsoever about classifying it as a disease. It's a type specimen, <a href=\"https://www.lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/\">the sparrow as opposed to the ostrich</a>. The same is true of heart attack, the flu, diabetes, and many more.</p><p>Some conditions satisfy a few of the criteria, but not others. Dwarfism seems to fail (5), and it might get its status as a disease only after studies show that the supposed dwarf falls way out of normal human height variation. Despite the best efforts of transhumanists, it's hard to convince people that aging is a disease, partly because it fails (3). Calling homosexuality a disease is a poor choice for many reasons, but one of them is certainly (4): it's not necessarily unpleasant. </p><p>The marginal conditions mentioned above are also in this category. Obesity arguably sort-of-satisfies criteria (1), (4), and (6), but it would be pretty hard to make a case for (2), (3), and (5).</p><p>So, is obesity really a disease? Well, is Pluto really a planet? Once we state that obesity satisfies some of the criteria but not others, it is meaningless to talk about an additional fact of whether it \"really deserves to be a disease\" or not.</p><p>If it weren't for those pesky <a href=\"https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/#more\">hidden inferences</a>...</p><p><strong id=\"Hidden_Inferences_From_Disease_Concept\">Hidden Inferences From Disease Concept</strong></p><p>The state of the disease node, meaningless in itself, is used to predict several other nodes with non-empirical content. In English: we make value decisions based on whether we call something a \"disease\" or not.</p><p>If something is a real disease, the patient deserves our sympathy and support; for example, <a href=\"http://www.theonion.com/articles/loved-ones-recall-local-mans-cowardly-battle-with,772/\">cancer sufferers must universally be described as \"brave\"</a>. If it is not a real disease, people are more likely to get our condemnation; for example Sandy's husband who calls her a \"pig\" for her inability to control her eating habits. The difference between \"shyness\" and \"social anxiety disorder\" is that people with the first get called \"weird\" and told to man up, and people with the second get special privileges and the sympathy of those around them.</p><p>And if something is a real disease, it is socially acceptable (maybe even mandated) to seek medical treatment for it. If it's not a disease, medical treatment gets derided as a \"quick fix\" or an \"abdication of personal responsibility\". I have talked to several doctors who are uncomfortable suggesting gastric bypass surgery, even in people for whom it is medically indicated, because they believe it is morally wrong to turn to medicine to solve a character issue.</p><span><figure><img src=\"http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif\" class=\"draft-image center\" style=\"width:91%\"></figure></span><p>While a condition's status as a \"real disease\" ought to be meaningless as a \"hanging node\" after the status of all other nodes have been determined, it has acquired political and philosophical implications because of its role in determining whether patients receive sympathy and whether they are permitted to seek medical treatment.</p><p>If we can determine whether a person should get sympathy, and whether they should be allowed to seek medical treatment, independently of the central node \"disease\" or of the criteria that feed into it, we will have successfully unasked the question \"are these marginal conditions real diseases\" and cleared up the confusion.</p><p><strong id=\"Sympathy_or_Condemnation_\">Sympathy or Condemnation?</strong></p><p>Our attitudes toward people with marginal conditions mainly reflect a deontologist libertarian (libertarian as in \"free will\", not as in \"against government\") model of blame. In this concept, people make decisions using their free will, a spiritual entity operating free from biology or circumstance. People who make good decisions are intrinsically good people and deserve good treatment; people who make bad decisions are intrinsically bad people and deserve bad treatment. But people who make bad decisions for reasons that are outside of their free will may not be intrinsically bad people, and may therefore be absolved from deserving bad treatment. For example, if a normally peaceful person has a brain tumor that affects areas involved in fear and aggression, they go on a crazy killing spree, and then they have their brain tumor removed and become a peaceful person again, many people would be willing to accept that the killing spree does not reflect negatively on them or open them up to deserving bad treatment, since it had biological and not spiritual causes.</p><p>Under this model, deciding whether a condition is biological or spiritual becomes very important, and the rationale for worrying over whether something \"is a real disease\" or not is plain to see. Without figuring out this extremely difficult question, we are at risk of either blaming people for things they don't deserve, or else letting them off the hook when they commit a sin, both of which, to libertarian deontologists, would be terrible things. But determining whether marginal conditions like depression have a spiritual or biological cause is difficult, and no one knows how to do it reliably.</p><p>Determinist consequentialists can do better. We believe it's biology all the way down. Separating spiritual from biological illnesses is impossible and unnecessary. Every condition, from brain tumors to poor taste in music, is \"biological\" insofar as it is encoded in things like cells and proteins and follows laws based on their structure.</p><p>But determinists don't just ignore the very important differences between brain tumors and poor taste in music. Some biological phenomena, like poor taste in music, are encoded in such a way that they are extremely vulnerable to what we can call social influences: praise, condemnation, introspection, and the like. Other biological phenomena, like brain tumors, are completely immune to such influences. This allows us to develop a more useful model of blame.</p><p>The consequentialist model of blame is very different from the deontological model. Because all actions are biologically determined, none are more or less metaphysically blameworthy than others, and none can mark anyone with the metaphysical status of \"bad person\" and make them \"deserve\" bad treatment. Consequentialists don't on a primary level want anyone to be treated badly, full stop; thus <a href=\"http://yudkowsky.net/obsolete/tmol-faq.html#theo_free\">is it written</a>: \"Saddam Hussein doesn't deserve so much as a stubbed toe.\" But if consequentialists don't believe in punishment for its own sake, they do believe in punishment for the sake of, well, consequences. Hurting bank robbers may not be a good in and of itself, but it will prevent banks from being robbed in the future. And, one might infer, although alcoholics may not deserve condemnation, societal condemnation of alcoholics makes alcoholism a less attractive option.</p><p>So here, at last, is a rule for which diseases we offer sympathy, and which we offer condemnation: if giving condemnation instead of sympathy decreases the incidence of the disease enough to be worth the hurt feelings, condemn; otherwise, sympathize. Though the rule is based on philosophy that the majority of the human race would disavow, it leads to intuitively correct consequences. Yelling at a cancer patient, shouting \"How dare you allow your cells to divide in an uncontrolled manner like this; is that the way your mother raised you??!\" will probably make the patient feel pretty awful, but it's not going to cure the cancer. Telling a lazy person \"Get up and do some work, you worthless bum,\" very well might cure the laziness. The cancer is a biological condition immune to social influences; the laziness is a biological condition susceptible to social influences, so we try to socially influence the laziness and not the cancer.</p><p>The question \"Do the obese deserve our sympathy or our condemnation,\" then, is asking whether condemnation is such a useful treatment for obesity that its utility outweights the disutility of hurting obese people's feelings. This question may have different answers depending on the particular obese person involved, the particular person doing the condemning, and the availability of other methods for treating the obesity, which brings us to...</p><p><strong id=\"The_Ethics_of_Treating_Marginal_Conditions\">The Ethics of Treating Marginal Conditions</strong></p><p>If a condition is susceptible to social intervention, but an effective biological therapy for it also exists, is it okay for people to use the biological therapy instead of figuring out a social solution? My gut answer is \"Of course, why wouldn't it be?\", but apparently lots of people find this controversial for some reason. </p><p>In a libertarian deontological system, throwing biological solutions at spiritual problems might be disrespectful or dehumanizing, or a band-aid that doesn't affect the deeper problem. To someone who believes it's biology all the way down, this is much less of a concern. </p><p>Others complain that the existence of an easy medical solution prevents people from learning personal responsibility. But here <a href=\"http://www.nickbostrom.com/ethics/statusquo.pdf\">we see the status-quo bias at work, and so can apply a preference reversal test</a>. If people really believe learning personal responsibility is more important than being not addicted to heroin, we would expect these people to support deliberately addicting schoolchildren to heroin so they can develop personal responsibility by coming off of it. Anyone who disagrees with this somewhat shocking proposal must believe, on some level, that having people who are not addicted to heroin is more important than having people develop whatever measure of personal responsibility comes from kicking their heroin habit the old-fashioned way. </p><p>But the most convincing explanation I have read for why so many people are opposed to medical solutions for social conditions is a signaling explanation by Robin Hans...wait! no!...by Katja Grace. On <a href=\"http://meteuphoric.wordpress.com/2009/09/21/why-do-animal-lovers-want-animals-to-feel-pain/\">her blog</a>, she says:</p><blockquote><em>...the situation reminds me of a pattern in similar cases I have noticed before. It goes like this. Some people make personal sacrifices, supposedly toward solving problems that don\u2019t threaten them personally. They sort recycling, buy free range eggs, buy fair trade, campaign for wealth redistribution etc. Their actions are seen as virtuous. They see those who don\u2019t join them as uncaring and immoral. A more efficient solution to the problem is suggested. It does not require personal sacrifice. People who have not previously sacrificed support it. Those who have previously sacrificed object on grounds that it is an excuse for people to get out of making the sacrifice. The supposed instrumental action, as the visible sign of caring, has become virtuous in its own right. Solving the problem effectively is an attack on the moral people.</em></blockquote><p>A case in which some people eat less enjoyable foods and exercise hard to avoid becoming obese, and then campaign against a pill that makes avoiding obesity easy demonstrates some of the same principles.</p><p>There are several very reasonable objections to treating any condition with drugs, whether it be a classical disease like cancer or a marginal condition like alcoholism. The drugs can have side effects. They can be expensive. They can build dependence. They may later be found to be placebos whose efficacy was overhyped by dishonest pharmaceutical advertising.. They may raise ethical issues with children, the mentally incapacitated, and other people who cannot decide for themselves whether or not to take them. But these issues do not magically become more dangerous in conditions typically regarded as \"character flaws\" rather than \"diseases\", and the same good-enough solutions that work for cancer or heart disease will work for alcoholism and other such conditions (but see <a href=\"https://www.lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/230p\">here</a>).</p><p>I see no reason why people who want effective treatment for a condition should be denied it or stigmatized for seeking it, whether it is traditionally considered \"medical\" or not.</p><p><strong id=\"Summary\">Summary</strong></p><p>People commonly debate whether social and mental conditions are real diseases. This masquerades as a medical question, but its implications are mainly social and ethical. We use the concept of disease to decide who gets sympathy, who gets blame, and who gets treatment.</p><p>Instead of continuing the fruitless \"disease\" argument, we should address these questions directly. Taking a determinist consequentialist position allows us to do so more effectively. We should blame and stigmatize people for conditions where blame and stigma are the most useful methods for curing or preventing the condition, and we should allow patients to seek treatment whenever it is available and effective.</p>", "sections": [{"title": "What is Disease?", "anchor": "What_is_Disease_", "level": 1}, {"title": "Hidden Inferences From Disease Concept", "anchor": "Hidden_Inferences_From_Disease_Concept", "level": 1}, {"title": "Sympathy or Condemnation?", "anchor": "Sympathy_or_Condemnation_", "level": 1}, {"title": "The Ethics of Treating Marginal Conditions", "anchor": "The_Ethics_of_Treating_Marginal_Conditions", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "354 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 355, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4FcxgdvdQP45D6Skg", "3nxs2WYDGzJbzcLMp", "Mc6QcrsbH5NRXbCRX", "gFMH3Cqw4XxwL69iy", "WBw8dDkAWohFjWQSk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 24, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2010-05-30T21:16:19.449Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-31T08:02:33.978Z", "modifiedAt": null, "url": null, "title": "Negative photon numbers observed", "slug": "negative-photon-numbers-observed", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:01.210Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F3WKCNYx7oQEjiu9b/negative-photon-numbers-observed", "pageUrlRelative": "/posts/F3WKCNYx7oQEjiu9b/negative-photon-numbers-observed", "linkUrl": "https://www.lesswrong.com/posts/F3WKCNYx7oQEjiu9b/negative-photon-numbers-observed", "postedAtFormatted": "Monday, May 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Negative%20photon%20numbers%20observed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANegative%20photon%20numbers%20observed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF3WKCNYx7oQEjiu9b%2Fnegative-photon-numbers-observed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Negative%20photon%20numbers%20observed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF3WKCNYx7oQEjiu9b%2Fnegative-photon-numbers-observed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF3WKCNYx7oQEjiu9b%2Fnegative-photon-numbers-observed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 37, "htmlBody": "<p>A real life paradox observed by quantum physicists: when not being observed, there can be a negative number of photons present.</p>\n<p><a href=\"http://www.economist.com/science/PrinterFriendly.cfm?story_id=13226725\">The Economist</a> summarizes the research. The <a href=\"http://arxiv.org/abs/0811.1625\">scientific paper</a> is available freely.&nbsp; <a href=\"http://en.wikipedia.org/wiki/EPR_paradox\">Wikipedia</a> discusses the paradox at length.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F3WKCNYx7oQEjiu9b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -12, "extendedScore": null, "score": -8e-06, "legacy": true, "legacyId": "38", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-31T08:19:15.248Z", "modifiedAt": null, "url": null, "title": "On Less Wrong traffic and new users -- and how you can help", "slug": "on-less-wrong-traffic-and-new-users-and-how-you-can-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:23.719Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bbf4ZWwcPQkRijEpt/on-less-wrong-traffic-and-new-users-and-how-you-can-help", "pageUrlRelative": "/posts/bbf4ZWwcPQkRijEpt/on-less-wrong-traffic-and-new-users-and-how-you-can-help", "linkUrl": "https://www.lesswrong.com/posts/bbf4ZWwcPQkRijEpt/on-less-wrong-traffic-and-new-users-and-how-you-can-help", "postedAtFormatted": "Monday, May 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Less%20Wrong%20traffic%20and%20new%20users%20--%20and%20how%20you%20can%20help&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Less%20Wrong%20traffic%20and%20new%20users%20--%20and%20how%20you%20can%20help%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbbf4ZWwcPQkRijEpt%2Fon-less-wrong-traffic-and-new-users-and-how-you-can-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Less%20Wrong%20traffic%20and%20new%20users%20--%20and%20how%20you%20can%20help%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbbf4ZWwcPQkRijEpt%2Fon-less-wrong-traffic-and-new-users-and-how-you-can-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbbf4ZWwcPQkRijEpt%2Fon-less-wrong-traffic-and-new-users-and-how-you-can-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 468, "htmlBody": "<p>This is a breakdown of Less Wrong's recent new user traffic, data sourced from the Less Wrong Google Analytics account.</p>\n<p style=\"padding-left: 30px; \">67% StumbleUpon<br />16% Google<br />5.4% Reddit<br />3.6% Hacker News<br />3% Harry Potter story<br />0.7% Facebook<br />0.3% Overcoming Bias<br />4% \"The Long Tail\"</p>\n<p><span style=\"border-collapse: collapse;\"><span style=\"border-collapse: separate;\">The 16% for Google is artificially high because many of those hits are users that are using Google as an address bar by searching for Less Wrong.</span></span></p>\n<p>So we get an order of magnitude more traffic from Stumble Upon than anywhere else -- sometimes thousands of new users a day. Stumble Upon has been Less Wrong's biggest referrer of new users from the beginning of the site. That was surprising to me and I suspect it is also surprising to you. Some of our very best users, like Alicorn, came from Stumble Upon.</p>\n<div style=\"margin-bottom: 1em;\"><strong style=\"font-weight: bold;\">Why does it matter?</strong></div>\n<div style=\"margin-bottom: 1em;\">Imagine your life without Less Wrong... now realize that the overwhelming majority of humans go through their entire lives without ever thinking of&nbsp;<a href=\"/lw/1to/what_is_bayesianism/\">Bayesianism</a>,&nbsp;<a href=\"/lw/dr/generalizing_from_one_example/\">fallacies</a>, how to actually&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">change your mind</a>, or even philosophical&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Zombies_(sequence)\">zombies</a>. Seriously, try to picture your life without Less Wrong. An article that recently made the rounds on the internet claimed that one real way to make yourself happier was to imagine your life without something that you liked.&nbsp;</div>\n<div style=\"margin-bottom: 1em;\">We try to take&nbsp;<a href=\"/tag/existential_risk/\">existential risk</a>&nbsp;seriously around these parts. Each marginal new user that reads anything on Less Wrong has a real chance of being the one that tips us from existential Loss to existential Win.</div>\n<p><strong style=\"font-weight: bold;\">What can you do?</strong></p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial;\">\n<li><a href=\"http://stumbleupon.com\">Sign up for Stumble Upon</a>&nbsp;and start \"thumbs up'ing\" or likeing LW articles that you sincerely like and want to recommend to others. You could start by stumbling one of my favorite Less Wrong articles on the power of a&nbsp;<a href=\"/lw/qk/that_alien_message/\">superintelligence</a>&nbsp;and the real meaning of making efficient use of sensory information.<br /><br />In order to get maximum Stumble power, you can't just stumble LW articles and only LW articles. You need to use Stumble Upon for a minute or two every now and then and vote up or down the random links it gives you. I know, it's annoying, but what are a few&nbsp;<a href=\"/lw/kn/torture_vs_dust_specks/\">dust specks</a>&nbsp;when we are talking about saving the world?</li>\n<li>Help our Google traffic by linking to Less Wrong using the word&nbsp;<a href=\"http://lesswrong.com\">rationality</a>. Less Wrong is the best web site out there on rationality. We should rank #1 on Google for rationality, not #57. At this point in Google's&nbsp;<a href=\"/lw/28r/is_google_paperclipping_the_web_the_perils_of/\">metaphorical paperclipping of the web</a>, some evidence of effort going into increased inbound links is a sign of a high-quality site.</li>\n<li>When you stumble something on Less Wrong you like or post a link to Less Wrong on the greater Internet, post here. You will be rewarded with large amounts of karma and kudos. Also, cake.</li>\n</ol>\n<p>Thanks to&amp;nbsp;<a href=\"/user/Louie\">Louie</a>&amp;nbsp;for help with this post.</p>\n<ol> </ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bbf4ZWwcPQkRijEpt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 25, "extendedScore": null, "score": 5.915270774470274e-07, "legacy": true, "legacyId": "2968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AN2cBr6xKWCB8dRQG", "baTWMegR42PAsH9qJ", "5wMcKNAwB6X4mp9og", "3wYTFWY3LKQCnAptN", "fTu69HzLSXqWgj9ib"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-31T15:23:20.972Z", "modifiedAt": null, "url": null, "title": "London UK, Saturday 2010-07-03: \"How to think rationally about the future\"", "slug": "london-uk-saturday-2010-07-03-how-to-think-rationally-about", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:57.603Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vsmiC3hzXWn4iQH9F/london-uk-saturday-2010-07-03-how-to-think-rationally-about", "pageUrlRelative": "/posts/vsmiC3hzXWn4iQH9F/london-uk-saturday-2010-07-03-how-to-think-rationally-about", "linkUrl": "https://www.lesswrong.com/posts/vsmiC3hzXWn4iQH9F/london-uk-saturday-2010-07-03-how-to-think-rationally-about", "postedAtFormatted": "Monday, May 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20London%20UK%2C%20Saturday%202010-07-03%3A%20%22How%20to%20think%20rationally%20about%20the%20future%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALondon%20UK%2C%20Saturday%202010-07-03%3A%20%22How%20to%20think%20rationally%20about%20the%20future%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsmiC3hzXWn4iQH9F%2Flondon-uk-saturday-2010-07-03-how-to-think-rationally-about%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=London%20UK%2C%20Saturday%202010-07-03%3A%20%22How%20to%20think%20rationally%20about%20the%20future%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsmiC3hzXWn4iQH9F%2Flondon-uk-saturday-2010-07-03-how-to-think-rationally-about", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsmiC3hzXWn4iQH9F%2Flondon-uk-saturday-2010-07-03-how-to-think-rationally-about", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 535, "htmlBody": "<p>Myself and Roko will be giving a presentation about LessWrong-style thinking to the UK Transhumanist Association on the afternoon of Saturday 3 July.&nbsp; Here's the <a href=\"http://www.meetup.com/London-Futurists/calendar/13675464/\">official announcement</a>:</p>\n<p style=\"padding-left: 30px;\">Title: \"How to think rationally about the future\"</p>\n<p style=\"padding-left: 30px;\">2pm-4pm, Saturday 3rd July. [but see above]</p>\n<p style=\"padding-left: 30px;\">Room 416<br />Fourth floor<br /><a href=\"http://maps.google.co.uk/maps?q=WC1E+7HX\">Birkbeck College</a><br />Torrington Square<br />LONDON <br />WC1E 7HX</p>\n<p style=\"padding-left: 30px;\">Speakers: Paul Crowley and Roko Mijic</p>\n<p style=\"padding-left: 30px;\">About the talk:</p>\n<p style=\"padding-left: 30px;\">Over the past forty years, science has built up a substantial body of experimental evidence that highlights dozens of alarming systematic failings in our capacity for reason. These errors are especially dangerous in an area as difficult to think about as the future of humanity, where deluding oneself is tempting and the \"reality check\" won't arrive until too late.</p>\n<p style=\"padding-left: 30px;\">How can we form accurate beliefs about the future in the face of these considerable obstacles? We'll outline ways of identifying and correcting cognitive biases, in particular the use of probability theory to quantify and manipulate uncertainty, and then apply these improved methods to try to paint a more accurate picture of what we all have to look forward to in the 21st century.</p>\n<p style=\"padding-left: 30px;\">About the speakers:</p>\n<p style=\"padding-left: 30px;\">Paul Crowley is a cryptographer and computer programmer whose work includes breaks in ciphers designed by Cisco and by Bruce Schneier. His website is <a class=\"moz-txt-link-freetext\" href=\"http://www.ciphergoth.org/\">http://www.ciphergoth.org</a></p>\n<p style=\"padding-left: 30px;\">Roko Mijic graduated from the University of Cambridge with a BA in Mathematics, and the Certificate of Advanced Study in Mathematics. He spent a year doing research into the foundations of knowledge representation at the University of Edinburgh and holds an MSc in informatics. He is currently an advisor for the Singularity Institute for Artificial Intelligence.</p>\n<p style=\"padding-left: 30px;\">Both speakers are contributors to the community website for refining the art of human rationality, <a class=\"moz-txt-link-freetext\" href=\"/\">http://LessWrong.com</a></p>\n<p style=\"padding-left: 30px;\">Further details:<br /><br />There's no charge to attend this meeting, and everyone is welcome.<br /><br />There will be plenty of opportunity to ask questions and to make comments.<br /><br />Discussion will continue after the event, in a nearby pub, for those who are able to stay.<br /><br />Why not join some of the UKH+ regulars for a drink and/or light lunch beforehand, any time after 12.30pm, in <a href=\"http://maps.google.co.uk/maps?q=36+Torrington+Place,+WC1E+7HJ\">The Marlborough Arms</a>, 36 Torrington Place, London WC1E 7HJ. To find us, look out for a table where there's a copy of the book \"The Singularity Is Near\" displayed.<br /><br />About the venue:<br /><br />Room 416 is on the fourth floor (via the lift near reception) in the main Birkbeck College building, in Torrington Square (which is a pedestrian-only square). Torrington Square is about 10 minutes walk from either Russell Square or Goodge St tube stations.</p>\n<p>The broad plan is for me to open by talking about cognitive biases, including possibly a live demonstration of anchoring bias (which may go wrong but seems worth a go), followed by Roko talking about the implications for thinking about the future, after which we'll take questions. Hopefully we can encourage more careful rational thinking about futurism and get a few more folk participating here; would be great to see as many of you as possible, especially wearing <a href=\"http://www.zazzle.co.uk/rmijic\">LessWrong.com T-shirts</a> :-)</p>\n<p>Also, this Sunday sees another <a href=\"/lw/2a0/lesswrong_meetup_london_uk_20100606_1600/\">LessWrong meetup</a> near Holborn - see some of you there!</p>\n<p>(Updated with venue information and more from meetup announcement)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vsmiC3hzXWn4iQH9F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 5.916150806910546e-07, "legacy": true, "legacyId": "2981", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3E66o8JghKM5gBM6L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-05-31T20:05:24.763Z", "modifiedAt": null, "url": null, "title": "Cultivating our own gardens", "slug": "cultivating-our-own-gardens", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:42.579Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3aiAjSPZccso5WgFd/cultivating-our-own-gardens", "pageUrlRelative": "/posts/3aiAjSPZccso5WgFd/cultivating-our-own-gardens", "linkUrl": "https://www.lesswrong.com/posts/3aiAjSPZccso5WgFd/cultivating-our-own-gardens", "postedAtFormatted": "Monday, May 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cultivating%20our%20own%20gardens&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACultivating%20our%20own%20gardens%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3aiAjSPZccso5WgFd%2Fcultivating-our-own-gardens%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cultivating%20our%20own%20gardens%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3aiAjSPZccso5WgFd%2Fcultivating-our-own-gardens", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3aiAjSPZccso5WgFd%2Fcultivating-our-own-gardens", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1104, "htmlBody": "<p><em>This is a post about moral philosophy, approached with a mathematical metaphor</em>.</p>\n<p>Here's an interesting problem in mathematics.&nbsp; Let's say you have a graph, made up of vertices and edges, with weights assigned to the edges.&nbsp; Think of the vertices as US cities and the edges as roads between them; the weight on each road is the length of the road. Now, knowing only this information, can you draw a map of the US on a sheet of paper? In mathematical terms, is there an isometric embedding of this graph in two-dimensional Euclidean space?</p>\n<p>When you think about this for a minute, it's clear that this is a problem about reconciling the local and the global.&nbsp; Start with New York and all its neighboring cities.&nbsp; You have a sort of star shape.&nbsp; You can certainly draw this on the plane; in fact, you have many degrees of freedom; you can arbitrarily pick one way to draw it.&nbsp; Now start adding more cities and more roads, and eventually the degrees of freedom diminish.&nbsp; If you made the wrong choices earlier on, you might paint yourself in a corner and have no way to keep all the distances consistent when you add a new city.&nbsp; This is known as a \"synchronization problem.\"&nbsp; Getting it to work locally is easy; getting all the local pieces reconciled with each other is hard.</p>\n<p>This is a lovely problem and some acquaintances of mine have written a paper about it.&nbsp; (<a class=\"one\" title=\"link here\" href=\"http://www.math.princeton.edu/~mcucurin/Sensors_ASAP_TOSN_final.pdf\" target=\"_blank\">http://www.math.princeton.edu/~mcucurin/Sensors_ASAP_TOSN_final.pdf</a>)&nbsp; I'll pick out some insights that seem relevant to what follows.&nbsp; First, some obvious approaches don't work very well.&nbsp; It might be thought we want to optimize over all possible embeddings, picking the one that has the lowest error in approximating distances between cities.&nbsp; You come up with a \"penalty function\" that's some sort of sum of errors, and use standard optimization techniques to minimize it.&nbsp; The trouble is, these approaches tend to work spottily -- in particular, they sometimes pick out local rather than global optima (so that the error can be quite high after all.)&nbsp;</p>\n<p>The approach in the paper I linked is different. We break the graph into overlapping smaller subgraphs, so small that they can only be embedded in one way (that's called rigidity) and then \"stitch\" them together consistently.&nbsp; The \"stitching\" is done with a very handy trick involving eigenvectors of sparse matrices.&nbsp; But the point I want to emphasize here is that you have to look at the small scale, and let all the little patches embed themselves as they like, before trying to reconcile them globally.</p>\n<p>Now, rather daringly, I want to apply this idea to ethics.&nbsp; (This is an expansion of a post people seemed to like: <span class=\"one\"><a class=\"one\" title=\"Human Values Differ\" href=\"/lw/1xa/human_values_differ_as_much_as_values_can_differ/1yi5\" target=\"_blank\">http://lesswrong.com/lw/1xa/human_values_differ_as_much_as_values_can_differ/1y )</a></span></p>\n<p><span class=\"one\">The thing is, human values differ enormously.&nbsp; The diversity of values is an empirical fact.&nbsp; <del>The Japanese did not have a word for \"thank you\" until the Portuguese gave them one; this is a simple example, but it absolutely shocked me, because I thought \"thank you\" was a universal concept.&nbsp; It's not.&nbsp;&nbsp;</del>&nbsp; (edited for lack of fact-checking.) And we do not all agree on what virtues are, or what the best way to raise children is, or what the best form of government is.&nbsp; There may be no principle that all humans agree on -- dissenters who believe that genocide is a good thing may be pretty awful people, but they undoubtedly exist.&nbsp; Creating the best possible world for humans is a synchronization problem, then -- we have to figure out a way to balance values that inevitably clash.&nbsp; Here, nodes are individuals, each individual is tied to its neighbors, and a choice of embedding is a particular action.&nbsp; The worse the embedding near an individual fits the \"true\" underlying manifold, the greater the \"penalty function\" and the more miserable that individual is, because the action goes against what he values.</span></p>\n<p><span class=\"one\">If we can extend the metaphor further, this is a problem for utilitarianism.&nbsp; Maximizing something globally -- say, happiness -- can be a dead end.&nbsp; It can hit a local maximum -- the maximum for those people who value happiness -- but do nothing for the people whose highest value is loyalty to their family, or truth-seeking, or practicing religion, or freedom, or martial valor.&nbsp; We can't really optimize, because a lot of people's values are other-regarding: we want Aunt Susie to stop smoking, because of the principle of the thing.&nbsp; Or more seriously, we want people in foreign countries to stop performing clitoridectomies, because of the principle of the thing.&nbsp; And Aunt Susie or the foreigners may feel differently.&nbsp; When you have a set of values that extends to the whole world, conflict is inevitable.</span></p>\n<p><span class=\"one\">The analogue to breaking down the graph is to keep values local.&nbsp; You have a small star-shaped graph of people you know personally and actions you're personally capable of taking.&nbsp; Within that star, you define your own values: what you're ready to cheer for, work for, or die for.&nbsp; You're free to choose those values for yourself -- you don't have to drop them because they're perhaps not optimal for the world's well-being.&nbsp; But beyond that radius, opinions are dangerous: both because you're more ignorant about distant issues, and because you run into this problem of globally reconciling conflicting values.&nbsp; Reconciliation is only possible if everyone's minding their own business. If things are really broken down into rigid components.&nbsp; It's something akin to what Thomas Nagel said against utilitarianism: </span></p>\n<p><span class=\"one\">\"Absolutism is associated with a view of oneself as a small being interacting with others in a large world.&nbsp; The justifications it requires are primarily interpersonal. Utilitarianism is associated with a view of oneself as a benevolent bureaucrat distributing such benefits as one can control to countless other beings, with whom one can have various relations or none.&nbsp; The justifications it requires are primarily administrative.\" (Mortal Questions, p. 68.)</span></p>\n<p><span class=\"one\">Anyhow, trying to embed our values on this dark continent of a manifold seems to require breaking things down into little local pieces. I think of that as \"cultivating our own gardens,\" to quote Candide. I don't want to be so confident as to have universal ideologies, but I think I may be quite confident and decisive in the little area that is mine: my personal relationships; my areas of expertise, such as they are; my own home and what I do in it; everything that I know I love and is worth my time and money; and bad things that I will not permit to happen in front of me, so long as I can help it.&nbsp; Local values, not global ones.&nbsp; </span></p>\n<p><span class=\"one\">Could any AI be \"friendly\" enough to keep things local?<br />&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3aiAjSPZccso5WgFd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 10, "extendedScore": null, "score": 5.916736239108017e-07, "legacy": true, "legacyId": "2983", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-01T00:43:11.646Z", "modifiedAt": null, "url": null, "title": "Seven Shiny Stories", "slug": "seven-shiny-stories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:07.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9sguwESkteCgqFMbj/seven-shiny-stories", "pageUrlRelative": "/posts/9sguwESkteCgqFMbj/seven-shiny-stories", "linkUrl": "https://www.lesswrong.com/posts/9sguwESkteCgqFMbj/seven-shiny-stories", "postedAtFormatted": "Tuesday, June 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seven%20Shiny%20Stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeven%20Shiny%20Stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9sguwESkteCgqFMbj%2Fseven-shiny-stories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seven%20Shiny%20Stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9sguwESkteCgqFMbj%2Fseven-shiny-stories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9sguwESkteCgqFMbj%2Fseven-shiny-stories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2037, "htmlBody": "<p>It has come to my attention that the contents of the <a href=\"/lw/1xh/living_luminously/\">luminosity sequence</a> were too abstract, to the point where explicitly fictional stories illustrating the use of the concepts would be helpful.&nbsp; Accordingly, there follow some such stories.</p>\n<p><strong>1. Words</strong> (an idea from <a href=\"/lw/1xq/let_there_be_light/\">Let There Be Light</a>, in which I advise harvesting priors about yourself from outside feedback)</p>\n<p>Maria likes compliments.&nbsp; She <em>loves</em> compliments.&nbsp; And when she doesn't get enough of them to suit her, she starts fishing, asking plaintive questions, making doe eyes to draw them out.&nbsp; It's starting to annoy people.&nbsp; Lately, instead of compliments, she's getting barbs and criticism and snappish remarks.&nbsp; It hurts - and it seems to hurt her more than it hurts others when they hear similar things.&nbsp; Maria wants to know what it is about her that would explain all of this.&nbsp; So she starts taking personality tests and looking for different styles of maintaining and thinking about relationships, looking for something that describes her.&nbsp; Eventually, she runs into a concept called \"love languages\" and realizes at once that she's a \"words\" person.&nbsp; Her friends aren't trying to hurt her - they don't realize how much she thrives on compliments, or how deeply insults can cut when they're dealing with someone who transmits affection verbally.&nbsp; Armed with this concept, she has a lens through which to interpret patterns of her own behavior; she also has a way to explain herself to her loved ones and get the wordy boosts she needs.</p>\n<p><strong>2. Widgets</strong> (an idea from <a href=\"/lw/1y0/the_abcs_of_luminosity/\">The ABC's of Luminosity</a>, in which I explain the value of correlating affect, behavior, and circumstance)</p>\n<p>Tony's performance at work is suffering.&nbsp; Not every day, but most days, he's too drained and distracted to perform the tasks that go into making widgets.&nbsp; He's in serious danger of falling behind his widget quota and needs to figure out why.&nbsp; Having just read a fascinating and brilliantly written post on Less Wrong about luminosity, he decides to keep track of where he is and what he's doing when he does and doesn't feel the drainedness.&nbsp; After a week, he's got a fairly robust correlation: he feels worst on days when he doesn't eat breakfast, which reliably occurs when he's stayed up too late, hit the snooze button four times, and had to dash out the door.&nbsp; Awkwardly enough, having been distracted all day tends to make him work more slowly at making widgets, which makes him less physically exhausted by the time he gets home and enables him to stay up later.&nbsp; To deal with that, he starts going for long runs on days when his work hasn't been very tiring, and pops melatonin; he easily drops off to sleep when his head hits the pillow at a reasonable hour, gets sounder sleep, scarfs down a bowl of Cheerios, and arrives at the widget factory energized and focused.<a id=\"more\"></a></p>\n<p><strong>3. Text</strong> (an idea from <a href=\"/lw/1yb/lights_camera_action/\">Lights, Camera, Action!</a>, in which I advocate aggressive and frequent introspection to collect as much data as possible)</p>\n<p>Dot reads about an experiment in which the subjects receive phone calls at random times and must tell researchers how happy they feel.&nbsp; Apparently the experiment turned up some really suboptimal patterns of behavior, and Dot's curious about what she'd learn that she could use to improve her life.&nbsp; She gets a friend to arrange delayed text messages to be sent to her phone at intervals supplied by a random number generator, and promises herself that she'll note what she's doing, thinking, and feeling at the moment she receives the text.&nbsp; She soon finds that she doesn't enjoy watching TV as much as she thinks she does; that it's probably worth the time to cook dinner rather than heating up something in the microwave because it's considerably tastier; that she can't really stand her cubicle neighbor; and that she thinks about her ex more than she'd have ever admitted.&nbsp; These thoughts were usually too fleeting to turn into actions; if she tried to remember them hours later, they'd be folded into some large story in which these momentary emotions were secondary.&nbsp; But treating them as notable data points to be taken into account gives them staying power.&nbsp; Dot starts keeping the TV remote under the book she's reading to remind herself what entertainment is more fulfilling.&nbsp; She buys fewer frozen meals and makes sure she's stocked up on staple ingredients.&nbsp; She agrees to swap cubicles with a co-worker down the hall.&nbsp; There's not all that much she can do about the ex, but at least when her friends ask her if everything's okay between them, she can answer more accurately.</p>\n<p><strong>4. Typing</strong> (an idea from <a href=\"/lw/1za/the_spotlight/\">The Spotlight</a>, in which I encourage extracting thoughts into a visible or audible form so as to allow their inspection without introspection)</p>\n<p>George is trying to figure out who he is.&nbsp; He's trying really hard.&nbsp; But when he tries to explain his behaviors and thoughts in terms of larger patterns that could answer the question, they inevitably sound suspiciously revisionist and self-serving, like he's conveniently forgetting some parts and artificially inflating others.&nbsp; He thinks he's generous, fun at parties, a great family man, loyal, easygoing.&nbsp; George decides that what he needs to do is catch what he's thinking at the moment he's thinking it, honestly and irrevocably, so he'll have an uncorrupted data set to work with.&nbsp; He fires up a word processor and starts typing, stream of consciousness.&nbsp; For a few paragraphs, it's mostly \"here I am, writing what I think\" and \"this is kind of dumb, I wonder if anything will come of it\", but eventually that gets old, and content starts to come out.&nbsp; Soon George has a few minutes of inner monologue written down.&nbsp; He writes the congratulatory things he thinks about himself, but also notes in parentheses the times he's acted contrary to these nice patterns (he took three helpings of cake that one time when there were fewer slices than guests, he spent half of the office celebration on his cellphone instead of participating, he missed his daughter's last birthday, he dropped a friend over a sports rivalry, he blew up when a co-worker reminded him one too many times to finish that spreadsheet).&nbsp; George writes the bad habits and vices he demonstrates, too.&nbsp; Most importantly, he resists the urge to hit backspace, although he freely contradicts himself if there's something he wants to correct.&nbsp; Then he saves the document, squirrels it away in a folder, and waits a week.&nbsp; The following Tuesday, he goes over it like a stranger had written it and notes what he'd think of this stranger, and what he'd advise him to do.</p>\n<p><strong>5. Contradiction</strong> (an idea from <a href=\"/lw/209/highlights_and_shadows/\">Highlights and Shadows</a>, in which I explain endorsement and repudiation of one's thoughts and dispositions)</p>\n<p>Penny knows she's not perfect.&nbsp; In fact, some of her traits and projects seem to outright contradict one another, so she really <em>knows </em>it.&nbsp; She wants to eat better, but she just <em>loves</em> pizza; she's trying to learn anger management, but sometimes people do things that really <em>are</em> wrong and it seems only suitable that she be upset with them; she's working on her tendency to nag her boyfriend because she knows it annoys him, but if he can't learn to put the toilet seat down, maybe he deserves to be annoyed.&nbsp; Penny decides to take a serious look at the contradictions and make decisions about which \"side\" she's on.&nbsp; Eventually, she concludes that if she's honest with herself, a life without pizza seems bleak and unrewarding; she'll make that her official exception to the rule, and work harder to eat better in every other way without the drag on motivation caused by withholding her one favorite food.&nbsp; On reflection, being angry - even at people who really do wrong things - isn't helping her or them, and so she throws herself into anger management classes with renewed vigor, looking for other, more productive channels to turn her moral evaluation towards.&nbsp; And - clearly - the nagging isn't helping its ostensible cause either.&nbsp; She doesn't endorse that, but she's not going to let her boyfriend's uncivilized behavior slide either.&nbsp; She'll agree to stop nagging when he slips up and hope this inspires him to remember more often.</p>\n<p><strong>6. Community</strong> (an idea from <a href=\"/lw/20r/city_of_lights/\">City of Lights</a>, in which I propose dividing yourself into subagents to tackle complex situations)</p>\n<p>Billy has the chance to study abroad in Australia for a year, and he's so mixed up about it, he can barely think straight.&nbsp; He can't decide if he wants to go, or why, or how he feels about the idea of missing it.&nbsp; Eventually, he decides this would be far easier if all the different nagging voices and clusters of desire were given names and allowed to talk to each other.&nbsp; He identifies the major relevant sub-agents as \"Clingyness\", which wants to stay in known surroundings; \"Adventurer\", which wants to seek new experiences and learn about the world; \"Obedience to Advisor\", which wants to do what Prof. So-and-So recommends; \"Academic\", who wants to do whatever will make Billy's r&eacute;sum&eacute; more impressive to future readers; and \"Fear of Spiders\", which would happily go nearly anywhere but the home of the Sydney funnelweb and is probably responsible for Billy's spooky dreams.&nbsp; When these voices have a chance to compete with each other, they expose questionable motivations: for instance, Academic determines that Prof. So-and-So only recommends staying at Billy's home institution because Billy is her research assistant, not because it would further Billy's intellectual growth, which reduces the comparative power of Obedience to Advisor.&nbsp; Adventurer renders Fear of Spiders irrelevant by pointing out that the black widow is native to the United States.&nbsp; Eventually, Academic and Adventurer, in coalition, beat out Clingyness (whom Billy is not strongly inclined to identify with), and Billy buys the ticket to Down Under.</p>\n<p><strong>7. Experiment </strong>(an idea from <a href=\"/lw/21l/lampshading/\">Lampshading</a>, where I describe how to make changes in oneself by setting oneself up to succeed at operating in accordance with the change, and determining what underlies the disliked behavior)</p>\n<p>Eva bursts into tears whenever she has a hard problem to deal with, like a stressful project at work or above-average levels of social drama amongst her friends.&nbsp; This is, of course, completely unproductive - in fact, in the case of drama, it worsens things - and Eva wants to stop it.&nbsp; First, she has to figure out why it happens.&nbsp; Are the tears caused by <em>sadness?</em>&nbsp; It turns out not - she can be brought to tears even by things that don't make her sad.&nbsp; The latest project from work was exciting and a great opportunity and it still made her cry.&nbsp; After a little work sorting through lists of things that make her cry, Eva concludes that it's linked to how much pressure she feels to solve the problem: for instance, if she's part of a team that's assigned a project, she's less likely to react this way than if she's operating solo, and if her friends embroiled in drama turn to her for help, she'll wind up tearful more often than if she's just a spectator with no special responsibility.&nbsp; Now she needs to set herself up not to cry.&nbsp; She decides to do this by making sure she has social support in her endeavors: if the boss gives her an assignment, she says to the next employee over, \"I should be able to handle this, but if I need help, can I count on you?\"&nbsp; That way, she can think of the task as something that isn't entirely on her.&nbsp; When next social drama rears its head, Eva reconceptualizes her part in the solution as finding and voicing the group's existing consensus, rather than personally creating a novel way to make everything better.&nbsp; While this new approach reduces the incidence of stress tears, it doesn't disassemble the underlying architecture that causes the tendency in the first place.&nbsp; That's more complicated to address: Eva spends some time thinking about why responsibility is such an emotional thing for her, and looks for ways to duplicate the sense of support she feels when she has help in situations where she doesn't.&nbsp; Eventually, it is not much of a risk that Eva will cry if presented with a problem to solve.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5uHdFgR938LGGxMKQ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9sguwESkteCgqFMbj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 129, "baseScore": 137, "extendedScore": null, "score": 0.000235, "legacy": true, "legacyId": "2984", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "", "canonicalPrevPostSlug": "on-enjoying-disagreeable-company", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 137, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "Y6TpEEKZq6HXfhWxd", "rLuZ6XrGpgjk9BNpX", "v4ngP587MDZ5rC48Y", "Zstm38omrpeu7iWeS", "tCTmAmAapB37dAz9Y", "vfHRahpgbp9YFPuGQ", "goCfoiQkniQwPryki"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-01T18:04:48.504Z", "modifiedAt": null, "url": null, "title": "Open Thread: June 2010", "slug": "open-thread-june-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:03.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aypPsdGCpYMysNrvi/open-thread-june-2010", "pageUrlRelative": "/posts/aypPsdGCpYMysNrvi/open-thread-june-2010", "linkUrl": "https://www.lesswrong.com/posts/aypPsdGCpYMysNrvi/open-thread-june-2010", "postedAtFormatted": "Tuesday, June 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20June%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20June%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaypPsdGCpYMysNrvi%2Fopen-thread-june-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20June%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaypPsdGCpYMysNrvi%2Fopen-thread-june-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaypPsdGCpYMysNrvi%2Fopen-thread-june-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>To whom it may concern:</p>\n<p><em>This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>\n<p>(After the critical success of <a href=\"/lw/2bi/open_thread_june_2010_part_2/\">part II</a>, and the strong box office sales of <a href=\"/lw/2cp/open_thread_june_2010_part_3/\">part III</a> in spite of mixed reviews, will <a href=\"/lw/2d9/open_thread_june_2010_part_4/\">part IV</a> finally see the June Open Thread jump the shark?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aypPsdGCpYMysNrvi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 5.919476054035933e-07, "legacy": true, "legacyId": "2985", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 663, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3Kkgek3PZ9snipyMY", "meLBCXvvGcTNCet4g", "L6yBGL4Hog5coegd8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-01T18:07:17.716Z", "modifiedAt": null, "url": null, "title": "Rationality quotes: June 2010", "slug": "rationality-quotes-june-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.949Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mxmGS9AFg2w8Ee3ZL/rationality-quotes-june-2010", "pageUrlRelative": "/posts/mxmGS9AFg2w8Ee3ZL/rationality-quotes-june-2010", "linkUrl": "https://www.lesswrong.com/posts/mxmGS9AFg2w8Ee3ZL/rationality-quotes-june-2010", "postedAtFormatted": "Tuesday, June 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20quotes%3A%20June%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20quotes%3A%20June%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmxmGS9AFg2w8Ee3ZL%2Frationality-quotes-june-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20quotes%3A%20June%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmxmGS9AFg2w8Ee3ZL%2Frationality-quotes-june-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmxmGS9AFg2w8Ee3ZL%2Frationality-quotes-june-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>This is our monthly thread for collecting these little gems and pearls of wisdom, rationality-related quotes you've seen recently, or had stored in your quotesfile for ages, and which might be handy to link to in one of our discussions.</p>\n<ul>\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mxmGS9AFg2w8Ee3ZL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 5.919480171447717e-07, "legacy": true, "legacyId": "2986", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 223, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-02T06:01:26.276Z", "modifiedAt": null, "url": null, "title": "Singularity Summit 2010 on Aug. 14-15 in San Francisco", "slug": "singularity-summit-2010-on-aug-14-15-in-san-francisco", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:26.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QXqfTDg4wM2PrDaKQ/singularity-summit-2010-on-aug-14-15-in-san-francisco", "pageUrlRelative": "/posts/QXqfTDg4wM2PrDaKQ/singularity-summit-2010-on-aug-14-15-in-san-francisco", "linkUrl": "https://www.lesswrong.com/posts/QXqfTDg4wM2PrDaKQ/singularity-summit-2010-on-aug-14-15-in-san-francisco", "postedAtFormatted": "Wednesday, June 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Summit%202010%20on%20Aug.%2014-15%20in%20San%20Francisco&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Summit%202010%20on%20Aug.%2014-15%20in%20San%20Francisco%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXqfTDg4wM2PrDaKQ%2Fsingularity-summit-2010-on-aug-14-15-in-san-francisco%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Summit%202010%20on%20Aug.%2014-15%20in%20San%20Francisco%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXqfTDg4wM2PrDaKQ%2Fsingularity-summit-2010-on-aug-14-15-in-san-francisco", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQXqfTDg4wM2PrDaKQ%2Fsingularity-summit-2010-on-aug-14-15-in-san-francisco", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 576, "htmlBody": "<p>The <a href=\"http://www.singularitysummit.com/\">Singularity Summit 2010</a> will be held on August 14th and 15th at the Hyatt Regency in San Francisco, and will feature Ray Kurzweil and famed Traditional Rationalist James Randi as speakers, in addition to numerous others. During last year's Summit (in New York City), there was a very large Less Wrong meetup with dozens of attendees, and it is quite possible that there will be one again this year. Anyone interested in planning such a meetup (not just attending) should contact the Singularity Institute at institute@intelligence.org. The Singularity Summit press release follows after the jump.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>Singularity Summit 2010 returns to San Francisco, explores intelligence augmentation</strong><br /> <em>Speakers include Futurist Ray Kurzweil, Magician-Skeptic James Randi </em></p>\n<p>Will it be one day become possible to boost human intelligence using brain implants, or create an artificial intelligence smarter than Einstein? In a 1993 paper presented to NASA, science fiction author and mathematician Vernor Vinge called such a hypothetical event a &ldquo;Singularity&rdquo;, saying &ldquo;From the human point of view this change will be a throwing away of all the previous rules, perhaps in the blink of an eye&rdquo;. Vinge pointed out that intelligence enhancement could lead to &ldquo;closing the loop&rdquo; between intelligence and technology, creating a positive feedback effect.</p>\n<p>This August 14-15, hundreds of AI researchers, robotics experts, philosophers, entrepreneurs, scientists, and interested laypeople will converge in San Francisco to address the Singularity and related issues at the only conference on the topic, the Singularity Summit. Experts in fields including animal intelligence, artificial intelligence, brain-computer interfacing, tissue regeneration, medical ethics, computational neurobiology, augmented reality, and more will share their latest research and explore its implications for the future of humanity.</p>\n<p>&ldquo;This year, the conference shifts to a focus on neuroscience, bioscience, cognitive enhancement, and other explorations of what Vernor Vinge called &lsquo;intelligence amplification&rsquo; &mdash; the other route to the Singularity,&rdquo; said Michael Vassar, president of the Singularity Institute, which is hosting the event.</p>\n<p>Irene Pepperberg, author of &ldquo;Alex &amp; Me,&rdquo; who has pushed the frontier of animal intelligence with her research on African Gray Parrots, will explore the ethical and practical implications of non-human intelligence enhancement and of the creation of new intelligent life less powerful than ourselves. Futurist-inventor Ray Kurzweil will discuss reverse-engineering the brain and his forthcoming book, <em>How the Mind Works and How to Build One</em>. Allan Synder, Director, Centre for the Mind at the University of Sydney, will explore the use of transcranial magnetic stimulation for the enhancement of narrow cognitive abilities. Joe Tsien will talk about the smarter rats and mice that he created by tuning the molecular substrate of the brain&rsquo;s learning mechanism. Steve Mann, &ldquo;the world&rsquo;s first cyborg,&rdquo; will demonstrate his latest geek-chic inventions: wearable computers now used by almost 100,000 people.</p>\n<p>Other speakers will include magician-skeptic and MacArthur Genius Award winner James Randi; Gregory Stock (<em>Redesigning Humans</em>), former Director of the Program on Medicine, Technology, and Society at UCLA&rsquo;s School of Public Health; Terry Sejnowski, Professor and Laboratory Head, Salk Institute Computational Neurobiology Laboratory, who believes we are just ten years away from being able to upload ourselves; Ellen Heber-Katz, Professor, Molecular and Cellular Oncogenesis Program at The Wistar Institute, who is investigating the molecular basis of wound regeneration in mutant mice, which can regenerate limbs, hearts, and spinal cords; Anita Goel, MD, physicist, and CEO of nanotechnology company Nanobiosym; and David Hanson, Founder &amp; CEO, Hanson Robotics, who is creating the world&rsquo;s most realistic humanoid robots.</p>\n<p>Interested readers can watch videos from past summits and register at <a href=\"http://www.singularitysummit.com/\">www.singularitysummit.com</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QXqfTDg4wM2PrDaKQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 5.920965124779127e-07, "legacy": true, "legacyId": "2990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-03T04:40:21.377Z", "modifiedAt": null, "url": null, "title": "Bayes' Theorem Illustrated (My Way)", "slug": "bayes-theorem-illustrated-my-way", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:35.658Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CMt3ijXYuCynhPWXa/bayes-theorem-illustrated-my-way", "pageUrlRelative": "/posts/CMt3ijXYuCynhPWXa/bayes-theorem-illustrated-my-way", "linkUrl": "https://www.lesswrong.com/posts/CMt3ijXYuCynhPWXa/bayes-theorem-illustrated-my-way", "postedAtFormatted": "Thursday, June 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayes'%20Theorem%20Illustrated%20(My%20Way)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayes'%20Theorem%20Illustrated%20(My%20Way)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCMt3ijXYuCynhPWXa%2Fbayes-theorem-illustrated-my-way%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayes'%20Theorem%20Illustrated%20(My%20Way)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCMt3ijXYuCynhPWXa%2Fbayes-theorem-illustrated-my-way", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCMt3ijXYuCynhPWXa%2Fbayes-theorem-illustrated-my-way", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2690, "htmlBody": "<p><em>(This post is elementary: it introduces a simple method of visualizing Bayesian calculations. In my defense, we've had <a href=\"/lw/1to/what_is_bayesianism/\">other</a> elementary posts before, and they've been found useful; plus, I'd really like this to be online somewhere, and it might as well be here.)</em></p>\n<p>I'll admit, those <a href=\"http://en.wikipedia.org/wiki/Monty_Hall_problem\">Monty-Hall</a>-<a href=\"/lw/1lh/drawing_two_aces/\">type</a> problems invariably trip me up. Or at least, they do if I'm not thinking <em>very</em> carefully -- doing quite a bit more work than other people seem to have to do.</p>\n<p>What's more, people's explanations of how to get the right answer have almost never been satisfactory to me. If I concentrate hard enough, I can usually follow the reasoning, sort of; but I never quite \"see it\", and nor do I feel equipped to solve similar problems in the future: it's as if the solutions seem to work only in retrospect.&nbsp;</p>\n<p><a href=\"/lw/dr/generalizing_from_one_example/\">Minds work differently</a>, <a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">illusion of transparency</a>, and all that.</p>\n<p>Fortunately, I eventually managed to identify the source of the problem, and I came up a way of thinking about -- <em>visualizing </em>-- such problems that suits my own intuition. Maybe there are others out there like me; this post is for them.</p>\n<p><a id=\"more\"></a></p>\n<p>I've <a href=\"http://wiki.lesswrong.com/wiki/Chat_Logs/2010-02-18\">mentioned before</a> that I like to think in very abstract terms. What this means in practice is that, if there's some simple, general, elegant point to be made, <em>tell it to me right away</em>. Don't start with some messy concrete example and attempt to \"work upward\", in the hope that difficult-to-grasp abstract concepts will be made more palatable by relating them to \"real life\". If you do that, I'm liable to get stuck in the trees and not see the forest. Chances are, I won't have much trouble understanding the abstract concepts; \"real life\", on the other hand...</p>\n<p>...well, let's just say I prefer to start at the top and work downward, as a general rule. Tell me how the trees relate to the forest, rather than the other way around.</p>\n<p>Many people have found Eliezer's <a href=\"http://yudkowsky.net/rational/bayes\">Intuitive Explanation of Bayesian Reasoning</a> to be an excellent introduction to <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' theorem</a>, and so I don't usually hesitate to recommend it to others. But for me personally, if I didn't know Bayes' theorem and you were trying to explain it to me, pretty much the worst thing you could do would be to start with some detailed scenario involving breast-cancer screenings. (And not just because it tarnishes beautiful mathematics with images of sickness and death, either!)</p>\n<p>So what's the right way to explain Bayes' theorem to me?</p>\n<p>Like this:</p>\n<p>We've got a bunch of hypotheses (states the world could be in) and we're trying to figure out which of them is true (that is, which state the world is actually in). As a concession to concreteness (and for ease of drawing the pictures), let's say we've got three (mutually exclusive and exhaustive) hypotheses -- possible world-states -- which we'll call H<sub>1</sub>, H<sub>2</sub>, and H<sub>3</sub>. We'll represent these as blobs in space:</p>\n<p><img src=\"http://imgur.com/NpNUV.png\" alt=\"Figure 0\" width=\"225\" height=\"415\" /></p>\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp; Figure 0</strong></p>\n<p><br />Now, we have some prior notion of how probable each of these hypotheses is -- that is, each has some <em>prior probability</em>. If we don't know anything at all that would make one of them more probable than another, they would each have probability 1/3. To illustrate a more typical situation, however, let's assume we have more information than that. Specifically, let's suppose our prior probability distribution is as follows: P(H<sub>1</sub>) = 30%, P(H<sub>2</sub>)=50%, P(H<sub>3</sub>) = 20%. We'll represent this by resizing our blobs accordingly:</p>\n<p><img src=\"http://i.imgur.com/8JAkA.png\" alt=\"Figure 1\" width=\"337\" height=\"533\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; <strong>Figure 1<br /></strong></p>\n<p>That's our <em>prior</em> knowledge. Next, we're going to collect some <em>evidence</em> and <em>update</em> our prior probability distribution to produce a <em>posterior</em> probability distribution. Specifically, we're going to run a test. The test we're going to run has three possible outcomes: Result A, Result B, and Result C. Now, since this test happens to have three possible results, it would be really nice if the test just flat-out told us which world we were living in -- that is, if (say) Result A meant that H<sub>1</sub> was true, Result B meant that H<sub>2</sub> was true, and Result 3 meant that H<sub>3</sub> was true. Unfortunately, the real world is messy and complex, and things aren't that simple. Instead, we'll suppose that each result can occur under each hypothesis, but that the different hypotheses have different effects on how likely each result is to occur. We'll assume for instance that if Hypothesis&nbsp; H<sub>1</sub> is true, we have a 1/2 chance of obtaining Result A, a 1/3 chance of obtaining Result B, and a 1/6 chance of obtaining Result C; which we'll write like this:</p>\n<p>P(A|H<sub>1</sub>) = 50%, P(B|H<sub>1</sub>) = 33.33...%, P(C|H<sub>1</sub>) = 16.166...%</p>\n<p>and illustrate like this:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgur.com/9jpzJ.png\" alt=\"\" width=\"384\" height=\"140\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 2</strong></p>\n<p>(Result A being represented by a triangle, Result B by a square, and Result C by a pentagon.)</p>\n<p>If Hypothesis H<sub>2</sub> is true, we'll assume there's a 10% chance of Result A, a 70% chance of Result B, and a 20% chance of Result C:</p>\n<p><img src=\"http://imgur.com/puWW1.png\" alt=\"Figure 3\" width=\"433\" height=\"180\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 3</strong></p>\n<p><strong><br /></strong>(P(A|H<sub>2</sub>) = 10% , P(B|H<sub>2</sub>) = 70%, P(C|H<sub>2</sub>) = 20%)<strong><br /></strong></p>\n<p>Finally, we'll say that if Hypothesis H<sub>3</sub> is true, there's a 5% chance of Result A, a 15% chance of Result B, and an 80% chance of Result C:<strong><br /></strong></p>\n<p><img src=\"http://imgur.com/DHitn.png\" alt=\"Figure 4\" width=\"384\" height=\"140\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 4</strong></p>\n<p>(P(A|H<sub>3</sub>) = 5%, P(B|H<sub>3</sub>) = 15% P(C|H<sub>3</sub>) = 80%)</p>\n<p>Figure 5 below thus shows our knowledge prior to running the test:</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgur.com/qlyGw.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 5</strong></p>\n<p>&nbsp;</p>\n<p>Note that we have now carved up our hypothesis-space more finely; our possible world-states are now things like \"Hypothesis H<sub>1</sub> is true and Result A occurred\", \"Hypothesis H<sub>1</sub> is true and Result B occurred\", etc., as opposed to merely \"Hypothesis H<sub>1</sub> is true\", etc. The numbers above the slanted line segments -- the <em>likelihoods</em> of the test results, assuming the particular hypothesis -- represent <em>what proportion</em> of the total probability mass assigned to the hypothesis H<sub>n</sub> is assigned to the conjunction of Hypothesis H<sub>n</sub> and Result X; thus, since P(H<sub>1</sub>) = 30%, and P(A|H<sub>1</sub>) = 50%, P(H<sub>1</sub> &amp; A) is therefore 50% of 30%, or, in other words, 15%.</p>\n<p>(That's really all Bayes' theorem is, right there, but -- shh! -- don't tell anyone yet!)</p>\n<p><br />Now, then, suppose we run the test, and we get...Result A.</p>\n<p>What do we do? We <em>cut off all the other branches</em>:</p>\n<p><img src=\"http://imgur.com/XBXi5.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 6</strong></p>\n<p>&nbsp;</p>\n<p>So our updated probability distribution now looks like this:</p>\n<p><img src=\"http://imgur.com/nXENh.png\" alt=\"\" /></p>\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 7</strong></p>\n<p><strong><br /></strong></p>\n<p>...except for one thing: probabilities are supposed to add up to 100%, not 21%. Well, since we've <em>conditioned</em> on Result A, that means that the 21% probability mass assigned to Result A is now the entirety of our probability mass -- 21% is the new 100%, you might say. So we simply adjust the numbers in such a way that they add up to 100% <em>and the proportions are the same</em>:</p>\n<p><img src=\"http://i.imgur.com/RIeff.png\" alt=\"\" width=\"253\" height=\"541\" /></p>\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 8</strong></p>\n<p>There! We've just performed a Bayesian update. And that's what it <em>looks like</em>.</p>\n<p>&nbsp;</p>\n<p>If, instead of Result A, we had gotten Result B,</p>\n<p><img src=\"http://2.bp.blogspot.com/_Ig9I_03TGBQ/TAXmNUu1BpI/AAAAAAAAAAM/s9iVIdtmPy0/s1600/figure09.png\" alt=\"Figure 9\" width=\"460\" height=\"480\" /></p>\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 9</strong></p>\n<p><strong><br /></strong></p>\n<p>then our updated probability distribution would have looked like this:</p>\n<p><img src=\"http://imgur.com/s9Tw5.png\" alt=\"\" /></p>\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 10</strong></p>\n<p>&nbsp;</p>\n<p>Similarly, for Result C:</p>\n<p><img src=\"http://imgur.com/9Ikc0.png\" alt=\"\" /></p>\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 11</strong></p>\n<p><em>Bayes' theorem</em> is the formula that calculates these updated probabilities. Using H to stand for a hypothesis (such as H<sub>1</sub>, H<sub>2</sub> or H<sub>3</sub>), and E a piece of evidence (such as Result A, Result B, or Result C), it says:</p>\n<p>P(H|E) = P(H)*P(E|H)/P(E)</p>\n<p>In words: to calculate the updated probability P(H|E), take the portion of the prior probability of H that is allocated to E (i.e. the quantity P(H)*P(E|H)), and calculate what fraction this is of the total prior probability of E (i.e. divide it by P(E)).</p>\n<p>What I like about this way of visualizing Bayes' theorem is that it makes the importance of prior probabilities -- in particular, the difference between P(H|E) and P(E|H) -- <em>visually obvious</em>. Thus, in the above example, we easily see that even though P(C|H<sub>3</sub>) is high (80%), P(H<sub>3</sub>|C) is much less high (around 51%) -- and once you have assimilated this visualization method, it should be easy to see that even more extreme examples (e.g. with P(E|H) huge and P(H|E) tiny) could be constructed.</p>\n<p>Now let's use this to examine two tricky probability puzzles, the infamous <a href=\"http://en.wikipedia.org/wiki/Monty_Hall_problem\">Monty Hall Problem</a> and Eliezer's <a href=\"/lw/1lh/drawing_two_aces/\">Drawing Two Aces</a>, and see how it illustrates the correct answers, as well as how one might go wrong.</p>\n<p>&nbsp;</p>\n<h3><strong>The Monty Hall Problem</strong></h3>\n<p>The situation is this: you're a contestant on a game show seeking to win a car. Before you are three doors, one of which contains a car, and the other two of which contain goats. You will make an initial \"guess\" at which door contains the car -- that is, you will select one of the doors, without opening it. At that point, the host will open a goat-containing door from among the two that you did not select. You will then have to decide whether to stick with your original guess and open the door that you originally selected, or switch your guess to the remaining unopened door. The question is whether it is to your advantage to switch -- that is, whether the car is more likely to be behind the remaining unopened door than behind the door you originally guessed.</p>\n<p>(If you haven't thought about this problem before, you may want to try to figure it out before continuing...)</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>The answer is that it <em>is</em> to your advantage to switch -- that, in fact, switching <em>doubles</em> the probability of winning the car.</p>\n<p>People often find this counterintuitive when they first encounter it -- where \"people\" includes the author of this post. There are two possible doors that could contain the car; why should one of them be more likely to contain it than the other?</p>\n<p>As it turns out, while constructing the diagrams for this post, I \"rediscovered\" the error that led me to incorrectly conclude that there is a 1/2 chance the car is behind the originally-guessed door and a 1/2 chance it is behind the remaining door the host didn't open. I'll present that error first, and then show how to correct it. Here, then, is the <em>wrong</em> solution:</p>\n<p>We start out with a perfectly correct diagram showing the prior probabilities:</p>\n<p><img src=\"http://imgur.com/aXwYS.png\" alt=\"\" /></p>\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 12</strong></p>\n<p>The possible hypotheses are Car in Door 1, Car in Door 2, and Car in Door 3; before the game starts, there is no reason to believe any of the three doors is more likely than the others to contain the car, and so each of these hypotheses has prior probability 1/3.</p>\n<p>The game begins with our selection of a door. That itself isn't <a href=\"http://wiki.lesswrong.com/wiki/Evidence\">evidence</a> about where the car is, of course -- we're assuming we have no particular information about that, other than that it's behind one of the doors (that's the whole point of the game!). Once we've done that, however, we will then have the opportunity to \"run a test\" to gain some \"experimental data\": the host will perform his task of opening a door that is guaranteed to contain a goat. We'll represent the result Host Opens Door 1 by a triangle, the result Host Opens Door 2 by a square, and the result Host Opens Door 3 by a pentagon -- thus carving up our hypothesis space more finely into possibilities such as \"Car in Door 1 and Host Opens Door 2\" , \"Car in Door 1 and Host Opens Door 3\", etc:</p>\n<p><img src=\"http://imgur.com/bIxZr.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 13</strong></p>\n<p><strong></strong><br />Before we've made our initial selection of a door, the host is equally likely to open either of the goat-containing doors. Thus, at the beginning of the game, the probability of each hypothesis of the form \"Car in Door X and Host Opens Door Y\" has a probability of 1/6, as shown. So far, so good; everything is still perfectly correct.</p>\n<p>Now we select a door; say we choose Door 2. The host then opens either Door 1 or Door 3, to reveal a goat. Let's suppose he opens Door 1; our diagram now looks like this:<br /><br /><br /><img src=\"http://imgur.com/0xMQs.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 14</strong></p>\n<p>But this shows equal probabilities of the car being behind Door 2 and Door 3!</p>\n<p><img src=\"http://imgur.com/07q9g.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; <strong>Figure 15</strong></p>\n<p>Did you catch the mistake?</p>\n<p>Here's the <em>correct</em> version:<br /><br /><em>As soon as we selected Door 2</em>, our diagram should have looked like this:</p>\n<p><img src=\"http://imgur.com/tKGgR.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 16</strong></p>\n<p>&nbsp;</p>\n<p>With Door 2 selected, the host no longer has the <em>option</em> of opening Door 2; if the car is in Door 1, he <em>must</em> open Door 3, and if the car is in Door 3, he <em>must</em> open Door 1. We thus see that if the car is behind Door 3, the host is twice as <a href=\"http://wiki.lesswrong.com/wiki/Likelihood_ratio\">likely</a> to open Door 1 (namely, 100%) as he is if the car is behind Door 2 (50%); his opening of Door 1 thus constitutes <a href=\"http://wiki.lesswrong.com/wiki/Amount_of_evidence\">some evidence</a> in favor of the hypothesis that the car is behind Door 3. So, when the host opens Door 1, our picture looks as follows:</p>\n<p><img src=\"http://imgur.com/5U47D.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 17</strong></p>\n<p>&nbsp;</p>\n<p>which yields the correct updated probability distribution:</p>\n<p><img src=\"http://imgur.com/JYay2.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; <strong>Figure 18</strong></p>\n<p>&nbsp;</p>\n<h3>Drawing Two Aces</h3>\n<p>Here is the statement of the problem, from <a href=\"/lw/1lh/drawing_two_aces/\">Eliezer's post</a>:</p>\n<blockquote>\n<p><br />Suppose I have a deck of four cards:&nbsp; The ace of spades, the ace of hearts, and two others (say, 2C and 2D).<br /><br />You draw two cards at random.<br /><br />(...)<br /><br />Now suppose I ask you \"Do you have an ace?\"<br /><br />You say \"Yes.\"<br /><br />I then say to you:&nbsp; \"Choose one of the aces you're holding at random (so if you have only one, pick that one).&nbsp; Is it the ace of spades?\"<br /><br />You reply \"Yes.\"<br /><br />What is the probability that you hold two aces?</p>\n</blockquote>\n<p><br />(Once again, you may want to think about it, if you haven't already, before continuing...)</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Here's how our picture method answers the question:</p>\n<p><br />Since the person holding the cards has at least one ace, the \"hypotheses\" (possible card combinations) are the five shown below:</p>\n<p><img src=\"http://imgur.com/a3dxW.png\" alt=\"\" /></p>\n<p><strong>&nbsp; &nbsp; &nbsp; Figure 19</strong></p>\n<p>Each has a prior probability of 1/5, since there's no reason to suppose any of them is more likely than any other. <br /><br />The \"test\" that will be run is selecting an ace at random from the person's hand, and seeing if it is the ace of spades. The possible results are:</p>\n<p><img src=\"http://imgur.com/XoXVj.png\" alt=\"\" width=\"330\" height=\"446\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Figure 20</strong></p>\n<p>&nbsp;</p>\n<p>Now we run the test, and get the answer \"YES\"; this puts us in the following situation:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgur.com/b2oLJ.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 21</strong></p>\n<p>&nbsp;</p>\n<p>The total prior probability of this situation (the YES answer) is (1/6)+(1/3)+(1/3) = 5/6; thus, since 1/6 is 1/5 of 5/6 (that is, (1/6)/(5/6) = 1/5), our updated probability is 1/5 -- which happens to be the same as the prior probability. (I won't bother displaying the final post-update picture here.)</p>\n<p>What this means is that the test we ran did not provide any additional information about whether the person has both aces beyond simply knowing that they have at least one ace; we might in fact say that the result of the test is <a href=\"http://wiki.lesswrong.com/wiki/Screening_off\">screened off</a> by the answer to the first question (\"Do you have an ace?\").</p>\n<p><br />On the other hand, if we had simply asked \"Do you have the ace of spades?\", the diagram would have looked like this:</p>\n<p><img src=\"http://imgur.com/CWtH4.png\" alt=\"\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 22</strong></p>\n<p>&nbsp;</p>\n<p>which, upon receiving the answer YES, would have become:</p>\n<p><img src=\"http://imgur.com/oc1YQ.png\" alt=\"\" /></p>\n<p>&nbsp; <strong>Figure 23</strong></p>\n<p>The total probability mass allocated to YES is 3/5, and, within that, the specific situation of interest has probability 1/5; hence the updated probability would be 1/3.</p>\n<p>So a YES answer in this experiment, unlike the other, would provide <a href=\"http://wiki.lesswrong.com/wiki/Evidence\">evidence</a> that the hand contains both aces; for if the hand contains both aces, the probability of a YES answer is 100% -- twice as large as it is in the contrary case (50%), giving a <a href=\"http://wiki.lesswrong.com/wiki/Likelihood_ratio\">likelihood ratio</a> of 2:1. By contrast, in the other experiment, the probability of a YES answer is only 50% even in the case where the hand contains both aces.</p>\n<p><br />This is what people who try to explain the difference by uttering the opaque phrase \"a random selection was involved!\" are actually talking about: the difference between</p>\n<p><img src=\"http://imgur.com/IWwTV.png\" alt=\"\" /></p>\n<p><strong>&nbsp; Figure 24</strong></p>\n<p>&nbsp;</p>\n<p>and</p>\n<p><img src=\"http://imgur.com/ugqVb.png\" alt=\"\" />.</p>\n<p><strong>&nbsp; Figure 25</strong></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>The method explained here is far from the only way of visualizing Bayesian updates, but I feel that it is among the most intuitive.</p>\n<p>&nbsp;</p>\n<p>(<em>I'd like to thank my sister, </em><a href=\"/user/Vive-ut-Vivas/\">Vive-ut-Vivas</a><em>, for help with some of the diagrams in this post.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 4, "5f5c37ee1b5cdee568cfb294": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CMt3ijXYuCynhPWXa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 144, "baseScore": 161, "extendedScore": null, "score": 0.000276, "legacy": true, "legacyId": "2988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 161, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>(This post is elementary: it introduces a simple method of visualizing Bayesian calculations. In my defense, we've had <a href=\"/lw/1to/what_is_bayesianism/\">other</a> elementary posts before, and they've been found useful; plus, I'd really like this to be online somewhere, and it might as well be here.)</em></p>\n<p>I'll admit, those <a href=\"http://en.wikipedia.org/wiki/Monty_Hall_problem\">Monty-Hall</a>-<a href=\"/lw/1lh/drawing_two_aces/\">type</a> problems invariably trip me up. Or at least, they do if I'm not thinking <em>very</em> carefully -- doing quite a bit more work than other people seem to have to do.</p>\n<p>What's more, people's explanations of how to get the right answer have almost never been satisfactory to me. If I concentrate hard enough, I can usually follow the reasoning, sort of; but I never quite \"see it\", and nor do I feel equipped to solve similar problems in the future: it's as if the solutions seem to work only in retrospect.&nbsp;</p>\n<p><a href=\"/lw/dr/generalizing_from_one_example/\">Minds work differently</a>, <a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">illusion of transparency</a>, and all that.</p>\n<p>Fortunately, I eventually managed to identify the source of the problem, and I came up a way of thinking about -- <em>visualizing </em>-- such problems that suits my own intuition. Maybe there are others out there like me; this post is for them.</p>\n<p><a id=\"more\"></a></p>\n<p>I've <a href=\"http://wiki.lesswrong.com/wiki/Chat_Logs/2010-02-18\">mentioned before</a> that I like to think in very abstract terms. What this means in practice is that, if there's some simple, general, elegant point to be made, <em>tell it to me right away</em>. Don't start with some messy concrete example and attempt to \"work upward\", in the hope that difficult-to-grasp abstract concepts will be made more palatable by relating them to \"real life\". If you do that, I'm liable to get stuck in the trees and not see the forest. Chances are, I won't have much trouble understanding the abstract concepts; \"real life\", on the other hand...</p>\n<p>...well, let's just say I prefer to start at the top and work downward, as a general rule. Tell me how the trees relate to the forest, rather than the other way around.</p>\n<p>Many people have found Eliezer's <a href=\"http://yudkowsky.net/rational/bayes\">Intuitive Explanation of Bayesian Reasoning</a> to be an excellent introduction to <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' theorem</a>, and so I don't usually hesitate to recommend it to others. But for me personally, if I didn't know Bayes' theorem and you were trying to explain it to me, pretty much the worst thing you could do would be to start with some detailed scenario involving breast-cancer screenings. (And not just because it tarnishes beautiful mathematics with images of sickness and death, either!)</p>\n<p>So what's the right way to explain Bayes' theorem to me?</p>\n<p>Like this:</p>\n<p>We've got a bunch of hypotheses (states the world could be in) and we're trying to figure out which of them is true (that is, which state the world is actually in). As a concession to concreteness (and for ease of drawing the pictures), let's say we've got three (mutually exclusive and exhaustive) hypotheses -- possible world-states -- which we'll call H<sub>1</sub>, H<sub>2</sub>, and H<sub>3</sub>. We'll represent these as blobs in space:</p>\n<p><img src=\"http://imgur.com/NpNUV.png\" alt=\"Figure 0\" width=\"225\" height=\"415\"></p>\n<p><strong id=\"___________________Figure_0\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp; Figure 0</strong></p>\n<p><br>Now, we have some prior notion of how probable each of these hypotheses is -- that is, each has some <em>prior probability</em>. If we don't know anything at all that would make one of them more probable than another, they would each have probability 1/3. To illustrate a more typical situation, however, let's assume we have more information than that. Specifically, let's suppose our prior probability distribution is as follows: P(H<sub>1</sub>) = 30%, P(H<sub>2</sub>)=50%, P(H<sub>3</sub>) = 20%. We'll represent this by resizing our blobs accordingly:</p>\n<p><img src=\"http://i.imgur.com/8JAkA.png\" alt=\"Figure 1\" width=\"337\" height=\"533\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; <strong>Figure 1<br></strong></p>\n<p>That's our <em>prior</em> knowledge. Next, we're going to collect some <em>evidence</em> and <em>update</em> our prior probability distribution to produce a <em>posterior</em> probability distribution. Specifically, we're going to run a test. The test we're going to run has three possible outcomes: Result A, Result B, and Result C. Now, since this test happens to have three possible results, it would be really nice if the test just flat-out told us which world we were living in -- that is, if (say) Result A meant that H<sub>1</sub> was true, Result B meant that H<sub>2</sub> was true, and Result 3 meant that H<sub>3</sub> was true. Unfortunately, the real world is messy and complex, and things aren't that simple. Instead, we'll suppose that each result can occur under each hypothesis, but that the different hypotheses have different effects on how likely each result is to occur. We'll assume for instance that if Hypothesis&nbsp; H<sub>1</sub> is true, we have a 1/2 chance of obtaining Result A, a 1/3 chance of obtaining Result B, and a 1/6 chance of obtaining Result C; which we'll write like this:</p>\n<p>P(A|H<sub>1</sub>) = 50%, P(B|H<sub>1</sub>) = 33.33...%, P(C|H<sub>1</sub>) = 16.166...%</p>\n<p>and illustrate like this:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgur.com/9jpzJ.png\" alt=\"\" width=\"384\" height=\"140\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 2</strong></p>\n<p>(Result A being represented by a triangle, Result B by a square, and Result C by a pentagon.)</p>\n<p>If Hypothesis H<sub>2</sub> is true, we'll assume there's a 10% chance of Result A, a 70% chance of Result B, and a 20% chance of Result C:</p>\n<p><img src=\"http://imgur.com/puWW1.png\" alt=\"Figure 3\" width=\"433\" height=\"180\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 3</strong></p>\n<p><strong><br></strong>(P(A|H<sub>2</sub>) = 10% , P(B|H<sub>2</sub>) = 70%, P(C|H<sub>2</sub>) = 20%)<strong><br></strong></p>\n<p>Finally, we'll say that if Hypothesis H<sub>3</sub> is true, there's a 5% chance of Result A, a 15% chance of Result B, and an 80% chance of Result C:<strong><br></strong></p>\n<p><img src=\"http://imgur.com/DHitn.png\" alt=\"Figure 4\" width=\"384\" height=\"140\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 4</strong></p>\n<p>(P(A|H<sub>3</sub>) = 5%, P(B|H<sub>3</sub>) = 15% P(C|H<sub>3</sub>) = 80%)</p>\n<p>Figure 5 below thus shows our knowledge prior to running the test:</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgur.com/qlyGw.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 5</strong></p>\n<p>&nbsp;</p>\n<p>Note that we have now carved up our hypothesis-space more finely; our possible world-states are now things like \"Hypothesis H<sub>1</sub> is true and Result A occurred\", \"Hypothesis H<sub>1</sub> is true and Result B occurred\", etc., as opposed to merely \"Hypothesis H<sub>1</sub> is true\", etc. The numbers above the slanted line segments -- the <em>likelihoods</em> of the test results, assuming the particular hypothesis -- represent <em>what proportion</em> of the total probability mass assigned to the hypothesis H<sub>n</sub> is assigned to the conjunction of Hypothesis H<sub>n</sub> and Result X; thus, since P(H<sub>1</sub>) = 30%, and P(A|H<sub>1</sub>) = 50%, P(H<sub>1</sub> &amp; A) is therefore 50% of 30%, or, in other words, 15%.</p>\n<p>(That's really all Bayes' theorem is, right there, but -- shh! -- don't tell anyone yet!)</p>\n<p><br>Now, then, suppose we run the test, and we get...Result A.</p>\n<p>What do we do? We <em>cut off all the other branches</em>:</p>\n<p><img src=\"http://imgur.com/XBXi5.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 6</strong></p>\n<p>&nbsp;</p>\n<p>So our updated probability distribution now looks like this:</p>\n<p><img src=\"http://imgur.com/nXENh.png\" alt=\"\"></p>\n<p><strong id=\"__________Figure_7\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 7</strong></p>\n<p><strong><br></strong></p>\n<p>...except for one thing: probabilities are supposed to add up to 100%, not 21%. Well, since we've <em>conditioned</em> on Result A, that means that the 21% probability mass assigned to Result A is now the entirety of our probability mass -- 21% is the new 100%, you might say. So we simply adjust the numbers in such a way that they add up to 100% <em>and the proportions are the same</em>:</p>\n<p><img src=\"http://i.imgur.com/RIeff.png\" alt=\"\" width=\"253\" height=\"541\"></p>\n<p><strong id=\"______________________Figure_8\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 8</strong></p>\n<p>There! We've just performed a Bayesian update. And that's what it <em>looks like</em>.</p>\n<p>&nbsp;</p>\n<p>If, instead of Result A, we had gotten Result B,</p>\n<p><img src=\"http://2.bp.blogspot.com/_Ig9I_03TGBQ/TAXmNUu1BpI/AAAAAAAAAAM/s9iVIdtmPy0/s1600/figure09.png\" alt=\"Figure 9\" width=\"460\" height=\"480\"></p>\n<p><strong id=\"______________________Figure_9\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 9</strong></p>\n<p><strong><br></strong></p>\n<p>then our updated probability distribution would have looked like this:</p>\n<p><img src=\"http://imgur.com/s9Tw5.png\" alt=\"\"></p>\n<p><strong id=\"_____________________Figure_10\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 10</strong></p>\n<p>&nbsp;</p>\n<p>Similarly, for Result C:</p>\n<p><img src=\"http://imgur.com/9Ikc0.png\" alt=\"\"></p>\n<p><strong id=\"_______________Figure_11\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 11</strong></p>\n<p><em>Bayes' theorem</em> is the formula that calculates these updated probabilities. Using H to stand for a hypothesis (such as H<sub>1</sub>, H<sub>2</sub> or H<sub>3</sub>), and E a piece of evidence (such as Result A, Result B, or Result C), it says:</p>\n<p>P(H|E) = P(H)*P(E|H)/P(E)</p>\n<p>In words: to calculate the updated probability P(H|E), take the portion of the prior probability of H that is allocated to E (i.e. the quantity P(H)*P(E|H)), and calculate what fraction this is of the total prior probability of E (i.e. divide it by P(E)).</p>\n<p>What I like about this way of visualizing Bayes' theorem is that it makes the importance of prior probabilities -- in particular, the difference between P(H|E) and P(E|H) -- <em>visually obvious</em>. Thus, in the above example, we easily see that even though P(C|H<sub>3</sub>) is high (80%), P(H<sub>3</sub>|C) is much less high (around 51%) -- and once you have assimilated this visualization method, it should be easy to see that even more extreme examples (e.g. with P(E|H) huge and P(H|E) tiny) could be constructed.</p>\n<p>Now let's use this to examine two tricky probability puzzles, the infamous <a href=\"http://en.wikipedia.org/wiki/Monty_Hall_problem\">Monty Hall Problem</a> and Eliezer's <a href=\"/lw/1lh/drawing_two_aces/\">Drawing Two Aces</a>, and see how it illustrates the correct answers, as well as how one might go wrong.</p>\n<p>&nbsp;</p>\n<h3 id=\"The_Monty_Hall_Problem\"><strong>The Monty Hall Problem</strong></h3>\n<p>The situation is this: you're a contestant on a game show seeking to win a car. Before you are three doors, one of which contains a car, and the other two of which contain goats. You will make an initial \"guess\" at which door contains the car -- that is, you will select one of the doors, without opening it. At that point, the host will open a goat-containing door from among the two that you did not select. You will then have to decide whether to stick with your original guess and open the door that you originally selected, or switch your guess to the remaining unopened door. The question is whether it is to your advantage to switch -- that is, whether the car is more likely to be behind the remaining unopened door than behind the door you originally guessed.</p>\n<p>(If you haven't thought about this problem before, you may want to try to figure it out before continuing...)</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>The answer is that it <em>is</em> to your advantage to switch -- that, in fact, switching <em>doubles</em> the probability of winning the car.</p>\n<p>People often find this counterintuitive when they first encounter it -- where \"people\" includes the author of this post. There are two possible doors that could contain the car; why should one of them be more likely to contain it than the other?</p>\n<p>As it turns out, while constructing the diagrams for this post, I \"rediscovered\" the error that led me to incorrectly conclude that there is a 1/2 chance the car is behind the originally-guessed door and a 1/2 chance it is behind the remaining door the host didn't open. I'll present that error first, and then show how to correct it. Here, then, is the <em>wrong</em> solution:</p>\n<p>We start out with a perfectly correct diagram showing the prior probabilities:</p>\n<p><img src=\"http://imgur.com/aXwYS.png\" alt=\"\"></p>\n<p><strong id=\"_______________Figure_12\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Figure 12</strong></p>\n<p>The possible hypotheses are Car in Door 1, Car in Door 2, and Car in Door 3; before the game starts, there is no reason to believe any of the three doors is more likely than the others to contain the car, and so each of these hypotheses has prior probability 1/3.</p>\n<p>The game begins with our selection of a door. That itself isn't <a href=\"http://wiki.lesswrong.com/wiki/Evidence\">evidence</a> about where the car is, of course -- we're assuming we have no particular information about that, other than that it's behind one of the doors (that's the whole point of the game!). Once we've done that, however, we will then have the opportunity to \"run a test\" to gain some \"experimental data\": the host will perform his task of opening a door that is guaranteed to contain a goat. We'll represent the result Host Opens Door 1 by a triangle, the result Host Opens Door 2 by a square, and the result Host Opens Door 3 by a pentagon -- thus carving up our hypothesis space more finely into possibilities such as \"Car in Door 1 and Host Opens Door 2\" , \"Car in Door 1 and Host Opens Door 3\", etc:</p>\n<p><img src=\"http://imgur.com/bIxZr.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 13</strong></p>\n<p><strong></strong><br>Before we've made our initial selection of a door, the host is equally likely to open either of the goat-containing doors. Thus, at the beginning of the game, the probability of each hypothesis of the form \"Car in Door X and Host Opens Door Y\" has a probability of 1/6, as shown. So far, so good; everything is still perfectly correct.</p>\n<p>Now we select a door; say we choose Door 2. The host then opens either Door 1 or Door 3, to reveal a goat. Let's suppose he opens Door 1; our diagram now looks like this:<br><br><br><img src=\"http://imgur.com/0xMQs.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 14</strong></p>\n<p>But this shows equal probabilities of the car being behind Door 2 and Door 3!</p>\n<p><img src=\"http://imgur.com/07q9g.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; <strong>Figure 15</strong></p>\n<p>Did you catch the mistake?</p>\n<p>Here's the <em>correct</em> version:<br><br><em>As soon as we selected Door 2</em>, our diagram should have looked like this:</p>\n<p><img src=\"http://imgur.com/tKGgR.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 16</strong></p>\n<p>&nbsp;</p>\n<p>With Door 2 selected, the host no longer has the <em>option</em> of opening Door 2; if the car is in Door 1, he <em>must</em> open Door 3, and if the car is in Door 3, he <em>must</em> open Door 1. We thus see that if the car is behind Door 3, the host is twice as <a href=\"http://wiki.lesswrong.com/wiki/Likelihood_ratio\">likely</a> to open Door 1 (namely, 100%) as he is if the car is behind Door 2 (50%); his opening of Door 1 thus constitutes <a href=\"http://wiki.lesswrong.com/wiki/Amount_of_evidence\">some evidence</a> in favor of the hypothesis that the car is behind Door 3. So, when the host opens Door 1, our picture looks as follows:</p>\n<p><img src=\"http://imgur.com/5U47D.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 17</strong></p>\n<p>&nbsp;</p>\n<p>which yields the correct updated probability distribution:</p>\n<p><img src=\"http://imgur.com/JYay2.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; <strong>Figure 18</strong></p>\n<p>&nbsp;</p>\n<h3 id=\"Drawing_Two_Aces\">Drawing Two Aces</h3>\n<p>Here is the statement of the problem, from <a href=\"/lw/1lh/drawing_two_aces/\">Eliezer's post</a>:</p>\n<blockquote>\n<p><br>Suppose I have a deck of four cards:&nbsp; The ace of spades, the ace of hearts, and two others (say, 2C and 2D).<br><br>You draw two cards at random.<br><br>(...)<br><br>Now suppose I ask you \"Do you have an ace?\"<br><br>You say \"Yes.\"<br><br>I then say to you:&nbsp; \"Choose one of the aces you're holding at random (so if you have only one, pick that one).&nbsp; Is it the ace of spades?\"<br><br>You reply \"Yes.\"<br><br>What is the probability that you hold two aces?</p>\n</blockquote>\n<p><br>(Once again, you may want to think about it, if you haven't already, before continuing...)</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Here's how our picture method answers the question:</p>\n<p><br>Since the person holding the cards has at least one ace, the \"hypotheses\" (possible card combinations) are the five shown below:</p>\n<p><img src=\"http://imgur.com/a3dxW.png\" alt=\"\"></p>\n<p><strong id=\"______Figure_19\">&nbsp; &nbsp; &nbsp; Figure 19</strong></p>\n<p>Each has a prior probability of 1/5, since there's no reason to suppose any of them is more likely than any other. <br><br>The \"test\" that will be run is selecting an ace at random from the person's hand, and seeing if it is the ace of spades. The possible results are:</p>\n<p><img src=\"http://imgur.com/XoXVj.png\" alt=\"\" width=\"330\" height=\"446\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Figure 20</strong></p>\n<p>&nbsp;</p>\n<p>Now we run the test, and get the answer \"YES\"; this puts us in the following situation:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://imgur.com/b2oLJ.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 21</strong></p>\n<p>&nbsp;</p>\n<p>The total prior probability of this situation (the YES answer) is (1/6)+(1/3)+(1/3) = 5/6; thus, since 1/6 is 1/5 of 5/6 (that is, (1/6)/(5/6) = 1/5), our updated probability is 1/5 -- which happens to be the same as the prior probability. (I won't bother displaying the final post-update picture here.)</p>\n<p>What this means is that the test we ran did not provide any additional information about whether the person has both aces beyond simply knowing that they have at least one ace; we might in fact say that the result of the test is <a href=\"http://wiki.lesswrong.com/wiki/Screening_off\">screened off</a> by the answer to the first question (\"Do you have an ace?\").</p>\n<p><br>On the other hand, if we had simply asked \"Do you have the ace of spades?\", the diagram would have looked like this:</p>\n<p><img src=\"http://imgur.com/CWtH4.png\" alt=\"\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; <strong>Figure 22</strong></p>\n<p>&nbsp;</p>\n<p>which, upon receiving the answer YES, would have become:</p>\n<p><img src=\"http://imgur.com/oc1YQ.png\" alt=\"\"></p>\n<p>&nbsp; <strong>Figure 23</strong></p>\n<p>The total probability mass allocated to YES is 3/5, and, within that, the specific situation of interest has probability 1/5; hence the updated probability would be 1/3.</p>\n<p>So a YES answer in this experiment, unlike the other, would provide <a href=\"http://wiki.lesswrong.com/wiki/Evidence\">evidence</a> that the hand contains both aces; for if the hand contains both aces, the probability of a YES answer is 100% -- twice as large as it is in the contrary case (50%), giving a <a href=\"http://wiki.lesswrong.com/wiki/Likelihood_ratio\">likelihood ratio</a> of 2:1. By contrast, in the other experiment, the probability of a YES answer is only 50% even in the case where the hand contains both aces.</p>\n<p><br>This is what people who try to explain the difference by uttering the opaque phrase \"a random selection was involved!\" are actually talking about: the difference between</p>\n<p><img src=\"http://imgur.com/IWwTV.png\" alt=\"\"></p>\n<p><strong id=\"__Figure_24\">&nbsp; Figure 24</strong></p>\n<p>&nbsp;</p>\n<p>and</p>\n<p><img src=\"http://imgur.com/ugqVb.png\" alt=\"\">.</p>\n<p><strong id=\"__Figure_25\">&nbsp; Figure 25</strong></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>The method explained here is far from the only way of visualizing Bayesian updates, but I feel that it is among the most intuitive.</p>\n<p>&nbsp;</p>\n<p>(<em>I'd like to thank my sister, </em><a href=\"/user/Vive-ut-Vivas/\">Vive-ut-Vivas</a><em>, for help with some of the diagrams in this post.)</em></p>", "sections": [{"title": "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 \u00a0\u00a0 Figure 0", "anchor": "___________________Figure_0", "level": 2}, {"title": "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Figure 7", "anchor": "__________Figure_7", "level": 2}, {"title": "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Figure 8", "anchor": "______________________Figure_8", "level": 2}, {"title": "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Figure 9", "anchor": "______________________Figure_9", "level": 2}, {"title": "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Figure 10", "anchor": "_____________________Figure_10", "level": 2}, {"title": "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Figure 11", "anchor": "_______________Figure_11", "level": 2}, {"title": "The Monty Hall Problem", "anchor": "The_Monty_Hall_Problem", "level": 1}, {"title": "\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Figure 12", "anchor": "_______________Figure_12", "level": 2}, {"title": "Drawing Two Aces", "anchor": "Drawing_Two_Aces", "level": 1}, {"title": "\u00a0 \u00a0 \u00a0 Figure 19", "anchor": "______Figure_19", "level": 2}, {"title": "\u00a0 Figure 24", "anchor": "__Figure_24", "level": 2}, {"title": "\u00a0 Figure 25", "anchor": "__Figure_25", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "195 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 195, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AN2cBr6xKWCB8dRQG", "Hug2ePykMkmPzSsx6", "baTWMegR42PAsH9qJ", "sSqoEw9eRP2kPKLCz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-03T20:30:29.518Z", "modifiedAt": null, "url": null, "title": "Hacking the CEV for Fun and Profit", "slug": "hacking-the-cev-for-fun-and-profit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.766Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HW5Q9cW9sgk4yCffd/hacking-the-cev-for-fun-and-profit", "pageUrlRelative": "/posts/HW5Q9cW9sgk4yCffd/hacking-the-cev-for-fun-and-profit", "linkUrl": "https://www.lesswrong.com/posts/HW5Q9cW9sgk4yCffd/hacking-the-cev-for-fun-and-profit", "postedAtFormatted": "Thursday, June 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hacking%20the%20CEV%20for%20Fun%20and%20Profit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHacking%20the%20CEV%20for%20Fun%20and%20Profit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHW5Q9cW9sgk4yCffd%2Fhacking-the-cev-for-fun-and-profit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hacking%20the%20CEV%20for%20Fun%20and%20Profit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHW5Q9cW9sgk4yCffd%2Fhacking-the-cev-for-fun-and-profit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHW5Q9cW9sgk4yCffd%2Fhacking-the-cev-for-fun-and-profit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<p class=\"MsoNormal\">It&rsquo;s the year 2045, and Dr. Evil and the Singularity Institute have been in a long and grueling race to be the first to achieve machine intelligence, thereby controlling the course of the Singularity and the fate of the universe. Unfortunately for Dr. Evil, SIAI is ahead in the game. Its <a href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a> is undergoing final testing, and <a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a> is scheduled to begin in a week. Dr. Evil learns of this news, but there&rsquo;s not much he can do, or so it seems.<span>&nbsp; </span>He has succeeded in developing brain scanning and emulation technology, but the emulation speed is still way too slow to be competitive.</p>\n<p class=\"MsoNormal\">There is no way to catch up with SIAI's superior technology in time, but Dr. Evil suddenly realizes that maybe he doesn&rsquo;t have to. CEV is supposed to give equal weighting to all of humanity, and surely uploads count as human. If he had enough storage space, he could simply upload himself, and then make a trillion copies of the upload. The rest of humanity would end up with less than 1% weight in CEV. Not perfect, but he could live with that. Unfortunately he only has enough storage for a few hundred uploads. What to do&hellip;</p>\n<p class=\"MsoNormal\">Ah ha, compression! A trillion identical copies of an object would compress down to be only a little bit larger than one copy. But would CEV count compressed identical copies to be separate individuals? Maybe, maybe not. To be sure, Dr. Evil gives each copy a unique experience before adding it to the giant compressed archive. Since they still share almost all of the same information, a trillion copies, after compression, just manages to fit inside the available space.</p>\n<p class=\"MsoNormal\">Now Dr. Evil sits back and relaxes. Come next week, the Singularity Institute and rest of humanity are in for a rather rude surprise!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 1, "NLwTnsH9RSotqXYLw": 1, "R6uagTfhhBeejGrrf": 1, "W6QZYSNt5FgWgvbdT": 1, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HW5Q9cW9sgk4yCffd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 75, "extendedScore": null, "score": 0.00014, "legacy": true, "legacyId": "2995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 75, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 207, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-04T16:08:40.556Z", "modifiedAt": null, "url": null, "title": "Virtue Ethics for Consequentialists", "slug": "virtue-ethics-for-consequentialists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.072Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZLBtZqsP79Cwioi2b/virtue-ethics-for-consequentialists", "pageUrlRelative": "/posts/ZLBtZqsP79Cwioi2b/virtue-ethics-for-consequentialists", "linkUrl": "https://www.lesswrong.com/posts/ZLBtZqsP79Cwioi2b/virtue-ethics-for-consequentialists", "postedAtFormatted": "Friday, June 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Virtue%20Ethics%20for%20Consequentialists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVirtue%20Ethics%20for%20Consequentialists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZLBtZqsP79Cwioi2b%2Fvirtue-ethics-for-consequentialists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Virtue%20Ethics%20for%20Consequentialists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZLBtZqsP79Cwioi2b%2Fvirtue-ethics-for-consequentialists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZLBtZqsP79Cwioi2b%2Fvirtue-ethics-for-consequentialists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1573, "htmlBody": "<p>Meta: Influenced by a <a href=\"http://xuenay.livejournal.com/332853.html\">cool blog post</a> by <a href=\"/user/Kaj_Sotala\">Kaj,</a> which was influenced by a cool <a href=\"/user/MichaelVassar\">Michael Vassar</a> (like pretty much everything else; the man sure has a lot of ideas). The name of this post is intended to be taken slightly more literally than the similarly titled <a title=\"Alicorn's post on confusion about deontology\" href=\"/lw/1og/deontology_for_consequentialists/\">Deontology for Consequentialists</a>.</p>\n<p>&nbsp;</p>\n<p>There's been a hip new trend going around the <a href=\"/lw/29c/be_a_visiting_fellow_at_the_singularity_institute/\">Singularity Institute Visiting Fellows</a> house lately, and it's not <a href=\"/lw/2ah/this_is_your_brain_on_ambiguity/22nx\">postmodernism</a>. It's <a title=\"An overview from the SEP.\" href=\"http://plato.stanford.edu/entries/ethics-virtue/\">virtue ethics</a>. \"What, virtue ethics?! Are you serious?\" Yup. I'm so contrarian I think <a href=\"/lw/2a8/abnormal_cryonics/\">cryonics isn't obvious</a> and that virtue ethics is better than consequentialism. This post will explain why.</p>\n<p>When I first heard about virtue ethics I assumed it was a clever way for people to justify things they did when the consequences were bad and the reasons were bad, too. People are very good at spinning tales about how virtuous they are, even more so than at finding good reasons that they could have done things that turned out unpopular, and it's hard to spin the consequences of your actions as good when everyone is keeping score. But it seems that moral theorists were mostly thinking in far mode and didn't have too much incentive to create a moral theory that benefited them the most, so my Hansonian hypothesis falls flat. Why did Plato and Aristotle and everyone up until the Enlightenment find virtue ethics appealing, then? Well...<a id=\"more\"></a></p>\n<p><strong>Moral philosophy was designed for humans, not for rational agents</strong>. When you're used to thinking about artificial intelligence, economics, and decision theory, it gets easy to forget that we're hyperbolic discounters: not anything resembling sane. Humans are not inherently expected utility maximizers, they're bounded agents with little capacity for reflection. Utility functions are great and all, but in the words of <a href=\"/lw/296/the_tragedy_of_the_social_epistemology_commons/21ca\">Zack M. Davis</a>, \"Humans don't <em>have</em> utility functions.\" Similarly, Kaj warns us: \"<a href=\"/lw/1qk/applying_utility_functions_to_humans_considered/\">be  extra careful when you try to apply the concept of a utility function  to human beings.</a>\" Back in the day nobody thought smarter-than-human intelligence was possible, and many still don't. Philosophers came up with ways for people to live their lives, have a good time, be respected, and do good things; they weren't even <em>trying</em> to create morals for anyone too far outside the norm of whatever society they inhabited at the time, or whatever society they imagined to be perfect. I personally think that the Buddha had some really interesting things to say and that his ideas about ethics are no exception (though I suspect he may have had <a href=\"http://en.wikipedia.org/wiki/Pain_asymbolia\">pain asymbolia</a>, which totally deserves its own post soon).&nbsp; Epicurus, Mill, and Bentham were great thinkers and all, but it's not obvious that what they were saying is best practice for individual people, even if their ideas about policy are strictly superior to alternative options. Virtue ethics is good for bounded agents: you don't have to waste memory on what a personalized rulebook says about different kinds of milk, and you don't have to think 15 inferential steps ahead to determine if you should drink skim or whole.</p>\n<p><strong>You can be a virtue ethicist whose virtue is to do the consequentialist thing to do </strong>(because your deontological morals say that's what is right). Consequentialists, deontologists, and virtue ethicists don't really disagree on any major points in day to day life, just in crazy situations like trolley problems. And anyway, they're all actually virtue ethicists: they're trying to do the 'consequentialist' or 'deontologist' things to do, which happen to usually be the same. Alicorn's decided to do her best to reduce existential risk, and I, being a pseudo-consequentialist, have also decided to do my best to reduce existential risk. Virtue ethicists can do these things too, but they can also abuse the <a href=\"/tag/consistency/\">consistency effects</a> such actions invariably come with. If you're a virtue ethicist it's easier to say \"I'm the type of person who will reply to all of the emails in my inbox and sort them into my <a href=\"https://addons.mozilla.org/en-US/firefox/addon/3209/\">GTD system</a>, because organization and contentiousness are virtues\" and use this as a way to motivate yourself. So go ahead and be a virtue ethicist for the consequences (...or a consequentialist because it's deontic). It's not illegal!</p>\n<p><strong>Retooled virtue ethics is better for your instrumental rationality</strong>. <em>The Happiness Hypothesis</em> critiqued the way Western ethics, both in the deontologist tradition started by Immanuel Kant and the consequentialist tradition started by Jeremy Bentham have been becoming increasingly reason-based:</p>\n<blockquote>The philosopher Edmund Pincoffs has argued that consequentialists and deontologists worked together to convince Westerners in the twentieth century that morality is the study of moral quandaries and dilemmas. Where the Greeks focused on the <em>character</em> of a person and asked what kind of person we should each aim to become, modern ethics focuses on <em>actions</em>, asking when a particular decision is right or wrong. Philosophers wrestle with life-and-death dilemmas: Kill one to save five? Allow aborted fetuses to be used as a source of stem cells? [...] This turn from character ethics to quandary ethics has turned moral education away from virtues and towards moral reasoning. If morality is about dilemmas, then moral education is training in problem solving. Children must be taught how to think about moral problems, especially how to overcome their natural egoism and take into their calculations the needs of others.<br /><br />[...] I believe that this turn from character to quandary was a profound mistake, for two reasons. First, it weakens morality and limits its scope. Where the ancients saw virtue and character at work in everything a person does, our modern conception confines morality to a set of situations that arise for each person only a few times in any given week [...] The second problem with the turn to moral reasoning is that it relies on bad psychology. Many moral education efforts since the 1970s take the rider off the elephant and train him to solve problems on his own. After being exposed to hours of case studies, classroom discussions about moral dilemmas, and videos about people who faced dilemmas and made the right choices, the child learns how (not what) to think. Then class ends, the rider gets back on the elephant, and nothing changes at recess. Trying to make children behave ethically by teaching them to reason well is like trying to make a dog happy by wagging its tail. It gets causality backwards.</blockquote>\n<p>To quote Kaj's response to the above:</p>\n<blockquote>\n<p>Reading this chapter, that critique and the description of how people like Benjamin Franklin made it into an explicit project to cultivate their various virtues one at a time, I could feel a very peculiar transformation take place within me. The best way I can describe it is that it felt like a part of my decision-making or world-evaluating machinery separated itself from the rest and settled into a new area of responsibility that I had previously not recognized as a separate one. While I had previously been primarily a consequentialist, that newly-specialized part declared its allegiance to virtue ethics, even though the rest of the machinery remained consequentialist. [...]<br /><br />What has this meant in practice? Well, I'm not quite sure of the long-term effects yet, but I think that my emotional machinery kind of separated from my general decision-making and planning machinery. Think of \"emotional machinery\" as a system that takes various sorts of information as input and produces different emotional states as output. Optimally, your emotional machinery should attempt to create emotions that push you towards taking the kinds of actions that are most appropriate given your goals. Previously I was sort of embedded in the world and the emotional system was taking its input from the entire whole: the way I was, the way the world was, and the way that those were intertwined. It was simultaneously trying to optimize for all three, with mixed results.<br /><br />But now, my self-model was set separate from the world-model, and my emotional machinery started running its evaluations primarily based on the self-model. The main questions became \"how could I develop myself\", \"how could I be more virtuous\" and \"how could I best act to improve the world\". From the last bit, you can see that I haven't lost the consequentialist layer in my decision-making: I am still trying to act in ways that improve the world. But now it's more like my emotional systems are taking input from the consequentialist planning system to figure out what virtues to concentrate on, instead of the consequentialist reasoning being completely intertwined with my emotional systems.</p>\n</blockquote>\n<p>Applying both consequentialist and virtue ethicist layers to the way you actually get things done in the real world seems to me a great idea. It recognizes that most of us don't actually have that much control over what we do. Acknowledging this and dealing with its consequences, and what it says about us, allows us to do the things we want and feel good about it at the same time.</p>\n<p>So, if you'd like, try to be a virtue ethicist for a week. If a key of epistemic rationality is having your beliefs pay rent in expected anticipation, then instrumental rationality is about having your actions pay rent in expected utility. Use science! If being a virtue ethicist helps even one person be more the person they want to be, like it did for Kaj, then this post was well worth the time spent.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 1, "8uNFGxejo5hykCEez": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZLBtZqsP79Cwioi2b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 44, "extendedScore": null, "score": 0.0005264750054754617, "legacy": true, "legacyId": "2962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 185, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yppdL4EXLWda5Wthn", "dLqFJaP2PjmztWNdX", "J5teWueouHJxcZkDy", "vuN57BvWyT7WZ3b6p"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-07T08:37:52.236Z", "modifiedAt": null, "url": null, "title": "Open Thread June 2010, Part 2", "slug": "open-thread-june-2010-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:08.694Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3Kkgek3PZ9snipyMY/open-thread-june-2010-part-2", "pageUrlRelative": "/posts/3Kkgek3PZ9snipyMY/open-thread-june-2010-part-2", "linkUrl": "https://www.lesswrong.com/posts/3Kkgek3PZ9snipyMY/open-thread-june-2010-part-2", "postedAtFormatted": "Monday, June 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20June%202010%2C%20Part%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20June%202010%2C%20Part%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Kkgek3PZ9snipyMY%2Fopen-thread-june-2010-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20June%202010%2C%20Part%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Kkgek3PZ9snipyMY%2Fopen-thread-june-2010-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Kkgek3PZ9snipyMY%2Fopen-thread-june-2010-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p>The title says it all.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3Kkgek3PZ9snipyMY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 5.936289318091444e-07, "legacy": true, "legacyId": "3006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 553, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-09T06:40:57.890Z", "modifiedAt": null, "url": null, "title": "Deploy Test", "slug": "deploy-test", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wmoore", "createdAt": "2009-02-17T05:49:50.396Z", "isAdmin": false, "displayName": "wmoore"}, "userId": "EgQZcMBqxf6sGmKfi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HBEukSjp77yjizqqx/deploy-test", "pageUrlRelative": "/posts/HBEukSjp77yjizqqx/deploy-test", "linkUrl": "https://www.lesswrong.com/posts/HBEukSjp77yjizqqx/deploy-test", "postedAtFormatted": "Wednesday, June 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deploy%20Test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeploy%20Test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBEukSjp77yjizqqx%2Fdeploy-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deploy%20Test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBEukSjp77yjizqqx%2Fdeploy-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBEukSjp77yjizqqx%2Fdeploy-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2, "htmlBody": "<p>More Testing...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HBEukSjp77yjizqqx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5.942063261701246e-07, "legacy": true, "legacyId": "3011", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-09T17:00:29.214Z", "modifiedAt": null, "url": null, "title": "Less Wrong Book Club and Study Group", "slug": "less-wrong-book-club-and-study-group", "viewCount": null, "lastCommentedAt": "2022-02-11T20:57:19.592Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tSDaxEq2WGHTRPcWB/less-wrong-book-club-and-study-group", "pageUrlRelative": "/posts/tSDaxEq2WGHTRPcWB/less-wrong-book-club-and-study-group", "linkUrl": "https://www.lesswrong.com/posts/tSDaxEq2WGHTRPcWB/less-wrong-book-club-and-study-group", "postedAtFormatted": "Wednesday, June 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20Book%20Club%20and%20Study%20Group&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20Book%20Club%20and%20Study%20Group%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSDaxEq2WGHTRPcWB%2Fless-wrong-book-club-and-study-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20Book%20Club%20and%20Study%20Group%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSDaxEq2WGHTRPcWB%2Fless-wrong-book-club-and-study-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSDaxEq2WGHTRPcWB%2Fless-wrong-book-club-and-study-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 363, "htmlBody": "<p>Do you want to <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger\">become stronger</a> in the way of Bayes? This post is intended for people whose understanding of Bayesian probability theory is currently somewhat tentative (between <a href=\"/lw/1yq/understanding_your_understanding\">levels 0 and 1</a> to use a previous post's terms), and who are interested in developing deeper knowledge through deliberate practice.</p>\n<p>Our intention is to form an online self-study group composed of peers, working with the assistance of a facilitator - but not necessarily of a teacher or of an expert in the topic. Some students may be somewhat more advanced along the path, and able to offer assistance to others.</p>\n<p>Our first text will be E.T. Jaynes' <a href=\"http://www.amazon.com/Probability-Theory-Logic-Science-Vol/dp/0521592712/ref=cm_cr_pr_product_top\"><em>Probability Theory: The Logic of Science</em></a>, which can be found in PDF form (in a slightly less polished version than the book edition) <a rel=\"nofollow\" href=\"http://www-biba.inrialpes.fr/Jaynes/prob.html\">here</a> or <a rel=\"nofollow\" href=\"http://omega.albany.edu:8008/JaynesBook.html\">here</a>.</p>\n<p>We will work through the text in sections, at a pace allowing thorough understanding: expect one new section every week, maybe every other week. A brief summary of the currently discussed section will be published as an update to this post, and simultaneously a comment will open the discussion with a few questions, or the statement of an exercise. Please use ROT13 whenever appropriate in your replies.</p>\n<p>A first comment below collects intentions to participate. Please reply to this comment <em>only</em> if you are genuinely interested in gaining a better understanding of Bayesian probability and willing to commit to spend a few hours per week reading through the section assigned or doing the exercises.</p>\n<p><a id=\"more\"></a>As a warm-up, participants are encouraged to start in on the book:</p>\n<h2>Preface</h2>\n<p>Most of the Preface can be safely skipped. It names the giants on whose shoulders Jaynes stood (\"History\", \"Foundations\"), deals briefly with the frequentist vs Bayesian controversy (\"Comparisons\"), discusses his \"Style of Presentation\" (and incidentally his distrust of <a href=\"/lw/gv/outside_the_laboratory/dr0\">infinite sets</a>), and contains the usual acknowledgements.<br /><br />One section, \"What is 'safe'?\", stands out as making several strong points about the use of probability theory. Sample: \"new data that we insist on analyzing in terms of old ideas (that is, models which are not questioned) <em>cannot lead us out of the old ideas</em>\". (The emphasis is Jaynes'. This has an almost Kuhnian flavor.)</p>\n<p>Discussion on the Preface starts <a href=\"/lw/2br/less_wrong_book_club_and_study_group/24td\">with this comment</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tSDaxEq2WGHTRPcWB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 43, "extendedScore": null, "score": 0.0005264750054754617, "legacy": true, "legacyId": "3015", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 161, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h", "4gevjbK77NQS6hybY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-09T18:18:17.750Z", "modifiedAt": null, "url": null, "title": "Bay Area Meetup Saturday 6/12", "slug": "bay-area-meetup-saturday-6-12", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:32.493Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Tarleton", "createdAt": "2009-03-05T18:07:15.687Z", "isAdmin": false, "displayName": "Nick_Tarleton"}, "userId": "nwrGYcfC4sPPn73Aw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a4hqTo3pTubuw7Tms/bay-area-meetup-saturday-6-12", "pageUrlRelative": "/posts/a4hqTo3pTubuw7Tms/bay-area-meetup-saturday-6-12", "linkUrl": "https://www.lesswrong.com/posts/a4hqTo3pTubuw7Tms/bay-area-meetup-saturday-6-12", "postedAtFormatted": "Wednesday, June 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bay%20Area%20Meetup%20Saturday%206%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABay%20Area%20Meetup%20Saturday%206%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa4hqTo3pTubuw7Tms%2Fbay-area-meetup-saturday-6-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bay%20Area%20Meetup%20Saturday%206%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa4hqTo3pTubuw7Tms%2Fbay-area-meetup-saturday-6-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa4hqTo3pTubuw7Tms%2Fbay-area-meetup-saturday-6-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p>There will be a meetup at 7PM this Saturday at the SIAI house (3755 Benton St, Santa Clara). <a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/13505031/\">(More info on the official page.)</a></p>\n<p>The usual set of people will be present, as well as new SIAI Visiting Fellows.</p>\n<p>(Sorry for the short notice.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a4hqTo3pTubuw7Tms", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 5.943522020724296e-07, "legacy": true, "legacyId": "3016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-10T00:11:30.121Z", "modifiedAt": "2020-08-10T22:13:18.179Z", "url": null, "title": "Your intuitions are not magic", "slug": "your-intuitions-are-not-magic", "viewCount": null, "lastCommentedAt": "2022-01-26T07:37:22.559Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic", "pageUrlRelative": "/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic", "linkUrl": "https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic", "postedAtFormatted": "Thursday, June 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Your%20intuitions%20are%20not%20magic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYour%20intuitions%20are%20not%20magic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPsp8ZpYLCDJjshpRb%2Fyour-intuitions-are-not-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Your%20intuitions%20are%20not%20magic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPsp8ZpYLCDJjshpRb%2Fyour-intuitions-are-not-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPsp8ZpYLCDJjshpRb%2Fyour-intuitions-are-not-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1124, "htmlBody": "<p>People who know a little bit of statistics - enough to use statistical techniques, not enough to understand why or how they work - often end up horribly misusing them. Statistical tests are complicated mathematical techniques, and to work, they tend to make numerous assumptions. The problem is that if those assumptions are not valid, most statistical tests do not cleanly fail and produce obviously false results. Neither do they require you to carry out impossible mathematical operations, like dividing by zero. Instead, they simply produce results that do not tell you what you think they tell you. As a formal system, pure math exists only inside our heads. We can try to apply it to the real world, but if we are misapplying it, nothing in the system itself will tell us that we're making a mistake.</p>\n<p>Examples of misapplied statistics have been discussed here before. Cyan discussed a \"test\" that <a href=\"/lw/1sk/case_study_abuse_of_frequentist_statistics/\">could only produce one outcome</a>. PhilGoetz critiqued a statistical method which implicitly assumed that taking a <a href=\"/lw/20i/even_if_you_have_a_nail_not_all_hammers_are_the/\">healthy dose of vitamins had a comparable effect as taking a toxic dose</a>.<br /><br />Even a very simple statistical technique, like taking the correlation between two variables, might be misleading if you forget about the assumptions it's making. When someone says \"correlation\", they are most commonly talking about <a href=\"http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient\">Pearson's correlation coefficient</a>, which seeks to gauge whether there's a linear relationship between two variables. In other words, if X increases, does Y also tend to increase. (Or decrease.) However, like with vitamin dosages and their effects on health, two variables might have a non-linear relationship. Increasing X might increase Y up to a certain point, after which increasing X would decrease Y. Simply calculating Pearson's correlation on two such variables might cause someone to get a low correlation, and therefore conclude that there's no relationship or there's only a weak relationship between the two. (See also <a href=\"http://en.wikipedia.org/wiki/Anscombe's_quartet\">Anscombe's quartet</a>.)<br /><br />The lesson here, then, is that not understanding how your analytical tools work will get you incorrect results when you try to analyze something. A person who doesn't stop to consider the assumptions of the techniques she's using is, in effect, thinking that her techniques are magical. No matter how she might use them, they will always produce the right results. Of course, assuming that makes about as much sense as assuming that your hammer is magical and can be used to repair anything. Even if you had a broken window, you could fix that by hitting it with your magic hammer. But I'm not <em>only</em> talking about statistics here, for the same principle can be applied in a more general manner.</p>\n<p><a></a><br />Every moment in our lives, we are trying to make estimates of the way the world works. Of what causal relationships there are, of what ways of describing the world make sense and which ones don't, which plans will work and which ones will fail. In order to make those estimates, we need to draw on a vast amount of information our brains have gathered throughout our lives. Our brains keep track of countless pieces of information that we will not usually even think about. Few people will explicitly keep track of the amount of different restaurants they've seen. Yet in general, if people are asked about the relative number of restaurants in various fast-food chains, their estimates <a href=\"http://www-personal.umich.edu/~jjonides/pdf/1992.pdf\">generally bear a close relation to the truth</a>.<br /><br />But like explicit statistical techniques, the brain makes numerous assumptions when building its models of the world. Newspapers are selective in their reporting of disasters, focusing on rare shocking ones above common mundane ones. Yet our brains assume that we hear about all those disasters because we've personally witnessed them, and that the distribution of disasters in the newspapers therefore reflects the distribution of disasters in the real world. Thus, people asked to estimate the frequency of different causes of death <a href=\"http://www.eric.ed.gov/ERICWebPortal/custom/portlets/recordDetails/detailmini.jsp?accno=EJ219546\">underestimate the frequency of those that are underreported</a> in the media, and overestimate the ones that are overreported.<br /><br />On this site, we've also discussed a <a href=\"http://wiki.lesswrong.com/wiki/Category:Biases\">variety of other ways</a> by which the brain's reasoning sometimes goes wrong: <a href=\"http://wiki.lesswrong.com/wiki/Absurdity_heuristic\">the absurdity heuristic</a>, <a href=\"http://wiki.lesswrong.com/wiki/Affect_heuristic\">the affect heuristic</a>, <a href=\"http://wiki.lesswrong.com/wiki/Affective_death_spiral\">the affective death spiral</a>, <a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">the availability heuristic</a>, <a href=\"http://wiki.lesswrong.com/wiki/Conjunction_fallacy\">the conjunction fallacy</a>... the list goes on and on.<br /><br />So what happens when you've read too many newspaper articles and then naively wonder about how frequent different disasters are? You are querying your unconscious processes about a certain kind of statistical relationship, and you get an answer back. But like the person who was naively misapplying her statistical tools, the process which generates the answers is a black box to you. You do not know how or why it works. If you would, you could tell when its results were reliable, when they needed to be explicitly corrected for, and when they were flat-out wrong.<br /><br />Sometimes we rely on <a href=\"/lw/19v/intuitive_differences_when_to_agree_to_disagree/\">our intuitions</a> even when they are being directly contradicted by math and science. The science seems absurd and unintuitive; our intuitions seem firm and clear. And indeed, sometimes there's a flaw in the science, and we are right to trust our intuitions. But on other occasions, our intuitions are wrong. Yet we frequently persist in holding onto our intuitions. And what is ironic is that we persist on holding onto them exactly because we do not know how they work, because we cannot see their insides and all the things inside them that could go wrong. We only get the feeling of certainty, a knowledge of <em>this being right</em>, and that feeling cannot be broken into parts that could be subjected to criticism to see if they add up.<br /><br />But like statistical techniques in general, our intuitions are not magic. Hitting a broken window with a hammer will not fix the window, no matter how reliable the hammer. It would certainly be <em>easy</em> and <em>convenient</em> if our intuitions always gave us the right results, just like it would be <em>easy</em> and <em>convenient</em> if our statistical techniques always gave us the right results. Yet carelessness can cost lives. Misapplying a statistical technique when evaluating the safety of a new drug might kill people or cause them to spend money on a useless treatment. Blindly following our intuitions can cause our careers, relationships or lives to crash and burn, because we did not think of the possibility that we might be wrong.<br /><br />That is why we need to study the <a href=\"http://en.wikipedia.org/wiki/Cognitive_science\">cognitive sciences</a>, figure out the way our intuitions work and how we might correct for mistakes. Above all, we need to learn to always question the workings of our minds, for we need to understand that they are not magical.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "dJ6eJxJrCEget7Wb6": 2, "3ee9k6NJfcGzL6kMS": 2, "z95PGFXtPpwakqkTA": 7, "4R8JYu4QF2FqzJxE5": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Psp8ZpYLCDJjshpRb", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 144, "baseScore": 150, "extendedScore": null, "score": 0.000252, "legacy": true, "legacyId": "3018", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 150, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["XrxsFR2WLXWzgLEoy", "pWi5WmvDcN4Hn7Bo6", "JF4mv4PbTp6ckN3vG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-10T05:01:06.970Z", "modifiedAt": null, "url": null, "title": "UDT agents as deontologists", "slug": "udt-agents-as-deontologists", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:33.309Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tyrrell_McAllister", "createdAt": "2009-03-05T19:59:57.157Z", "isAdmin": false, "displayName": "Tyrrell_McAllister"}, "userId": "HSANMQBsHiGrZzwTB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jnKaDMvdkFEoBT2qK/udt-agents-as-deontologists", "pageUrlRelative": "/posts/jnKaDMvdkFEoBT2qK/udt-agents-as-deontologists", "linkUrl": "https://www.lesswrong.com/posts/jnKaDMvdkFEoBT2qK/udt-agents-as-deontologists", "postedAtFormatted": "Thursday, June 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20UDT%20agents%20as%20deontologists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUDT%20agents%20as%20deontologists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnKaDMvdkFEoBT2qK%2Fudt-agents-as-deontologists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=UDT%20agents%20as%20deontologists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnKaDMvdkFEoBT2qK%2Fudt-agents-as-deontologists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjnKaDMvdkFEoBT2qK%2Fudt-agents-as-deontologists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1794, "htmlBody": "<p>One way (the usual way?) to think of an agent running&nbsp;<a href=\"/lw/15m/towards_a_new_decision_theory/\">Updateless Decision Theory</a>&nbsp;is to imagine that the agent always cares about all possible worlds&nbsp;according to how probable&nbsp;those worlds seemed to the agent's builders when they wrote the agent's source code<sup><strong>[see added footnote 1</strong></sup><sup><strong>&nbsp;below]</strong></sup><span style=\"font-size: small; font-weight: normal;\">.</span><strong><span style=\"font-size: small; font-weight: normal;\"> &nbsp;In particular, the agent never develops any additional concern for whatever turns out to be the actual world</span></strong><strong><span style=\"font-size: small; font-weight: normal;\"><sup>[2]</sup>.<span style=\"font-size: 11px;\"><span style=\"font-size: small;\">&nbsp;&nbsp;This is what puts the \"U\" in \"UDT\".</span></span></span></strong></p>\n<p>I suggest an alternative conception of a UDT agent, without changing the UDT formalism. According to this view, the agent cares<em><span style=\"font-style: normal;\">&nbsp;about&nbsp;<span style=\"font-style: italic;\">only&nbsp;<span style=\"font-style: normal;\">the actual world. &nbsp;In fact, at any time, the agent cares&nbsp;about&nbsp;only&nbsp;one small facet of the actual world &mdash; namely, whether the agent's act at that time maximizes a certain fixed act-evaluating function. &nbsp;In effect, a UDT agent is the ultimate deontologist: &nbsp;It doesn't care at all about the actual consequences that result from its action. &nbsp;One implication of this conception&nbsp;is that a UDT agent cannot be <em><a href=\"/lw/tn/the_true_prisoners_dilemma/\">truly</a></em>&nbsp;<a href=\"/lw/3l/counterfactual_mugging/\">counterfactually mugged</a>.</span></span></span></em></p>\n<p><em><span style=\"font-style: normal;\"><span style=\"font-style: italic;\"><span style=\"font-style: normal;\"><strong>[ETA:</strong>&nbsp;For completeness, I give a description of UDT&nbsp;<a href=\"http://dl.dropbox.com/u/34639481/Updateless_Decision_Theory.pdf\">here</a>&nbsp;(pdf).]</span></span></span></em></p>\n<p><em></em></p>\n<p><a id=\"more\"></a></p>\n<p>Vladimir Nesov's&nbsp;<a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a>&nbsp;presents us with the following scenario:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></p>\n<blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Imagine that one day,&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://www.overcomingbias.com/2008/01/newcombs-proble.html\"><span style=\"font-family: mceinline;\">Omega</span></a>&nbsp;comes to you and says that it has just tossed a fair coin, and given that the coin came up tails, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But see, the Omega tells you that if the coin came up heads instead of tails, it'd give you $10000, but only if you'd agree to give it $100 if the coin came up tails.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Omega can predict your decision in case it asked you to give it $100, even if that hasn't actually happened, it can compute the&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://www.overcomingbias.com/2008/07/counterfactual.html\">counterfactual truth</a>. The Omega is also known to be absolutely honest and trustworthy, no word-twisting, so the facts are really as it says, it really tossed a coin and really would've given you $10000.</p>\n</blockquote>\n<p>An agent following UDT will give the $100. &nbsp;Imagine that we were building an agent, and that we will receive whatever utility follows from the agent's actions. &nbsp;Then it's easy to see why we should build our agent to give Omega the money in this scenario. &nbsp;After all, at the time we build our agent, we know that Omega <em>might</em> one day flip a fair coin with the intentions Nesov describes. &nbsp;Whatever probability this has of happening, our expected earnings are greater if we program our agent to give Omega the $100 on tails.</p>\n<p>More generally, if we suppose that we get whatever utility will follow from our agent's actions, then we can do no better than to program the agent to follow UDT. &nbsp;But since we have to program the UDT agent <em>now</em>, the act-evaluating function that determines how the agent will act needs to be fixed with the probabilities that we know <em>now</em>. &nbsp;This will suffice to maximize our expected utility given our best knowledge at the time when we build the agent.</p>\n<p>So, it makes sense for a builder to program an agent to follow UDT on expected-utility grounds. &nbsp;We can understand the builder's motivations. &nbsp;We can get inside the builder's head, so to speak.</p>\n<p>But what about the agent's head? &nbsp;The brilliance of Nesov's scenario is that it is so hard, on first hearing it, to imagine why a reasonable agent would give Omega the money&nbsp;<em>knowing</em>&nbsp;that the only result will be that they gave up $100. &nbsp;It's easy enough to follow the UDT formalism. &nbsp;But what on earth could the UDT agent itself be thinking? &nbsp;Yes, trying to figure this out is an exercise in anthropomorphization.&nbsp;&nbsp;Nonetheless, I think that it is worthwhile if we are going to use UDT to try to understand what <em>we</em>&nbsp;ought to do.</p>\n<p>Here are three ways to conceive of the agent's thinking when it gives Omega the $100. &nbsp;They form a sort of spectrum.</p>\n<ol>\n<li>One extreme view: &nbsp;The agent considers all the possible words to be on equal ontological footing. &nbsp;There is no sense in which any one of them is distinguished as \"actual\" by the agent. &nbsp;It conceives of itself as acting simultaneously in all the possible worlds so as to maximize utility over all of them. &nbsp;Sometimes this entails acting in one world so as to make things worse <em>in that world</em>. &nbsp;But, no matter which world this is, there is nothing special about it. &nbsp;The only property of the world that has any ontologically significance is the probability weight given to that world at the time that the agent was built. (I believe that this is roughly the view that Wei Dai himself takes, but I may be wrong.)</li>\n<li>An intermediate view: &nbsp;The agent thinks that there is only one actual world. &nbsp;That is, there is an ontological fact of the matter about which world is actual. &nbsp;However, the other possible worlds continue to exist in some sense, although they are merely possible, not actual. &nbsp;Nonetheless, the agent continues to&nbsp;<em style=\"font-style: italic; \">care</em>&nbsp;about all of the possible worlds, and this amount of care never changes. &nbsp;After being counterfactually mugged, the agent is happy to know that, in some merely-possible world, Omega gave the agent&nbsp;$10000.</li>\n<li>The other extreme: &nbsp;As in (2), the agent thinks that there is only one actual world. &nbsp;Contrary to (2), the agent cares about only this world. &nbsp;However, the agent is a <a href=\"http://plato.stanford.edu/entries/ethics-deontological/\">deontologist</a>. &nbsp;When deciding how to act, all that it cares about is whether its act in this world is \"right\", where \"right\" means \"maximizes the fixed act-evaluating function that was built into me.\"</li>\n</ol>\n<p>View (3) is the one that I wanted to develop in this post.&nbsp;&nbsp;On this view, the \"probability distribution\" in the act-evaluating function no longer has any epistemic meaning for the agent. &nbsp;The act-evaluating function is just a particular computation which, for the agent, constitutes the essence of rightness. &nbsp;Yes, the computation involves considering some counterfactuals, but to consider those counterfactuals does not entail any ontological commitment.</p>\n<p>Thus, when the agent has been counterfactually mugged, it's not (as in (1)) happy because it cares about expected utility over all possible worlds. &nbsp;It's not (as in (2)) happy because, in some merely-possible world, Omega gave it $10000. &nbsp;On this view, the agent considers all those \"possible worlds\" to have been rendered impossible by what it has learned since it was built. &nbsp;The reason the agent is happy is that <em>it did the right thing</em>. &nbsp;Merely doing the right thing has given the agent all the utility it could hope for. &nbsp;More to the point, the agent got that utility <em>in the actual world.</em> &nbsp;The agent knows that it did the right thing, so it genuinely does not care about what actual consequences will follow from its action.</p>\n<ol> </ol>\n<p>In other words, although the agent lost $100, it really gained from the interaction with Omega. &nbsp;This suggests that we try to consider a \"true\" analog of the Counterfactual Mugging. &nbsp;In&nbsp;<a href=\"/lw/tn/the_true_prisoners_dilemma/\">The True Prisoner's Dilemma</a>, Eliezer Yudkowsky presents a version of the Prisoner's Dilemma in which it's viscerally clear that the payoffs at stake capture <em>everything</em> that we care about, not just our selfish values. &nbsp;The point is to make the problem&nbsp;<a href=\"/lw/tn/the_true_prisoners_dilemma/n0j\">about&nbsp;</a><a href=\"/lw/tn/the_true_prisoners_dilemma/n0j\">utilons</a>, and not about some stand-in, such as years in prison or dollars.</p>\n<p>In a True&nbsp;Counterfactual Mugging, Omega would ask the agent to give up <em>utility.</em>&nbsp;&nbsp;Here we see that the UDT agent cannot possibly do as Omega asks. &nbsp;Whatever it chooses to do will turn out to have <em>in fact</em> maximized its utility. &nbsp;Not just expected utility, but actual utility. In the original Counterfactual Mugging, the agent looks like something of a chump who gave up $100 for nothing. &nbsp;But in the&nbsp;True&nbsp;Counterfactual Mugging, our deontological agent lives with the satisfaction that, no matter what it does, it lives in the best of all possible worlds.</p>\n<p>&nbsp;</p>\n<hr />\n<p>[1] <strong>ETA</strong>: Under UDT, the agent assigns a utility to having all of the possible worlds&nbsp;P1, P2, . . . undergo respective execution histories E1, E2, . . .. &nbsp;(The way that a world evolves may depend in part on the agent's action). &nbsp;That is, for each vector &lt;E1, E2, . . .&gt; of ways that these worlds could respectively evolve, the agent assigns a utility U(&lt;E1, E2, . . .&gt;). &nbsp;Due to criticisms by Vladimir Nesov (beginning <a href=\"/lw/2bt/udt_agents_are_deontologists/24qm\">here</a>), I have realized that this post only applies to instances of UDT in which the utility function U takes the form that it has in standard decision theories.&nbsp; In this case, each world P<em>i</em> has its own probability pr(P<em>i</em>)&nbsp;and its own utility function u<sub><em>i</em></sub>&nbsp;that takes an execution history of P<em>i</em>&nbsp;alone as input, and the function U takes the form</p>\n<p style=\"padding-left: 30px; \">U(&lt;E1, E2, . . .&gt;) =&nbsp;&Sigma;<sub><em>i</em></sub>&nbsp;pr(P<em>i</em>)&nbsp;u<sub><em>i</em></sub>(E<em>i</em>).</p>\n<p>The probabilities pr(P<em>i</em>) are what I'm talking about when I mention probabilities in this post. &nbsp;Wei Dai is interested in instances of UDT with more general utility functions U. &nbsp;However, to my knowledge, this special kind of utility function is the only one in terms of which he's talked about the <a href=\"/lw/1iy/what_are_probabilities_anyway/\">meanings of&nbsp;probabilities&nbsp;of possible worlds</a>&nbsp;in UDT. &nbsp;See in particular this quote from the original UDT post:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">If your preferences for what happens in one such program is independent of what happens in another, then we can represent them by a probability distribution on the set of programs plus a utility function on the execution of each individual program.</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">(A \"program\" is what Wei Dai calls a possible world in that post.) &nbsp;The utility function U is \"baked in\" to the UDT agent at the time it's created. &nbsp;Therefore, so too are the probabilities pr(P<em>i</em>).</span></p>\n<p>[2] By&nbsp;\"the actual world\", I do <em>not</em>&nbsp;mean one of the worlds in the many-worlds interpretation (MWI) of quantum mechanics. &nbsp;I mean something more like the <a href=\"/lw/qp/timeless_physics/\">entire path</a> traversed by the quantum state vector of the universe through its corresponding Hilbert space. &nbsp;Distinct possible worlds are distinct paths that the state of the universe might (for all we know) be traversing in this Hilbert space. &nbsp;All the \"many worlds\" of the MWI together constitute a <em>single</em> world in the sense used here.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p><strong>ETA:</strong>&nbsp;This post was originally titled \"UDT agents are deontologists\". &nbsp;I changed the title to \"UDT agents as deontologists\" to emphasize that I am describing a way to <em>view</em> UDT agents. &nbsp;That is, I am describing an interpretive framework for understanding the agent's thinking.&nbsp; My proposal is analogous to Dennett's \"intentional stance\".&nbsp; To take the intentional stance is not to make a claim about what a conscious organism is doing.&nbsp; Rather, it is to make use of a framework for organizing our understanding of the organism's behavior.&nbsp; Similarly, I am not suggesting that UDT somehow gets things wrong.&nbsp; I am saying that it might be more natural for us if we think of the UDT agent as a deontologist, instead of as an agent that never changes its belief about which possible worlds will actually happen.&nbsp; I say a little bit more about this in <a href=\"/lw/2bt/udt_agents_as_deontologists/24t1\">this comment</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jnKaDMvdkFEoBT2qK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 13, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "3017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["de3xjFaACCAk6imzv", "HFyWNBnDNEDsDNLrZ", "mg6jDEuQEjBGtibX7", "J7Gkz8aDxxSEQKXTN", "rrW7yf42vQYDf8AcH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-10T17:25:32.430Z", "modifiedAt": null, "url": null, "title": "H+ Summit Meetup Harvard 6/12", "slug": "h-summit-meetup-harvard-6-12", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:33.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S7yJJKBQeap53BMAn/h-summit-meetup-harvard-6-12", "pageUrlRelative": "/posts/S7yJJKBQeap53BMAn/h-summit-meetup-harvard-6-12", "linkUrl": "https://www.lesswrong.com/posts/S7yJJKBQeap53BMAn/h-summit-meetup-harvard-6-12", "postedAtFormatted": "Thursday, June 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20H%2B%20Summit%20Meetup%20Harvard%206%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AH%2B%20Summit%20Meetup%20Harvard%206%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS7yJJKBQeap53BMAn%2Fh-summit-meetup-harvard-6-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=H%2B%20Summit%20Meetup%20Harvard%206%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS7yJJKBQeap53BMAn%2Fh-summit-meetup-harvard-6-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS7yJJKBQeap53BMAn%2Fh-summit-meetup-harvard-6-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p><em>Just realized I hadn't seen a post about this, and couldn't find mention of it by searching...</em></p>\n<p>The <a href=\"http://hplussummit2010east.eventbrite.com/\">H+ Summit</a>&nbsp;is happening this Saturday and Sunday at Harvard. &nbsp;I assume there are quite a few <a href=\"http://humanityplus.org/\">Humanity+</a>&nbsp;fellow-travelers here who might be going (though it's kindof late notice if this is the first time you've heard of it).</p>\n<p>I'm interested in a Lw meet-up or anything of the sort on Saturday evening (June 12). &nbsp;Who's in?</p>\n<p>&nbsp;</p>\n<p><strong>EDIT (6/12):</strong>&nbsp;Meeting at h+ afterparty instead. &nbsp;It will be 6 to 8 at Sprout. &nbsp;<a href=\"http://diybio.org/hplusbeer/\">http://diybio.org/hplusbeer/</a></p>\n<p>I'll be the one named Thom Blake.</p>\n<p><a id=\"more\"></a></p>\n<p><span style=\"font-family: mceinline;\">&nbsp;</span><strong><span style=\"font-family: mceinline;\">EDIT (6/11): </span></strong><span style=\"font-family: mceinline;\">Location: </span><a href=\"http://www.grendelsden.com/\"><span style=\"font-family: mceinline;\">Grendel's Den</span></a><span style=\"font-family: mceinline;\">&nbsp;&nbsp;Time: About 6ish. &nbsp;We might move if it's not suitable - apparently it may be overrun by sports fans.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S7yJJKBQeap53BMAn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 5.946425906799825e-07, "legacy": true, "legacyId": "3027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-14T00:35:15.424Z", "modifiedAt": null, "url": null, "title": "How to always have interesting conversations", "slug": "how-to-always-have-interesting-conversations", "viewCount": null, "lastCommentedAt": "2022-02-02T20:44:48.850Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Kaj_Sotala", "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d9CcQ24ukbL8WcMpB/how-to-always-have-interesting-conversations", "pageUrlRelative": "/posts/d9CcQ24ukbL8WcMpB/how-to-always-have-interesting-conversations", "linkUrl": "https://www.lesswrong.com/posts/d9CcQ24ukbL8WcMpB/how-to-always-have-interesting-conversations", "postedAtFormatted": "Monday, June 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20always%20have%20interesting%20conversations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20always%20have%20interesting%20conversations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd9CcQ24ukbL8WcMpB%2Fhow-to-always-have-interesting-conversations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20always%20have%20interesting%20conversations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd9CcQ24ukbL8WcMpB%2Fhow-to-always-have-interesting-conversations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd9CcQ24ukbL8WcMpB%2Fhow-to-always-have-interesting-conversations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 699, "htmlBody": "<p>One of the things that makes Michael Vassar an interesting person to be around is that he has an opinion about everything. If you locked him up in an empty room with grey walls, it would probably take the man about thirty seconds before he&#x27;d start analyzing the historical influence of the Enlightenment on the tradition of locking people up in empty rooms with grey walls.<br/><br/>Likewise, in the <a href=\"/lw/2bs/bay_area_meetup_saturday_612/\">recent LW meetup</a>, I noticed that I was naturally drawn to the people who most easily ended up talking about interesting things. I spent a while just listening to <a href=\"/user/HughRistik/\">HughRistik&#x27;s</a> theories on the differences between men and women, for instance. There were a few occasions when I engaged in some small talk with new people, but not all of them took very long, as I failed to lead the conversation into territory where one of us would have plenty of opinions.<br/><br/>I have two major deficiencies in trying to mimic this behavior. One, I&#x27;m by nature more of a listener than speaker. I usually prefer to let other people talk so that I can just soak up the information being offered. Second, my <a href=\"http://www.overcomingbias.com/2008/08/use-the-native.html\">native way of thought</a> is closer to text than speech. At best, I can generate thoughts as fast as I can type. But in speech, I often have difficulty formulating my thoughts into coherent sentences fast enough and frequently hesitate.<br/><br/>Both of these problems are solvable by having a sufficiently well built-up storage of <a href=\"http://wiki.lesswrong.com/wiki/Cached_thought\">cached thoughts</a> that I don&#x27;t need to generate everything in real time. On the occasions when a conversations happens to drift into a topic I&#x27;m sufficiently familiar with, I&#x27;m often able to overcome the limitations and contribute meaningfully to the discussion. This implies two things. First, that I need to generate cached thoughts in more subjects than I currently have. Seconds, that I need an ability to more reliably steer conversation into subjects that I actually do have cached thoughts about.<br/><br/>Below is a preliminary &quot;conversational map&quot; I generated as an exercise. The top three subjects - the weather, the other person&#x27;s background (job and education), people&#x27;s hobbies - are classical small talk subjects. Below them are a bunch of subjects that I feel like I can spend at least a while talking about, and possible paths leading from one subject to another. My goal in generating the map is to create a huge web of interesting subjects, so that I can use the small talk openings to bootstrap the conversation into basically anything I happen to be interested in.</p><span><figure><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1564851876/CChart_tct3qw.jpg\" class=\"draft-image center\" style=\"width:40%\" /></figure></span><p>This map is still pretty small, but it can be expanded to an arbitrary degree. (This is also one of the times when I wish my netbook had a bigger screen.) I <strong>thought</strong> that I didn&#x27;t have very many things that I could easily talk with people about, but once I started explicitly brainstorming for them, I realized that there were a lot of those.<br/><br/>My intention is to spend a while generating conversational charts like this and then spend some time fleshing out the actual transitions between subjects. The benefit from this process should be two-fold. Practice in creating transitions between subjects will make it easier to generate such transitions in real time conversations. And if I can&#x27;t actually come up with anything in real time, I can fall back to the cache of transitions and subjects that I&#x27;ve built up.<br/><br/>Naturally, the process needs to be guided by what the other person shows an interest in. If they show no interest in some subject I mention, it&#x27;s time to move the topic to another cluster. Many of the subjects in this chart are also pretty inflammable: there are environments where pretty much everything in the politics cluster should probably be kept off-limits, for instance. Exercise your common sense when building and using your own conversational charts.<br/><br/>(Thanks to <a href=\"/user/JustinShovelain\">Justin Shovelain</a> for mentioning that Michael Vassar seems to have a big huge conversational web that all his discussions take place in. That notion was one of the original sources for this idea.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d9CcQ24ukbL8WcMpB", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 68, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "3048", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 351, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["a4hqTo3pTubuw7Tms"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-14T06:14:36.760Z", "modifiedAt": null, "url": null, "title": "Open Thread June 2010, Part 3", "slug": "open-thread-june-2010-part-3", "viewCount": null, "lastCommentedAt": "2019-08-21T09:26:03.173Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/meLBCXvvGcTNCet4g/open-thread-june-2010-part-3", "pageUrlRelative": "/posts/meLBCXvvGcTNCet4g/open-thread-june-2010-part-3", "linkUrl": "https://www.lesswrong.com/posts/meLBCXvvGcTNCet4g/open-thread-june-2010-part-3", "postedAtFormatted": "Monday, June 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20June%202010%2C%20Part%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20June%202010%2C%20Part%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmeLBCXvvGcTNCet4g%2Fopen-thread-june-2010-part-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20June%202010%2C%20Part%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmeLBCXvvGcTNCet4g%2Fopen-thread-june-2010-part-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmeLBCXvvGcTNCet4g%2Fopen-thread-june-2010-part-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><em style=\"font-style: italic;\">This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>\n<div>The thrilling conclusion of what is likely to be an inaccurately named trilogy of June Open Threads.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "meLBCXvvGcTNCet4g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 5.957100266732975e-07, "legacy": true, "legacyId": "3049", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 627, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-15T00:30:52.358Z", "modifiedAt": null, "url": null, "title": "Book Club Update and Chapter 1", "slug": "book-club-update-and-chapter-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:55.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aKkgRiGhLvKxmEPvi/book-club-update-and-chapter-1", "pageUrlRelative": "/posts/aKkgRiGhLvKxmEPvi/book-club-update-and-chapter-1", "linkUrl": "https://www.lesswrong.com/posts/aKkgRiGhLvKxmEPvi/book-club-update-and-chapter-1", "postedAtFormatted": "Tuesday, June 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Club%20Update%20and%20Chapter%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Club%20Update%20and%20Chapter%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKkgRiGhLvKxmEPvi%2Fbook-club-update-and-chapter-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Club%20Update%20and%20Chapter%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKkgRiGhLvKxmEPvi%2Fbook-club-update-and-chapter-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKkgRiGhLvKxmEPvi%2Fbook-club-update-and-chapter-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1401, "htmlBody": "<p>This post summarizes response to the <a href=\"/lw/2br/less_wrong_book_club_and_study_group/\">Less Wrong Book Club and Study Group</a> proposal, floats a tentative virtual meetup schedule, and offers some mechanisms for keeping up to date with the group's work. We end with summaries of Chapter 1.</p>\n<h2>Statistics</h2>\n<p>The proposal for a LW book club and study group, initially focusing on E.T. Jaynes' <em>Probability Theory: The Logic of Science</em> (a.k.a. PT:TLOS), drew an impressive response with 57 declarations of intent to participate. (I may have missed some or misinterpreted as intending to participate some who were merely interested. <a href=\"http://spreadsheets.google.com/ccc?key=0Au9LcqMYAIwldFViMEVza0hibmoxQ3FaeVBPRmFweWc&amp;hl=en\">This spreadsheet</a> contains participant data and can be edited by anyone (under revision control). Please feel free to add, remove or change your information.) The group has people from no less than 11 different countries, in time zones ranging from GMT-7 to GMT+10.</p>\n<h2>Live discussion schedule and venues<br /></h2>\n<p>Many participants have expressed an interest in having informal or chatty discussions over a less permanent medium than LW itself, which should probably be reserved for more careful observations. The schedule below is offered as a basis for further negotiation. You can edit the spreadsheet linked above with your preferred times, and by the next iteration if a different clustering emerges I will report on that.</p>\n<ul>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?day=22&amp;month=6&amp;year=2010&amp;hour=18&amp;min=0&amp;sec=0&amp;p1=0\">Tuesdays at UTC 18:00</a> (that is 1pm Bay Area, 8pm in Europe, etc. - see linked schedule for more)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?day=27&amp;month=6&amp;year=2010&amp;hour=11&amp;min=0&amp;sec=0&amp;p1=0\">Wednesdays at UTC 11:00</a> (seems preferred by Australian participants)</li>\n<li>Sundays at UTC 18:00 (some have requested a weekend meeting)</li>\n</ul>\n<p>The unofficial <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">Less Wrong IRC channel</a> is the preferred venue. An experimental <a href=\"https://wave.google.com/wave/waveref/googlewave.com/w+OtubJLjPF \">Google Wave</a> has also been started which may be a useful adjunct, in particular as we come to need mathematical notations in our discussions.</p>\n<p>I recommend reading the suggested material <em>before</em> attending live discussion sessions.</p>\n<p><a id=\"more\"></a></p>\n<h2>Objectives, math prerequisites</h2>\n<p>The intent of the group is to engage in \"earnest study of the great literature in our area of interest\" (to paraphrase from the <a href=\"http://industriallogic.com/papers/kh.html\">Knowledge Hydrant pattern language</a>, a useful resource for study groups).</p>\n<p>Earnest study aims at understanding a work deeply. Probably (particularly so in the case of PT:TLOS) the most useful way to do so is sequentially, in the order the author presented their ideas. Therefore, we aim for a pace that allows participants to extract as much insight as possible from each piece of the work, before moving on to the next, which is assumed to build on it.</p>\n<p>Exercises are useful stopping-points to check for understanding. When the text contains equations or proofs, reproducing the derivations or checking the calculations can also be a good way to ensure deep understanding.</p>\n<p>PT:TLOS is (from personal experience) relatively accessible on rusty high school math (in particular requires little calculus) until at least partway through Chapter 6 (which is where I am at the moment). Just these few chapters contain many key insights about the Bayesian view of probability and are well worth the effort.</p>\n<h2>Format</h2>\n<p>My proposal for the format is as follows. I will post one new top-level post per chapter, so as to give people following through RSS a chance to catch updates. Each chapter, however, may require splitting up into more than one chunk to be manageable. I intend to aim for a weekly rhythm: the monday after the first chunk of a new chapter is posted, I will post the next chunk, and so on. If you're worried about missing an update, check the top-level post for the current chapter weekly on mondays.</p>\n<p>Each update will identify the current chunk, and will link to a comment containing one or more \"opening questions\" to jump-start discussion.</p>\n<p>Updates also briefly summarize the previous chunk and highlights of the discussion arising from it. (Participants in the live chat sessions are encouraged to designate one person to summarize the discussion and post the summary as a comment.) By the time a new chapter is to be opened, the previous post will contain a digest form of the group's collective take on the chapter just worked through. The cumulative effect will be a \"Less Wrong's notes on PT:TLOS\", useful in itself for newcomers.</p>\n<h2>Chapter 1: Plausible Reasoning<br /></h2>\n<p>In this chapter Jaynes fleshes out a theme introduced in the preface: \"Probability theory as extended logic\".</p>\n<p><strong>Sections: Deductive and Plausible Reasoning - Analogies with Physical Theories - The Thinking Computer - Introducing the Robot (week of 14/06)<br /></strong></p>\n<p>Classical (Aristotelian) logic - modus ponens, modus tollens - allows deduction (teasing apart the concepts of <a href=\"/lw/2cq/book_club_update_and_chapter_1/25iv\">deduction, induction, abduction</a> isn't trivial). But what if we're interested not just in \"definitely true or false\" but \"is this plausible\", as we are in the kind of everyday thinking Jaynes provides examples of? Plausible reasoning is a weaker form of inference than deduction, but one Jaynes argues plays an important role even in (say) mathematics.</p>\n<p>Jaynes' aim is to construct a working model of our faculty of \"common sense\", in the same sense that the Wright brothers could form a working model of the faculty of flight, not by vague <a href=\"/lw/vx/failure_by_analogy/\">resort to analogy</a> as in the Icarus myth, but by producing a machine embodying a precise understanding. (Jaynes, however, speaks favorably of analogical thinking: \"Good mathematicians see analogies between theorems; great mathematicians seen analogies between analogies\". He acknowledges that this line of argument itself stems from analogy with physics.)</p>\n<p>Accordingly, Jaynes <a href=\"/lw/2cq/book_club_update_and_chapter_1/25ix\">frames</a> what is to follow as building an \"inference robot\". Jaynes notes, \"the question of the reasoning process used by actual human brains is charged with emotion and grotesque misunderstandings\", and so this frame will be helpful in keeping us focused on useful questions with <a href=\"http://wiki.lesswrong.com/wiki/Making_beliefs_pay_rent\">observable consequences</a>. It is tempting to also read a practical intent - just as robots can carry out specialized mechanical tasks on behalf of humans, so could an inference robot keep track of more details than our unaided common senses - we must however be careful not to project onto Jaynes some conception of a \"Bayesian AI\".</p>\n<p><strong>Sections: </strong><strong>Boolean Algebra</strong> - <strong>Adequate Sets of Operations</strong> - <strong>The Basic Desiderata</strong> - <strong>Comments</strong> - <strong>Common Language vs Formal Logic</strong> - <strong>Nitpicking</strong><strong> (week of 21/06)</strong></p>\n<p>Jaynes next introduces the familiar formal notation of Boolean algebra to represent truth-values of propositions, their conjunction and disjunction, and denial. (Equality denotes equality of truth-values, rather than equality of propositions.) Some care is required to distinguish common usage of terms such as \"<a href=\"/lw/2cq/book_club_update_and_chapter_1/26xp\">or</a>\", \"implies\", \"if\", etc. from their denotation in the Boolean algebra of truth-values. From the axioms of idempotence, commutativity, associativity, distributivity and duality, we can build up any number of more sophisticated consequences.</p>\n<p>One such consequence, sketched out next, is that any function of <em>n</em> boolean variables can be expressed as a sum (logical OR) involving only conjunctions (logical AND) of each variable or its negation. Each of <img src=\"http://latex.codecogs.com/png.latex?2^{2^{n}}\" alt=\"\" width=\"24\" height=\"17\" />different logic functions can thus be expressed in terms of only <img src=\"http://latex.codecogs.com/png.latex?2^n\" alt=\"\" width=\"19\" height=\"14\" /> building blocks and only three operations (conjunction, disjunction, negation). In fact an even smaller set of operations is adequate to construct all Boolean functions: it is possible to express all three in terms of the NAND (negation of AND) operation, for instance. (A key argument in Chapter 2 hinges on this reduction of logic functions to an \"adequate set\".)</p>\n<p>The \"inference robot\", then, is to reason in terms of degrees of plausibility assigned to propositions: plausibility is a generalization of truth-value. We are generally concerned with \"conditional probability\"; how plausible something is given what else we know. This is represented in the familiar notation A|B (\" the plausibility of A given that B is true\", or \"A given B\"). The robot is assumed to be provided sensible, non-contradictory input.</p>\n<p>Jaynes next considers the \"basic desiderata\" for such an extension. First, they should be real numbers. (This is motivated by an appeal to <a href=\"/lw/2cq/book_club_update_and_chapter_1/26nl\">convenience of implementation</a>; the Comments defend this in greater detail, and a more formal justification can be found in the Appendices.) By convention, greater plausibility will be represented with a greater number, and the robot's \"sense of direction\", that is, the consequences it draws from increases or decreases in the plausibility of the \"givens\", must conform to common sense. (This will play a key role in Chapter 2.) Finally, the robot is to be <em>consistent</em> and <em>non-ideological</em>: it must always draw the same conclusions from identical premises, it must not arbitrarily ignore information available to it, and it must represent equivalent states of knowledge by equivalent values of plausibility.</p>\n<p>(The Comments section is well worth reading, as it introduces the <a href=\"/lw/oi/mind_projection_fallacy\">Mind Projection Fallacy</a> which LW readers who have gone through the Sequences should be familiar with.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aKkgRiGhLvKxmEPvi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 5.959404117870698e-07, "legacy": true, "legacyId": "3050", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This post summarizes response to the <a href=\"/lw/2br/less_wrong_book_club_and_study_group/\">Less Wrong Book Club and Study Group</a> proposal, floats a tentative virtual meetup schedule, and offers some mechanisms for keeping up to date with the group's work. We end with summaries of Chapter 1.</p>\n<h2 id=\"Statistics\">Statistics</h2>\n<p>The proposal for a LW book club and study group, initially focusing on E.T. Jaynes' <em>Probability Theory: The Logic of Science</em> (a.k.a. PT:TLOS), drew an impressive response with 57 declarations of intent to participate. (I may have missed some or misinterpreted as intending to participate some who were merely interested. <a href=\"http://spreadsheets.google.com/ccc?key=0Au9LcqMYAIwldFViMEVza0hibmoxQ3FaeVBPRmFweWc&amp;hl=en\">This spreadsheet</a> contains participant data and can be edited by anyone (under revision control). Please feel free to add, remove or change your information.) The group has people from no less than 11 different countries, in time zones ranging from GMT-7 to GMT+10.</p>\n<h2 id=\"Live_discussion_schedule_and_venues\">Live discussion schedule and venues<br></h2>\n<p>Many participants have expressed an interest in having informal or chatty discussions over a less permanent medium than LW itself, which should probably be reserved for more careful observations. The schedule below is offered as a basis for further negotiation. You can edit the spreadsheet linked above with your preferred times, and by the next iteration if a different clustering emerges I will report on that.</p>\n<ul>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?day=22&amp;month=6&amp;year=2010&amp;hour=18&amp;min=0&amp;sec=0&amp;p1=0\">Tuesdays at UTC 18:00</a> (that is 1pm Bay Area, 8pm in Europe, etc. - see linked schedule for more)</li>\n<li><a href=\"http://www.timeanddate.com/worldclock/fixedtime.html?day=27&amp;month=6&amp;year=2010&amp;hour=11&amp;min=0&amp;sec=0&amp;p1=0\">Wednesdays at UTC 11:00</a> (seems preferred by Australian participants)</li>\n<li>Sundays at UTC 18:00 (some have requested a weekend meeting)</li>\n</ul>\n<p>The unofficial <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">Less Wrong IRC channel</a> is the preferred venue. An experimental <a href=\"https://wave.google.com/wave/waveref/googlewave.com/w+OtubJLjPF \">Google Wave</a> has also been started which may be a useful adjunct, in particular as we come to need mathematical notations in our discussions.</p>\n<p>I recommend reading the suggested material <em>before</em> attending live discussion sessions.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Objectives__math_prerequisites\">Objectives, math prerequisites</h2>\n<p>The intent of the group is to engage in \"earnest study of the great literature in our area of interest\" (to paraphrase from the <a href=\"http://industriallogic.com/papers/kh.html\">Knowledge Hydrant pattern language</a>, a useful resource for study groups).</p>\n<p>Earnest study aims at understanding a work deeply. Probably (particularly so in the case of PT:TLOS) the most useful way to do so is sequentially, in the order the author presented their ideas. Therefore, we aim for a pace that allows participants to extract as much insight as possible from each piece of the work, before moving on to the next, which is assumed to build on it.</p>\n<p>Exercises are useful stopping-points to check for understanding. When the text contains equations or proofs, reproducing the derivations or checking the calculations can also be a good way to ensure deep understanding.</p>\n<p>PT:TLOS is (from personal experience) relatively accessible on rusty high school math (in particular requires little calculus) until at least partway through Chapter 6 (which is where I am at the moment). Just these few chapters contain many key insights about the Bayesian view of probability and are well worth the effort.</p>\n<h2 id=\"Format\">Format</h2>\n<p>My proposal for the format is as follows. I will post one new top-level post per chapter, so as to give people following through RSS a chance to catch updates. Each chapter, however, may require splitting up into more than one chunk to be manageable. I intend to aim for a weekly rhythm: the monday after the first chunk of a new chapter is posted, I will post the next chunk, and so on. If you're worried about missing an update, check the top-level post for the current chapter weekly on mondays.</p>\n<p>Each update will identify the current chunk, and will link to a comment containing one or more \"opening questions\" to jump-start discussion.</p>\n<p>Updates also briefly summarize the previous chunk and highlights of the discussion arising from it. (Participants in the live chat sessions are encouraged to designate one person to summarize the discussion and post the summary as a comment.) By the time a new chapter is to be opened, the previous post will contain a digest form of the group's collective take on the chapter just worked through. The cumulative effect will be a \"Less Wrong's notes on PT:TLOS\", useful in itself for newcomers.</p>\n<h2 id=\"Chapter_1__Plausible_Reasoning\">Chapter 1: Plausible Reasoning<br></h2>\n<p>In this chapter Jaynes fleshes out a theme introduced in the preface: \"Probability theory as extended logic\".</p>\n<p><strong id=\"Sections__Deductive_and_Plausible_Reasoning___Analogies_with_Physical_Theories___The_Thinking_Computer___Introducing_the_Robot__week_of_14_06_\">Sections: Deductive and Plausible Reasoning - Analogies with Physical Theories - The Thinking Computer - Introducing the Robot (week of 14/06)<br></strong></p>\n<p>Classical (Aristotelian) logic - modus ponens, modus tollens - allows deduction (teasing apart the concepts of <a href=\"/lw/2cq/book_club_update_and_chapter_1/25iv\">deduction, induction, abduction</a> isn't trivial). But what if we're interested not just in \"definitely true or false\" but \"is this plausible\", as we are in the kind of everyday thinking Jaynes provides examples of? Plausible reasoning is a weaker form of inference than deduction, but one Jaynes argues plays an important role even in (say) mathematics.</p>\n<p>Jaynes' aim is to construct a working model of our faculty of \"common sense\", in the same sense that the Wright brothers could form a working model of the faculty of flight, not by vague <a href=\"/lw/vx/failure_by_analogy/\">resort to analogy</a> as in the Icarus myth, but by producing a machine embodying a precise understanding. (Jaynes, however, speaks favorably of analogical thinking: \"Good mathematicians see analogies between theorems; great mathematicians seen analogies between analogies\". He acknowledges that this line of argument itself stems from analogy with physics.)</p>\n<p>Accordingly, Jaynes <a href=\"/lw/2cq/book_club_update_and_chapter_1/25ix\">frames</a> what is to follow as building an \"inference robot\". Jaynes notes, \"the question of the reasoning process used by actual human brains is charged with emotion and grotesque misunderstandings\", and so this frame will be helpful in keeping us focused on useful questions with <a href=\"http://wiki.lesswrong.com/wiki/Making_beliefs_pay_rent\">observable consequences</a>. It is tempting to also read a practical intent - just as robots can carry out specialized mechanical tasks on behalf of humans, so could an inference robot keep track of more details than our unaided common senses - we must however be careful not to project onto Jaynes some conception of a \"Bayesian AI\".</p>\n<p><strong>Sections: </strong><strong>Boolean Algebra</strong> - <strong>Adequate Sets of Operations</strong> - <strong>The Basic Desiderata</strong> - <strong>Comments</strong> - <strong>Common Language vs Formal Logic</strong> - <strong>Nitpicking</strong><strong> (week of 21/06)</strong></p>\n<p>Jaynes next introduces the familiar formal notation of Boolean algebra to represent truth-values of propositions, their conjunction and disjunction, and denial. (Equality denotes equality of truth-values, rather than equality of propositions.) Some care is required to distinguish common usage of terms such as \"<a href=\"/lw/2cq/book_club_update_and_chapter_1/26xp\">or</a>\", \"implies\", \"if\", etc. from their denotation in the Boolean algebra of truth-values. From the axioms of idempotence, commutativity, associativity, distributivity and duality, we can build up any number of more sophisticated consequences.</p>\n<p>One such consequence, sketched out next, is that any function of <em>n</em> boolean variables can be expressed as a sum (logical OR) involving only conjunctions (logical AND) of each variable or its negation. Each of <img src=\"http://latex.codecogs.com/png.latex?2^{2^{n}}\" alt=\"\" width=\"24\" height=\"17\">different logic functions can thus be expressed in terms of only <img src=\"http://latex.codecogs.com/png.latex?2^n\" alt=\"\" width=\"19\" height=\"14\"> building blocks and only three operations (conjunction, disjunction, negation). In fact an even smaller set of operations is adequate to construct all Boolean functions: it is possible to express all three in terms of the NAND (negation of AND) operation, for instance. (A key argument in Chapter 2 hinges on this reduction of logic functions to an \"adequate set\".)</p>\n<p>The \"inference robot\", then, is to reason in terms of degrees of plausibility assigned to propositions: plausibility is a generalization of truth-value. We are generally concerned with \"conditional probability\"; how plausible something is given what else we know. This is represented in the familiar notation A|B (\" the plausibility of A given that B is true\", or \"A given B\"). The robot is assumed to be provided sensible, non-contradictory input.</p>\n<p>Jaynes next considers the \"basic desiderata\" for such an extension. First, they should be real numbers. (This is motivated by an appeal to <a href=\"/lw/2cq/book_club_update_and_chapter_1/26nl\">convenience of implementation</a>; the Comments defend this in greater detail, and a more formal justification can be found in the Appendices.) By convention, greater plausibility will be represented with a greater number, and the robot's \"sense of direction\", that is, the consequences it draws from increases or decreases in the plausibility of the \"givens\", must conform to common sense. (This will play a key role in Chapter 2.) Finally, the robot is to be <em>consistent</em> and <em>non-ideological</em>: it must always draw the same conclusions from identical premises, it must not arbitrarily ignore information available to it, and it must represent equivalent states of knowledge by equivalent values of plausibility.</p>\n<p>(The Comments section is well worth reading, as it introduces the <a href=\"/lw/oi/mind_projection_fallacy\">Mind Projection Fallacy</a> which LW readers who have gone through the Sequences should be familiar with.)</p>", "sections": [{"title": "Statistics", "anchor": "Statistics", "level": 1}, {"title": "Live discussion schedule and venues", "anchor": "Live_discussion_schedule_and_venues", "level": 1}, {"title": "Objectives, math prerequisites", "anchor": "Objectives__math_prerequisites", "level": 1}, {"title": "Format", "anchor": "Format", "level": 1}, {"title": "Chapter 1: Plausible Reasoning", "anchor": "Chapter_1__Plausible_Reasoning", "level": 1}, {"title": "Sections: Deductive and Plausible Reasoning - Analogies with Physical Theories - The Thinking Computer - Introducing the Robot (week of 14/06)", "anchor": "Sections__Deductive_and_Plausible_Reasoning___Analogies_with_Physical_Theories___The_Thinking_Computer___Introducing_the_Robot__week_of_14_06_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "83 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tSDaxEq2WGHTRPcWB", "C4EjbrvG3PvZzizZb", "ZTRiSNmeGQK8AkdN2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-15T02:33:06.249Z", "modifiedAt": null, "url": null, "title": "What Does Make a Difference? It\u2019s Really Simple", "slug": "what-does-make-a-difference-it-s-really-simple", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:34.185Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DaBzJwajrtn8PAfXj/what-does-make-a-difference-it-s-really-simple", "pageUrlRelative": "/posts/DaBzJwajrtn8PAfXj/what-does-make-a-difference-it-s-really-simple", "linkUrl": "https://www.lesswrong.com/posts/DaBzJwajrtn8PAfXj/what-does-make-a-difference-it-s-really-simple", "postedAtFormatted": "Tuesday, June 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Does%20Make%20a%20Difference%3F%20It%E2%80%99s%20Really%20Simple&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Does%20Make%20a%20Difference%3F%20It%E2%80%99s%20Really%20Simple%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDaBzJwajrtn8PAfXj%2Fwhat-does-make-a-difference-it-s-really-simple%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Does%20Make%20a%20Difference%3F%20It%E2%80%99s%20Really%20Simple%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDaBzJwajrtn8PAfXj%2Fwhat-does-make-a-difference-it-s-really-simple", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDaBzJwajrtn8PAfXj%2Fwhat-does-make-a-difference-it-s-really-simple", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 432, "htmlBody": "<div class=\"snap_preview\">\n<p><a title=\"This is a self referential Link\" href=\"http://brainstormers.wordpress.com/2009/11/20/what-does-make-a-difference-its-really-simple/\">This</a> is really simple:<br /> Suppose you want to check if some action of yours makes a difference.<br /> How to do it?<br /> The wrong thing to do:<br /> Think of the consequences of your action and evaluate them to see if they fit your purposes. If they do, go on and do it.</p>\n<p>The reason for this being wrong: If someone else does something with the same consequences, and if your doing or not your action makes no difference to the fact that THAT person will do it, then you are not necessary for those consequences, they would happen anyway.<br /> This is also true if something, not a person, would do an action with the same consequences.</p>\n<p>The right thing to do: Consider what would happen if you DIDN&rsquo;T do your action. Subtract that from what would happen if you DID do your action.<br /> This is the difference it would make if you did it.</p>\n<p>There is a reason it is called a &lsquo;difference&rsquo;, it is the difference between you doing it and you not doing it.</p>\n<p>Example: Suppose you think you will make a difference by carefully considering your vote, and voting.</p>\n<p>Wrong: Well, I&rsquo;m partially causally responsible for the election of X so my action would make a difference.</p>\n<p>Right: If I do vote or if I don&rsquo;t vote, the same candidates will be elected. Therefore my vote makes no difference.<br /> (In more than 16000 elections in the USA it was NEVER the case that one vote would have made a difference)</p>\n<p>The AWFUL argument people usually say: But what if everyone did it?</p>\n<p>The reason it does not work: Everyone will NOT do it. Yes. That simple.</p>\n<p>The reason it is awful: Compare &ldquo;I don&rsquo;t think I should go to the movies today, what if everione did it?&rdquo;</p>\n<p>So, when you are willing to make a difference, not feel good, not do what everyone does, not clean your consciousness. When you want to REALLY, REALLY make a difference, you should consider the difference between doing and not doing it.</p>\n<p>It is that simple.</p>\n<p>&nbsp;</p>\n<p>(Note to the Less Wrong entry: I know most people here know that politics is <em><a title=\"Not the only one, the biggest one perhaps\" href=\"/lw/gw/politics_is_the_mindkiller/\">a</a></em> mind killer, but delving deeper into the simple argument, notice it works for any case of overdetermination, such as posting a comment or entry that someone else would. If you realize now how important it is to figure out what is important, don't forget <a title=\"Anna Salomon on what matters\" href=\"http://vimeo.com/7397629\">more</a> than <a title=\"Bostrom on Opportunity Waste\" href=\"http://www.nickbostrom.com/astronomical/waste.html\">one</a> person have already done it, use them.)</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DaBzJwajrtn8PAfXj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -2, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "2424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-19T03:42:34.927Z", "modifiedAt": null, "url": null, "title": "Cambridge Less Wrong meetup Sunday 27 June", "slug": "cambridge-less-wrong-meetup-sunday-27-june", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:45.947Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3rv9vCXF6a9smNiRd/cambridge-less-wrong-meetup-sunday-27-june", "pageUrlRelative": "/posts/3rv9vCXF6a9smNiRd/cambridge-less-wrong-meetup-sunday-27-june", "linkUrl": "https://www.lesswrong.com/posts/3rv9vCXF6a9smNiRd/cambridge-less-wrong-meetup-sunday-27-june", "postedAtFormatted": "Saturday, June 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20Less%20Wrong%20meetup%20Sunday%2027%20June&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20Less%20Wrong%20meetup%20Sunday%2027%20June%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3rv9vCXF6a9smNiRd%2Fcambridge-less-wrong-meetup-sunday-27-june%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20Less%20Wrong%20meetup%20Sunday%2027%20June%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3rv9vCXF6a9smNiRd%2Fcambridge-less-wrong-meetup-sunday-27-june", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3rv9vCXF6a9smNiRd%2Fcambridge-less-wrong-meetup-sunday-27-june", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p>I will be at Clear Conscience Cafe at <a href=\"http://maps.google.com/maps?q=Clear+Conscience+Cafe+Cambridge+MA&amp;oe=utf-8&amp;client=firefox-a&amp;ie=UTF8&amp;hl=en&amp;hq=Clear+Conscience+Cafe&amp;hnear=Cambridge,+MA&amp;ll=42.365056,-71.102411&amp;spn=0.003789,0.006539&amp;t=h&amp;z=18&amp;iwloc=A\">581 Massachusetts Avenue</a> Cambridge, MA looking for other Less Wrong people on Sunday the 27th. This breaks the <a href=\"/lw/22a/boston_area_meetup_april_18/\">previously established pattern</a> of meeting on the third Sunday of each month, to avoid conflicting with Father's Day. As usual, I will bring my boxes and challenge anyone who comes to a puzzle. No one has answered correctly yet; come be the first!</p>\n<p><br />Edited to add: 4pm</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3rv9vCXF6a9smNiRd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 5.971939524200439e-07, "legacy": true, "legacyId": "3068", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8wHa4DLW4pADHMPwJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-19T04:34:36.537Z", "modifiedAt": null, "url": null, "title": "Open Thread June 2010, Part 4", "slug": "open-thread-june-2010-part-4", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:35.817Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L6yBGL4Hog5coegd8/open-thread-june-2010-part-4", "pageUrlRelative": "/posts/L6yBGL4Hog5coegd8/open-thread-june-2010-part-4", "linkUrl": "https://www.lesswrong.com/posts/L6yBGL4Hog5coegd8/open-thread-june-2010-part-4", "postedAtFormatted": "Saturday, June 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20June%202010%2C%20Part%204&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20June%202010%2C%20Part%204%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL6yBGL4Hog5coegd8%2Fopen-thread-june-2010-part-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20June%202010%2C%20Part%204%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL6yBGL4Hog5coegd8%2Fopen-thread-june-2010-part-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL6yBGL4Hog5coegd8%2Fopen-thread-june-2010-part-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 37, "htmlBody": "<p style=\"margin: 0px 0px 1em;\"><em style=\"font-style: italic;\">This  thread is for the discussion of Less Wrong topics that have not appeared  in recent posts. If a discussion gets unwieldy, celebrate by turning it  into a top-level post.</em></p>\n<p>This thread brought to you by quantum immortality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L6yBGL4Hog5coegd8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 5.972049307935808e-07, "legacy": true, "legacyId": "3069", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 336, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-19T16:40:39.852Z", "modifiedAt": null, "url": null, "title": "Surface syllogisms and the sin-based model of causation", "slug": "surface-syllogisms-and-the-sin-based-model-of-causation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:47.696Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8FPJeyvgNL3CTa4Ng/surface-syllogisms-and-the-sin-based-model-of-causation", "pageUrlRelative": "/posts/8FPJeyvgNL3CTa4Ng/surface-syllogisms-and-the-sin-based-model-of-causation", "linkUrl": "https://www.lesswrong.com/posts/8FPJeyvgNL3CTa4Ng/surface-syllogisms-and-the-sin-based-model-of-causation", "postedAtFormatted": "Saturday, June 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Surface%20syllogisms%20and%20the%20sin-based%20model%20of%20causation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASurface%20syllogisms%20and%20the%20sin-based%20model%20of%20causation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8FPJeyvgNL3CTa4Ng%2Fsurface-syllogisms-and-the-sin-based-model-of-causation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Surface%20syllogisms%20and%20the%20sin-based%20model%20of%20causation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8FPJeyvgNL3CTa4Ng%2Fsurface-syllogisms-and-the-sin-based-model-of-causation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8FPJeyvgNL3CTa4Ng%2Fsurface-syllogisms-and-the-sin-based-model-of-causation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 719, "htmlBody": "<p>The White House says there will be a temporary ban on new deep-water drilling, and BP will have to pay the salaries of oilmen who have no work during that ban.&nbsp; I scratched my head trying to figure out the logic behind this.&nbsp; This was my first attempt:</p>\n<ol>\n<li>BP caused an oil spill.</li>\n<li>The oil spill caused a ban on drilling.</li>\n<li>The ban on drilling caused oilmen to be out of work.</li>\n<li>Therefore, BP caused oilmen to be out of work.</li>\n<li>Therefore, BP should pay these oilmen.</li>\n</ol>\n<p>This logic works equally well in this case:</p>\n<ol>\n<li>Rachel Carson wrote <em>Silent Spring</em>.</li>\n<li><em>Silent Spring</em> caused a ban on DDT use.</li>\n<li>The ban on DDT use caused factory workers to be out of work.</li>\n<li>Therefore, Rachel Carson caused factory workers to be out of work.</li>\n<li>Therefore, Rachel Carson should pay these workers.</li>\n</ol>\n<p>But \"everyone\" would agree that the second example is fallacious.&nbsp; Are people so angry at BP that they can't think at all?</p>\n<p><a id=\"more\"></a>Then I came up with this second argument.&nbsp; (\"At fault\" is legalese for \"caused by an immoral or illegal action.\")</p>\n<ol>\n<li>An oil spill caused a ban on drilling.</li>\n<li>The ban on drilling caused oilmen to be out of work.</li>\n<li>Therefore, the oil spill caused oilmen to be out of work.</li>\n<li>The party at fault should pay the injured party.</li>\n<li>BP is at fault for the oil spill.</li>\n<li>Therefore, BP should pay these oilmen.</li>\n</ol>\n<p>Applied to Rachel Carson:</p>\n<ol>\n<li><em>Silent Spring </em>caused a ban on DDT.</li>\n<li>The ban on DDT use caused factory workers to be out of work.</li>\n<li>The party at fault should pay the injured party.</li>\n<li>The producers of DDT are at fault for environmental damage. </li>\n<li>Therefore, those producers should pay these factory workers.</li>\n</ol>\n<p>Both these chains of reasoning are still faulty, but they're more similar to the reactions of most people.&nbsp; They are faulty because they're not specific about the connection between the fault and the injured party, or about what an \"injury\" is.&nbsp; In the second case, there is no injury to the workers; the company simply stopped employing them, and could only be held morally responsible for this under something like feudalism.&nbsp; In the BP case, you could argue that non-BP oilmen were injured, because they want to work and their (non-BP) employers want to hire them, but outside forces prevented them.</p>\n<p>However, being at fault for the oil spill is not the same as being at fault for (causing by immoral action) the ban on drilling.&nbsp; The word \"cause\" is too vague for moral responsibility to be transitive over it; and \"X caused Z\" does not preclude \"Y caused Z\".&nbsp; The ban on drilling is not a ban only on drilling by BP; this means that the powers that be decided the ban on drilling is good for the country, not a punishment of BP.&nbsp; It is a decision that the expected cost of further drilling outweighs the expected benefits.&nbsp; There is no moral failing and no one at fault, and either the government should pay them, or the oilmen should bite it the way any workers do when their industry has a downturn and rely on existing safety nets such as unemployment insurance.&nbsp; (This is completely different from the case of fishermen put out of work directly by the oil spill; I believe it makes sense for BP to pay them.)</p>\n<p>Figuring out how moral responsibility propagates through a chain of events is complicated.&nbsp; I propose that people are using the \"sin-based\" model of cause and effect.&nbsp; This model says that all bad outcomes are caused by moral failings.&nbsp; (On the radio yesterday, I heard a woman being interviewed whose house had been destroyed by a landslide.&nbsp; The first question the interviewer asked was, \"Whose fault was this?\")</p>\n<p>In the sin-based model, when you enumerate a chain of events that is causally linked, and some events are bad outcomes, all you need to do is transfer blame for the bad outcomes to the moral failings preceding them in the chain.&nbsp; Oilmen are out of work; you construct a chain of events leading to them being out of work, identify the closest preceding moral failure in the chain, and pin the blame on that moral failing.&nbsp; No need for painful <em>thinking</em>!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8FPJeyvgNL3CTa4Ng", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 22, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "3074", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-19T19:37:44.349Z", "modifiedAt": null, "url": null, "title": "Defeating Ugh Fields In Practice", "slug": "defeating-ugh-fields-in-practice", "viewCount": null, "lastCommentedAt": "2021-10-04T13:36:36.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rvm7tmfEQ2RstJBPG/defeating-ugh-fields-in-practice", "pageUrlRelative": "/posts/Rvm7tmfEQ2RstJBPG/defeating-ugh-fields-in-practice", "linkUrl": "https://www.lesswrong.com/posts/Rvm7tmfEQ2RstJBPG/defeating-ugh-fields-in-practice", "postedAtFormatted": "Saturday, June 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Defeating%20Ugh%20Fields%20In%20Practice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefeating%20Ugh%20Fields%20In%20Practice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRvm7tmfEQ2RstJBPG%2Fdefeating-ugh-fields-in-practice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Defeating%20Ugh%20Fields%20In%20Practice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRvm7tmfEQ2RstJBPG%2Fdefeating-ugh-fields-in-practice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRvm7tmfEQ2RstJBPG%2Fdefeating-ugh-fields-in-practice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 622, "htmlBody": "<p>Unsurprisingly related to:&nbsp;<a href=\"/lw/21b/ugh_fields/\">Ugh fields</a>.</p>\n<p>If I had to choose a single piece of evidence off of which to argue that the rationality assumption of neoclassical economics is totally, irretrievably incorrect, it's <a href=\"http://www.nytimes.com/2010/06/14/health/14meds.html\">this article about financial incentives and medication compliance.</a>&nbsp;In short, offering people small cash incentives vastly improves their adherence to life-saving medical regimens. That's right. For a significant number of people, a small chance at winning $10-100 can be the difference between whether or not they stick to a regimen that has a very good chance of saving their life. This technique has even shown promise in getting drug addicts and psychiatric patients to adhere to their regimens, for as little as a $20 gift certificate. This problem, in the aggregate, is estimated to cost about 5% of total health care spending -$100 billion - and that may not properly account for the utility lost by those who are harmed beyond repair. To claim that people are making a reasoned decision between the payoffs of taking and not-taking their medication, and that they be persuaded to change their behaviour by a payoff of about $900 a year (or less), is to crush reality into a theory that cannot hold it. This is doubly true when you consider that some of these people were fairly affluent.&nbsp;</p>\n<p>A likely explanation of this detrimental irrationality is something close to an&nbsp;<a style=\"color: #8a8a8b; text-decoration: inherit;\" href=\"/lw/21b/ugh_fields/\">Ugh field</a>. It must be miserable having a life-threatening illness. Being reminded of it by taking a pill every single day (or more frequently) is not pleasant. Then there's the question of whether you already took the pill. Because if you take it twice in one day, you'll end up in the hospital. And&nbsp;Heaven forfend your treatment involves needles.&nbsp;Thus, people avoid taking their medicine because the process becomes so unpleasant, even though they know they really should be taking it.</p>\n<p>As this&nbsp;experiment&nbsp;shows, this serious problem has a simple and elegant solution: make taking their medicine fun. As one person in the article describes it, using a low-reward lottery made taking his meds \"like a game;\" he couldn't wait to check the dispenser to see if he'd won (and take his meds again). Instead of thinking about how they have some terrible condition, they get excited thinking about how they could be winning money. The Ugh field has been demolished, with the once-feared procedure now associated with a tried-and-true intermittent reward system. It also wouldn't surprise me the least if people who are unlikely to adhere to a medical regimen are the kind of people who really enjoy playing the lottery.<a id=\"more\"></a></p>\n<p>This also explains why rewarding success may be more useful than punishing failure in the long run: if a kid does his homework because otherwise he doesn't get dessert, it's labor. If he gets some reward for getting it done, it becomes a positive. The problem is that if she knows what the reward is, she may anchor on already having the reward, turning it back into negative reinforcement - if you promise your kid a trip to Disneyland if they get above a 3.5, and they get a 3.3, they feel like they actually&nbsp;<em>lost</em>&nbsp;something. The use of a gambling mechanism may be key for this. If your reward is a <em>chance</em>&nbsp;at a real reward, you don't anchor as already having the reward, but the reward still excites you.</p>\n<p>I believe that the fact that such a significant problem can be overcome with such a trivial solution has tremendous implications, the enumeration of all of which would make for a very unwieldy post. A particularly noteworthy issue is the difficulty of applying such a technique to one's own actions, a problem which I believe has a fairly large number of workable solutions. That's what comments, and, potentially, follow-up posts are for.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ipJwbLxhR83ZksN6Z": 1, "xHjy88N2uJvGdgzfw": 1, "r7qAjcbfhj2256EHH": 1, "dqx5k65wjFfaiJ9sQ": 1, "EuDw6uxQW2ZBRFhMo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rvm7tmfEQ2RstJBPG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 91, "extendedScore": null, "score": 0.000157, "legacy": true, "legacyId": "3055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 91, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EFQ3F6kmt4WHXRqik"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-20T00:03:09.699Z", "modifiedAt": null, "url": null, "title": "What if AI doesn't quite go FOOM?", "slug": "what-if-ai-doesn-t-quite-go-foom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.116Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wv6a9kA6EApiYD5sL/what-if-ai-doesn-t-quite-go-foom", "pageUrlRelative": "/posts/wv6a9kA6EApiYD5sL/what-if-ai-doesn-t-quite-go-foom", "linkUrl": "https://www.lesswrong.com/posts/wv6a9kA6EApiYD5sL/what-if-ai-doesn-t-quite-go-foom", "postedAtFormatted": "Sunday, June 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20if%20AI%20doesn't%20quite%20go%20FOOM%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20if%20AI%20doesn't%20quite%20go%20FOOM%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwv6a9kA6EApiYD5sL%2Fwhat-if-ai-doesn-t-quite-go-foom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20if%20AI%20doesn't%20quite%20go%20FOOM%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwv6a9kA6EApiYD5sL%2Fwhat-if-ai-doesn-t-quite-go-foom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwv6a9kA6EApiYD5sL%2Fwhat-if-ai-doesn-t-quite-go-foom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1397, "htmlBody": "<p><em><strong>Intro</strong></em></p>\n<p>This article seeks to explore possible futures in a world where artificial intelligence turns out NOT to be able to quickly, recursively self-improve so as to influence our world with arbitrarily large strength and subtlety, i.e, \"go FOOM.\"&nbsp; Note that I am not arguing that AI won't FOOM. Eliezer has made several <a href=\"/lw/wf/hard_takeoff/\">good arguments</a> for why AI probably will FOOM, and I don't necessarily disagree.&nbsp; I am simply calling attention to the non-zero probability that it won't FOOM, and then asking what we might do to prepare for a world in which it doesn't.</p>\n<p><em><strong>Failure Modes</strong></em></p>\n<p>I can imagine three different ways in which AI could fail to FOOM in the next 100 years or so.&nbsp; Option 1 is a \"human fail.\"&nbsp; Option 1 means we destroy ourselves or succumb to some other existential risk before the first FOOM-capable AI boots up.&nbsp; I would love to hear in the comments section about (a) which existential risks people think are most likely to seriously threaten us before the advent of AI, and (b) what, if anything, a handful of people with moderate resources (i.e., people who hang around on Less Wrong) might do to effectively combat some of those risks.<br /><br />Option 2 is a \"hardware fail.\" Option 2 means that Moore's Law turns out to have an upper bound; if physics doesn't show enough complexity beneath the level of quarks, or if quantum-sized particles are so irredeemably random as to be intractable for computational purposes, then it might not be possible for even the most advanced intelligence to significantly improve on the basic hardware design of the supercomputers of, say, the year 2020.&nbsp; This would limit the computing power available per dollar, and so the level of computing power required for a self-improving AI might not be affordable for generations, if ever. <a href=\"http://www.nickbostrom.com/superintelligence.html\">Nick Bostrom</a> has some interesting thoughts along these lines, ultimately guessing (as of 2008) that the odds of a super-intelligence forming by 2033 was less than 50%.<br /><br />Option 3 is a \"software fail.\"&nbsp; Option 3 means that *programming* efficiency turns out to have an upper bound; if there are natural information-theoretical limits on how efficiently a set number of operations can be used to perform an arbitrary task, then it might not be possible for even the most advanced intelligence to significantly improve on its basic software design; the supercomputer would be more than 'smart' enough to understand itself and to re-write itself, but there would simply not *be* an alternate script for the source code that was actually more effective.<br /><br />These three options are not necessarily exhaustive; they are just the possibilities that have immediately occurred to me, with some help from <a href=\"/user/JoshuaZ/\">User: JoshuaZ</a>.</p>\n<p><em><strong>\"Superintelligent Enough\" AI</strong></em></p>\n<p>An important point to keep in mind is that even if self-improving AI faces hard limits before becoming arbitrarily powerful, AI might still be more than powerful enough to effortlessly dominate future society.&nbsp; I am sure my numbers are off by many orders of magnitude, but by way of illustration only, suppose that current supercomputers run at a speed of roughly 10^20 ops/second, and that successfully completing Eliezer's <a href=\"http://intelligence.org/upload/CEV.html\">coherent extrapolated volition</a> project would require a processing speed of roughly 10^36 ops/second.&nbsp; There is obviously quite a lot of space here for a miniature FOOM.&nbsp; If one of today's supercomputers starts to go FOOM and then hits hard limits at 10^25 ops/second, it wouldn't be able to identify humankind's CEV, but it might be able to, e.g, take over every electronic device capable of receiving transmissions, such as cars, satellites, and first-world factories.&nbsp; If this happens around the year 2020, a mini-FOOMed AI might also be able to take over homes, medical prosthetics, robotic soldiers, and credit cards.<br /><br />Sufficient investments in security and encryption might keep such an AI out of some corners of our economy, but right now, major operating systems aren't even proof against casual human trolls, let alone a dedicated AI thinking at faster-than-human speeds.&nbsp; I do not understand encryption well, and so it is possible that some plausible level of investment in computer security could, contrary to my assumptions, actually manage to protect human control over individual computers for the foreseeable future.&nbsp; Even if key industrial resources were adequately secured, though, a moderately super-intelligent AI might be capable of modeling the politics of current human leaders well enough to manipulate them into steering Earth onto a path of its choosing, as in Issac Asimov's <a href=\"http://en.wikipedia.org/wiki/The_Evitable_Conflict\">The Evitable Conflict</a>.<br /><br />If enough superintelligences develop at close enough to the same moment in time and have different enough values, they might in theory reach some sort of equilibrium that does not involve any one of them taking over the world.&nbsp; As Eliezer has <a href=\"/lw/w9/total_nano_domination/\">argued</a> (scroll down to 2nd half of the linked page), though, the stability of a race between intelligent agents should mostly be expected to *decrease* as those agents swallow their own intellectual and physical supply chains.&nbsp; If a supercomputer can take over larger and larger chunks of the Internet as it gets smarter and smarter, or if a supercomputer can effectively control what happens in more and more factories as it gets smarter and smarter, then there's less and less reason to think that supercomputing empires will \"grow\" at roughly the same pace -- the first empire to grow to a given size is likely to grow faster than its rivals until it takes over the world.&nbsp; Note that this could happen even if the AI is nowhere near smart enough to start mucking about with uploaded \"ems\" or nanoreplicators.&nbsp; Even in a boringly normal near-future scenario, a computer with even modest self-improvement and self-aggrandizement capabilities might be able to take over the world.&nbsp; Imagine something like the ending to David Brin's <a href=\"http://en.wikipedia.org/wiki/Earth_%28novel%29\"><em>Earth</em></a>, stripped of the mystical symbolism and the egalitarian optimism.</p>\n<p><em><strong>Ensuring a \"Nice Place to Live\"</strong></em></p>\n<p>I don't know what Eliezer's timeline is for attempting to develop provably Friendly AI, but it might be worthwhile to attempt to develop a second-order stopgap.&nbsp; Eliezer's CEV is supposed to function as a first-order stopgap; it won't achieve all of our goals, but it will ensure that we all get to grow up in a Nice Place to Live while we figure out what those goals are.&nbsp; Of course, that only happens if someone develops a CEV-capable AI.&nbsp; Eliezer seems quite worried about the possibility that someone will develop a FOOMing unFriendly AI before Friendly AI can get off the ground, but is anything being done about this besides just rushing to finish Friendly AI?</p>\n<p>Perhaps we need some kind of mini-FOOMing marginally Friendly AI whose only goal is to ensure that nothing seizes control of the world's computing resources until SIAI can figure out how to get CEV to work.&nbsp; Although no \"utility function\" can be specified for a general AI without risking paper-clip tiling, it might be possible to formulate a \"homeostatic function\" at relatively low risk.&nbsp; An AI that \"valued\" keeping the world looking roughly the way it does now, that was specifically instructed *never* to seize control of more than X number of each of several thousand different kinds of resources, and whose principal intended activity was to search for, hunt down, and destroy AIs that seemed to be growing too powerful too quickly might be an acceptable risk.&nbsp; Even if such a \"shield AI\" were not provably friendly, it might pose a smaller risk of tiling the solar system than the status quo, since the status quo is full of irresponsible people who like to tinker with seed AIs.</p>\n<p>An interesting side question is whether this would be counterproductive in a world where Failure Mode 2 (hard limits on hardware) or Failure Mode 3 (hard limits on software) were serious concerns.&nbsp; Assuming that, eventually, a provably friendly AI can be developed, then, several years after that, it's likely that millions of people can be convinced that it would be really good to activate the provably friendly AI, and humans might be able to dedicate enough resources to specifically overcome the second-order stopgap \"shield AI\" that was knocking out other people's un-provably Friendly AIs.&nbsp; But if the shield AI worked too well and got too close to the hard upper bound on the power of an AI, then it might not be possible to unmake the shield, even with added resources and with no holds barred.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "oiRp4T6u5poc8r9Tj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wv6a9kA6EApiYD5sL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 16, "extendedScore": null, "score": 5.974514990843417e-07, "legacy": true, "legacyId": "3075", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 191, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tjH8XPxAnr6JRbh7k", "5hX44Kuz5No6E6RS9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-20T06:25:13.679Z", "modifiedAt": null, "url": null, "title": "Applying Behavioral Psychology on Myself", "slug": "applying-behavioral-psychology-on-myself", "viewCount": null, "lastCommentedAt": "2021-04-04T17:57:35.081Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EJuZcWnk8j7eNQPqq/applying-behavioral-psychology-on-myself", "pageUrlRelative": "/posts/EJuZcWnk8j7eNQPqq/applying-behavioral-psychology-on-myself", "linkUrl": "https://www.lesswrong.com/posts/EJuZcWnk8j7eNQPqq/applying-behavioral-psychology-on-myself", "postedAtFormatted": "Sunday, June 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applying%20Behavioral%20Psychology%20on%20Myself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplying%20Behavioral%20Psychology%20on%20Myself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJuZcWnk8j7eNQPqq%2Fapplying-behavioral-psychology-on-myself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applying%20Behavioral%20Psychology%20on%20Myself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJuZcWnk8j7eNQPqq%2Fapplying-behavioral-psychology-on-myself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEJuZcWnk8j7eNQPqq%2Fapplying-behavioral-psychology-on-myself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 553, "htmlBody": "<p><em>In which I attempt to apply findings from behavioral psychology to my own life.</em></p>\n<h4>Behavioral Psychology Finding #1: Habituation</h4>\n<p>The psychological process of \"extinction\" or \"habituation\" occurs when a stimulus is administered repeatedly to an animal, causing the animal's response to gradually diminish.&nbsp; You can imagine that if you were to eat your favorite food for breakfast every morning, it wouldn't be your favorite food after a while.&nbsp; Habituation tends to happen the fastest when the following three conditions are met:</p>\n<ul>\n<li>The stimulus is delivered frequently</li>\n<li>The stimulus is delivered in small doses</li>\n<li>The stimulus is delivered at regular intervals</li>\n</ul>\n<p>Source is <a href=\"http://diigo.com/0aq7v\">here</a>.</p>\n<h4>Applied Habituation</h4>\n<p>I had a project I was working on that was really important to me, but whenever I started working on it I would get demoralized.&nbsp; So I habituated myself to the project: I alternated 2 minutes of work with 2 minutes of sitting in the yard for about 20 minutes.&nbsp; This worked.</p>\n<p><a id=\"more\"></a><br />Interestingly enough, about halfway through this exercise I realized that what was really making it difficult for me to work on my project was the fact that it involved so many choices.&nbsp; So as my 20 minutes progressed, I started spending my 2 minutes trying to make as difficult decisions as possible.&nbsp; This habituation to decision demoralization seems to have had an immediate, fairly lasting impact on a wide variety of activities.<br /><br />I'm really looking forward to hearing from someone who attempts to apply habituation to an <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\" target=\"_blank\">ugh field</a>.</p>\n<h4>Applied Habituation in Reverse<br /></h4>\n<p>If you want to enjoy your favorite song until the day you die, dance to it infrequently at irregular intervals while it plays full blast.&nbsp; (Reversed conditions for habituation.)</p>\n<h4>Behavioral Psychology Finding #2: Intermittent Reinforcement</h4>\n<p>The reason why slot machines are so engaging is because they deliver rewards at random.&nbsp; If slot machines payed small rewards out on every round, playing them would be like work.</p>\n<h4>Applied Intermittent Reinforcement</h4>\n<p>For a while, there was a time-consuming chore that I was required to do every evening.&nbsp; I would often put it off until 2-3 AM and work while sleepy as a result.<br /><br />To solve this problem, I started eating a gummy worm with 50% probability each time I did the chore at a pre-determined time early in the evening.&nbsp; (I gave myself the first two gummy worms with 100% probability to start things off.)&nbsp; My success rate with this method was very high.</p>\n<h4>Further Research</h4>\n<p>Another self-help technique I've had tremendous success with is using Linux's cron utility to cause Firefox tabs to open periodically and tell me to switch activities if I'm wasting time.&nbsp; However, I've found that forcing myself to switch activities is highly stressful.<br /><br />Perhaps it's possible to habituate the negative response to activity switching by having practice sessions where you periodically switch between distraction and work?&nbsp; Or maybe you could use intermittent reinforcement and randomly decide to give yourself something nice if you're successful in an upgrade to a higher-quality activity.</p>\n<p>(I'm not experimenting with these at the moment because I'm currently fairly happy with my work/relaxation balance.)</p>\n<p><em>Thanks to <a href=\"/lw/2cv/defeating_ugh_fields_in_practice/\">Psychohistorian</a> for reminding me I wanted to write about this.&nbsp; I'm hoping he won't get mad at me for writing on the same topic he did so soon after his post.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AodfCFefLAuwDyj7Z": 1, "udPbn9RthmgTtHMiG": 1, "fkABsGCJZ6y9qConW": 1, "dBPou4ihoQNY4cquv": 1, "WqLn4pAWi5hn6McHQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EJuZcWnk8j7eNQPqq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 70, "extendedScore": null, "score": 0.000132, "legacy": true, "legacyId": "3076", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>In which I attempt to apply findings from behavioral psychology to my own life.</em></p>\n<h4 id=\"Behavioral_Psychology_Finding__1__Habituation\">Behavioral Psychology Finding #1: Habituation</h4>\n<p>The psychological process of \"extinction\" or \"habituation\" occurs when a stimulus is administered repeatedly to an animal, causing the animal's response to gradually diminish.&nbsp; You can imagine that if you were to eat your favorite food for breakfast every morning, it wouldn't be your favorite food after a while.&nbsp; Habituation tends to happen the fastest when the following three conditions are met:</p>\n<ul>\n<li>The stimulus is delivered frequently</li>\n<li>The stimulus is delivered in small doses</li>\n<li>The stimulus is delivered at regular intervals</li>\n</ul>\n<p>Source is <a href=\"http://diigo.com/0aq7v\">here</a>.</p>\n<h4 id=\"Applied_Habituation\">Applied Habituation</h4>\n<p>I had a project I was working on that was really important to me, but whenever I started working on it I would get demoralized.&nbsp; So I habituated myself to the project: I alternated 2 minutes of work with 2 minutes of sitting in the yard for about 20 minutes.&nbsp; This worked.</p>\n<p><a id=\"more\"></a><br>Interestingly enough, about halfway through this exercise I realized that what was really making it difficult for me to work on my project was the fact that it involved so many choices.&nbsp; So as my 20 minutes progressed, I started spending my 2 minutes trying to make as difficult decisions as possible.&nbsp; This habituation to decision demoralization seems to have had an immediate, fairly lasting impact on a wide variety of activities.<br><br>I'm really looking forward to hearing from someone who attempts to apply habituation to an <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\" target=\"_blank\">ugh field</a>.</p>\n<h4 id=\"Applied_Habituation_in_Reverse\">Applied Habituation in Reverse<br></h4>\n<p>If you want to enjoy your favorite song until the day you die, dance to it infrequently at irregular intervals while it plays full blast.&nbsp; (Reversed conditions for habituation.)</p>\n<h4 id=\"Behavioral_Psychology_Finding__2__Intermittent_Reinforcement\">Behavioral Psychology Finding #2: Intermittent Reinforcement</h4>\n<p>The reason why slot machines are so engaging is because they deliver rewards at random.&nbsp; If slot machines payed small rewards out on every round, playing them would be like work.</p>\n<h4 id=\"Applied_Intermittent_Reinforcement\">Applied Intermittent Reinforcement</h4>\n<p>For a while, there was a time-consuming chore that I was required to do every evening.&nbsp; I would often put it off until 2-3 AM and work while sleepy as a result.<br><br>To solve this problem, I started eating a gummy worm with 50% probability each time I did the chore at a pre-determined time early in the evening.&nbsp; (I gave myself the first two gummy worms with 100% probability to start things off.)&nbsp; My success rate with this method was very high.</p>\n<h4 id=\"Further_Research\">Further Research</h4>\n<p>Another self-help technique I've had tremendous success with is using Linux's cron utility to cause Firefox tabs to open periodically and tell me to switch activities if I'm wasting time.&nbsp; However, I've found that forcing myself to switch activities is highly stressful.<br><br>Perhaps it's possible to habituate the negative response to activity switching by having practice sessions where you periodically switch between distraction and work?&nbsp; Or maybe you could use intermittent reinforcement and randomly decide to give yourself something nice if you're successful in an upgrade to a higher-quality activity.</p>\n<p>(I'm not experimenting with these at the moment because I'm currently fairly happy with my work/relaxation balance.)</p>\n<p><em>Thanks to <a href=\"/lw/2cv/defeating_ugh_fields_in_practice/\">Psychohistorian</a> for reminding me I wanted to write about this.&nbsp; I'm hoping he won't get mad at me for writing on the same topic he did so soon after his post.</em></p>", "sections": [{"title": "Behavioral Psychology Finding #1: Habituation", "anchor": "Behavioral_Psychology_Finding__1__Habituation", "level": 1}, {"title": "Applied Habituation", "anchor": "Applied_Habituation", "level": 1}, {"title": "Applied Habituation in Reverse", "anchor": "Applied_Habituation_in_Reverse", "level": 1}, {"title": "Behavioral Psychology Finding #2: Intermittent Reinforcement", "anchor": "Behavioral_Psychology_Finding__2__Intermittent_Reinforcement", "level": 1}, {"title": "Applied Intermittent Reinforcement", "anchor": "Applied_Intermittent_Reinforcement", "level": 1}, {"title": "Further Research", "anchor": "Further_Research", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "39 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Rvm7tmfEQ2RstJBPG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-20T07:42:43.674Z", "modifiedAt": "2020-05-04T17:31:40.711Z", "url": null, "title": "Rationality & Criminal Law: Some Questions", "slug": "rationality-and-criminal-law-some-questions", "viewCount": null, "lastCommentedAt": "2020-05-04T14:04:48.897Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "simplicio", "createdAt": "2010-03-06T04:03:43.272Z", "isAdmin": false, "displayName": "simplicio"}, "userId": "fDQ7ty7YmE3PxgqNh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ptcKg7TZgj2WvLKAQ/rationality-and-criminal-law-some-questions", "pageUrlRelative": "/posts/ptcKg7TZgj2WvLKAQ/rationality-and-criminal-law-some-questions", "linkUrl": "https://www.lesswrong.com/posts/ptcKg7TZgj2WvLKAQ/rationality-and-criminal-law-some-questions", "postedAtFormatted": "Sunday, June 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20%26%20Criminal%20Law%3A%20Some%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20%26%20Criminal%20Law%3A%20Some%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptcKg7TZgj2WvLKAQ%2Frationality-and-criminal-law-some-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20%26%20Criminal%20Law%3A%20Some%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptcKg7TZgj2WvLKAQ%2Frationality-and-criminal-law-some-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptcKg7TZgj2WvLKAQ%2Frationality-and-criminal-law-some-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 930, "htmlBody": "<p>The following will explore a couple of areas in which I feel that the criminal justice system of many Western countries might be deficient, from the standpoint of rationality. I am very much interested to know your thoughts on these and other questions of the law, as far as they relate to rational considerations.</p>\n<h2>Moral Luck</h2>\n<p>Moral luck refers to the phenomenon in which behaviour by an agent is adjudged differently based on factors outside the agent's control.</p>\n<p>Suppose that Alice and Yelena, on opposite ends of town, drive home drunk from the bar, and both dazedly speed through a red light, unaware of their surroundings. Yelena gets through nonetheless, but Alice hits a young pedestrian, killing him instantly.&nbsp;Alice is liable to be tried for manslaughter or some similar charge; Yelena, if she is caught, will only receive the drunk driving charge and lose her license.</p>\n<p>Raymond, a day after finding out that his ex is now in a relationship with Pardip, accosts Pardip at his home and attempts to stab him in the chest; Pardip smashes a piece of crockery over Raymond's head, knocking him unconscious. Raymond is convicted of attempted murder, receiving typically 3-5 years chez nous (in Canada). If he had succeeded, he would have received a life sentence, with parole in 10-25 years.</p>\n<p>Why should Alice be punished by the law and demonized by the public so much more than Yelena, when their actions were identical, differing only by the sheerest accident? Why should Raymond receive a lighter sentence for being an unsuccessful murderer?</p>\n<p>Some <em>prima facie</em>&nbsp;plausible justifications:</p>\n<ul>\n<li>Identical behaviour is hard to judge - perhaps Yelena was really keeping a better eye on the road than Alice; perhaps Raymond would have performed a non-fatal stabbing.</li>\n</ul>\n<div>But in Yelena's case, the law is already blind to such things anyway. You don't get a lesser drunk driving charge if you can prove you're pretty good at driving drunk. In the case of Raymond, attempted <strong>murder</strong>&nbsp;already implies that the intent to kill must be proven, else the charge would have been dropped to assault or some such.</div>\n<ul>\n<li>The law needs to crack down harder when there are actual victims, in order to provide the victims and families a sense of justice done.</li>\n</ul>\n<div>This is understandable, but surely if we accept this argument, we could nonetheless satisfy the concerns above by punishing the morally lucky more severely, not punishing the morally unlucky less severely.</div>\n<ul>\n<li>This could result in far too many serious, high-level trials.</li>\n</ul>\n<div>This might be true as far as it goes; however, enforcing strong sentences on the morally lucky would certainly provide a stronger deterrent, which would provide a countervailing tendency to the above.</div>\n<h2>Trial by Jury; Trial by Judge</h2>\n<p>Those of us who like classic films may remember&nbsp;<em>12 Angry Men</em>&nbsp;(1957) with Henry Fonda. This was a remarkably good film about a jury deliberating on the murder trial of a poor young man from a bad neighbourhood, accused of killing his father. It portrays the indifference (one juror wants to be out in time for the baseball game), prejudice and conformity of many of the jurors, and how this is overcome by one man of integrity who decides to insist on a thorough look through the evidence and testimony.</p>\n<p>I do not wish to&nbsp;<a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">generalize from fictional examples</a>; however, such factors are manifestly at play in real trials, in which Henry Fonda cannot necessarily be relied upon to save the day.</p>\n<p>Komponisto has written on the&nbsp;<a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">Knox case</a>, in which an Italian jury came to a very questionable (to put it mildly) conclusion based on the evidence presented to them; other examples will doubtless spring to mind (a famous one in this neck of the woods is the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Steven_Truscott\">Stephen Truscott</a>&nbsp;case - the evidence against Truscott being entirely circumstantial.</p>\n<p>More information on trial by jury and its limitations may be found &nbsp;<a href=\"http://books.google.ca/books?id=z213Luirh44C&amp;pg=PA539&amp;lpg=PA539&amp;dq=trial+by+jury+study+efficacy&amp;source=bl&amp;ots=S0Ttgbw557&amp;sig=kx_YFKumiS0cx93RU3spjU7RP9g&amp;hl=en&amp;ei=P7kdTLaTAo7aNZqzycYM&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=8&amp;ved=0CEUQ6AEwBw#v=onepage&amp;q=trial%20by%20jury%20study%20efficacy&amp;f=false\">here</a>. Recently the UK has made some moves to trial by judge for certain cases, specifically fraud cases in which jury tampering is a problem.</p>\n<p>The justifications cited for trial by jury typically include the egalitarian nature of the practice, in which it can be guaranteed that those making final legal decisions do not form a special class over and above the ordinary citizens whose lives they effect.</p>\n<p>A heartening example of this was mentioned in Thomas Levenson's fascinating book &nbsp;<em>Newton and the Counterfeiter.</em>&nbsp;Being sent to&nbsp;<a href=\"http://en.wikipedia.org/wiki/Newgate_Prison\">Newgate gaol</a>&nbsp;was, infamously in the 17th and 18th centuries, an effective death sentence in and of itself; moreover, a surprisingly large number of crimes at this time were capital crimes (the counterfeiter whom Newton eventually convicted was hanged). In this climate of harsh punishment, juries typically only returned guilty verdicts either when evidence was extremely convincing or when the crime was especially heinous. Effectively, they counteracted the harshness of the legal system by upping the burden of proof for relatively minor crimes.</p>\n<p>So juries sometimes provide a safeguard against abuse of justice by elites. However, is this price for democratizing justice too high, given the ease with which citizens naive about the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Dark_arts\">Dark Arts</a>&nbsp;may be manipulated? (Of course, judges are by no means perfect Bayesians either; however, I would expect them to be significantly less gullible.)</p>\n<p>Are there any other systems that might be tried, besides these canonical two?&nbsp;What about the question of representation? Does the adversarial system, in which two sides are represented by advocates charged with defending their interests, conduce well to truth and justice, or is there a better alternative? For any alternatives you might consider: are they naive or savvy about human nature? What is the normative role of punishment, exactly?</p>\n<p>How would the justice system look if LessWrong had to rewrite it from scratch?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ptcKg7TZgj2WvLKAQ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 21, "extendedScore": null, "score": 5.975486669564271e-07, "legacy": true, "legacyId": "3077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The following will explore a couple of areas in which I feel that the criminal justice system of many Western countries might be deficient, from the standpoint of rationality. I am very much interested to know your thoughts on these and other questions of the law, as far as they relate to rational considerations.</p>\n<h2 id=\"Moral_Luck\">Moral Luck</h2>\n<p>Moral luck refers to the phenomenon in which behaviour by an agent is adjudged differently based on factors outside the agent's control.</p>\n<p>Suppose that Alice and Yelena, on opposite ends of town, drive home drunk from the bar, and both dazedly speed through a red light, unaware of their surroundings. Yelena gets through nonetheless, but Alice hits a young pedestrian, killing him instantly.&nbsp;Alice is liable to be tried for manslaughter or some similar charge; Yelena, if she is caught, will only receive the drunk driving charge and lose her license.</p>\n<p>Raymond, a day after finding out that his ex is now in a relationship with Pardip, accosts Pardip at his home and attempts to stab him in the chest; Pardip smashes a piece of crockery over Raymond's head, knocking him unconscious. Raymond is convicted of attempted murder, receiving typically 3-5 years chez nous (in Canada). If he had succeeded, he would have received a life sentence, with parole in 10-25 years.</p>\n<p>Why should Alice be punished by the law and demonized by the public so much more than Yelena, when their actions were identical, differing only by the sheerest accident? Why should Raymond receive a lighter sentence for being an unsuccessful murderer?</p>\n<p>Some <em>prima facie</em>&nbsp;plausible justifications:</p>\n<ul>\n<li>Identical behaviour is hard to judge - perhaps Yelena was really keeping a better eye on the road than Alice; perhaps Raymond would have performed a non-fatal stabbing.</li>\n</ul>\n<div>But in Yelena's case, the law is already blind to such things anyway. You don't get a lesser drunk driving charge if you can prove you're pretty good at driving drunk. In the case of Raymond, attempted <strong>murder</strong>&nbsp;already implies that the intent to kill must be proven, else the charge would have been dropped to assault or some such.</div>\n<ul>\n<li>The law needs to crack down harder when there are actual victims, in order to provide the victims and families a sense of justice done.</li>\n</ul>\n<div>This is understandable, but surely if we accept this argument, we could nonetheless satisfy the concerns above by punishing the morally lucky more severely, not punishing the morally unlucky less severely.</div>\n<ul>\n<li>This could result in far too many serious, high-level trials.</li>\n</ul>\n<div>This might be true as far as it goes; however, enforcing strong sentences on the morally lucky would certainly provide a stronger deterrent, which would provide a countervailing tendency to the above.</div>\n<h2 id=\"Trial_by_Jury__Trial_by_Judge\">Trial by Jury; Trial by Judge</h2>\n<p>Those of us who like classic films may remember&nbsp;<em>12 Angry Men</em>&nbsp;(1957) with Henry Fonda. This was a remarkably good film about a jury deliberating on the murder trial of a poor young man from a bad neighbourhood, accused of killing his father. It portrays the indifference (one juror wants to be out in time for the baseball game), prejudice and conformity of many of the jurors, and how this is overcome by one man of integrity who decides to insist on a thorough look through the evidence and testimony.</p>\n<p>I do not wish to&nbsp;<a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">generalize from fictional examples</a>; however, such factors are manifestly at play in real trials, in which Henry Fonda cannot necessarily be relied upon to save the day.</p>\n<p>Komponisto has written on the&nbsp;<a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">Knox case</a>, in which an Italian jury came to a very questionable (to put it mildly) conclusion based on the evidence presented to them; other examples will doubtless spring to mind (a famous one in this neck of the woods is the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Steven_Truscott\">Stephen Truscott</a>&nbsp;case - the evidence against Truscott being entirely circumstantial.</p>\n<p>More information on trial by jury and its limitations may be found &nbsp;<a href=\"http://books.google.ca/books?id=z213Luirh44C&amp;pg=PA539&amp;lpg=PA539&amp;dq=trial+by+jury+study+efficacy&amp;source=bl&amp;ots=S0Ttgbw557&amp;sig=kx_YFKumiS0cx93RU3spjU7RP9g&amp;hl=en&amp;ei=P7kdTLaTAo7aNZqzycYM&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=8&amp;ved=0CEUQ6AEwBw#v=onepage&amp;q=trial%20by%20jury%20study%20efficacy&amp;f=false\">here</a>. Recently the UK has made some moves to trial by judge for certain cases, specifically fraud cases in which jury tampering is a problem.</p>\n<p>The justifications cited for trial by jury typically include the egalitarian nature of the practice, in which it can be guaranteed that those making final legal decisions do not form a special class over and above the ordinary citizens whose lives they effect.</p>\n<p>A heartening example of this was mentioned in Thomas Levenson's fascinating book &nbsp;<em>Newton and the Counterfeiter.</em>&nbsp;Being sent to&nbsp;<a href=\"http://en.wikipedia.org/wiki/Newgate_Prison\">Newgate gaol</a>&nbsp;was, infamously in the 17th and 18th centuries, an effective death sentence in and of itself; moreover, a surprisingly large number of crimes at this time were capital crimes (the counterfeiter whom Newton eventually convicted was hanged). In this climate of harsh punishment, juries typically only returned guilty verdicts either when evidence was extremely convincing or when the crime was especially heinous. Effectively, they counteracted the harshness of the legal system by upping the burden of proof for relatively minor crimes.</p>\n<p>So juries sometimes provide a safeguard against abuse of justice by elites. However, is this price for democratizing justice too high, given the ease with which citizens naive about the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Dark_arts\">Dark Arts</a>&nbsp;may be manipulated? (Of course, judges are by no means perfect Bayesians either; however, I would expect them to be significantly less gullible.)</p>\n<p>Are there any other systems that might be tried, besides these canonical two?&nbsp;What about the question of representation? Does the adversarial system, in which two sides are represented by advocates charged with defending their interests, conduce well to truth and justice, or is there a better alternative? For any alternatives you might consider: are they naive or savvy about human nature? What is the normative role of punishment, exactly?</p>\n<p>How would the justice system look if LessWrong had to rewrite it from scratch?</p>", "sections": [{"title": "Moral Luck", "anchor": "Moral_Luck", "level": 1}, {"title": "Trial by Jury; Trial by Judge", "anchor": "Trial_by_Jury__Trial_by_Judge", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "163 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 163, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rHBdcHGLJ7KvLJQPk", "G9dptrW9CJi7wNg3b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-06-20T07:42:43.674Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-22T12:15:54.408Z", "modifiedAt": null, "url": null, "title": "Poll: What value extra copies?", "slug": "poll-what-value-extra-copies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:47.852Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gYvuPhQZyzFqnxXMx/poll-what-value-extra-copies", "pageUrlRelative": "/posts/gYvuPhQZyzFqnxXMx/poll-what-value-extra-copies", "linkUrl": "https://www.lesswrong.com/posts/gYvuPhQZyzFqnxXMx/poll-what-value-extra-copies", "postedAtFormatted": "Tuesday, June 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Poll%3A%20What%20value%20extra%20copies%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APoll%3A%20What%20value%20extra%20copies%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYvuPhQZyzFqnxXMx%2Fpoll-what-value-extra-copies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Poll%3A%20What%20value%20extra%20copies%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYvuPhQZyzFqnxXMx%2Fpoll-what-value-extra-copies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYvuPhQZyzFqnxXMx%2Fpoll-what-value-extra-copies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p>In the future, it may be possible for you to scan your own brain and create copies of yourself. With the power of a controllable superintelligent AI, it may even be possible to create very accurate instances of your past self (and you could take action today or in the near future to make this easier by using <a href=\"http://en.wikipedia.org/wiki/Lifelog\">lifelogging</a> tools such as <a href=\"http://techon.nikkeibp.co.jp/english/NEWS_EN/20100604/183197/\">these glasses</a>).</p>\n<p>So I ask Less Wrong: how valuable do you think creating extra identical, non-interacting copies of yourself is? (each copy existing in its own computational world, which is identical to yours with no copy-copy or world-world interaction)</p>\n<p>For example, would you endure a day's hard labor to create an extra self-copy? A month? A year? Consider the hard labor to be digging a trench with a pickaxe, with a harsh taskmaster who can punish you if you slack off.</p>\n<p>Do you think having 10 copies of yourself made in the future is 10 times as good as having 1 copy made? Or does your utility in copies drop off sub-linearly?</p>\n<p>Last time I spoke to Robin Hanson, he was extremely keen on having a lot of copies of himself created (though I think he was prepared for these copies to be emulant-wage-slaves).</p>\n<p>I have <a href=\"http://rmijic.questionform.com/public/Extra-copies\">created a poll for LW</a> to air its views on this question, then in my next post I'll outline and defend my answer, and lay out some fairly striking implications that this has for existential risk mitigation.</p>\n<p>For those on a hardcore-altruism trip, you may substitute any person or entity that you find more valuable than your own good self: would you sacrifice a day of this entity's life for an extra copy? A year? etc.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p><strong>UPDATE</strong>: Wei Dai has asked this question before, in his post \"<a href=\"/lw/1hg/the_moral_status_of_independent_identical_copies/\">The moral status of independent identical copies</a>\" - though his post focuses more on lock-step copies that are identical over time, whereas here I am interested in both lock-step identical copies and statistically identical copies (a statistically identical copy has the same probability distribution of futures as you do).</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gYvuPhQZyzFqnxXMx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 6, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "3078", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 177, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DNyMJmLf5o26seqvX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-23T05:48:20.854Z", "modifiedAt": null, "url": null, "title": "A Rational Education", "slug": "a-rational-education", "viewCount": null, "lastCommentedAt": "2011-05-15T09:34:17.770Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gztAhEueePQi3RNHs/a-rational-education", "pageUrlRelative": "/posts/gztAhEueePQi3RNHs/a-rational-education", "linkUrl": "https://www.lesswrong.com/posts/gztAhEueePQi3RNHs/a-rational-education", "postedAtFormatted": "Wednesday, June 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Rational%20Education&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Rational%20Education%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgztAhEueePQi3RNHs%2Fa-rational-education%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Rational%20Education%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgztAhEueePQi3RNHs%2Fa-rational-education", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgztAhEueePQi3RNHs%2Fa-rational-education", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1235, "htmlBody": "<p>Within the next month I will be enrolling in an(other) undergraduate university course. This being the case I must make a selection of both course and major. While I could make such decisions on impulsive unconscious preference satisfaction and guesswork on what subjects happen to provide the most value I could also take the opportunity to address the decision more rationally and objectively. There are some relevant questions to ask that I know LessWrong readers can help me answer.</p>\n<ol>\n<li>Which subjects and courses can make the best contribution to Epistemic Rationality?</li>\n<li>Which subjects and courses provide the most Instrumental Rationality benefits?</li>\n<li>Given all available information about the universe and what inferences can be drawn about my preferences and abilities what course structure should I choose?</li>\n<li>Which course do you just happen to <em>like</em>?</li>\n</ol>\n<p><a id=\"more\"></a></p>\n<h2>1. Which subjects and courses can make the best contribution to Epistemic Rationality?</h2>\n<p>I happen to care about <a href=\"http://wiki.lesswrong.com/wiki/Rationality\">Epistemic Rationality</a> for its own sake. Both for me personally and in those whom I encounter. It is <a href=\"/lw/xy/the_fun_theory_sequence/\">Fun</a>! This means that I like both to add new information to my <a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">Map</a> and to develop skills that enhance my general ability to build and improve upon that map.</p>\n<p>Not all knowledge is created equal. While whole posts could be dedicated to what things are the most important to know. I don't want to learn gigabytes of statistics on sport performances. I prefer, and may be tempted to argue that it is fundamentally better, to learn concepts than facts and in particular concepts that are the most related to fundamental reality. This includes physics and the most applicable types of mathematics (eg. probability theory).</p>\n<p>For some types of knowledge that are worth learning university is not a desirable place to learn them. Philosophy is Fun. But the philosophy I would learn at university is too influenced by traditional knowledge and paying rent to impressive figures. The optimal&nbsp; behavior when studying or researching philosophy is not to <a href=\"/lw/of/dissolving_the_question/\">Dissolve the Question</a>. It is to convey that the question is deep and contentious, affiliate with one 'side' and do battle within an obsolete and suboptimal way of <a href=\"/lw/o3/superexponential_conceptspace_and_simple_words/\">Carving Reality</a>. My frank opinion is that many philosophers need to spend more time programming, creating simulated realities, or at least doing mathematics before they can hope to make a useful contribution to thought. (I'm voicing a potentially controversial position here that I know some would agree with but for which I am also inviting debate.)</p>\n<p>There are some subjects that are better served for improving thinking itself as well as merely learning existing thoughts. I'll list some that spring to mind but I suspect some of them may be red herrings and there are others you may be able to suggest that I just haven't considered.</p>\n<ul>\n<li><a href=\"http://www.studentcentre.unimelb.edu.au/eastern/course_information/majors/bsc/majors_-_structures_and_contents/maths_and_stats\">Mathematics and Statistics</a> with specialization in (Statistics/Stochastic Processes) or (Applied Mathematics)</li>\n<li><a href=\"http://www.bsc.unimelb.edu.au/bachelor/study/physics\"></a><a href=\"http://www.bsc.unimelb.edu.au/bachelor/study/mathphys\">Mathematical Physics</a></li>\n<li><a href=\"http://www.bsc.unimelb.edu.au/bachelor/study/informatics/structure\">Science Informatics</a></li>\n<li><a href=\"http://www.bsc.unimelb.edu.au/bachelor/study/psychology\">Psychology</a></li>\n<li>Bizarre as it first seemed to me: <a href=\"https://handbook.unimelb.edu.au/view/2010/F04-AA\">Bachelor of Commerce</a>(Economics) with a concurrent <a href=\"http://www.undergraduates.ms.unimelb.edu.au/new_generation/dip_math_sci/dip_math_sci.html\">Diploma in Mathematics and Statistics</a>(Statistics/Stochastic Processes).</li>\n</ul>\n<h2>2. Which subjects and courses provide the most Instrumental Rationality benefits?</h2>\n<p>Fun is great, so is having accurate maps. But there are practical considerations too. You can't have fun if you starve and fun may not last too long if you are unable to contribute directly or financially to the efforts that ensure the future of humanity. Again there are two considerations:</p>\n<ul>\n<li> What learning facilitates making <a href=\"http://wiki.lesswrong.com/wiki/Rationality\">Instrumentally Rational</a> choices (either in the abstract or practical sense)? The previously mentioned courses are relevant and subjects like game theory naturally spring to mind.</li>\n<li>What learning actually facilitates achieving something useful or otherwise fulfilling one's <a href=\"http://www.google.com.au/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBQQFjAA&amp;url=http%3A%2F%2Fsinginst.org%2Fupload%2FCEV.html&amp;ei=QCIhTOO0KsuIkAXPnuj6Dw&amp;usg=AFQjCNEG70mnF2d9M4oZWL1H3_qkV4WMMg&amp;sig2=9lupStXR_CQ37X69Csqq_A\">CEV</a>? In many cases this will be entirely different to the subjects I have mentioned.</li>\n</ul>\n<h2>3. Given all available information about the universe and what inferences can be drawn about my preferences and abilities what course structure should I choose?</h2>\n<p>This is an invitation to Other-Optimize me. Please give me advice. Remember that giving advice is a signal of high status and as such is often an enjoyable experience to engage in. This is also a rare opportunity - you may be patronizing and I will not even respond in kind or with a curt dismissal. You can even be smug and condescending if that is what it takes for me to extract your insights!</p>\n<p>Now, I should note that my decision to do another undergraduate degree is in no way based on a belief that it is just what I need to do to gain success. I already have more than enough education behind me (I have previously studied IT, AI and teaching).</p>\n<ul>\n<li> My source of income is something that I do independently and is not something that university attendance will unduly interfere with (especially since I can take a laptop to lectures).</li>\n<li>Working entirely independently does not satisfy the human need to be engaged in cooperative endeavor. In the long term this can interfere with both work performance, provoke Akrasia and diminish satisfaction. I do not particularly like working in an office. Studying (and probably tutoring) is ideal.</li>\n<li>Doing something that you are really, really good at that also gives social recognition is psychologically beneficial. Sitting exams is a more efficient way for me to satisfy the need for recognition than attempting to win at office politics.</li>\n<li>\"Full Time\" study is not at all \"full time\" for me. It is more like a part time hobby.</li>\n</ul>\n<p>(Call bullshit on that if you think I am rationalizing or believe there are better alternatives to give me what you infer from here or elsewhere that I want.)</p>\n<h4>Now, assuming that I am going to be studying an undergraduate course, which course maximizes the expected benefit?</h4>\n<p>Something I am considering is a double major Bachelor of Science(pharmacology, mathematical statistics). <a href=\"/lw/2cv/defeating_ugh_fields_in_practice/264j\">Recent</a> <a href=\"/lw/2co/how_to_always_have_interesting_conversations/25xk\">conversations </a>that I have participated in here give an indication as to my existing interest in pharmacology. I have some plans in mind that would contribute to furthering human knowledge on non-patented pharmaceutical substances. In particular life-extension drugs and <a href=\"http://en.wikipedia.org/wiki/Nootropic\">nootropics</a>. This is an area that I believe is drastically overlooked, to the extent of being species wide negligence. Consider this to be a significant goal that I want my studying to contribute to.</p>\n<p>The most effective contribution I can make there will likely involve leveraging financial resources that I earn elsewhere but I mostly have financial considerations covered. I also want to ensure I know what is going on and know what needs to be done at a detailed level. That means learning pharmacology. But it also means learning statistics of some sort. What statistics should I learn? Should I focus on improving my understanding of Bayesian statistics or should I immerse myself in some more ad-hoc frequentest tools that can be used to look impressive?</p>\n<h2>4. Which course do you just happen to <em>like</em>?</h2>\n<p>What other subjects are relevant to the sort of concepts we like discussing here? Perhaps something from sociology or psych? I have breadth subjects I need to fill, which gives me the chance to look at some topics in somewhat more depth than just a post (but sometimes possibly less depth than a whole post sequence!) I'm also rather curious which subjects like-minded people just wish they had a chance to study. If you were <a href=\"http://www.gateworld.net/sg1/s4/reviews/406.shtml\">trapped in the SGC in a groundhog day time loop</a> which topics would you want to learn?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gztAhEueePQi3RNHs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 5.984381990961191e-07, "legacy": true, "legacyId": "3090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Within the next month I will be enrolling in an(other) undergraduate university course. This being the case I must make a selection of both course and major. While I could make such decisions on impulsive unconscious preference satisfaction and guesswork on what subjects happen to provide the most value I could also take the opportunity to address the decision more rationally and objectively. There are some relevant questions to ask that I know LessWrong readers can help me answer.</p>\n<ol>\n<li>Which subjects and courses can make the best contribution to Epistemic Rationality?</li>\n<li>Which subjects and courses provide the most Instrumental Rationality benefits?</li>\n<li>Given all available information about the universe and what inferences can be drawn about my preferences and abilities what course structure should I choose?</li>\n<li>Which course do you just happen to <em>like</em>?</li>\n</ol>\n<p><a id=\"more\"></a></p>\n<h2 id=\"1__Which_subjects_and_courses_can_make_the_best_contribution_to_Epistemic_Rationality_\">1. Which subjects and courses can make the best contribution to Epistemic Rationality?</h2>\n<p>I happen to care about <a href=\"http://wiki.lesswrong.com/wiki/Rationality\">Epistemic Rationality</a> for its own sake. Both for me personally and in those whom I encounter. It is <a href=\"/lw/xy/the_fun_theory_sequence/\">Fun</a>! This means that I like both to add new information to my <a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">Map</a> and to develop skills that enhance my general ability to build and improve upon that map.</p>\n<p>Not all knowledge is created equal. While whole posts could be dedicated to what things are the most important to know. I don't want to learn gigabytes of statistics on sport performances. I prefer, and may be tempted to argue that it is fundamentally better, to learn concepts than facts and in particular concepts that are the most related to fundamental reality. This includes physics and the most applicable types of mathematics (eg. probability theory).</p>\n<p>For some types of knowledge that are worth learning university is not a desirable place to learn them. Philosophy is Fun. But the philosophy I would learn at university is too influenced by traditional knowledge and paying rent to impressive figures. The optimal&nbsp; behavior when studying or researching philosophy is not to <a href=\"/lw/of/dissolving_the_question/\">Dissolve the Question</a>. It is to convey that the question is deep and contentious, affiliate with one 'side' and do battle within an obsolete and suboptimal way of <a href=\"/lw/o3/superexponential_conceptspace_and_simple_words/\">Carving Reality</a>. My frank opinion is that many philosophers need to spend more time programming, creating simulated realities, or at least doing mathematics before they can hope to make a useful contribution to thought. (I'm voicing a potentially controversial position here that I know some would agree with but for which I am also inviting debate.)</p>\n<p>There are some subjects that are better served for improving thinking itself as well as merely learning existing thoughts. I'll list some that spring to mind but I suspect some of them may be red herrings and there are others you may be able to suggest that I just haven't considered.</p>\n<ul>\n<li><a href=\"http://www.studentcentre.unimelb.edu.au/eastern/course_information/majors/bsc/majors_-_structures_and_contents/maths_and_stats\">Mathematics and Statistics</a> with specialization in (Statistics/Stochastic Processes) or (Applied Mathematics)</li>\n<li><a href=\"http://www.bsc.unimelb.edu.au/bachelor/study/physics\"></a><a href=\"http://www.bsc.unimelb.edu.au/bachelor/study/mathphys\">Mathematical Physics</a></li>\n<li><a href=\"http://www.bsc.unimelb.edu.au/bachelor/study/informatics/structure\">Science Informatics</a></li>\n<li><a href=\"http://www.bsc.unimelb.edu.au/bachelor/study/psychology\">Psychology</a></li>\n<li>Bizarre as it first seemed to me: <a href=\"https://handbook.unimelb.edu.au/view/2010/F04-AA\">Bachelor of Commerce</a>(Economics) with a concurrent <a href=\"http://www.undergraduates.ms.unimelb.edu.au/new_generation/dip_math_sci/dip_math_sci.html\">Diploma in Mathematics and Statistics</a>(Statistics/Stochastic Processes).</li>\n</ul>\n<h2 id=\"2__Which_subjects_and_courses_provide_the_most_Instrumental_Rationality_benefits_\">2. Which subjects and courses provide the most Instrumental Rationality benefits?</h2>\n<p>Fun is great, so is having accurate maps. But there are practical considerations too. You can't have fun if you starve and fun may not last too long if you are unable to contribute directly or financially to the efforts that ensure the future of humanity. Again there are two considerations:</p>\n<ul>\n<li> What learning facilitates making <a href=\"http://wiki.lesswrong.com/wiki/Rationality\">Instrumentally Rational</a> choices (either in the abstract or practical sense)? The previously mentioned courses are relevant and subjects like game theory naturally spring to mind.</li>\n<li>What learning actually facilitates achieving something useful or otherwise fulfilling one's <a href=\"http://www.google.com.au/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBQQFjAA&amp;url=http%3A%2F%2Fsinginst.org%2Fupload%2FCEV.html&amp;ei=QCIhTOO0KsuIkAXPnuj6Dw&amp;usg=AFQjCNEG70mnF2d9M4oZWL1H3_qkV4WMMg&amp;sig2=9lupStXR_CQ37X69Csqq_A\">CEV</a>? In many cases this will be entirely different to the subjects I have mentioned.</li>\n</ul>\n<h2 id=\"3__Given_all_available_information_about_the_universe_and_what_inferences_can_be_drawn_about_my_preferences_and_abilities_what_course_structure_should_I_choose_\">3. Given all available information about the universe and what inferences can be drawn about my preferences and abilities what course structure should I choose?</h2>\n<p>This is an invitation to Other-Optimize me. Please give me advice. Remember that giving advice is a signal of high status and as such is often an enjoyable experience to engage in. This is also a rare opportunity - you may be patronizing and I will not even respond in kind or with a curt dismissal. You can even be smug and condescending if that is what it takes for me to extract your insights!</p>\n<p>Now, I should note that my decision to do another undergraduate degree is in no way based on a belief that it is just what I need to do to gain success. I already have more than enough education behind me (I have previously studied IT, AI and teaching).</p>\n<ul>\n<li> My source of income is something that I do independently and is not something that university attendance will unduly interfere with (especially since I can take a laptop to lectures).</li>\n<li>Working entirely independently does not satisfy the human need to be engaged in cooperative endeavor. In the long term this can interfere with both work performance, provoke Akrasia and diminish satisfaction. I do not particularly like working in an office. Studying (and probably tutoring) is ideal.</li>\n<li>Doing something that you are really, really good at that also gives social recognition is psychologically beneficial. Sitting exams is a more efficient way for me to satisfy the need for recognition than attempting to win at office politics.</li>\n<li>\"Full Time\" study is not at all \"full time\" for me. It is more like a part time hobby.</li>\n</ul>\n<p>(Call bullshit on that if you think I am rationalizing or believe there are better alternatives to give me what you infer from here or elsewhere that I want.)</p>\n<h4 id=\"Now__assuming_that_I_am_going_to_be_studying_an_undergraduate_course__which_course_maximizes_the_expected_benefit_\">Now, assuming that I am going to be studying an undergraduate course, which course maximizes the expected benefit?</h4>\n<p>Something I am considering is a double major Bachelor of Science(pharmacology, mathematical statistics). <a href=\"/lw/2cv/defeating_ugh_fields_in_practice/264j\">Recent</a> <a href=\"/lw/2co/how_to_always_have_interesting_conversations/25xk\">conversations </a>that I have participated in here give an indication as to my existing interest in pharmacology. I have some plans in mind that would contribute to furthering human knowledge on non-patented pharmaceutical substances. In particular life-extension drugs and <a href=\"http://en.wikipedia.org/wiki/Nootropic\">nootropics</a>. This is an area that I believe is drastically overlooked, to the extent of being species wide negligence. Consider this to be a significant goal that I want my studying to contribute to.</p>\n<p>The most effective contribution I can make there will likely involve leveraging financial resources that I earn elsewhere but I mostly have financial considerations covered. I also want to ensure I know what is going on and know what needs to be done at a detailed level. That means learning pharmacology. But it also means learning statistics of some sort. What statistics should I learn? Should I focus on improving my understanding of Bayesian statistics or should I immerse myself in some more ad-hoc frequentest tools that can be used to look impressive?</p>\n<h2 id=\"4__Which_course_do_you_just_happen_to_like_\">4. Which course do you just happen to <em>like</em>?</h2>\n<p>What other subjects are relevant to the sort of concepts we like discussing here? Perhaps something from sociology or psych? I have breadth subjects I need to fill, which gives me the chance to look at some topics in somewhat more depth than just a post (but sometimes possibly less depth than a whole post sequence!) I'm also rather curious which subjects like-minded people just wish they had a chance to study. If you were <a href=\"http://www.gateworld.net/sg1/s4/reviews/406.shtml\">trapped in the SGC in a groundhog day time loop</a> which topics would you want to learn?</p>", "sections": [{"title": "1. Which subjects and courses can make the best contribution to Epistemic Rationality?", "anchor": "1__Which_subjects_and_courses_can_make_the_best_contribution_to_Epistemic_Rationality_", "level": 1}, {"title": "2. Which subjects and courses provide the most Instrumental Rationality benefits?", "anchor": "2__Which_subjects_and_courses_provide_the_most_Instrumental_Rationality_benefits_", "level": 1}, {"title": "3. Given all available information about the universe and what inferences can be drawn about my preferences and abilities what course structure should I choose?", "anchor": "3__Given_all_available_information_about_the_universe_and_what_inferences_can_be_drawn_about_my_preferences_and_abilities_what_course_structure_should_I_choose_", "level": 1}, {"title": "Now, assuming that I am going to be studying an undergraduate course, which course maximizes the expected benefit?", "anchor": "Now__assuming_that_I_am_going_to_be_studying_an_undergraduate_course__which_course_maximizes_the_expected_benefit_", "level": 2}, {"title": "4. Which course do you just happen to like?", "anchor": "4__Which_course_do_you_just_happen_to_like_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "152 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 149, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4aGvLnHvYgX9pZHS", "Mc6QcrsbH5NRXbCRX", "82eMd5KLiJ5Z6rTrr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-23T14:33:29.651Z", "modifiedAt": null, "url": null, "title": "Is cryonics necessary?: Writing yourself into the future", "slug": "is-cryonics-necessary-writing-yourself-into-the-future", "viewCount": null, "lastCommentedAt": "2020-05-13T20:48:26.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gworley", "createdAt": "2009-03-26T17:18:20.404Z", "isAdmin": false, "displayName": "G Gordon Worley III"}, "userId": "gjoi5eBQob27Lww62", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iRMBxyrKubkwhxccq/is-cryonics-necessary-writing-yourself-into-the-future", "pageUrlRelative": "/posts/iRMBxyrKubkwhxccq/is-cryonics-necessary-writing-yourself-into-the-future", "linkUrl": "https://www.lesswrong.com/posts/iRMBxyrKubkwhxccq/is-cryonics-necessary-writing-yourself-into-the-future", "postedAtFormatted": "Wednesday, June 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20cryonics%20necessary%3F%3A%20Writing%20yourself%20into%20the%20future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20cryonics%20necessary%3F%3A%20Writing%20yourself%20into%20the%20future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRMBxyrKubkwhxccq%2Fis-cryonics-necessary-writing-yourself-into-the-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20cryonics%20necessary%3F%3A%20Writing%20yourself%20into%20the%20future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRMBxyrKubkwhxccq%2Fis-cryonics-necessary-writing-yourself-into-the-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRMBxyrKubkwhxccq%2Fis-cryonics-necessary-writing-yourself-into-the-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1392, "htmlBody": "<p>Cryonics appears to be the best hope for continuing a person's existence beyond physical death until other technologies provide better solutions.&nbsp; But despite its best-in-class status, cryonics has several serious downsides.</p>\n<p>First and foremost, cryonics is expensive&mdash;well beyond a price that even a third of humanity can afford.&nbsp; Economies of scale may eventually bring the cost down, but in the mean time billions of people will die without the benefit of cryonics, and, even when the cost bottoms out, it will likely still be too expensive for people living at subsistence levels.&nbsp; Secondly, many people consider cryonics immoral or at least socially unacceptable, so even those who accept the idea of cryonics and want to pursue taking personal advantage of it are usually socially pressured out of signing up for cryonics.&nbsp; Combined, these two forces reduce the pool of people who will act to sign up for cryonics to be less than even a fraction of a percent of the human population.</p>\n<p>Given that cryonics is effectively not an option for almost everyone on the planet, if we're serious about preserving lives into the future then we have to consider other options, especially ones that are morally and socially acceptable to most of humanity.&nbsp; Pushed by my own need to find an alternative to cryonics, I began trying to think of ways I could be restored after physical death.</p>\n<p>If I am unable to preserve the physical components that currently make me up, it seems that the next best thing I can do is to record in some way as much of the details of the functioning of those physical components as possible.&nbsp; Since we don't yet have the brain emulation technology that would make cryonics irrelevant for the still living, I need a lower tech way to making a record of myself.&nbsp; And of all the ways I might try to record myself, none seems to better balance robustness, cost, and detail than writing.</p>\n<p>Writing myself into the future&mdash;now we're on to something.<a id=\"more\"></a></p>\n<p>At first this plan didn't feel like such a winner, though:&nbsp; How can I continue myself just through writing?&nbsp; Even if I write down everything I can about myself&mdash;memories, medical history, everything&mdash;how can that really be all that's needed to restore me (or even most of me)?&nbsp; But when we begin to break down what writing everything we can about ourselves really gives us, writing ourselves into the future begins to make more sense.</p>\n<p>For most of humanity, what makes you who you are is largely the same between all people.&nbsp; Since percentages would make it seem that I have too precise an idea of how much, let's put it like this:&nbsp; up to your eyebrows, all humans (except those with extreme abnormalities) are essentially the same.&nbsp; Because we share the same evolutionary past as all of our conspecifics, the biology and psychology of our brains is statistically the same.&nbsp; We each have our quirks of genetics and development, but even those are statistically similar among people who share our quirks.&nbsp; Thus with just a few bits of data we can already record most of what makes you who you are.</p>\n<p>Most people find this idea unsettling when they first encounter it and have an urge to look away or disagree.&nbsp; \"How can I, the very unique me, be almost completely the same as everyone else?\"&nbsp; Since this is Less Wrong and not a more general forum, though, I'll assume you're still with me at this point.&nbsp; If not, I recommend reading some of the introductory sequences on the site.</p>\n<p>So if we begin with a human template, add in a few modifiers for particular genetic and developmental quirks, we get to a sort of blank human that gets us most of the way to restoring you after physical death.&nbsp; To complete the restoration, we need to inject the stuff that sets you uniquely apart even from your fellow humans who share your statistically regular quirks:&nbsp; your memories.&nbsp; If the record of your memories is good enough, this should effectively create a person who is so much like you as to be indistinguishable from the original, i.e. restore you.</p>\n<p>But, you may ask, is this restoration of you from writing really still you in the same way that the you restored from cryonics is you?&nbsp; Maybe.&nbsp; To me, it is.&nbsp; Despite what subjective experience feels like, there doesn't seem to be anything in the brain that makes you who you are besides the general process of your brain and its memories.&nbsp; Transferring yourself from your current brain to another brain or a brain emulation via writing doesn't seem that much different from transferring yourself via neuron replacement or some other technique except that writing introduces a lossy compression step, necessitated only by a lack of access to better technology.&nbsp; Writing yourself into the future isn't the best solution, but it does seem to be an effective stopgap to death.</p>\n<hr />\n<p>If you're still with me, we have a few nagging questions to answer.&nbsp; Consider this an FAQ for writing yourself into the future.</p>\n<p><strong>How good a record is good enough?</strong>&nbsp; In truth, I don't think we even know enough to get the order of magnitude right.&nbsp; The best I can offer is that you need to record as much as you are willing to.&nbsp; The more you record, the more there will be to work with, and the less chance there will be of insufficient data.&nbsp; It may turn out that you simply can't record enough to create a good restoration of a person from writing, but this is little different from the risk in cryonics of not being well preserved enough to restore despite best efforts.&nbsp; If you're willing to take the risk that cryonics won't work as well as you hope, you should be willing to accept that writing yourself into the future might not work as well as you hope.</p>\n<p><strong>How is writing yourself into the future more socially acceptable than cryonics?&nbsp; </strong>Basically, because people already do this all the time, although not with an eye toward their eventually restoration.&nbsp; People regularly keep journals, write blogs, write autobiographies, and pass on stories of their lives, even if only orally.&nbsp; You can write a record of yourself, fully intending for it to be used to restore you at some future time, without ever having to do anything that is morally or socially unacceptable to other people (at least, for people in most societies) other than perhaps specify in your writing of yourself that you want it to be used to restore you after you die.</p>\n<p><strong>How is writing yourself into the future more accessible to the poor?</strong>&nbsp; If a person is literate and has access to some form of durable writing material, they can write themselves into the future, limited only by their access to durable writing material and reliable storage.&nbsp; Of course, many people are not literate, but the cost of teaching literacy is far lower than the cost of cryonics, and literacy has other benefits beyond writing yourself into the future, so it's an easy sell to increase literacy even to people who are opposed to the idea of life extension.</p>\n<p><strong>Will the restoration really be me?</strong>&nbsp; Let me address this in another way.&nbsp; You, like everything else, are a part of the universe.&nbsp; Unlike what we believe to be true of most of the stuff in the universe, though, the stuff that makes up what we call you is aware of its existence.&nbsp; As best we can tell, the way that you are aware of your existence is because you have a way of recalling previous events during your existence.&nbsp; If we take away the store and recall of experience, we're left with some stuff that can do essentially everything it could when it had memory, but will not have any concept of existing outside the current moment.&nbsp; Put the store and recall back in, though, and suddenly what we would recognize as self-awareness returns.</p>\n<p><strong>Other questions?</strong>&nbsp; Post them and I'll try to address them.&nbsp; I have a feeling that there will be some strong disagreement from people who disagree with me about what self-awareness means and how the brain works, and I'll try to explain my position as best I can to them, but I'm also interested in any other questions that people might have since there are likely many issues I haven't even considered yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iRMBxyrKubkwhxccq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 15, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "1690", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 147, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-25T01:08:28.125Z", "modifiedAt": null, "url": null, "title": "Spaced Repetition Database for the Mysterious Answers to Mysterious Questions Sequence", "slug": "spaced-repetition-database-for-the-mysterious-answers-to", "viewCount": null, "lastCommentedAt": "2020-08-21T23:49:58.337Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "divia", "createdAt": "2009-02-28T01:56:35.966Z", "isAdmin": false, "displayName": "divia"}, "userId": "CQzR9QRTNKQ9Qmsjc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iRfKKYhZAG8fWDjJr/spaced-repetition-database-for-the-mysterious-answers-to", "pageUrlRelative": "/posts/iRfKKYhZAG8fWDjJr/spaced-repetition-database-for-the-mysterious-answers-to", "linkUrl": "https://www.lesswrong.com/posts/iRfKKYhZAG8fWDjJr/spaced-repetition-database-for-the-mysterious-answers-to", "postedAtFormatted": "Friday, June 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spaced%20Repetition%20Database%20for%20the%20Mysterious%20Answers%20to%20Mysterious%20Questions%20Sequence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpaced%20Repetition%20Database%20for%20the%20Mysterious%20Answers%20to%20Mysterious%20Questions%20Sequence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRfKKYhZAG8fWDjJr%2Fspaced-repetition-database-for-the-mysterious-answers-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spaced%20Repetition%20Database%20for%20the%20Mysterious%20Answers%20to%20Mysterious%20Questions%20Sequence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRfKKYhZAG8fWDjJr%2Fspaced-repetition-database-for-the-mysterious-answers-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRfKKYhZAG8fWDjJr%2Fspaced-repetition-database-for-the-mysterious-answers-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>I'm a big fan of <a href=\"http://supermemo.com/english/princip.htm\">spaced repetition software</a>. &nbsp;There's a lot I could say about how awesome I think it is and how much it has helped me, but the SuperMemo website covers the benefits better than I could. &nbsp;I will mention two things that surprised me. &nbsp;First, I had no idea how much fun it would be; I actually really enjoy doing the reviews every day. &nbsp;(For me this is hugely important, since it's unlikely I would have kept up with it otherwise.) Second, it's proven more useful than I had anticipated for maintaining coherence of beliefs across emotional states. &nbsp;</p>\n<p>I've tried memorizing a variety types of things such as emacs commands, my favorite quotations, advice about how to communicate with children, and characters from books. &nbsp;One of my more recent projects has been making notecards of the lesswrong <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">sequences</a>. &nbsp;I tried to follow the <a href=\"http://www.supermemo.com/articles/20rules.htm\">rules for formulating knowledge</a>&nbsp;from the SuperMemo website, but deciding which bits to encode and how is subjective. &nbsp;For reference, I asked my boyfriend to make a few too so we could compare, and his looked pretty different from mine.&nbsp;</p>\n<p>So, with those caveats, I thought I might as well share what I'd come up with. &nbsp;As <a href=\"http://paulbuchheit.blogspot.com/2007/04/perfect-is-enemy-of-good-enough-and.html\">Paul Buchheit says</a>, \"'Good enough' is the enemy of 'At all'\". &nbsp;If you download <a href=\"http://ichi2.net/anki/\">Anki</a>, my favorite spaced repetition software (free and cross-platform) and go to Download &gt; Shared Deck in the Menu, you should be able to search for and get my Less Wrong Sequences cards. &nbsp;I also put them up <a href=\"http://divia.posterous.com/mysterious-answers-to-mysterious-questions-no\">here</a>, with the ones my boyfriend made of the first post for comparison.</p>\n<p>I had read all the sequences before, but I have found that since I've started using the cards I've noticed the concepts coming up in my life more often, so I think my experiment has been useful.</p>\n<p>Let me know what you think!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iRfKKYhZAG8fWDjJr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 58, "extendedScore": null, "score": 0.00011826597547322854, "legacy": true, "legacyId": "3102", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-25T16:46:08.379Z", "modifiedAt": null, "url": null, "title": "MWI, copies and probability", "slug": "mwi-copies-and-probability", "viewCount": null, "lastCommentedAt": "2020-02-13T19:12:05.854Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w2mC29nvmLbcua9yo/mwi-copies-and-probability", "pageUrlRelative": "/posts/w2mC29nvmLbcua9yo/mwi-copies-and-probability", "linkUrl": "https://www.lesswrong.com/posts/w2mC29nvmLbcua9yo/mwi-copies-and-probability", "postedAtFormatted": "Friday, June 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MWI%2C%20copies%20and%20probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMWI%2C%20copies%20and%20probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw2mC29nvmLbcua9yo%2Fmwi-copies-and-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MWI%2C%20copies%20and%20probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw2mC29nvmLbcua9yo%2Fmwi-copies-and-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw2mC29nvmLbcua9yo%2Fmwi-copies-and-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1042, "htmlBody": "<p>Followup to: <a href=\"/lw/2di/what_value_extra_copies/\">Poll: What value extra copies?</a><a href=\"/lw/2di/what_value_extra_copies/\"></a></p>\n<p>For those of you who didn't follow Eliezer's <a href=\"/lw/r5/the_quantum_physics_sequence/\">Quantum Physics Sequence</a>, let me reiterate that there is something very messed up about the universe we live in. Specifically, the Many Worlds Interpretation (MWI) of quantum mechanics states that our entire classical world gets copied something like 10<sup>40&plusmn;20</sup> times per second<sup>1</sup>. You are not a line through time, but a branching tree.</p>\n<p>If you think carefully about Descartes' \"I think therefore I am\" type skepticism, and approach your stream of sensory observations from such a skeptical point of view, you should note that if you really were just one branch-line in a person-tree, it would feel exactly the same as if you were a unique person-line through time, because looking backwards, a tree looks like a line, and your memory can only look backwards.&nbsp;</p>\n<p>However, the rules of quantum mechanics mean that the integral of the modulus squared of the amplitude density, &int;|&Psi;|<sup>2</sup>, is conserved in the copying process. Therefore, the tree that is you has branches that get thinner (where thickness is &int;|&Psi;|<sup>2</sup> over the localized density \"blob\" that represents that branch) as they branch off. In fact they get thinner in such a way that if you gathered them together into a bundle, the bundle would be as thick as the trunk it came from.</p>\n<p>Now, since each copying event creates a slightly different classical universe, the copies in each of the sub-branches will each experience random events going differently. This means that over a timescale of decades, they will be totally \"different\" people, with different jobs, probably different partners and will live in different places though they will (of course) have your DNA, approximate physical appearance, and an identical history up until the time they branched off. For timescales on the order of a day, I suspect that almost all of the copies will be virtually identical to you, even down to going to bed at the same time, having exactly the same schedule that day, thinking almost all of the same thoughts etc.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><strong>MWI mixes copies and probability</strong></p>\n<p>When a \"random\" event happens, either the event was pseudorandom (like a large digit of pi) or it was a copy event, meaning that both (or all) outcomes were realized elsewhere in the wavefunction. This means that in many situations, when you say \"there is a probability p of event X happening\", what this really means is \"proportion p of my copy-children will experience X\".</p>\n<p>&nbsp;</p>\n<p><strong>LW doesn't care about copies</strong></p>\n<p>In <a href=\"/lw/2di/what_value_extra_copies/\">Poll: What value extra copies?</a>, I asked what value people placed upon non-interacting extra copies of themselves, asking both about lock-step identical and statistically identical copies. The overwhelming opinion was that neither were of much value. For example, <a href=\"/lw/2di/poll_what_value_extra_copies/26ok\">Sly comments</a>:<sup>2</sup></p>\n<p><em>\"I would place 0 value on a copy that does not interact with me. This might be odd, but a copy of me that is non-interacting is indistinguishable from a copy of someone else that is non-interacting. Why does it matter that it is a copy of me?\"</em></p>\n<p>&nbsp;</p>\n<p><strong>How to get away with attempted murder </strong></p>\n<p>Suppose you throw a grenade with a quantum detonator at Sly. The detonator will sample a qbit in an even superposition of states 1 and 0. On a 0 it explodes, instantly vaporizing sly (it's a very powerful grenade). On a 1, it defuses the grenade and dispenses a $100 dollar note. Suppose that you throw it and observe that it doesn't explode:</p>\n<p style=\"padding-left: 30px;\"><em>(A) does Sly charge you with attempted murder, or does he thank you for giving him $100 in exchange for something that had no value to him anyway?</em></p>\n<p style=\"padding-left: 30px;\"><em>(B) if he thanks you for the free $100, does he ask for another one of those nice free hundred dollar note dispensers? (This is the \"quantum suicide\" option<br /></em></p>\n<p style=\"padding-left: 30px;\"><em>(C) if he says \"the one you've already given me was great, but no more please\", then presumably if you throw another one against his will, he will thank you for the free $100 again. And so on ad infinitum. Sly is temporally inconsistent if this option is chosen.</em></p>\n<p>&nbsp;</p>\n<p>The punch line is that the physics we run on gives us a very strong reason to care about the welfare of copies of ourselves, which is (according to my survey) a counter-intuitive result.&nbsp;</p>\n<p>EDIT: Quite a few people are biting the quantum suicide bullet. I think I'll have to talk about that next. Also, <a href=\"/lw/2e0/mwi_copies_and_probability/273h\">Wei Dai summarizes</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><em>Another way to think about this is that many of us seem to share the follow three intuitions about non-interacting extra copies, out of which we have to give up at least one to retain logical consistency:</em></span></p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em>We value extra copies in other quantum branches.</em></li>\n<li><em>We don't value extra copies that are just spatially separated from us (and are not too far away).</em></li>\n<li><em>We ought to value both kinds of copies the same way.</em></li>\n</ol> \n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding-left: 30px;\">\n<li><em>Giving up 1 is the position of \"quantum immortality\".</em></li>\n<li><em>Giving up 2 seems to be Roko's position in this post.</em></li>\n<li><em>Giving up 3 would imply that our values are rather arbitrary: there seems to be no morally relevant differences between these two kinds of copies, so why should we value one and not the other? But according to the \"complexity of value\" position, perhaps this isn't really a big problem.</em></li>\n</ul>\n<p>&nbsp;</p>\n<p>I might add a fourth option that many people in the comments seem to be going after: (4) We don't intrinsically value copies in other branches, we just have a subjective anticipation of becoming them.&nbsp;</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<hr style=\"padding-left: 30px;\" />\n<p style=\"padding-left: 30px;\">1: The copying events are not discrete, rather they consist of a continuous deformation of probability amplitude in state space, but the shape of that deformation looks a lot like a continuous approximation to a discrete copying event, and the classical rules of physics approximately govern the time evolution of the \"copies\" as if they were completely independent. This last statement is the phenomenon of decoherence. The uncertainty in the copying rate is due to my ignorance, and I would welcome a physicist correcting me.</p>\n<p style=\"padding-left: 30px;\">2: There were many others who expressed roughly similar views, and I don't hold it as a \"black mark\" to pick the option that I am advising against, rather I encourage people to honestly put forward their opinions in a spirit of communal learning.</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1c9": 1, "PbShukhzpLsWpGXkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w2mC29nvmLbcua9yo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 19, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "3096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Followup to: <a href=\"/lw/2di/what_value_extra_copies/\">Poll: What value extra copies?</a><a href=\"/lw/2di/what_value_extra_copies/\"></a></p>\n<p>For those of you who didn't follow Eliezer's <a href=\"/lw/r5/the_quantum_physics_sequence/\">Quantum Physics Sequence</a>, let me reiterate that there is something very messed up about the universe we live in. Specifically, the Many Worlds Interpretation (MWI) of quantum mechanics states that our entire classical world gets copied something like 10<sup>40\u00b120</sup> times per second<sup>1</sup>. You are not a line through time, but a branching tree.</p>\n<p>If you think carefully about Descartes' \"I think therefore I am\" type skepticism, and approach your stream of sensory observations from such a skeptical point of view, you should note that if you really were just one branch-line in a person-tree, it would feel exactly the same as if you were a unique person-line through time, because looking backwards, a tree looks like a line, and your memory can only look backwards.&nbsp;</p>\n<p>However, the rules of quantum mechanics mean that the integral of the modulus squared of the amplitude density, \u222b|\u03a8|<sup>2</sup>, is conserved in the copying process. Therefore, the tree that is you has branches that get thinner (where thickness is \u222b|\u03a8|<sup>2</sup> over the localized density \"blob\" that represents that branch) as they branch off. In fact they get thinner in such a way that if you gathered them together into a bundle, the bundle would be as thick as the trunk it came from.</p>\n<p>Now, since each copying event creates a slightly different classical universe, the copies in each of the sub-branches will each experience random events going differently. This means that over a timescale of decades, they will be totally \"different\" people, with different jobs, probably different partners and will live in different places though they will (of course) have your DNA, approximate physical appearance, and an identical history up until the time they branched off. For timescales on the order of a day, I suspect that almost all of the copies will be virtually identical to you, even down to going to bed at the same time, having exactly the same schedule that day, thinking almost all of the same thoughts etc.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><strong id=\"MWI_mixes_copies_and_probability\">MWI mixes copies and probability</strong></p>\n<p>When a \"random\" event happens, either the event was pseudorandom (like a large digit of pi) or it was a copy event, meaning that both (or all) outcomes were realized elsewhere in the wavefunction. This means that in many situations, when you say \"there is a probability p of event X happening\", what this really means is \"proportion p of my copy-children will experience X\".</p>\n<p>&nbsp;</p>\n<p><strong id=\"LW_doesn_t_care_about_copies\">LW doesn't care about copies</strong></p>\n<p>In <a href=\"/lw/2di/what_value_extra_copies/\">Poll: What value extra copies?</a>, I asked what value people placed upon non-interacting extra copies of themselves, asking both about lock-step identical and statistically identical copies. The overwhelming opinion was that neither were of much value. For example, <a href=\"/lw/2di/poll_what_value_extra_copies/26ok\">Sly comments</a>:<sup>2</sup></p>\n<p><em>\"I would place 0 value on a copy that does not interact with me. This might be odd, but a copy of me that is non-interacting is indistinguishable from a copy of someone else that is non-interacting. Why does it matter that it is a copy of me?\"</em></p>\n<p>&nbsp;</p>\n<p><strong id=\"How_to_get_away_with_attempted_murder_\">How to get away with attempted murder </strong></p>\n<p>Suppose you throw a grenade with a quantum detonator at Sly. The detonator will sample a qbit in an even superposition of states 1 and 0. On a 0 it explodes, instantly vaporizing sly (it's a very powerful grenade). On a 1, it defuses the grenade and dispenses a $100 dollar note. Suppose that you throw it and observe that it doesn't explode:</p>\n<p style=\"padding-left: 30px;\"><em>(A) does Sly charge you with attempted murder, or does he thank you for giving him $100 in exchange for something that had no value to him anyway?</em></p>\n<p style=\"padding-left: 30px;\"><em>(B) if he thanks you for the free $100, does he ask for another one of those nice free hundred dollar note dispensers? (This is the \"quantum suicide\" option<br></em></p>\n<p style=\"padding-left: 30px;\"><em>(C) if he says \"the one you've already given me was great, but no more please\", then presumably if you throw another one against his will, he will thank you for the free $100 again. And so on ad infinitum. Sly is temporally inconsistent if this option is chosen.</em></p>\n<p>&nbsp;</p>\n<p>The punch line is that the physics we run on gives us a very strong reason to care about the welfare of copies of ourselves, which is (according to my survey) a counter-intuitive result.&nbsp;</p>\n<p>EDIT: Quite a few people are biting the quantum suicide bullet. I think I'll have to talk about that next. Also, <a href=\"/lw/2e0/mwi_copies_and_probability/273h\">Wei Dai summarizes</a>:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><em>Another way to think about this is that many of us seem to share the follow three intuitions about non-interacting extra copies, out of which we have to give up at least one to retain logical consistency:</em></span></p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em>We value extra copies in other quantum branches.</em></li>\n<li><em>We don't value extra copies that are just spatially separated from us (and are not too far away).</em></li>\n<li><em>We ought to value both kinds of copies the same way.</em></li>\n</ol> \n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding-left: 30px;\">\n<li><em>Giving up 1 is the position of \"quantum immortality\".</em></li>\n<li><em>Giving up 2 seems to be Roko's position in this post.</em></li>\n<li><em>Giving up 3 would imply that our values are rather arbitrary: there seems to be no morally relevant differences between these two kinds of copies, so why should we value one and not the other? But according to the \"complexity of value\" position, perhaps this isn't really a big problem.</em></li>\n</ul>\n<p>&nbsp;</p>\n<p>I might add a fourth option that many people in the comments seem to be going after: (4) We don't intrinsically value copies in other branches, we just have a subjective anticipation of becoming them.&nbsp;</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<hr style=\"padding-left: 30px;\">\n<p style=\"padding-left: 30px;\">1: The copying events are not discrete, rather they consist of a continuous deformation of probability amplitude in state space, but the shape of that deformation looks a lot like a continuous approximation to a discrete copying event, and the classical rules of physics approximately govern the time evolution of the \"copies\" as if they were completely independent. This last statement is the phenomenon of decoherence. The uncertainty in the copying rate is due to my ignorance, and I would welcome a physicist correcting me.</p>\n<p style=\"padding-left: 30px;\">2: There were many others who expressed roughly similar views, and I don't hold it as a \"black mark\" to pick the option that I am advising against, rather I encourage people to honestly put forward their opinions in a spirit of communal learning.</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>", "sections": [{"title": "MWI mixes copies and probability", "anchor": "MWI_mixes_copies_and_probability", "level": 1}, {"title": "LW doesn't care about copies", "anchor": "LW_doesn_t_care_about_copies", "level": 1}, {"title": "How to get away with attempted murder ", "anchor": "How_to_get_away_with_attempted_murder_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "162 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gYvuPhQZyzFqnxXMx", "hc9Eg6erp6hk9bWhn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-26T02:50:22.302Z", "modifiedAt": null, "url": null, "title": "Unknown knowns: Why did you choose to be monogamous?", "slug": "unknown-knowns-why-did-you-choose-to-be-monogamous", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:18.905Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "WrongBot", "createdAt": "2010-05-27T15:54:44.899Z", "isAdmin": false, "displayName": "WrongBot"}, "userId": "toqZ5PS8KdJ2mXzgN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hEeapgs2Y8tNyZkXD/unknown-knowns-why-did-you-choose-to-be-monogamous", "pageUrlRelative": "/posts/hEeapgs2Y8tNyZkXD/unknown-knowns-why-did-you-choose-to-be-monogamous", "linkUrl": "https://www.lesswrong.com/posts/hEeapgs2Y8tNyZkXD/unknown-knowns-why-did-you-choose-to-be-monogamous", "postedAtFormatted": "Saturday, June 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Unknown%20knowns%3A%20Why%20did%20you%20choose%20to%20be%20monogamous%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnknown%20knowns%3A%20Why%20did%20you%20choose%20to%20be%20monogamous%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhEeapgs2Y8tNyZkXD%2Funknown-knowns-why-did-you-choose-to-be-monogamous%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Unknown%20knowns%3A%20Why%20did%20you%20choose%20to%20be%20monogamous%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhEeapgs2Y8tNyZkXD%2Funknown-knowns-why-did-you-choose-to-be-monogamous", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhEeapgs2Y8tNyZkXD%2Funknown-knowns-why-did-you-choose-to-be-monogamous", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 829, "htmlBody": "<p>Many of us are familiar with Donald Rumsfeld's famous (and surprisingly useful) taxonomy of knowledge:</p>\n<blockquote>\n<p>There are known knowns. These are things we know that we know. There are known unknowns. That is to say, there are things that we now know we don&rsquo;t know. But there are also unknown unknowns. These are things we do not know we don&rsquo;t know.</p>\n</blockquote>\n<p>But this taxonomy (as originally described) omits an important fourth category: <strong>unknown knowns</strong>, the things we don't know that we know. This category encompasses the knowledge of many of our own personal beliefs, what I call unquestioned defaults. For example, most modern Americans possess the unquestioned default belief that they have some moral responsibility for their own freely-chosen actions. In the twelfth century, most Europeans possessed the unquestioned default belief that the Christian god existed. And so on. These unknown knowns are largely the products of a particular culture; they require homogeneity of belief to remain unknown.</p>\n<p>By definition, we are each completely ignorant of our own unknown knowns. So even when our culture gives us a fairly accurate map of the territory, we'll never notice the <a href=\"http://en.wikipedia.org/wiki/Mercator_projection\">Mercator projection</a>'s effect. Unless it's pointed out to us or we find contradictory evidence, that is. A single observation can be all it takes, if you're paying attention and asking questions. The answers might not change your mind, but you'll still come out of the process with more knowledge than you went in with.</p>\n<p><a id=\"more\"></a></p>\n<p>When I was eighteen I went on a date with a girl I'll call Emma, who conscientiously informed me that she already had two boyfriends: she was, she said, <a href=\"http://en.wikipedia.org/wiki/Polyamory\">polyamorous</a>. I had previously had some vague awareness that there had been a free love movement in the sixties that encouraged \"alternative lifestyles\", but that awareness was not a sufficient motivation for me to challenge my default belief that romantic relationships could only be conducted one at a time. Acknowledging default settings is not easy.</p>\n<p>The chance to date a pretty girl, though, can be sufficient motivation for a great many things (as is also the case with pretty boys). It was certainly a good enough reason to ask myself, \"Self, what's so great about this monogamy thing?\"</p>\n<p>I couldn't come up with any particularly compelling answers, so I called Emma up and we planned a second date.</p>\n<p>Since that fateful day, I've been involved in both polyamorous and monogamous relationships, and I've become quite confident that I am happier, more fulfilled, and a better romantic partner when I am polyamorous. This holds even when I'm dating only one person; polyamorous relationships have a kind of freedom to them that is impossible to obtain any other way, as well as a set of similarly unique responsibilities.</p>\n<p>In this discussion I am targeting monogamy because its discovery has had an effect on my life that is orders of magnitude greater than that of any other previously-unknown known. Others I've spoken with have had similar experiences. If you haven't had it before, you now have the same opportunity that I lucked into several years ago, if you choose to exploit it.</p>\n<p>This, then, is your exercise: spend five minutes thinking about why your choice of monogamy is preferable to all of the other inhabitants of relationship-style-space, <em>for you</em>. Other options that have been explored and documented include:</p>\n<ul>\n<li>Non-consensual non-monogamy, the most popular alternative.</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Swinging\">Swinging</a>, in which couples engage in social, recreational sex, mostly with other couples.</li>\n<li>Polyamory, the practice, desire, or acceptance of having more than one intimate relationship at a time with the knowledge and consent of everyone involved.&nbsp;This category is <em>extremely</em>&nbsp;broad, but the most common variations include: \n<ul>\n<li>Polyfidelity, in which &gt;2 people form a single committed relationship that does not allow outside partners.</li>\n<li>Hierarchical polyamory, in which each individual has (usually) one primary partner and some number of secondary partners. These labels typically reflect the level of commitment involved, and are not a ranking of preference.</li>\n<li>\"Intimate networks\", in which each person maintains some number of independent relationships without explicit rankings or descriptions, such that a graph (the data structure) is the best way to describe all the individuals and relationships involved.</li>\n</ul>\n</li>\n</ul>\n<p>These types of polyamory cover many of the available options, but there are others; some are as yet unknown. Some relationship styles are better than others, subject to your ethics, history, and personality. I suspect that monogamy is genuinely the best option for many people, perhaps even most. But it's impossible for you to know that until you know that you have a choice.</p>\n<p>If you have a particularly compelling argument for or against a particular relationship style, please share it. But if romantic jealousy is your deciding factor in favor of monogamy, you may want to hold off on <a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think/\">forming a belief that will be hard to change</a>; my next post will be about techniques for managing and reducing romantic jealousy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 1, "Zwv9eHi7KGg5KA9oM": 1, "AWz5ryH8SpAgTeydh": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hEeapgs2Y8tNyZkXD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 68, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "3110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 68, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 669, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["buixYfcXBah9hbSNZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-29T00:46:12.048Z", "modifiedAt": null, "url": null, "title": "Book Club Update, Chapter 2 of Probability Theory", "slug": "book-club-update-chapter-2-of-probability-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pvj3F37YAs443bbrH/book-club-update-chapter-2-of-probability-theory", "pageUrlRelative": "/posts/Pvj3F37YAs443bbrH/book-club-update-chapter-2-of-probability-theory", "linkUrl": "https://www.lesswrong.com/posts/Pvj3F37YAs443bbrH/book-club-update-chapter-2-of-probability-theory", "postedAtFormatted": "Tuesday, June 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Club%20Update%2C%20Chapter%202%20of%20Probability%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Club%20Update%2C%20Chapter%202%20of%20Probability%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPvj3F37YAs443bbrH%2Fbook-club-update-chapter-2-of-probability-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Club%20Update%2C%20Chapter%202%20of%20Probability%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPvj3F37YAs443bbrH%2Fbook-club-update-chapter-2-of-probability-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPvj3F37YAs443bbrH%2Fbook-club-update-chapter-2-of-probability-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 816, "htmlBody": "<p><strong>Previously</strong>: <a href=\"/lw/2br/less_wrong_book_club_and_study_group/\">Book Club introductory post</a> - <a href=\"/lw/2cq/book_club_update_and_chapter_1/\">First update and Chapter 1 summary</a></p>\n<p>Discussion on chapter 1 has wound down, we move on to Chapter 2 (I have updated the previous post with a summary of chapter 1 with links to the discussion as appropriate). But first, a few announcements.</p>\n<h2>How to participate</h2>\n<p>This is both for people who have previously registered interest, as well as newcomers. <a href=\"http://spreadsheets.google.com/ccc?key=0Au9LcqMYAIwldFViMEVza0hibmoxQ3FaeVBPRmFweWc&amp;hl=en\">This spreadsheet</a> is our best attempt at coordinating 80+ Less Wrong readers interested in participating in \"earnest study of the great literature in our area of interest\".</p>\n<p><strong>If you are still participating</strong>, please let the group know - all you have to do is fill in the \"Active (Chapter)\" column. Write in an \"X\" if you are checked out, or the number of the chapter you are currently reading. This will let us measure attrition, as well as adapt the pace if necessary. <strong>If you would like to join</strong>, please add yourself to the spreadsheet. <strong>If you would like to participate in live chat</strong> about the material, please indicate your time zone and preferred meeting time. As always, your feedback on the process itself is more than welcome.</p>\n<p>Refer to the <a href=\"/lw/2cq/book_club_update_and_chapter_1/\">previous post</a> for more details on how to participate and meeting schedules.</p>\n<h2>Chapter 2: The Quantitative Rules</h2>\n<p>In this chapter Jaynes carefully introduces and justifies the elementary laws of plausibility, from which all later results are derived.</p>\n<p>(Disclosure: I wasn't able to follow all the math in this chapter but I didn't let it deter me; the applications in later chapters are more accessible. We'll take things slow, and draw on such expertise as has been <a href=\"/lw/2br/less_wrong_book_club_and_study_group/24pj\">offered</a> by more advanced members of the group. At worst this chapter can be enjoyed on a purely <a href=\"http://www.hulver.com/scoop/story/2008/10/20/175328/72\">literary</a> basis.)</p>\n<p><strong>Sections: The Product Rule - The Sum Rule. Exercises: 2.1 and 2.2</strong></p>\n<p>Chapter 2 works out the consequences of the qualitative desiderata introduced at the end of Chapter 1.</p>\n<p>The first step is to consider the evaluation of the plausibility (AB|C), from the possibly relevant inputs: (B|C), (A|C), (A|BC) and (B|AC). Considerations of symmetry and the desideratum of consistency lead to a <strong>functional equation</strong> known as the \"associativity equation\": F(F(x,z),z)=F(x,F(y,z)), characterizing the the function F such that (AB|C)=F[(B|C),(A|BC)]. The derivation that follows requires <a href=\"/lw/2ek/book_club_update_chapter_2_of_probability_theory/27or\">some calculus</a>, and shows by differentiating then integrating back the form of the product rule:</p>\n<p>w(AB|C)=w(A|BC)w(B|C)=w(B|AC)w(A|C)</p>\n<p>Having obtained this, the next step is to establish how (A|B) is related to (not-A|B). The functional equation in this case is</p>\n<p>x*S(S(y)/x)=y*S(S(x)/y)</p>\n<p>and the derivation, after some <a href=\"/lw/2ek/book_club_update_chapter_2_of_probability_theory/281u\">more calculus</a>, leads to S(x)=(1-x^m)^(1/m). But the value of m is irrelevant, and so we end up with the two following rules:</p>\n<p>p(AB|C)=p(A|BC)p(B|C)=p(B|AC)p(A|C)</p>\n<p>p(not-A|B)+p(A|B)=1</p>\n<p>The <a href=\"/lw/2ek/book_club_update_chapter_2_of_probability_theory/28ne\">exercises</a> provide a first opportunity to explore how these two rules yield a great many other ways of assessing probabilities of more complex propositions, for instance p(C|A+B), based on the elementary probabilities.</p>\n<p><strong>Sections: Qualitative Properties - Numerical Values - Notation and Finite Sets Policy - Comments. Exercises: 2.3</strong></p>\n<p>Jaynes next turns back to the relation between \"plausible reasoning\" and deductive logic, showing the latter as a limiting case of the former. The weaker syllogisms shown in Chapter 1 correspond to inequalities that can be derived from the product rule, and the direction of these inequalities start to point to likelihood ratios.</p>\n<p>The product and sum rules allow us to consider the particular case when we have a finite set of mutually exclusive and exhaustive propositions, and background information which is symmetrical about each such proposition: it says the same about any one of them that it says about any other. Considering two such situations, where the propositions are the same but the <em>labels</em> we give them are different, Jaynes shows that, given our starting desiderata, we cannot do other than to assign the same probabilities to propositions which we are unable to distinguish otherwise than by their labels.</p>\n<p>This is the <strong>principle of indifference</strong>; its significance is that even though what we have derived so far is an infinity of functions p(x) generated by the parameter <em>m</em>, the desiderata entirely \"pin down\" the numerical values in this particular situation.</p>\n<p>So far in this chapter we had been using p(x) as a function relating the plausibilities of propositions, such that p(x) was an arbitrary monotonic function of the plausibility x. At this point Jaynes suggests that we \"turn this around\" and say that <em>x</em> is a function of <em>p</em>. These values of p, <em>probabilities,</em> become the primary mathematical objects, while the plausibilities \"have faded entirely out of the picture. We will just have no further use for them\".</p>\n<p>The principle of indifference now allows us to start computing numerical values for \"urn probabilities\", which will be the main topic of the next chapter.</p>\n<p>Exercise 2.3 is notable for providing a formal treatment of the <a href=\"/lw/ji/conjunction_fallacy/\">conjunction fallacy</a>.</p>\n<p>Chapter 2 ends with a cautionary note on the topic of justifying results on infinite sets only based on a \"well-behaved\" process of passing to the limit of a series of finite cases. The Comments section addresses the <a href=\"/lw/2ek/book_club_update_chapter_2_of_probability_theory/29ix\">\"subjective\" vs \"objective\"</a> distinction.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pvj3F37YAs443bbrH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 6.002086846515774e-07, "legacy": true, "legacyId": "3116", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Previously</strong>: <a href=\"/lw/2br/less_wrong_book_club_and_study_group/\">Book Club introductory post</a> - <a href=\"/lw/2cq/book_club_update_and_chapter_1/\">First update and Chapter 1 summary</a></p>\n<p>Discussion on chapter 1 has wound down, we move on to Chapter 2 (I have updated the previous post with a summary of chapter 1 with links to the discussion as appropriate). But first, a few announcements.</p>\n<h2 id=\"How_to_participate\">How to participate</h2>\n<p>This is both for people who have previously registered interest, as well as newcomers. <a href=\"http://spreadsheets.google.com/ccc?key=0Au9LcqMYAIwldFViMEVza0hibmoxQ3FaeVBPRmFweWc&amp;hl=en\">This spreadsheet</a> is our best attempt at coordinating 80+ Less Wrong readers interested in participating in \"earnest study of the great literature in our area of interest\".</p>\n<p><strong>If you are still participating</strong>, please let the group know - all you have to do is fill in the \"Active (Chapter)\" column. Write in an \"X\" if you are checked out, or the number of the chapter you are currently reading. This will let us measure attrition, as well as adapt the pace if necessary. <strong>If you would like to join</strong>, please add yourself to the spreadsheet. <strong>If you would like to participate in live chat</strong> about the material, please indicate your time zone and preferred meeting time. As always, your feedback on the process itself is more than welcome.</p>\n<p>Refer to the <a href=\"/lw/2cq/book_club_update_and_chapter_1/\">previous post</a> for more details on how to participate and meeting schedules.</p>\n<h2 id=\"Chapter_2__The_Quantitative_Rules\">Chapter 2: The Quantitative Rules</h2>\n<p>In this chapter Jaynes carefully introduces and justifies the elementary laws of plausibility, from which all later results are derived.</p>\n<p>(Disclosure: I wasn't able to follow all the math in this chapter but I didn't let it deter me; the applications in later chapters are more accessible. We'll take things slow, and draw on such expertise as has been <a href=\"/lw/2br/less_wrong_book_club_and_study_group/24pj\">offered</a> by more advanced members of the group. At worst this chapter can be enjoyed on a purely <a href=\"http://www.hulver.com/scoop/story/2008/10/20/175328/72\">literary</a> basis.)</p>\n<p><strong id=\"Sections__The_Product_Rule___The_Sum_Rule__Exercises__2_1_and_2_2\">Sections: The Product Rule - The Sum Rule. Exercises: 2.1 and 2.2</strong></p>\n<p>Chapter 2 works out the consequences of the qualitative desiderata introduced at the end of Chapter 1.</p>\n<p>The first step is to consider the evaluation of the plausibility (AB|C), from the possibly relevant inputs: (B|C), (A|C), (A|BC) and (B|AC). Considerations of symmetry and the desideratum of consistency lead to a <strong>functional equation</strong> known as the \"associativity equation\": F(F(x,z),z)=F(x,F(y,z)), characterizing the the function F such that (AB|C)=F[(B|C),(A|BC)]. The derivation that follows requires <a href=\"/lw/2ek/book_club_update_chapter_2_of_probability_theory/27or\">some calculus</a>, and shows by differentiating then integrating back the form of the product rule:</p>\n<p>w(AB|C)=w(A|BC)w(B|C)=w(B|AC)w(A|C)</p>\n<p>Having obtained this, the next step is to establish how (A|B) is related to (not-A|B). The functional equation in this case is</p>\n<p>x*S(S(y)/x)=y*S(S(x)/y)</p>\n<p>and the derivation, after some <a href=\"/lw/2ek/book_club_update_chapter_2_of_probability_theory/281u\">more calculus</a>, leads to S(x)=(1-x^m)^(1/m). But the value of m is irrelevant, and so we end up with the two following rules:</p>\n<p>p(AB|C)=p(A|BC)p(B|C)=p(B|AC)p(A|C)</p>\n<p>p(not-A|B)+p(A|B)=1</p>\n<p>The <a href=\"/lw/2ek/book_club_update_chapter_2_of_probability_theory/28ne\">exercises</a> provide a first opportunity to explore how these two rules yield a great many other ways of assessing probabilities of more complex propositions, for instance p(C|A+B), based on the elementary probabilities.</p>\n<p><strong id=\"Sections__Qualitative_Properties___Numerical_Values___Notation_and_Finite_Sets_Policy___Comments__Exercises__2_3\">Sections: Qualitative Properties - Numerical Values - Notation and Finite Sets Policy - Comments. Exercises: 2.3</strong></p>\n<p>Jaynes next turns back to the relation between \"plausible reasoning\" and deductive logic, showing the latter as a limiting case of the former. The weaker syllogisms shown in Chapter 1 correspond to inequalities that can be derived from the product rule, and the direction of these inequalities start to point to likelihood ratios.</p>\n<p>The product and sum rules allow us to consider the particular case when we have a finite set of mutually exclusive and exhaustive propositions, and background information which is symmetrical about each such proposition: it says the same about any one of them that it says about any other. Considering two such situations, where the propositions are the same but the <em>labels</em> we give them are different, Jaynes shows that, given our starting desiderata, we cannot do other than to assign the same probabilities to propositions which we are unable to distinguish otherwise than by their labels.</p>\n<p>This is the <strong>principle of indifference</strong>; its significance is that even though what we have derived so far is an infinity of functions p(x) generated by the parameter <em>m</em>, the desiderata entirely \"pin down\" the numerical values in this particular situation.</p>\n<p>So far in this chapter we had been using p(x) as a function relating the plausibilities of propositions, such that p(x) was an arbitrary monotonic function of the plausibility x. At this point Jaynes suggests that we \"turn this around\" and say that <em>x</em> is a function of <em>p</em>. These values of p, <em>probabilities,</em> become the primary mathematical objects, while the plausibilities \"have faded entirely out of the picture. We will just have no further use for them\".</p>\n<p>The principle of indifference now allows us to start computing numerical values for \"urn probabilities\", which will be the main topic of the next chapter.</p>\n<p>Exercise 2.3 is notable for providing a formal treatment of the <a href=\"/lw/ji/conjunction_fallacy/\">conjunction fallacy</a>.</p>\n<p>Chapter 2 ends with a cautionary note on the topic of justifying results on infinite sets only based on a \"well-behaved\" process of passing to the limit of a series of finite cases. The Comments section addresses the <a href=\"/lw/2ek/book_club_update_chapter_2_of_probability_theory/29ix\">\"subjective\" vs \"objective\"</a> distinction.</p>", "sections": [{"title": "How to participate", "anchor": "How_to_participate", "level": 1}, {"title": "Chapter 2: The Quantitative Rules", "anchor": "Chapter_2__The_Quantitative_Rules", "level": 1}, {"title": "Sections: The Product Rule - The Sum Rule. Exercises: 2.1 and 2.2", "anchor": "Sections__The_Product_Rule___The_Sum_Rule__Exercises__2_1_and_2_2", "level": 2}, {"title": "Sections: Qualitative Properties - Numerical Values - Notation and Finite Sets Policy - Comments. Exercises: 2.3", "anchor": "Sections__Qualitative_Properties___Numerical_Values___Notation_and_Finite_Sets_Policy___Comments__Exercises__2_3", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "42 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tSDaxEq2WGHTRPcWB", "aKkgRiGhLvKxmEPvi", "QAK43nNCTQQycAcYe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-29T23:47:39.284Z", "modifiedAt": null, "url": null, "title": "A Challenge for LessWrong", "slug": "a-challenge-for-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:36.776Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "simplicio", "createdAt": "2010-03-06T04:03:43.272Z", "isAdmin": false, "displayName": "simplicio"}, "userId": "fDQ7ty7YmE3PxgqNh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uP8NuRLEsGsg9sfRR/a-challenge-for-lesswrong", "pageUrlRelative": "/posts/uP8NuRLEsGsg9sfRR/a-challenge-for-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/uP8NuRLEsGsg9sfRR/a-challenge-for-lesswrong", "postedAtFormatted": "Tuesday, June 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Challenge%20for%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Challenge%20for%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuP8NuRLEsGsg9sfRR%2Fa-challenge-for-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Challenge%20for%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuP8NuRLEsGsg9sfRR%2Fa-challenge-for-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuP8NuRLEsGsg9sfRR%2Fa-challenge-for-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 786, "htmlBody": "<p>The user divia, in her most excellent&nbsp;<a href=\"/lw/2e6/spaced_repetition_database_for_the_mysterious/\">post</a> on spaced repetition software, quotes <a href=\"http://paulbuchheit.blogspot.com/2007/04/perfect-is-enemy-of-good-enough-and.html\">Paul Buchheit</a> as saying</p>\n<blockquote>\n<p>\"Good enough\" is the enemy of \"At all\"!</p>\n</blockquote>\n<p>This is an important truth which bears repetition, and to which I shall return.</p>\n<h4>\"Rationalists should win\"</h4>\n<p><a href=\"/lw/9p/extreme_rationality_its_not_that_great/\">Many</a> <a href=\"/lw/9t/extreme_rationality_it_could_be_great\">hands</a> <a href=\"/lw/6t/the_benefits_of_rationality/\">have</a> <a href=\"/lw/j/the_costs_of_rationality/\">been</a> <a href=\"/lw/296/the_tragedy_of_the_social_epistemology_commons/\">wrung</a> hereabouts on the subject of rationality's instrumental value (or lack thereof) in the everyday lives of LWers. <em>Are we winning? </em>Some consider this doubtful.<sup>1</sup></p>\n<p>Now, I have a couple of issues with the question being framed in such a way.</p>\n<ul>\n<li>Benefits of rationality are often negative benefits - in the sense that they will involve <em>not being stupid</em>&nbsp;as opposed to <em>being especially smart.</em>&nbsp;But \"Why I didn't take on that crippling mortgage\" doesn't make a very good post.</li>\n<li>Weapons-grade rationality &agrave;&nbsp;la&nbsp;LessWrong&nbsp;is a <em>refinement</em>&nbsp;to the reactor-grade rationality practiced by self-described skeptics - for most cases, it is not a quantum leap forward. The general skeptic community is already winning in certain senses (e.g., a non-religious outlook correlates strongly with income and level of education), although causal direction is hard to determine.</li>\n<li>Truth-seeking is ethical for its own sake.</li>\n<li>I, for one, am having a hell of a good time! I count that as a win.</li>\n</ul>\n<div>However, these relatively minor considerations aside, surely we can do better. <em>In order to win, rationalists should play.</em></div>\n<h4>Nonrandom acts of rationality</h4>\n<p>The LessWrong community finds itself in the fairly privileged position of being (1) mostly financially well-off; (2) well-educated and articulate; (3) connected; (4) of non-trivial size. Therefore, I would like to suggest a project for any &amp; all users who might be interested.<a id=\"more\"></a></p>\n<p><em>Let us become a solution in search of problems.</em></p>\n<p>Perform one or more &nbsp;<strong>manageable &amp; modest &nbsp;</strong> rationally &amp; ethically motivated actions between now and July 31, 2010 (indicate intent to participate, and brainstorm, below). These actions must have a reasonable chance of being an unequivocal net positive for the shared values of this community. Finally, post what you have done to this thread's comments, in as much detail as possible.</p>\n<p>Some examples:</p>\n<ul>\n<li>Write a letter on behalf of&nbsp;<a href=\"http://www.amnesty.org/en/activism-center\">Amnesty International</a>&nbsp;in support of their anti-torture campaigns.</li>\n<li>Make an appointment to give blood.</li>\n<li>Contact and harangue one of your elected representatives. For example, I may write to my Minister of Health about the excellent harm-reduction work being done in Vancouver by &nbsp;<a href=\"http://supervisedinjection.vch.ca/\">Insite</a>, a safe-injection site for IV drug users whose efficacy in decreasing public drug use and successfully referring patients to detox&nbsp;has been confirmed in published articles in the &nbsp;<em>Lancet</em>&nbsp;and &nbsp;<em>New England Journal of Medicine.</em>&nbsp;(Insite is controversial, with people like the previous minister opposing it for purely ideological reasons. Politics is the people-killer.)</li>\n<li>Donate a one-time amount somewhere around 10% of your weekly disposable income to a reputable charity - I may go with &nbsp;<a href=\"http://en.wikipedia.org/wiki/Spread_the_Net\">Spread the Net</a>&nbsp;- or to an organization promoting our values in your own area (e.g., the &nbsp;<a href=\"http://ncse.com/\">NCSE</a>, or indeed the &nbsp;<a href=\"http://intelligence.org/\">SIAI</a>).</li>\n<li><a href=\"http://www.amandadefensefund.org/Ways_To_Help.html\">Give your Air Miles</a>&nbsp;&nbsp;to the Amanda Knox Defense Fund.</li>\n</ul>\n<div>(<strong>ETA:</strong>&nbsp;Some have suggested that these examples are terrible - a case of feel-good vs. effective actions. If you know more effective actions, for goodness' sake, please post them in the comments!)</div>\n<div>Remember, \"Good enough\" is the enemy of \"At all\"! Even if your action is just to click on&nbsp;<a href=\"http://www.thehungersite.com/clickToGive/home.faces?siteId=1&amp;link=ctg_ths_home_from_lit_thankyou_sitenav\">The Hunger Site</a>&nbsp;and all its affilliates every morning while you sip your rooibos, that's better than nothing and is unequivocally praiseworthy - post it.&nbsp;Your action does not have to be clever, just helpful.</div>\n<div>Bonus points for investigating the efficacy and cost-effectiveness of what you do (the suggestions above are not&nbsp;vetted with respect to this question).</div>\n<div>Also, if you already do something along these lines, please post it in the comments.</div>\n<h2>What about LessWrong acting as a group?</h2>\n<p>I would &nbsp;<strong>love</strong>&nbsp;to see a group-level action on our part occur; however, after some time spent brainstorming, I haven't hit upon any really salient ones that are not amenable to individual action. Perhaps a concerted letter-writing campaign? I suspect that is a weak idea, and that there are much better ones out there. Who's up for world-optimization?</p>\n<h2>Potential objection</h2>\n<blockquote>\n<p><span style=\"font-weight: normal; font-size: small;\">These actions are mostly sub-optimal, consequentially speaking. The SIAI/[insert favourite cause here] is a better idea for a donation, since it promises to solve all the above problems in one go. These are just band-aids.</span></p>\n</blockquote>\n<div>This may or may not be true; however, I am mostly asking people (myself included) to do this exercise&nbsp;<em>instead of nothing or very little</em>. If you already give to SIAI or something else that might save the world, and no disposable income or time is left over, then ignore this post (although I would be very interested to know the scope of your involvement in the comments).</div>\n<div><br /></div>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<div><sup>1</sup> Although by George, we have Newcomb's problem licked!</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uP8NuRLEsGsg9sfRR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 21, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "3118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The user divia, in her most excellent&nbsp;<a href=\"/lw/2e6/spaced_repetition_database_for_the_mysterious/\">post</a> on spaced repetition software, quotes <a href=\"http://paulbuchheit.blogspot.com/2007/04/perfect-is-enemy-of-good-enough-and.html\">Paul Buchheit</a> as saying</p>\n<blockquote>\n<p>\"Good enough\" is the enemy of \"At all\"!</p>\n</blockquote>\n<p>This is an important truth which bears repetition, and to which I shall return.</p>\n<h4 id=\"_Rationalists_should_win_\">\"Rationalists should win\"</h4>\n<p><a href=\"/lw/9p/extreme_rationality_its_not_that_great/\">Many</a> <a href=\"/lw/9t/extreme_rationality_it_could_be_great\">hands</a> <a href=\"/lw/6t/the_benefits_of_rationality/\">have</a> <a href=\"/lw/j/the_costs_of_rationality/\">been</a> <a href=\"/lw/296/the_tragedy_of_the_social_epistemology_commons/\">wrung</a> hereabouts on the subject of rationality's instrumental value (or lack thereof) in the everyday lives of LWers. <em>Are we winning? </em>Some consider this doubtful.<sup>1</sup></p>\n<p>Now, I have a couple of issues with the question being framed in such a way.</p>\n<ul>\n<li>Benefits of rationality are often negative benefits - in the sense that they will involve <em>not being stupid</em>&nbsp;as opposed to <em>being especially smart.</em>&nbsp;But \"Why I didn't take on that crippling mortgage\" doesn't make a very good post.</li>\n<li>Weapons-grade rationality \u00e0&nbsp;la&nbsp;LessWrong&nbsp;is a <em>refinement</em>&nbsp;to the reactor-grade rationality practiced by self-described skeptics - for most cases, it is not a quantum leap forward. The general skeptic community is already winning in certain senses (e.g., a non-religious outlook correlates strongly with income and level of education), although causal direction is hard to determine.</li>\n<li>Truth-seeking is ethical for its own sake.</li>\n<li>I, for one, am having a hell of a good time! I count that as a win.</li>\n</ul>\n<div>However, these relatively minor considerations aside, surely we can do better. <em>In order to win, rationalists should play.</em></div>\n<h4 id=\"Nonrandom_acts_of_rationality\">Nonrandom acts of rationality</h4>\n<p>The LessWrong community finds itself in the fairly privileged position of being (1) mostly financially well-off; (2) well-educated and articulate; (3) connected; (4) of non-trivial size. Therefore, I would like to suggest a project for any &amp; all users who might be interested.<a id=\"more\"></a></p>\n<p><em>Let us become a solution in search of problems.</em></p>\n<p>Perform one or more &nbsp;<strong>manageable &amp; modest &nbsp;</strong> rationally &amp; ethically motivated actions between now and July 31, 2010 (indicate intent to participate, and brainstorm, below). These actions must have a reasonable chance of being an unequivocal net positive for the shared values of this community. Finally, post what you have done to this thread's comments, in as much detail as possible.</p>\n<p>Some examples:</p>\n<ul>\n<li>Write a letter on behalf of&nbsp;<a href=\"http://www.amnesty.org/en/activism-center\">Amnesty International</a>&nbsp;in support of their anti-torture campaigns.</li>\n<li>Make an appointment to give blood.</li>\n<li>Contact and harangue one of your elected representatives. For example, I may write to my Minister of Health about the excellent harm-reduction work being done in Vancouver by &nbsp;<a href=\"http://supervisedinjection.vch.ca/\">Insite</a>, a safe-injection site for IV drug users whose efficacy in decreasing public drug use and successfully referring patients to detox&nbsp;has been confirmed in published articles in the &nbsp;<em>Lancet</em>&nbsp;and &nbsp;<em>New England Journal of Medicine.</em>&nbsp;(Insite is controversial, with people like the previous minister opposing it for purely ideological reasons. Politics is the people-killer.)</li>\n<li>Donate a one-time amount somewhere around 10% of your weekly disposable income to a reputable charity - I may go with &nbsp;<a href=\"http://en.wikipedia.org/wiki/Spread_the_Net\">Spread the Net</a>&nbsp;- or to an organization promoting our values in your own area (e.g., the &nbsp;<a href=\"http://ncse.com/\">NCSE</a>, or indeed the &nbsp;<a href=\"http://intelligence.org/\">SIAI</a>).</li>\n<li><a href=\"http://www.amandadefensefund.org/Ways_To_Help.html\">Give your Air Miles</a>&nbsp;&nbsp;to the Amanda Knox Defense Fund.</li>\n</ul>\n<div>(<strong>ETA:</strong>&nbsp;Some have suggested that these examples are terrible - a case of feel-good vs. effective actions. If you know more effective actions, for goodness' sake, please post them in the comments!)</div>\n<div>Remember, \"Good enough\" is the enemy of \"At all\"! Even if your action is just to click on&nbsp;<a href=\"http://www.thehungersite.com/clickToGive/home.faces?siteId=1&amp;link=ctg_ths_home_from_lit_thankyou_sitenav\">The Hunger Site</a>&nbsp;and all its affilliates every morning while you sip your rooibos, that's better than nothing and is unequivocally praiseworthy - post it.&nbsp;Your action does not have to be clever, just helpful.</div>\n<div>Bonus points for investigating the efficacy and cost-effectiveness of what you do (the suggestions above are not&nbsp;vetted with respect to this question).</div>\n<div>Also, if you already do something along these lines, please post it in the comments.</div>\n<h2 id=\"What_about_LessWrong_acting_as_a_group_\">What about LessWrong acting as a group?</h2>\n<p>I would &nbsp;<strong>love</strong>&nbsp;to see a group-level action on our part occur; however, after some time spent brainstorming, I haven't hit upon any really salient ones that are not amenable to individual action. Perhaps a concerted letter-writing campaign? I suspect that is a weak idea, and that there are much better ones out there. Who's up for world-optimization?</p>\n<h2 id=\"Potential_objection\">Potential objection</h2>\n<blockquote>\n<p><span style=\"font-weight: normal; font-size: small;\">These actions are mostly sub-optimal, consequentially speaking. The SIAI/[insert favourite cause here] is a better idea for a donation, since it promises to solve all the above problems in one go. These are just band-aids.</span></p>\n</blockquote>\n<div>This may or may not be true; however, I am mostly asking people (myself included) to do this exercise&nbsp;<em>instead of nothing or very little</em>. If you already give to SIAI or something else that might save the world, and no disposable income or time is left over, then ignore this post (although I would be very interested to know the scope of your involvement in the comments).</div>\n<div><br></div>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<div><sup>1</sup> Although by George, we have Newcomb's problem licked!</div>", "sections": [{"title": "\"Rationalists should win\"", "anchor": "_Rationalists_should_win_", "level": 2}, {"title": "Nonrandom acts of rationality", "anchor": "Nonrandom_acts_of_rationality", "level": 2}, {"title": "What about LessWrong acting as a group?", "anchor": "What_about_LessWrong_acting_as_a_group_", "level": 1}, {"title": "Potential objection", "anchor": "Potential_objection", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "172 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 172, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iRfKKYhZAG8fWDjJr", "LgavAYtzFQZKg95WC", "B3b29FJboqnANJRDz", "DXBbQBHACYwAdKKyx", "9Z3pezjiWLfNANg9P", "YgCi9vBphbG7hmnb5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-06-30T17:21:53.836Z", "modifiedAt": null, "url": null, "title": "Applied Bayes' Theorem: Reading People", "slug": "applied-bayes-theorem-reading-people", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:05.760Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KL9iocwykHq53Esrv/applied-bayes-theorem-reading-people", "pageUrlRelative": "/posts/KL9iocwykHq53Esrv/applied-bayes-theorem-reading-people", "linkUrl": "https://www.lesswrong.com/posts/KL9iocwykHq53Esrv/applied-bayes-theorem-reading-people", "postedAtFormatted": "Wednesday, June 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applied%20Bayes'%20Theorem%3A%20Reading%20People&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplied%20Bayes'%20Theorem%3A%20Reading%20People%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKL9iocwykHq53Esrv%2Fapplied-bayes-theorem-reading-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applied%20Bayes'%20Theorem%3A%20Reading%20People%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKL9iocwykHq53Esrv%2Fapplied-bayes-theorem-reading-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKL9iocwykHq53Esrv%2Fapplied-bayes-theorem-reading-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2458, "htmlBody": "<p>Or, how to recognize <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' theorem</a> when you meet one making small talk at a cocktail party.</p>\n<p>Knowing the theory of rationality is good, but it is of little use unless we know how to apply it. Unfortunately, humans tend to be poor at applying raw theory, instead needing several examples before it becomes instinctive. I found some very useful examples in the book <a href=\"http://www.amazon.com/Reading-People-Understand-Behavior-Anytime/dp/0345425871\"><em>Reading People: How to Understand People and Predict Their Behavior - Anytime, Anyplace</em></a>. While I didn't think that it communicated the skill of actually reading people very well, I did notice that it did have one chapter (titled \"Discovering Patterns: Learning to See the Forest, Not Just the Trees\") that could almost have been a collection of Less Wrong posts. It also serves as an excellent example of applying Bayes' theorem in every-day life.<br /><br />In \"<a href=\"/lw/1to/what_is_bayesianism/\">What is Bayesianism?</a>\" I said that the first core tenet of Bayesianism is \"Any given observation has many different possible causes\". <em>Reading People</em> says:<br /><em></em></p>\n<blockquote>\n<p><em>If this book could deliver but one message, it would be that to read people effectively you must gather enough information about them to establish a consistent pattern.</em> Without that pattern, your conclusions will be about as reliable as a tarot card reading.</p>\n</blockquote>\n<p>In fact, the author is saying that Bayes' theorem applies when you're trying to read people (if this is not immediately obvious, just keep reading). Any particular piece of evidence about a person could have various causes. For example, in a later chapter we are offered a list of possible reasons for why someone may have dressed inappropriately for an occasion. They might (1) be seeking attention, (2) lack common sense, (3) be self-centered and insensitive to others, (4) be trying to show that they are spontaneous, rebellious, or noncomformists and don't care what other people think, (5) not have been taught how to dress and act appropriately, (6) be trying to imitate someone they admire, (7) value comfort and convenience over all else, or (8) simply not have the right attire for the occasion.<br /><br />Similarly, very short hair on a man might indicate that he (1) is in the military, or was at some point in his life, (2) works for an organization that demands very short hair, such as a police force or fire department, (3) is trendy, artistic or rebellious, (4) is conservative, (5) is undergoing or recovering from a medical treatment, (6) thinks he looks better with short hair, (7) plays sports, or (8) keeps his hair short for practical reasons.<br /><br />So much for reading people being easy. This, again, is the essence of Bayes' theorem: even though somebody being in the military might almost certainly mean that they'd have short hair, them having a short hair does not necessarily mean that they are in the military. On the other hand, if someone has short hair, is clearly knowledgeable about weapons and tactics, displays a no-nonsense attitude, is in good shape, and has a very Spartan home... well, though it's still not for certain, it seems likely to me that of all the people having all of these attributes, quite a few of them are in the military or in similar occupations.<br /><a id=\"more\"></a><br />The book offers a seven-step guide for finding patterns in people. I'll go through them one at a time, pointing out what they say in Bayesian and heuristic/bias terms. Note that this is not a definitive list: if you can come up with more Bayesian angles to the book, post them in the comments.<br /><br /><strong>1. Start with the person's most striking traits, and as you gather more information see if his other traits are consistent or inconsistent.<br /></strong></p>\n<p>As computationally bounded agents, we can't simply take in all the available data at once: we have to start off some particularly striking traits and start building a picture from there. However, humans are notorious about anchoring too much (<a href=\"/lw/j7/anchoring_and_adjustment/\">Anchoring and Adjustment</a>), so we are reminded <em>to actively seek disconfirmation </em>to any initial theory we have.</p>\n<blockquote>\n<p>I constantly test additional information agaisnt my first impression, always watching for patterns to develop. Each piece of the puzzle - a person's appearance, her tone of voice, hygiene and so on - may validate my first impression, disprove it, or have little impact on it. If most of the new information points in a different direction than my first impression did, I revise that impression. Then I consider whether my revised impression holds up as even more clues are revealed - and revise it again, if need be.</p>\n</blockquote>\n<p>Here, the author is keeping in mind <a href=\"/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a>. If you could <em>anticipate in advance </em>the direction of any update, you should just update now. You should not expect to be able to get the right answer right away and never need to seriously update it. Nor should you expect to suddenly counter some piece of evidence that, on its own, would make you switch to becoming confident in something completely different. An ideal Bayesian agent will expect their beliefs to be in a constant state of gradual revision as the evidence comes in, and people with human cognitive architectures should also make an explicit effort to make their impressions update as fluidly as possible.</p>\n<p>Another thing that's said about first impressions also bears to be noted:</p>\n<blockquote>\n<p>People often try hard to make a good first impression. The challenge is to continue to examine your first impression of someone with an open mind as you have more time, information, and opportunity.</p>\n</blockquote>\n<p><a href=\"/lw/jt/what_evidence_filtered_evidence/\">Filtered evidence</a>, in its original formulation, was a set of evidence that had been chosen for the specific purpose of persuading you of something. Here I am widening the definition somewhat, and also applying to cases where the other person cannot exclude <em>all </em>the evidence they dislike, but are regardless capable of biasing it in a direction of their choice. The evidence presented at a first meeting is usually filtered evidence. (Such situations are actually complicated <a href=\"http://en.wikipedia.org/wiki/Signaling_games\">signaling games</a>, and a full Bayesian analysis would take into account all the broader game-theoretic implications. Filtered evidence is just one part of it.)</p>\n<p><a href=\"/lw/jl/what_is_evidence/\">Evidence is an event tangled by links of cause and effect with whatever you want to know about</a>. On a first meeting, a person might be doing their best to appear friendly, say. Usually being a friendly person will lead them to behave in specific ways which are characteristic of friendly people. But if they are seeking to convey a good impression of themselves, their behavior may not be caused by an inherent friendliness anymore. The behavior is not tangled with friendliness, but with a desire to appear friendly.</p>\n<p><strong>2. Consider each characteristic in light of the circumstances, not in isolation.</strong></p>\n<p>The second core tenet in What is Bayesianism was \"How we interpret <em>any</em> event, and the new information we get from anything, depends on information we <em>already</em> had.\"</p>\n<blockquote>\n<p>If you told me simply that a young man wears a large hoop earring, you couldn't expect me to tell you what that entails. It might make a great parlor game, but in real life I would never hazard a guess based on so little information. If the man is from a culture in which most young men wear large earrings, it might mean that he's a <em>conformist. </em>If, on the other hand, he is the son of a Philadelphia lawyer, he may be <em>rebellious. </em>If he plays in a rock band, he may be <em>trendy.</em></p>\n</blockquote>\n<p>A Bayesian translation of this might read roughly as follows. \"Suppose you told me simply that a young man wears a large hoop earring. You are asking me to suggest some personality trait that's causing him to wear them, but there is not enough evidence to <a href=\"http://wiki.lesswrong.com/wiki/Locating_the_hypothesis\">locate a hypothesis</a>. If we knew that the man is from a culture where most young men wear large earrings, we might know that conformists would be even more likely to wear earrings. If the number of conformists was sufficiently large, then a young man from that culture, chosen randomly on the basis of wearing earrings, might very likely be a conformist, simply because conformist earring-wearers make up such a large part of the earring-wearer population.</p>\n<p>(Or to say that in a more mathy way, say we started with a .4 chance of a young man being a conformist, a .6 chance for a young man to be wearing earrings, and a .9 chance for the conformists to be wearing earrings. Then we'd calculate (0.9 * 0.4) / (0.6) and get a 0.6 chance for the man in question to be conformist. We don't have exact numbers like these in our heads, of course, but we do have a rough idea.)</p>\n<p>But then, he might also be the son of a Philadelphia lawyer, say, and then we'd get a good chance for him being rebellious. Or if he were a rock band member, he might be trendy. We don't know which of these <a href=\"http://en.wikipedia.org/wiki/Reference_class_problem\">reference classes</a> we should use; whether we should think we're picking a young man at random from a group of earring-wearing young men from an earring-wearing culture or from all the sons of lawyers. We <em>could</em> try to take a prior over his membership in any of the relevant reference classes, saying for instance that there was a .05 chance of him being a member of an earring culture, or a .004 chance of him being the son of a lawyer and so on. In other words, we'd think that we're picking a young earring-wearing man from the group of <em>all </em>earring-wearing men on Earth. Then we'd have a (0.05 * 0.6 =) 0.03 chance of him being a conformist due to being from an earring culture, et cetera. But then we'd distribute our probability mass over such a large amount of hypotheses that they'd all be very unlikely: the group of all earring-wearing men is so big that drawing at random could produce pretty much any personality trait. Figuring out the most likely alternative of all those countless alternatives might make a great parlor game, but in real life it'd be nothing you'd like to bet on.</p>\n<p>If you told me that he was also carrying an electric guitar... well, that still wouldn't be enough to get a very high probability on any of those alternatives, but it sure would help increase the initial probability of the \"plays in a rock band\" hypothesis. Of course, he could play in a rock band <em>and</em> be from a culture where people usually wore earrings.\"</p>\n<p><strong>3. Look for extremes. The importance of a trait or characteristic may be a matter of degree.</strong></p>\n<p>This is basically just a reformulation of the above points, with an emphasis on the fact that extreme traits are easier to notice. But again, extreme signs don't tell us much in isolation, so we need to look for the broader pattern.</p>\n<blockquote>\n<p>The significance of any trait, however extreme, usually will not become clear until you learn enough about someone to see a pattern develop. As you look for the pattern, give special attention to any other traits consistent with the most extreme ones. They're usually like a beacon in the night, leading you in the right direction.<strong></strong></p>\n</blockquote>\n<p><strong>4. Identify deviations from the pattern.</strong></p>\n<p>(I'll skip this one.)</p>\n<p><strong>5. Ask yourself if what you're seeing reflects a temporary state of mind or a permanent quality.</strong></p>\n<p>Again, any given observation has many different possible causes. Sometimes a behavior is caused not by any particular personality trait, but the person simply happening to be in a particular mood, which might be rare for them.</p>\n<p>This is possibly old hat by now, but just to be sure: The probability that behavior X is caused by cause A, sayeth Bayes' theorem, is the probability that A happened in the first place times (since they must both be true) the probability that A would cause X at all. That's divided by the summed chance for <em>anything else</em> to have caused X.</p>\n<p>A psedo-frequentist interpretation might compare this to the probability of drawing an ace out of a deck of cards. (I'm not sure if the following analogy is useful or makes sense to anyone besides me, but let's give it a shot.) Suppose you get to draw cards from a deck, but even after drawing them you're never allowed to look at them, and can only guess whether you're holding the most valuable ones. The chance that you'll draw a particular card is one divided by the total number of cards. You'd have a better chance of drawing it if you got to draw more cards. Imagine the probability of \"(A happened) * (A would cause X)\" as the amount of cards you'll get to draw from the deck of all hypotheses. You need to divide that with the probability that <em>all </em>hypotheses combined have, alternative explanations included, so think of the probability of the alternate hypotheses as the amount of other cards in the deck. Then your chance of drawing an ace of hearts (the correct hypothesis) is maximized if you get to draw as many cards as possible <em>and </em>the alternative hypotheses have as little probability (as few non-ace-of-hearts cards in the deck) as possible. Not considering the alternate hypotheses is like thinking you'll have a high chance of drawing the correct card, when you don't know how many cards there are in the deck total.</p>\n<p>If you're hoping to draw the correct hypothesis about the reasons for someone's behavior, then consider carefully whether you want to use the \"this is a permanent quality\" or the \"this is just a transient mood\" explanation. Frequently, drawing the \"this is just a transient mood\" cards will give you a better shot at grabbing the hypothesis with the most valuable card.</p>\n<p>See also <a href=\"http://wiki.lesswrong.com/wiki/Fundamental_attribution_error\">Correspondence Bias</a>.</p>\n<p><strong><br />6. Distinguish between elective and nonelective traits. Some things you control; other things control you.</strong></p>\n<p>As noted in the discussion about first impressions, people have an interest in manipulating the impression that others give them. The easier it is to manipulate an impression, and the more common it is that people have an interest in biasing that impression, the less reliable of a guide it is. Elective traits such as clothing, jewelry and accessories can be altered almost at will, and are therefore relatively weak evidence.</p>\n<p>Nonelective traits offer stronger evidence, particularly if they're extreme: things such as extreme overweight, physical handicaps, mental disorders and debiliating diseases often have a deep-rooted effect on personality and behavior. Many other nonelective traits such as height or facial features that are not very unusual don't usually merit special consideration - unless the person has invested signficant resources to permanently altering them.<strong></strong></p>\n<p><strong><br />7. Give special attention to certain highly predictive traits.</strong></p>\n<p>Left as an exercise for readers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Man2iP6ftuTPze9K": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KL9iocwykHq53Esrv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 36, "extendedScore": null, "score": 6.0072763788461e-07, "legacy": true, "legacyId": "3117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Or, how to recognize <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' theorem</a> when you meet one making small talk at a cocktail party.</p>\n<p>Knowing the theory of rationality is good, but it is of little use unless we know how to apply it. Unfortunately, humans tend to be poor at applying raw theory, instead needing several examples before it becomes instinctive. I found some very useful examples in the book <a href=\"http://www.amazon.com/Reading-People-Understand-Behavior-Anytime/dp/0345425871\"><em>Reading People: How to Understand People and Predict Their Behavior - Anytime, Anyplace</em></a>. While I didn't think that it communicated the skill of actually reading people very well, I did notice that it did have one chapter (titled \"Discovering Patterns: Learning to See the Forest, Not Just the Trees\") that could almost have been a collection of Less Wrong posts. It also serves as an excellent example of applying Bayes' theorem in every-day life.<br><br>In \"<a href=\"/lw/1to/what_is_bayesianism/\">What is Bayesianism?</a>\" I said that the first core tenet of Bayesianism is \"Any given observation has many different possible causes\". <em>Reading People</em> says:<br><em></em></p>\n<blockquote>\n<p><em>If this book could deliver but one message, it would be that to read people effectively you must gather enough information about them to establish a consistent pattern.</em> Without that pattern, your conclusions will be about as reliable as a tarot card reading.</p>\n</blockquote>\n<p>In fact, the author is saying that Bayes' theorem applies when you're trying to read people (if this is not immediately obvious, just keep reading). Any particular piece of evidence about a person could have various causes. For example, in a later chapter we are offered a list of possible reasons for why someone may have dressed inappropriately for an occasion. They might (1) be seeking attention, (2) lack common sense, (3) be self-centered and insensitive to others, (4) be trying to show that they are spontaneous, rebellious, or noncomformists and don't care what other people think, (5) not have been taught how to dress and act appropriately, (6) be trying to imitate someone they admire, (7) value comfort and convenience over all else, or (8) simply not have the right attire for the occasion.<br><br>Similarly, very short hair on a man might indicate that he (1) is in the military, or was at some point in his life, (2) works for an organization that demands very short hair, such as a police force or fire department, (3) is trendy, artistic or rebellious, (4) is conservative, (5) is undergoing or recovering from a medical treatment, (6) thinks he looks better with short hair, (7) plays sports, or (8) keeps his hair short for practical reasons.<br><br>So much for reading people being easy. This, again, is the essence of Bayes' theorem: even though somebody being in the military might almost certainly mean that they'd have short hair, them having a short hair does not necessarily mean that they are in the military. On the other hand, if someone has short hair, is clearly knowledgeable about weapons and tactics, displays a no-nonsense attitude, is in good shape, and has a very Spartan home... well, though it's still not for certain, it seems likely to me that of all the people having all of these attributes, quite a few of them are in the military or in similar occupations.<br><a id=\"more\"></a><br>The book offers a seven-step guide for finding patterns in people. I'll go through them one at a time, pointing out what they say in Bayesian and heuristic/bias terms. Note that this is not a definitive list: if you can come up with more Bayesian angles to the book, post them in the comments.<br><br><strong>1. Start with the person's most striking traits, and as you gather more information see if his other traits are consistent or inconsistent.<br></strong></p>\n<p>As computationally bounded agents, we can't simply take in all the available data at once: we have to start off some particularly striking traits and start building a picture from there. However, humans are notorious about anchoring too much (<a href=\"/lw/j7/anchoring_and_adjustment/\">Anchoring and Adjustment</a>), so we are reminded <em>to actively seek disconfirmation </em>to any initial theory we have.</p>\n<blockquote>\n<p>I constantly test additional information agaisnt my first impression, always watching for patterns to develop. Each piece of the puzzle - a person's appearance, her tone of voice, hygiene and so on - may validate my first impression, disprove it, or have little impact on it. If most of the new information points in a different direction than my first impression did, I revise that impression. Then I consider whether my revised impression holds up as even more clues are revealed - and revise it again, if need be.</p>\n</blockquote>\n<p>Here, the author is keeping in mind <a href=\"/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a>. If you could <em>anticipate in advance </em>the direction of any update, you should just update now. You should not expect to be able to get the right answer right away and never need to seriously update it. Nor should you expect to suddenly counter some piece of evidence that, on its own, would make you switch to becoming confident in something completely different. An ideal Bayesian agent will expect their beliefs to be in a constant state of gradual revision as the evidence comes in, and people with human cognitive architectures should also make an explicit effort to make their impressions update as fluidly as possible.</p>\n<p>Another thing that's said about first impressions also bears to be noted:</p>\n<blockquote>\n<p>People often try hard to make a good first impression. The challenge is to continue to examine your first impression of someone with an open mind as you have more time, information, and opportunity.</p>\n</blockquote>\n<p><a href=\"/lw/jt/what_evidence_filtered_evidence/\">Filtered evidence</a>, in its original formulation, was a set of evidence that had been chosen for the specific purpose of persuading you of something. Here I am widening the definition somewhat, and also applying to cases where the other person cannot exclude <em>all </em>the evidence they dislike, but are regardless capable of biasing it in a direction of their choice. The evidence presented at a first meeting is usually filtered evidence. (Such situations are actually complicated <a href=\"http://en.wikipedia.org/wiki/Signaling_games\">signaling games</a>, and a full Bayesian analysis would take into account all the broader game-theoretic implications. Filtered evidence is just one part of it.)</p>\n<p><a href=\"/lw/jl/what_is_evidence/\">Evidence is an event tangled by links of cause and effect with whatever you want to know about</a>. On a first meeting, a person might be doing their best to appear friendly, say. Usually being a friendly person will lead them to behave in specific ways which are characteristic of friendly people. But if they are seeking to convey a good impression of themselves, their behavior may not be caused by an inherent friendliness anymore. The behavior is not tangled with friendliness, but with a desire to appear friendly.</p>\n<p><strong id=\"2__Consider_each_characteristic_in_light_of_the_circumstances__not_in_isolation_\">2. Consider each characteristic in light of the circumstances, not in isolation.</strong></p>\n<p>The second core tenet in What is Bayesianism was \"How we interpret <em>any</em> event, and the new information we get from anything, depends on information we <em>already</em> had.\"</p>\n<blockquote>\n<p>If you told me simply that a young man wears a large hoop earring, you couldn't expect me to tell you what that entails. It might make a great parlor game, but in real life I would never hazard a guess based on so little information. If the man is from a culture in which most young men wear large earrings, it might mean that he's a <em>conformist. </em>If, on the other hand, he is the son of a Philadelphia lawyer, he may be <em>rebellious. </em>If he plays in a rock band, he may be <em>trendy.</em></p>\n</blockquote>\n<p>A Bayesian translation of this might read roughly as follows. \"Suppose you told me simply that a young man wears a large hoop earring. You are asking me to suggest some personality trait that's causing him to wear them, but there is not enough evidence to <a href=\"http://wiki.lesswrong.com/wiki/Locating_the_hypothesis\">locate a hypothesis</a>. If we knew that the man is from a culture where most young men wear large earrings, we might know that conformists would be even more likely to wear earrings. If the number of conformists was sufficiently large, then a young man from that culture, chosen randomly on the basis of wearing earrings, might very likely be a conformist, simply because conformist earring-wearers make up such a large part of the earring-wearer population.</p>\n<p>(Or to say that in a more mathy way, say we started with a .4 chance of a young man being a conformist, a .6 chance for a young man to be wearing earrings, and a .9 chance for the conformists to be wearing earrings. Then we'd calculate (0.9 * 0.4) / (0.6) and get a 0.6 chance for the man in question to be conformist. We don't have exact numbers like these in our heads, of course, but we do have a rough idea.)</p>\n<p>But then, he might also be the son of a Philadelphia lawyer, say, and then we'd get a good chance for him being rebellious. Or if he were a rock band member, he might be trendy. We don't know which of these <a href=\"http://en.wikipedia.org/wiki/Reference_class_problem\">reference classes</a> we should use; whether we should think we're picking a young man at random from a group of earring-wearing young men from an earring-wearing culture or from all the sons of lawyers. We <em>could</em> try to take a prior over his membership in any of the relevant reference classes, saying for instance that there was a .05 chance of him being a member of an earring culture, or a .004 chance of him being the son of a lawyer and so on. In other words, we'd think that we're picking a young earring-wearing man from the group of <em>all </em>earring-wearing men on Earth. Then we'd have a (0.05 * 0.6 =) 0.03 chance of him being a conformist due to being from an earring culture, et cetera. But then we'd distribute our probability mass over such a large amount of hypotheses that they'd all be very unlikely: the group of all earring-wearing men is so big that drawing at random could produce pretty much any personality trait. Figuring out the most likely alternative of all those countless alternatives might make a great parlor game, but in real life it'd be nothing you'd like to bet on.</p>\n<p>If you told me that he was also carrying an electric guitar... well, that still wouldn't be enough to get a very high probability on any of those alternatives, but it sure would help increase the initial probability of the \"plays in a rock band\" hypothesis. Of course, he could play in a rock band <em>and</em> be from a culture where people usually wore earrings.\"</p>\n<p><strong id=\"3__Look_for_extremes__The_importance_of_a_trait_or_characteristic_may_be_a_matter_of_degree_\">3. Look for extremes. The importance of a trait or characteristic may be a matter of degree.</strong></p>\n<p>This is basically just a reformulation of the above points, with an emphasis on the fact that extreme traits are easier to notice. But again, extreme signs don't tell us much in isolation, so we need to look for the broader pattern.</p>\n<blockquote>\n<p>The significance of any trait, however extreme, usually will not become clear until you learn enough about someone to see a pattern develop. As you look for the pattern, give special attention to any other traits consistent with the most extreme ones. They're usually like a beacon in the night, leading you in the right direction.<strong></strong></p>\n</blockquote>\n<p><strong id=\"4__Identify_deviations_from_the_pattern_\">4. Identify deviations from the pattern.</strong></p>\n<p>(I'll skip this one.)</p>\n<p><strong id=\"5__Ask_yourself_if_what_you_re_seeing_reflects_a_temporary_state_of_mind_or_a_permanent_quality_\">5. Ask yourself if what you're seeing reflects a temporary state of mind or a permanent quality.</strong></p>\n<p>Again, any given observation has many different possible causes. Sometimes a behavior is caused not by any particular personality trait, but the person simply happening to be in a particular mood, which might be rare for them.</p>\n<p>This is possibly old hat by now, but just to be sure: The probability that behavior X is caused by cause A, sayeth Bayes' theorem, is the probability that A happened in the first place times (since they must both be true) the probability that A would cause X at all. That's divided by the summed chance for <em>anything else</em> to have caused X.</p>\n<p>A psedo-frequentist interpretation might compare this to the probability of drawing an ace out of a deck of cards. (I'm not sure if the following analogy is useful or makes sense to anyone besides me, but let's give it a shot.) Suppose you get to draw cards from a deck, but even after drawing them you're never allowed to look at them, and can only guess whether you're holding the most valuable ones. The chance that you'll draw a particular card is one divided by the total number of cards. You'd have a better chance of drawing it if you got to draw more cards. Imagine the probability of \"(A happened) * (A would cause X)\" as the amount of cards you'll get to draw from the deck of all hypotheses. You need to divide that with the probability that <em>all </em>hypotheses combined have, alternative explanations included, so think of the probability of the alternate hypotheses as the amount of other cards in the deck. Then your chance of drawing an ace of hearts (the correct hypothesis) is maximized if you get to draw as many cards as possible <em>and </em>the alternative hypotheses have as little probability (as few non-ace-of-hearts cards in the deck) as possible. Not considering the alternate hypotheses is like thinking you'll have a high chance of drawing the correct card, when you don't know how many cards there are in the deck total.</p>\n<p>If you're hoping to draw the correct hypothesis about the reasons for someone's behavior, then consider carefully whether you want to use the \"this is a permanent quality\" or the \"this is just a transient mood\" explanation. Frequently, drawing the \"this is just a transient mood\" cards will give you a better shot at grabbing the hypothesis with the most valuable card.</p>\n<p>See also <a href=\"http://wiki.lesswrong.com/wiki/Fundamental_attribution_error\">Correspondence Bias</a>.</p>\n<p><strong id=\"6__Distinguish_between_elective_and_nonelective_traits__Some_things_you_control__other_things_control_you_\"><br>6. Distinguish between elective and nonelective traits. Some things you control; other things control you.</strong></p>\n<p>As noted in the discussion about first impressions, people have an interest in manipulating the impression that others give them. The easier it is to manipulate an impression, and the more common it is that people have an interest in biasing that impression, the less reliable of a guide it is. Elective traits such as clothing, jewelry and accessories can be altered almost at will, and are therefore relatively weak evidence.</p>\n<p>Nonelective traits offer stronger evidence, particularly if they're extreme: things such as extreme overweight, physical handicaps, mental disorders and debiliating diseases often have a deep-rooted effect on personality and behavior. Many other nonelective traits such as height or facial features that are not very unusual don't usually merit special consideration - unless the person has invested signficant resources to permanently altering them.<strong></strong></p>\n<p><strong id=\"7__Give_special_attention_to_certain_highly_predictive_traits_\"><br>7. Give special attention to certain highly predictive traits.</strong></p>\n<p>Left as an exercise for readers.</p>", "sections": [{"title": "2. Consider each characteristic in light of the circumstances, not in isolation.", "anchor": "2__Consider_each_characteristic_in_light_of_the_circumstances__not_in_isolation_", "level": 1}, {"title": "3. Look for extremes. The importance of a trait or characteristic may be a matter of degree.", "anchor": "3__Look_for_extremes__The_importance_of_a_trait_or_characteristic_may_be_a_matter_of_degree_", "level": 1}, {"title": "4. Identify deviations from the pattern.", "anchor": "4__Identify_deviations_from_the_pattern_", "level": 1}, {"title": "5. Ask yourself if what you're seeing reflects a temporary state of mind or a permanent quality.", "anchor": "5__Ask_yourself_if_what_you_re_seeing_reflects_a_temporary_state_of_mind_or_a_permanent_quality_", "level": 1}, {"title": "6. Distinguish between elective and nonelective traits. Some things you control; other things control you.", "anchor": "6__Distinguish_between_elective_and_nonelective_traits__Some_things_you_control__other_things_control_you_", "level": 1}, {"title": "7. Give special attention to certain highly predictive traits.", "anchor": "7__Give_special_attention_to_certain_highly_predictive_traits_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AN2cBr6xKWCB8dRQG", "bMkCEZoBNhgRBtzoj", "jiBFC7DcCrZjGmZnJ", "kJiPnaQPiy4p9Eqki", "6s3xABaXKPdFwA3FS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-07-01T18:25:06.938Z", "modifiedAt": "2020-02-10T10:38:01.983Z", "url": null, "title": "What Cost for Irrationality?", "slug": "what-cost-for-irrationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:26.735Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ujTE9FLWveYz9WTxZ/what-cost-for-irrationality", "pageUrlRelative": "/posts/ujTE9FLWveYz9WTxZ/what-cost-for-irrationality", "linkUrl": "https://www.lesswrong.com/posts/ujTE9FLWveYz9WTxZ/what-cost-for-irrationality", "postedAtFormatted": "Thursday, July 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Cost%20for%20Irrationality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Cost%20for%20Irrationality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FujTE9FLWveYz9WTxZ%2Fwhat-cost-for-irrationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Cost%20for%20Irrationality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FujTE9FLWveYz9WTxZ%2Fwhat-cost-for-irrationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FujTE9FLWveYz9WTxZ%2Fwhat-cost-for-irrationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2033, "htmlBody": "<p><em>This is the first part in a <a href=\"https://www.lesswrong.com/s/n44Fqx5W4BhMugCMS\">mini-sequence</a> presenting content from Keith E. Stanovich&apos;s excellent book <a href=\"http://www.amazon.com/What-Intelligence-Tests-Miss-Psychology/dp/030012385X\">What Intelligence Tests Miss: The psychology of rational thought</a>. It will culminate in a review of the book itself.</em></p><br><p>People who care a lot about <a href=\"https://www.lesswrong.com/lw/31/what_do_we_mean_by_rationality/\">rationality</a> may frequently be asked why they do so. There are various answers, but I think that <a href=\"https://www.lesswrong.com/lw/go/why_truth_and/\">many of ones discussed here</a> won&apos;t be very persuasive to people who don&apos;t already have an interest in the issue. But in real life, most people don&apos;t try to stay healthy because of various <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">far-mode</a> arguments for the virtue of health: instead, they try to stay healthy in order to avoid various forms of illness. In the same spirit, I present you with a list of real-world events that have been caused by failures of rationality.</p><p>What happens if you, or the people around you, are not rational? Well, in order from least serious to worst, you may...</p><p><strong>Have a worse quality of living</strong>. <em><a href=\"http://wiki.lesswrong.com/wiki/Status_quo_bias\">Status Quo bias</a></em> is a general human tendency to prefer the default state, regardless of whether the default is actually good or not. In the 1980&apos;s, Pacific Gas and Electric conducted a survey of their customers. Because the company was serving a lot of people in a variety of regions, some of their customers suffered from more outages than others. Pacific Gas asked customers with unreliable service whether they&apos;d be willing to pay extra for more reliable service, and customers with reliable service whether they&apos;d be willing to accept a less reliable service in exchange for a discount. The customers were presented with increases and decreases of various percentages, and asked which ones they&apos;d be willing to accept. The percentages were same for both groups, only with the other having increases instead of decreases. Even though both groups had the same income, customers of both groups overwhelmingly wanted to stay with their status quo. Yet the service difference between the groups was large: the unreliable service group suffered 15 outages per year of 4 hours&apos; average duration and the reliable service group suffered 3 outages per year of 2 hours&apos; average duration! (Though <a href=\"https://www.lesswrong.com/lw/2et/what_cost_for_irrationality/283a\">note</a> <a href=\"https://www.lesswrong.com/lw/2et/what_cost_for_irrationality/2833\">caveats</a>.)</p><p>A study by Philips Electronics found that one half of their products had nothing wrong in them, but the consumers couldn&apos;t figure out how to use the devices. This can be partially explained by egocentric bias on behalf of the engineers. Cognitive scientist Chip Heath notes that he has &quot;a DVD remote control with 52 buttons on it, and every one of them is there because some engineer along the line knew how to use that button and believed I would want to use it, too. People who design products are experts... and they can&apos;t imagine what it&apos;s like to be as ignorant as the rest of us.&quot;</p><p><strong>Suffer financial harm.</strong> John Allen Paulos is a professor of mathematics at Temple University. Yet he fell prey to serious irrationality which began when he purchased WorldCom stock at $47 per share in early 2000. As bad news about the industry began mounting, WorldCom&apos;s stock price started falling - and as it did so, Paulos kept buying, regardless of accumulating evidence that he should be selling. Later on, he admitted that his &quot;purchases were not completely rational&quot; and that &quot;I bought shares even though I knew better&quot;. He was still buying - partially on borrowed money - when the stock price was $5. When it momentarily rose to $7, he finally decided to sell. Unfortunately, he didn&apos;t get off from work until the market closed, and on the next market day the stock had lost a third of its value. Paulos finally sold everything, at a huge loss.</p><p>Stock market losses due to irrationality are not atypical. From the beginning of 1998 to the end of 2001, the Firsthand Technology Value mutual fund had an average gain of 16 percent per year. Yet the average investor who invested in the fund <em>lost</em> 31.6 percent of her money over the same period. Investors actually lost a total of $1.9 <em>billion</em> by investing in a fund which was producing 16 percent of a profit per year. That happened because the fund was very volatile, causing people to invest and cash out at exactly the wrong times. When it gained, it gained a lot, and when it lost, it lost a lot. When people saw that it had been making losses, they sold, and when they saw it had been making gains, they bought. In other words, they bought when high and sold when low - exactly the opposite of what you&apos;re supposed to do if you want to make a profit. Reporting on a study of 700 mutual funds during 1998-2001, finanical reporter Jason Zweig noted that &quot;to a remarkable degree, investors underperformed their funds&apos; reported returns - sometimes by as much as 75 percentage points per year.&quot;</p><p><strong>Be manipulated and robbed of personal autonomy.</strong> Subjects were asked to divide 100 usable livers to 200 children awaiting a transplant. With two groups of children, group A with 100 children and group B with 100 children, the overwhelming response was to allocate 50 livers to each, which seems reasonable. But when group A had 100 children, each with an 80 percent chance of surviving when transplanted, and group B had 100 children, each with a 20 percent chance of surviving when transplanted, people still chose the equal allocation method even if this caused the unnecessary deaths of 30 children. Well, that&apos;s just a question of values and not rationality, right? Turns out that if the patients were ranked from 1 to 200 in terms of prognosis, people were relatively comfortable with distributing organs to the top 100 patients. It was only when the question was framed as &quot;group A versus group B&quot; that people suddenly felt they didn&apos;t want to abandon group B entirely. Of course, these are exactly the same dilemma. One could almost say that the person who got to choose which framing to use was <em>getting to decide on behalf of the people being asked the question.</em></p><br><p>Two groups of subjects were given information about eliminating affirmative action and adopting a race-neutral policy at several universities. One group was told that under race-neutral conditions, the probability of a black student being admitted would decline from 42 percent to 13 percent and the probability of a white student being admitted would rise from 25 percent to 27 percent. The other group was told that under race-neutral admissions, the number of black students being admitted would decrease by 725 and the number of white students would increase by 725. These two framings were both saying the same thing, but you can probably guess the outcome: support for affirmative action was much higher in the percentage group.</p><p>In a hypothetical country, a family with no children and an income of $35,000 pays $4,000 in tax, while a family with no children and an income of $100,000 pays $26,000 in tax. Now suppose that there&apos;s a $500 tax reduction for having a child for a family with an income of $35,000. Should the family with an income of $100,000 be given a larger reduction because of their higher income? Here, most people would say no. But suppose that instead, the baseline is that a family of two children with an income of $35,000 pays $3,000 in tax and that a family of two children with an income of $100,000 pays $25,000 in tax. We propose to make the families with no children pay more tax - that is, have a &quot;childless penalty&quot;. Say that the family with the income of $100,000 and one child has their taxes set at $26,000 and the same family with no children has their taxes set at $27,000 - there&apos;s a childless penalty of $1,000 per child. Should the poorer family which makes $35,000 and has no children also pay the same $2,000 childless penalty as the richer family? Here, most people would also say no - they&apos;d want the &quot;bonus&quot; for children to be equal for low- and high-income families, but they do not want the &quot;penalty&quot; for lacking children to be the high for same and low income.</p><p><strong>End up falsely accused or imprisoned.</strong> In 2003, an attorney was released from prison in England when her conviction of murdering her two infants was overturned. Five months later, another person was released from prison when her charge of having murdered her children was also overturned. In both cases, the evidence presented against them had been ambiguous. What had convinced the jury was that in both cases, a pediatrician had testified that the odds of two children in the same family dying of infant death syndrome was 73 million to 1. Unfortunately, he had arrived to this figure by squaring the odds of a single death. Squaring the odds of a single event to arrive at the odds for it happening twice only works if the two events are independent. But that assumption is likely to be false in the case of multiple deaths in the same family, where numerous environmental and genetic factors may have affected both deaths.</p><p>In the late 1980s and early 1990s, many parents were excited and overjoyed to hear of a technique coming out of Australia that enabled previously totally non-verbal autistic children to communicate. It was uncritically promoted in highly visible media such as <em>60 Minutes, Parade </em>magazine and the <em>Washington Post</em>. The claim was that autistic individuals and other children with developmental disabilities who&apos;d previously been nonverbal had typed highly literate messages on a keyboard when their hands and arms had been supported over by the typewriter by a sympathetic &quot;facilitator&quot;. As Stanovich describes: &quot;Throughout the early 1990s, behavioral science researchers the world over watched in horrified anticipation, almost as if observing cars crash in slow motion, while a predictable tragedy unfolded before their eyes.&quot; The hopes of countless parents were dashed when it was shown that the &quot;facilitators&quot; had been - consciously or unconsciously - directing the children&apos;s hands on the right keys. It <em>should</em> have been obvious that spreading such news before the technique had been properly scientifically examined was dangerously irresponsible - and it gets worse. During some &quot;faciliation&quot; sessions, children &quot;reported&quot; having been sexually abused by their parents, and were removed from their homes as a result. (Though they were eventually returned.)</p><p><strong>End up dead.</strong> After 9/11, people became afraid of flying and started doing so less. Instead, they began driving more. Unfortunately, car travel has a much higher chance of death than air travel. Researchers have estimated that over 300 more people died in the last months of 2001 because they drove instead of flying. Another group calculated that for flying to be as dangerous as driving, there would have to be an incident on the scale of 9/11 once a month!</p><p><strong>Have your society collapse.</strong> Possibly even more horrifying is the tale of Albania, which had previously been a communist dictatorship but had made considerable financial progress from 1992 to 1997. In 1997, however, <em>one half of the adult population</em> had fallen victim to <a href=\"http://en.wikipedia.org/wiki/Ponzi_scheme\">Ponzi schemes</a>. In a Ponzi scheme, the investment itself isn&apos;t actually making any money, but rather early investors are paid off with the money from late investors, and eventually the system has to collapse when no new investors can be recruited. But when schemes offering a 30 percent <em>monthly</em> return began to become popular in Albania, competitors offering a 50-60 or even a 100 percent monthly return soon showed up, and people couldn&apos;t resist the temptation. Eventually both the government and economy of Albania collapsed. Stanovich describes:</p><blockquote>People took out mortgages on their homes in order to participate. Others sold their homes. Many put their entire life savings into the schemes. At their height, an amount equal to 50 percent of the country&apos;s GDP was invested in Ponzi schemes. Before the schemes collapsed, they actually began to compete with wage income and distort the economy. For example, one business owner saw his workforce quickly slip from 130 employees to 70 because people began to think they could invest in the Ponzi schemes instead of actually working for their income.</blockquote><br><p>The estimated death toll was <a href=\"http://en.wikipedia.org/wiki/1997_rebellion_in_Albania\">between 1,700 and 2,000</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "pnSXfWXbQihrFadeD": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ujTE9FLWveYz9WTxZ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 76, "baseScore": 91, "extendedScore": null, "score": 0.000173, "legacy": true, "legacyId": "3125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 77, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 119, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["RcZCwxFiZzE6X7nsv", "YshRbqZHYFoEMqFAu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-07-01T21:20:42.638Z", "modifiedAt": null, "url": null, "title": "Open Thread: July 2010", "slug": "open-thread-july-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.510Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DKLxoWcQmt4xapsQJ/open-thread-july-2010", "pageUrlRelative": "/posts/DKLxoWcQmt4xapsQJ/open-thread-july-2010", "linkUrl": "https://www.lesswrong.com/posts/DKLxoWcQmt4xapsQJ/open-thread-july-2010", "postedAtFormatted": "Thursday, July 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20July%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20July%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDKLxoWcQmt4xapsQJ%2Fopen-thread-july-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20July%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDKLxoWcQmt4xapsQJ%2Fopen-thread-july-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDKLxoWcQmt4xapsQJ%2Fopen-thread-july-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><em style=\"font-style: italic;\">This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\"><em><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><span style=\"line-height: normal;\"><a href=\"/lw/2ft/open_thread_july_2010_part_2/\">Part 2</a></span></span></em></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DKLxoWcQmt4xapsQJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 6.010857907821947e-07, "legacy": true, "legacyId": "3126", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 698, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WqQD5WWcCEjPvcYeX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}