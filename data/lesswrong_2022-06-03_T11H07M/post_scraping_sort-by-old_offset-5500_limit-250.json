{"results": [{"createdAt": null, "postedAt": "2012-01-01T17:24:18.070Z", "modifiedAt": null, "url": null, "title": "January 2012 Media Thread", "slug": "january-2012-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:30.834Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xh3is79A7qkoMM6pu/january-2012-media-thread", "pageUrlRelative": "/posts/xh3is79A7qkoMM6pu/january-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/xh3is79A7qkoMM6pu/january-2012-media-thread", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20January%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJanuary%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxh3is79A7qkoMM6pu%2Fjanuary-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=January%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxh3is79A7qkoMM6pu%2Fjanuary-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxh3is79A7qkoMM6pu%2Fjanuary-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>There was a<a class=\"fullwidth\" title=\"recentdiscussion\" href=\"/lw/92w/are_yearlymonthly_book_suggestion_threads_a_good/\" target=\"_blank\"> recent discussion</a> considering the idea of a monthly Book (later expanded to movies, links, etc) thread. The poll was pretty unanimous that this was A Good Idea (tm), so let's give it a try!<br /><br />Post what you're reading or watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing!</p>\n<p>&nbsp;</p>\n<p>I encourage minimal down-voting in the comments here, because this is a thread for sharing subjective experiences, and I would like people to feel comfortable posting their personal opinion without fearing a karma backlash. If you disagree with a person's recommendation, respond with a comment instead.&nbsp;</p>\n<p>&nbsp;</p>\n<p>I think this is a post that lends itself to comment trees, so I will start one with the categories: Books, Movies, Other, and Meta.</p>\n<p>I am interested in hearing if you all think it's a good idea to separate out fiction and non-fiction. (So that the Book thread doesn't have Discworld next to Influence, for example. Or so the movie thread doesn't have documentaries interspersed with rom-coms or what-not)</p>\n<p>&nbsp;</p>\n<p>\n<hr />\n<br />Poster's Note: Wheee! I made my first working link on the discussion post writing interface! I can learn stuffs!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xh3is79A7qkoMM6pu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 4, "extendedScore": null, "score": 8.245972722198389e-07, "legacy": true, "legacyId": "11843", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["E8gc4H7Xorx4ebq8d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T19:25:40.308Z", "modifiedAt": null, "url": null, "title": "Advice Request: Baconmas Website", "slug": "advice-request-baconmas-website", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h3xDc3FYTwRFJW2Y4/advice-request-baconmas-website", "pageUrlRelative": "/posts/h3xDc3FYTwRFJW2Y4/advice-request-baconmas-website", "linkUrl": "https://www.lesswrong.com/posts/h3xDc3FYTwRFJW2Y4/advice-request-baconmas-website", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20Request%3A%20Baconmas%20Website&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20Request%3A%20Baconmas%20Website%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh3xDc3FYTwRFJW2Y4%2Fadvice-request-baconmas-website%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20Request%3A%20Baconmas%20Website%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh3xDc3FYTwRFJW2Y4%2Fadvice-request-baconmas-website", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh3xDc3FYTwRFJW2Y4%2Fadvice-request-baconmas-website", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p><strong>The Gist:</strong> I started <a href=\"http://baconmas.com/\">this blog</a> to get people excited about a science-themed holiday. I want your suggestions before I advertise it to everyone I know!</p>\n<p>Two years ago, I came up with the idea of celebrating Sir Francis Bacon's birthday (Jan. 22) as a festive science-themed holiday called Baconmas. (The name has the additional bonus that it's easy to convince people to come to a party if there will be bacon there.) I had a good Baconmas party in 2010 and a better one in 2011, and now I want to let other people in on the fun.</p>\n<p><a href=\"http://baconmas.com/\">So I made a website.</a></p>\n<p>It's currently \"in beta\"; I wrote a couple of preparatory things, but haven't yet shown it to the vast majority of my friends. I want to maximize the chance that it goes a bit viral when I do, because a science-themed holiday really needs to exist. So I'd like any suggestions you have, before I go \"alpha\" with it. So as to not cause anchoring, I'll put down in the first comment the things I already plan to do- if you could make all your original suggestions first, then read those plans and others' comments, then add more suggestions, that should maximize the good ideas. Thanks!</p>\n<p>(Oh, and it goes without saying that you should celebrate Baconmas if at all possible. It's been a lot of fun for me.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h3xDc3FYTwRFJW2Y4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "11845", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T19:36:50.337Z", "modifiedAt": null, "url": null, "title": "An ethical debate that's turning very sour", "slug": "an-ethical-debate-that-s-turning-very-sour", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:19.721Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Multiheaded", "createdAt": "2011-07-02T10:10:20.692Z", "isAdmin": false, "displayName": "Multiheaded"}, "userId": "moGiw35FowgiAnzfg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/frGvBCjLmH3yQpBGy/an-ethical-debate-that-s-turning-very-sour", "pageUrlRelative": "/posts/frGvBCjLmH3yQpBGy/an-ethical-debate-that-s-turning-very-sour", "linkUrl": "https://www.lesswrong.com/posts/frGvBCjLmH3yQpBGy/an-ethical-debate-that-s-turning-very-sour", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20ethical%20debate%20that's%20turning%20very%20sour&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20ethical%20debate%20that's%20turning%20very%20sour%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfrGvBCjLmH3yQpBGy%2Fan-ethical-debate-that-s-turning-very-sour%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20ethical%20debate%20that's%20turning%20very%20sour%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfrGvBCjLmH3yQpBGy%2Fan-ethical-debate-that-s-turning-very-sour", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfrGvBCjLmH3yQpBGy%2Fan-ethical-debate-that-s-turning-very-sour", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 542, "htmlBody": "<p>(Okay, this has more or less run its course. Deleting the post.)</p>\r\n<p>&nbsp;</p>\r\n<p>http://lesswrong.com/lw/90l/welcome_to_less_wrong_2012/5kk8</p>\r\n<p>Right now I seem to be losing my last scraps of clear-headedness concerning the topic.</p>\r\n<p>I'd dearly like anyone who feels they're up to it to 1) step in to provide a more rounded perspective and 2) help me the hell out in whatever way you consider useful, because such moral predicaments are known to be a point of obsession for me and haunt me for a long time (see <a href=\"/lw/y4/three_worlds_collide_08/4g7q\">my first post on LW</a>).</p>\r\n<p>&nbsp;</p>\r\n<p>Relevant: some of the exchange I just had with Orthonormal, with him inquiring about a possible factor to my behavior:</p>\r\n<p>&nbsp;</p>\r\n<blockquote>\r\n<p><em>Orthonormal: </em></p>\r\n<p>I'm not going to do this publicly (for several obvious reasons), but I want to ask: do you occasionally worry that you have some sociopathic impulses?</p>\r\n<p><em>Me:</em></p>\r\n<p>I certainly do (and I don't view that in a solely negative light at that), and I understand that I might be exhibiting some complexly motivated cognition here. But this understanding isn't swaying my opinion.</p>\r\n<p><em>Orthonormal:&nbsp;</em></p>\r\n<p>Oh, I wasn't arguing against you- I was just curious.<br /><br />I noticed that you had an unusual amount of passion on this topic, and wanted to test the generalization that the most passionate proponents of prohibiting something (or keeping it prohibited) are often those who feel some temptation themselves- both because an external prohibition seems like it helps their internal self-control, and because it signals to themselves and others that they're not the sort of person who would do that thing. (Getting evidence for that theory was more significant to me than any actual info about sociopathic impulses.)</p>\r\n</blockquote>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;EDIT: thanks to everyone who responded.</p>\r\n<p>I'll be coming back to this. In the meanwhile, to make my post less bad-useless-kind-of-meta, here's a comment by drethelin that offers a better arguments for and summary of my position than any of mine:</p>\r\n<blockquote>\r\n<p>I broadly agree that babies aren't people, but I still think infanticide should be illegal, simply because killing begets insensitivity to killing. I know this has the sound of a slippery slope argument, but there is evidence that desire for sadism in most people is low, and increases as they commit sadistic acts, and that people feel similarly about murder.<br /><br />From The Better Angels of Our Nature: \"Serial killers too carry out their first murder with trepidation, distaste, and in its wake, disappointment: the experience had not been as arousing as it had been in their imaginations. But as time passes and their appetite is rewhetted, they find the next on easier and more gratifying, and then they escalate the cruelty to feed what turns into an addiction.\"<br /><br />Similarly, cathartic violence against non-person objects (http://en.wikipedia.org/wiki/Catharsis#Therapeutic_uses) can lead to further aggression in personal interactions.<br /><br />I don't think we want to encourage or allow killing of anything anywhere near as close to people as babies. The psychological effects on people who kill their own children and on a society that views the killing of babies as good are too potentially terrible. Without actual data, I can say I would never want to live in a society that valued people as little as Sparta did.</p>\r\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "frGvBCjLmH3yQpBGy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -3, "extendedScore": null, "score": -7e-06, "legacy": true, "legacyId": "11846", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-02T01:18:01.561Z", "modifiedAt": null, "url": null, "title": "[META] Introduction thread attitudes and discussions", "slug": "meta-introduction-thread-attitudes-and-discussions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:02.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GydByr7RBHZYzAqiv/meta-introduction-thread-attitudes-and-discussions", "pageUrlRelative": "/posts/GydByr7RBHZYzAqiv/meta-introduction-thread-attitudes-and-discussions", "linkUrl": "https://www.lesswrong.com/posts/GydByr7RBHZYzAqiv/meta-introduction-thread-attitudes-and-discussions", "postedAtFormatted": "Monday, January 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Introduction%20thread%20attitudes%20and%20discussions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Introduction%20thread%20attitudes%20and%20discussions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGydByr7RBHZYzAqiv%2Fmeta-introduction-thread-attitudes-and-discussions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Introduction%20thread%20attitudes%20and%20discussions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGydByr7RBHZYzAqiv%2Fmeta-introduction-thread-attitudes-and-discussions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGydByr7RBHZYzAqiv%2Fmeta-introduction-thread-attitudes-and-discussions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 337, "htmlBody": "<p>So, we have <a href=\"http://wiki.lesswrong.com/wiki/Special_threads\">three introduction threads</a>, and recently have had several contentious ones, like AspiringKnitter's <a href=\"/lw/b9/welcome_to_less_wrong/5h82\">introduction as a theist</a> or Bakkot's <a href=\"/lw/90l/welcome_to_less_wrong_2012/5kk8\">introduction as a proponent of infanticide</a>. It seems to me like there's value in trying to come up with a unified policy on how to respond to introductions. Replies seem to run the emotional gamut, and different norms often conflict. Beyond that, practical concerns suggest arguments may be poorly suited to introduction threads.</p>\n<p>Take MagnetoHydroDynamics's <a href=\"/lw/90l/welcome_to_less_wrong_2012/5k40\">introduction as not interested in cryonics</a> as an example of a smaller discussion that spawned meta-discussions about attitudes. Epistemic hygiene suggests that <a href=\"/lw/90l/welcome_to_less_wrong_2012/5k5s\">challenging wrong beliefs</a> is a positive, but I found myself dismayed to find the only reply to a newbie's first post was a factual correction, and so posted a welcoming comment with a joke. Later, wedrifid posed a test for MHD, which MHD passed. Kaj_Sotala questioned wedrifid's tone on the grounds that MHD was a newcomer, and I'm not entirely sure what to think. On the one hand, I can easily come up with a friendly rewriting on wedrifid's test, and I think friendliness to newcomers is a sound strategy. But on the other hand, while testing newcomers can turn some away, it does so in a selective way. Should newcomers be shielded from some criticism in introduction threads, or should it be the same as the rest of LW? Are there inappropriate attitudes in introduction threads?</p>\n<p>&nbsp;</p>\n<p>The practical concern is that pages only display 500 comments at once, which is why we have three threads now. But on <strong>the first day</strong> of 2012, our 2012 thread already has <strong>over half</strong> the comments it'll be able to display simultaneously. (The post has been up for a week, not a day, but <em>still</em>.) At time of writing, 150 of the 270 comments are children of Bakkot's comment, arguing about infanticide. It seems obvious to me that we ought to seriously discourage promoting discussion on ideas in introduction threads, and instead point people to open threads / a primer on writing discussion posts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GydByr7RBHZYzAqiv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 31, "extendedScore": null, "score": 8.247742132647257e-07, "legacy": true, "legacyId": "11848", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-02T04:12:11.143Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Something to Protect", "slug": "seq-rerun-something-to-protect", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:19.802Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BusipEdhNFvT5SiBv/seq-rerun-something-to-protect", "pageUrlRelative": "/posts/BusipEdhNFvT5SiBv/seq-rerun-something-to-protect", "linkUrl": "https://www.lesswrong.com/posts/BusipEdhNFvT5SiBv/seq-rerun-something-to-protect", "postedAtFormatted": "Monday, January 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Something%20to%20Protect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Something%20to%20Protect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBusipEdhNFvT5SiBv%2Fseq-rerun-something-to-protect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Something%20to%20Protect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBusipEdhNFvT5SiBv%2Fseq-rerun-something-to-protect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBusipEdhNFvT5SiBv%2Fseq-rerun-something-to-protect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/nb/something_to_protect/\">Something to Protect</a> was originally published on 30 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Many people only start to grow as a rationalist when they find something that they care about more than they care about rationality itself. It takes something really scary to cause you to override your intuitions with math.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/94u/seq_rerun_trust_in_bayes/\">Trust in Bayes</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BusipEdhNFvT5SiBv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.248392805632844e-07, "legacy": true, "legacyId": "11849", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB", "cuasP7oujr2etWpNJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-02T10:32:22.178Z", "modifiedAt": null, "url": null, "title": "An inducible group-\"meditation\" for use in rationality dojos", "slug": "an-inducible-group-meditation-for-use-in-rationality-dojos", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Sqv6SPDboNF84j38K", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mZQraFnmnocox3FHb/an-inducible-group-meditation-for-use-in-rationality-dojos", "pageUrlRelative": "/posts/mZQraFnmnocox3FHb/an-inducible-group-meditation-for-use-in-rationality-dojos", "linkUrl": "https://www.lesswrong.com/posts/mZQraFnmnocox3FHb/an-inducible-group-meditation-for-use-in-rationality-dojos", "postedAtFormatted": "Monday, January 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20inducible%20group-%22meditation%22%20for%20use%20in%20rationality%20dojos&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20inducible%20group-%22meditation%22%20for%20use%20in%20rationality%20dojos%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZQraFnmnocox3FHb%2Fan-inducible-group-meditation-for-use-in-rationality-dojos%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20inducible%20group-%22meditation%22%20for%20use%20in%20rationality%20dojos%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZQraFnmnocox3FHb%2Fan-inducible-group-meditation-for-use-in-rationality-dojos", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZQraFnmnocox3FHb%2Fan-inducible-group-meditation-for-use-in-rationality-dojos", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1691, "htmlBody": "<p>Note: The following outline of my research proposal is unfinished. I posted it in the discussion section to spur conversation and get constructive criticism (successfully, I might add). If you have any suggestions, then please make them. I will be monitoring the discussion and improving the proposal until I feel it is ready to be posted as a main article.</p>\n<p><strong>Introduction</strong></p>\n<p>I think I may have found a novel use for an old technique, which may or may not have implications for rational decision making. I am open to constructive criticism or even deconstructive criticism if you make a sound argument. Ultimately, I would like the experiment to be put to the test. If you have the supplies and know-how to carry it out, then feel free to try it and report your findings.<br /><br /><strong>The Goal</strong>:</p>\n<ul>\n<li>&nbsp;Catalyze the brainstorming process in a way that increases both the number and quality of ideas made.</li>\n</ul>\n<p><strong>Methods</strong>:</p>\n<ul>\n<li>Find a problem that needs solving. <a href=\"/lw/qt/class_project/\">Unifying general relativity and quantum mechanics</a> is a good, but ambitious, example. Some more likely problems that could be solved are: \"How might I solve my relationship problems,\" or \"how can I advertise my company's product to its target demographic\", or \"what are some ideas to make quick money.\"</li>\n<li>Find 2-3 rationalists who understand the problem well. They don't need to be expert rationalists; the most important part is that they know the difference between <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">rationalization and rationality</a>. In the QM example above, and in most scientific applications of the method, all players should have access to the experimental data.</li>\n<li>Assign two of the three rationalists to the \"brainstormers\" group (name subject to change), whose primary concern is to make logical connections between the data to form hypotheses.</li>\n<li>Assign the odd-rationalist-out as the Confessor, whose primary concern, like in <a href=\"/lw/y4/three_worlds_collide_08/\">TWC</a>, is to preserve sanity. It is the Confessor's job to catch the brainstormers when they make a logical leap or use biased reasoning. Some tactics the Confessor might use are the <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">rationalist taboo</a>, the <a href=\"http://wiki.lesswrong.com/wiki/Reversal_test\">reversal test</a>, and argument from the <a href=\"/lw/2k/the_least_convenient_possible_world/\">least convenient world</a><span style=\"font-size:8.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">.</span></li>\n<li>This is a scaled up version of what the brain seems to do. We need the brainstormers and the Confessor to act the part of <a href=\"/lw/20/the_apologist_and_the_revolutionary\">the Apologist and the Revolutionary</a>, respectively.</li>\n<li>The Confessor - brainstormer dynamic is interesting in its own right, but I believe it can be improved. Now bear with me, because the optional step is for the brainstormers to smoke Cannabis. Not too much, but just enough so that connections between ideas are more quickly apparent to them. Remember, the goal is to have the brainstormers make many connections. They need to output quantity over quality, while the Confessor picks out anything that is quality and gently guides the brainstormers toward more quality ideas. Think of it like <a href=\"http://en.wikipedia.org/wiki/R/K_selection_theory#r-selection_.28unstable_environments.29\">r-selected evolution</a>.</li>\n</ul>\n<ul>\n<li>Ideally, we would split twelve rationalists into four groups of three (the alternative is to use the same group repeatedly). Group 1 would be told to just brainstorm the problem. Group 2 would be told to choose one among them to be the Confessor. Group 3 and 4 would be told the same, but their brainstormers would be given either Cannabis or a placebo.</li>\n<li>A placebo can be made by extracting the cannabinoids using ethanol or glycerine. All that should be left after extraction is plant matter, and the tincture can be used later for medicinal or recreational purposes. The placebo Cannabis and active Cannabis will have to be rolled into joints because extraction removes some of the plant's pigmentation. If you have access to a lab, then you might follow&nbsp; <a href=\"http://www.rexresearch.com/hhusb/hh6thc.htm#HH62\">this procedure</a>&nbsp; for the extraction; otherwise, use&nbsp; <a href=\"http://www.greenbridgemed.com/how-to-make-cannabis-tinctures-at-home/\">this guide</a>&nbsp; for doing the extraction at home. </li>\n<li>If you don't want to go to the trouble of making the placebo, then you may skip the control group and only do groups 1 - 3. It would be nice to get some preliminary data, even if skewed slightly by the placebo effect.</li>\n<li>For data collection, the Confessor will note down any idea made by the brainstormers, marking the ones which were discarded. After a given amount of time, enumerate the data and compare the groups. The hypothesized result is that the smoking group will make the greatest quantity of ideas, followed by the non-smoking partitioned group, followed by the normal brainstorming group. It is also hypothesized that the smoking group will make the greatest quality ideas, due to a combination of the highly creative nature of ideas made while high (explained below) and the Confessor's job of immediately discrediting any faulty reasoning.</li>\n</ul>\n<p><strong>Some evidence that the Cannabis route might be a good one to pursue</strong> (more references to be added):</p>\n<ul>\n<li>There is evidence that Cannabis engages the mind in semantic Hyper-Priming<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20122742\"><sup>1</sup></a><sup>,</sup><a href=\"http://mindhacks.com/2010/03/09/how-cannabis-makes-thoughts-tumble/\"><sup>2</sup></a>, meaning that distantly-related concepts are primed quickly after having been exposed to an idea. For instance, a smoker might quickly respond to the word \"fish\" with \"submarine,\" whereas someone who is sober might respond with \"fin.\" If I understand it correctly, then this doesn't mean the smoker <em>cannot</em> answer \"fin,\" only that the more distantly related concepts are given a higher priority than they normally would. One can see why this might be advantageous for brainstorming, but I suggest to take my - and mindhack.com's - interpretation of the paper with a grain of salt until someone with access can read it in full.</li>\n<li>*Cannabis allows erroneous perspectives to be rapidly dismissed in the light of new evidence. While high, it is <a href=\"/lw/i9/the_importance_of_saying_oops/\">easy to put one's pride aside and say \"oops\"</a>. This is made especially easy if the user has cached understanding of the art of rationality. In other words: they will listen to the Confessor.* &lt;-- (I haven't found any literature to support this claim, yet. It seems true in my experience, but it might not be true for everyone. If the brainstormers prove to be too clingy, we could alter the method by changing the Confessor's name to Kiritsugu and having the brainstormers agree to always defer to the Kiritsugu's better judgement. The Kiritsugu will have to take care to examine its own judgement and only discard the truly irrational ideas).</li>\n</ul>\n<p><strong>Anecdotal evidence:<br /></strong></p>\n<ul>\n<li>Artists, writers, and even <a href=\"http://marijuana-uses.com/mr-x/\">scientists</a> have long used Cannabis and other psychoactive drugs as a tool to make \"insights.\" I'm defining insight as the connection and/or creation of ideas (erroneous or otherwise), possibly due to hyper-priming. The Confessor, in the early pioneers' case, was usually their sober self. As Hemingway wrote, \"write drunk; edit sober.\"</li>\n<li><a href=\"http://forum.grasscity.com/real-life-stories/480214-curious-if-anyone-does-any-brainstorming-while-toking.html\">Less gifted stoners have been doing this for ages</a>&nbsp; but they - for the most part - are completely undisciplined, believe in dubious pseudoscience, and/or don't have a rational observer to moderate them.</li>\n<li>This is going a bit meta, but the outline to the outline of this idea was made while I was high. It was the first time I smoked since having been introduced to Less Wrong and <a href=\"http://yudkowsky.net/rational/virtues\">\"The Way\"</a>, and I was surprised to find that I still had most of my wits about me. Although I would often begin down paths that were just <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">Rationalizations</a>, I usually caught myself. In the instances where I didn't catch myself, and it seemed like a legitimately good insight, I wrote the idea down for future (sober) consideration.</li>\n<li>One of the good, practical, non-meta insights I made that night was a life plan. My plan up until this point had been to finish my undergraduate degree and then immediately go to grad school, relying on my schooling and a bit of luck to maybe hopefully turn into a somewhat-successful scientist somewhere along the road. The problem is that I suffer from quite a bit of<a href=\"http://wiki.lesswrong.com/wiki/Akrasia\"></a> procrastination, in part because I don't know exactly what I want to do. I don't have any strong passions or any real motivation. My college career, so far, has been an uphill battle against crippling akrasia<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\"></a>.</li>\n<li>Aided by Cannabis, I finally saw the obvious: I need to make an effort to find a passion. My new plan is to get a job as a computer programmer after finishing undergrad, but to continue self-teaching in Biology and other sciences. I've already taken the first step by having Computer Science as my minor, and I can help my resume along <em>right now</em> by getting involved in open source projects. As for self-teaching, that's made easy by open courseware like that found on <a href=\"http://www.khanacademy.org/\">Khan Academy</a>, <a href=\"http://ocw.mit.edu/index.htm\">MIT</a>, and <a href=\"http://www.openculture.com/freeonlinecourses\">other places</a>, and I always have the old-fashioned solution of just reading textbooks. After following my interests for a while and learning what things I really, really like to learn about, <em>then</em> I'll go to grad school with an actual PhD thesis in mind and money in the bank. </li>\n<li>I'm attributing these insights (the life plan, some other ideas I'm not mentioning, and even the hypothesis itself) to hyper-priming and later editing, but they might have just been made because I was focused on the problems. Hence the need for an experimentally-controlled test. </li>\n</ul>\n<p><strong>Conclusion</strong></p>\n<ul>\n<li>Cannabis allows connections to be made between concepts which normally seem unrelated. This is an experience commonly reported by users, and experimentally verified. Some of these connections will inevitably be false, but others might be true, and a third party - a Confessor - might be able to distinguish truth from falsehood. Whether the Confessor - brainstormer dynamic is any more efficient or productive than a normal brainstorming session is an open question, and the only way to really know is to test the hypothesis.</li>\n</ul>\n<ul>\n</ul>\n<p><strong>References</strong></p>\n<div class=\"csl-bib-body\" style=\"line-height: 2; padding-left: 2em; text-indent:-2em;\">\n<div class=\"csl-entry\">How cannabis makes thoughts&nbsp;tumble. (n.d.).<em>Mind Hacks</em>. Retrieved from http://mindhacks.com/2010/03/09/how-cannabis-makes-thoughts-tumble/</div>\n<div class=\"csl-entry\">Morgan, C. J. A., Rothwell, E., Atkinson, H., Mason, O., &amp; Curran, H. V. (2010). Hyper-priming in cannabis users: a naturalistic study of the effects of cannabis on semantic memory function. <em>Psychiatry Research</em>, <em>176</em>(2-3), 213-218. doi:10.1016/j.psychres.2008.09.002</div>\n</div>\n<p><strong>What I'm missing. To be included later:<br /></strong></p>\n<ul>\n<li>References to the benefits and techniques of traditional brainstorming. In lieu of that, for now, here's&nbsp; <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">this</a>&nbsp; and&nbsp; <a href=\"/lw/hu/the_third_alternative/\">this </a>.</li>\n<li>More references to Cannabis research.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mZQraFnmnocox3FHb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 7, "extendedScore": null, "score": 8.249813515292214e-07, "legacy": true, "legacyId": "11851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Note: The following outline of my research proposal is unfinished. I posted it in the discussion section to spur conversation and get constructive criticism (successfully, I might add). If you have any suggestions, then please make them. I will be monitoring the discussion and improving the proposal until I feel it is ready to be posted as a main article.</p>\n<p><strong id=\"Introduction\">Introduction</strong></p>\n<p>I think I may have found a novel use for an old technique, which may or may not have implications for rational decision making. I am open to constructive criticism or even deconstructive criticism if you make a sound argument. Ultimately, I would like the experiment to be put to the test. If you have the supplies and know-how to carry it out, then feel free to try it and report your findings.<br><br><strong>The Goal</strong>:</p>\n<ul>\n<li>&nbsp;Catalyze the brainstorming process in a way that increases both the number and quality of ideas made.</li>\n</ul>\n<p><strong>Methods</strong>:</p>\n<ul>\n<li>Find a problem that needs solving. <a href=\"/lw/qt/class_project/\">Unifying general relativity and quantum mechanics</a> is a good, but ambitious, example. Some more likely problems that could be solved are: \"How might I solve my relationship problems,\" or \"how can I advertise my company's product to its target demographic\", or \"what are some ideas to make quick money.\"</li>\n<li>Find 2-3 rationalists who understand the problem well. They don't need to be expert rationalists; the most important part is that they know the difference between <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">rationalization and rationality</a>. In the QM example above, and in most scientific applications of the method, all players should have access to the experimental data.</li>\n<li>Assign two of the three rationalists to the \"brainstormers\" group (name subject to change), whose primary concern is to make logical connections between the data to form hypotheses.</li>\n<li>Assign the odd-rationalist-out as the Confessor, whose primary concern, like in <a href=\"/lw/y4/three_worlds_collide_08/\">TWC</a>, is to preserve sanity. It is the Confessor's job to catch the brainstormers when they make a logical leap or use biased reasoning. Some tactics the Confessor might use are the <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">rationalist taboo</a>, the <a href=\"http://wiki.lesswrong.com/wiki/Reversal_test\">reversal test</a>, and argument from the <a href=\"/lw/2k/the_least_convenient_possible_world/\">least convenient world</a><span style=\"font-size:8.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font:minor-latin;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;mso-hansi-theme-font:minor-latin;mso-bidi-font-family:&quot;Times New Roman&quot;; mso-bidi-theme-font:minor-bidi;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA\">.</span></li>\n<li>This is a scaled up version of what the brain seems to do. We need the brainstormers and the Confessor to act the part of <a href=\"/lw/20/the_apologist_and_the_revolutionary\">the Apologist and the Revolutionary</a>, respectively.</li>\n<li>The Confessor - brainstormer dynamic is interesting in its own right, but I believe it can be improved. Now bear with me, because the optional step is for the brainstormers to smoke Cannabis. Not too much, but just enough so that connections between ideas are more quickly apparent to them. Remember, the goal is to have the brainstormers make many connections. They need to output quantity over quality, while the Confessor picks out anything that is quality and gently guides the brainstormers toward more quality ideas. Think of it like <a href=\"http://en.wikipedia.org/wiki/R/K_selection_theory#r-selection_.28unstable_environments.29\">r-selected evolution</a>.</li>\n</ul>\n<ul>\n<li>Ideally, we would split twelve rationalists into four groups of three (the alternative is to use the same group repeatedly). Group 1 would be told to just brainstorm the problem. Group 2 would be told to choose one among them to be the Confessor. Group 3 and 4 would be told the same, but their brainstormers would be given either Cannabis or a placebo.</li>\n<li>A placebo can be made by extracting the cannabinoids using ethanol or glycerine. All that should be left after extraction is plant matter, and the tincture can be used later for medicinal or recreational purposes. The placebo Cannabis and active Cannabis will have to be rolled into joints because extraction removes some of the plant's pigmentation. If you have access to a lab, then you might follow&nbsp; <a href=\"http://www.rexresearch.com/hhusb/hh6thc.htm#HH62\">this procedure</a>&nbsp; for the extraction; otherwise, use&nbsp; <a href=\"http://www.greenbridgemed.com/how-to-make-cannabis-tinctures-at-home/\">this guide</a>&nbsp; for doing the extraction at home. </li>\n<li>If you don't want to go to the trouble of making the placebo, then you may skip the control group and only do groups 1 - 3. It would be nice to get some preliminary data, even if skewed slightly by the placebo effect.</li>\n<li>For data collection, the Confessor will note down any idea made by the brainstormers, marking the ones which were discarded. After a given amount of time, enumerate the data and compare the groups. The hypothesized result is that the smoking group will make the greatest quantity of ideas, followed by the non-smoking partitioned group, followed by the normal brainstorming group. It is also hypothesized that the smoking group will make the greatest quality ideas, due to a combination of the highly creative nature of ideas made while high (explained below) and the Confessor's job of immediately discrediting any faulty reasoning.</li>\n</ul>\n<p><strong>Some evidence that the Cannabis route might be a good one to pursue</strong> (more references to be added):</p>\n<ul>\n<li>There is evidence that Cannabis engages the mind in semantic Hyper-Priming<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20122742\"><sup>1</sup></a><sup>,</sup><a href=\"http://mindhacks.com/2010/03/09/how-cannabis-makes-thoughts-tumble/\"><sup>2</sup></a>, meaning that distantly-related concepts are primed quickly after having been exposed to an idea. For instance, a smoker might quickly respond to the word \"fish\" with \"submarine,\" whereas someone who is sober might respond with \"fin.\" If I understand it correctly, then this doesn't mean the smoker <em>cannot</em> answer \"fin,\" only that the more distantly related concepts are given a higher priority than they normally would. One can see why this might be advantageous for brainstorming, but I suggest to take my - and mindhack.com's - interpretation of the paper with a grain of salt until someone with access can read it in full.</li>\n<li>*Cannabis allows erroneous perspectives to be rapidly dismissed in the light of new evidence. While high, it is <a href=\"/lw/i9/the_importance_of_saying_oops/\">easy to put one's pride aside and say \"oops\"</a>. This is made especially easy if the user has cached understanding of the art of rationality. In other words: they will listen to the Confessor.* &lt;-- (I haven't found any literature to support this claim, yet. It seems true in my experience, but it might not be true for everyone. If the brainstormers prove to be too clingy, we could alter the method by changing the Confessor's name to Kiritsugu and having the brainstormers agree to always defer to the Kiritsugu's better judgement. The Kiritsugu will have to take care to examine its own judgement and only discard the truly irrational ideas).</li>\n</ul>\n<p><strong id=\"Anecdotal_evidence_\">Anecdotal evidence:<br></strong></p>\n<ul>\n<li>Artists, writers, and even <a href=\"http://marijuana-uses.com/mr-x/\">scientists</a> have long used Cannabis and other psychoactive drugs as a tool to make \"insights.\" I'm defining insight as the connection and/or creation of ideas (erroneous or otherwise), possibly due to hyper-priming. The Confessor, in the early pioneers' case, was usually their sober self. As Hemingway wrote, \"write drunk; edit sober.\"</li>\n<li><a href=\"http://forum.grasscity.com/real-life-stories/480214-curious-if-anyone-does-any-brainstorming-while-toking.html\">Less gifted stoners have been doing this for ages</a>&nbsp; but they - for the most part - are completely undisciplined, believe in dubious pseudoscience, and/or don't have a rational observer to moderate them.</li>\n<li>This is going a bit meta, but the outline to the outline of this idea was made while I was high. It was the first time I smoked since having been introduced to Less Wrong and <a href=\"http://yudkowsky.net/rational/virtues\">\"The Way\"</a>, and I was surprised to find that I still had most of my wits about me. Although I would often begin down paths that were just <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">Rationalizations</a>, I usually caught myself. In the instances where I didn't catch myself, and it seemed like a legitimately good insight, I wrote the idea down for future (sober) consideration.</li>\n<li>One of the good, practical, non-meta insights I made that night was a life plan. My plan up until this point had been to finish my undergraduate degree and then immediately go to grad school, relying on my schooling and a bit of luck to maybe hopefully turn into a somewhat-successful scientist somewhere along the road. The problem is that I suffer from quite a bit of<a href=\"http://wiki.lesswrong.com/wiki/Akrasia\"></a> procrastination, in part because I don't know exactly what I want to do. I don't have any strong passions or any real motivation. My college career, so far, has been an uphill battle against crippling akrasia<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\"></a>.</li>\n<li>Aided by Cannabis, I finally saw the obvious: I need to make an effort to find a passion. My new plan is to get a job as a computer programmer after finishing undergrad, but to continue self-teaching in Biology and other sciences. I've already taken the first step by having Computer Science as my minor, and I can help my resume along <em>right now</em> by getting involved in open source projects. As for self-teaching, that's made easy by open courseware like that found on <a href=\"http://www.khanacademy.org/\">Khan Academy</a>, <a href=\"http://ocw.mit.edu/index.htm\">MIT</a>, and <a href=\"http://www.openculture.com/freeonlinecourses\">other places</a>, and I always have the old-fashioned solution of just reading textbooks. After following my interests for a while and learning what things I really, really like to learn about, <em>then</em> I'll go to grad school with an actual PhD thesis in mind and money in the bank. </li>\n<li>I'm attributing these insights (the life plan, some other ideas I'm not mentioning, and even the hypothesis itself) to hyper-priming and later editing, but they might have just been made because I was focused on the problems. Hence the need for an experimentally-controlled test. </li>\n</ul>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<ul>\n<li>Cannabis allows connections to be made between concepts which normally seem unrelated. This is an experience commonly reported by users, and experimentally verified. Some of these connections will inevitably be false, but others might be true, and a third party - a Confessor - might be able to distinguish truth from falsehood. Whether the Confessor - brainstormer dynamic is any more efficient or productive than a normal brainstorming session is an open question, and the only way to really know is to test the hypothesis.</li>\n</ul>\n<ul>\n</ul>\n<p><strong id=\"References\">References</strong></p>\n<div class=\"csl-bib-body\" style=\"line-height: 2; padding-left: 2em; text-indent:-2em;\">\n<div class=\"csl-entry\">How cannabis makes thoughts&nbsp;tumble. (n.d.).<em>Mind Hacks</em>. Retrieved from http://mindhacks.com/2010/03/09/how-cannabis-makes-thoughts-tumble/</div>\n<div class=\"csl-entry\">Morgan, C. J. A., Rothwell, E., Atkinson, H., Mason, O., &amp; Curran, H. V. (2010). Hyper-priming in cannabis users: a naturalistic study of the effects of cannabis on semantic memory function. <em>Psychiatry Research</em>, <em>176</em>(2-3), 213-218. doi:10.1016/j.psychres.2008.09.002</div>\n</div>\n<p><strong id=\"What_I_m_missing__To_be_included_later_\">What I'm missing. To be included later:<br></strong></p>\n<ul>\n<li>References to the benefits and techniques of traditional brainstorming. In lieu of that, for now, here's&nbsp; <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">this</a>&nbsp; and&nbsp; <a href=\"/lw/hu/the_third_alternative/\">this </a>.</li>\n<li>More references to Cannabis research.</li>\n</ul>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Anecdotal evidence:", "anchor": "Anecdotal_evidence_", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"title": "What I'm missing. To be included later:", "anchor": "What_I_m_missing__To_be_included_later_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xAXrEpF5FYjwqKMfZ", "HawFh7RvDM4RyoJ2d", "neQ7eXuaXpiYw7SBy", "ZiQqsgGX6a42Sfpii", "wCqfCLs8z5Qw4GbKS", "DoLQN5ryZ9XkZjq5h", "uHYYA32CKgKT3FagE", "erGipespbbzdG5zYb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-03T06:08:04.431Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Newcomb's Problem and Regret of Rationality", "slug": "seq-rerun-newcomb-s-problem-and-regret-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:19.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/654rxsFBmku6YDpdM/seq-rerun-newcomb-s-problem-and-regret-of-rationality", "pageUrlRelative": "/posts/654rxsFBmku6YDpdM/seq-rerun-newcomb-s-problem-and-regret-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/654rxsFBmku6YDpdM/seq-rerun-newcomb-s-problem-and-regret-of-rationality", "postedAtFormatted": "Tuesday, January 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Newcomb's%20Problem%20and%20Regret%20of%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Newcomb's%20Problem%20and%20Regret%20of%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F654rxsFBmku6YDpdM%2Fseq-rerun-newcomb-s-problem-and-regret-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Newcomb's%20Problem%20and%20Regret%20of%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F654rxsFBmku6YDpdM%2Fseq-rerun-newcomb-s-problem-and-regret-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F654rxsFBmku6YDpdM%2Fseq-rerun-newcomb-s-problem-and-regret-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>Today's post, <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of Rationality</a> was originally published on 31 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Newcomb's problem is a very famous decision theory problem in which the rational move appears to be consistently punished. This is the wrong attitude to take. Rationalists should win. If your particular ritual of cognition consistently fails to yield good results, change the ritual.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/955/seq_rerun_something_to_protect/\">Something to Protect</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "654rxsFBmku6YDpdM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.254209745192161e-07, "legacy": true, "legacyId": "11871", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x", "BusipEdhNFvT5SiBv", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-03T10:20:25.440Z", "modifiedAt": null, "url": null, "title": "Simple theory of IMDB bias", "slug": "simple-theory-of-imdb-bias", "viewCount": null, "lastCommentedAt": "2020-05-19T19:51:40.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bYYyfCgWNcnqJaYGz/simple-theory-of-imdb-bias", "pageUrlRelative": "/posts/bYYyfCgWNcnqJaYGz/simple-theory-of-imdb-bias", "linkUrl": "https://www.lesswrong.com/posts/bYYyfCgWNcnqJaYGz/simple-theory-of-imdb-bias", "postedAtFormatted": "Tuesday, January 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simple%20theory%20of%20IMDB%20bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimple%20theory%20of%20IMDB%20bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbYYyfCgWNcnqJaYGz%2Fsimple-theory-of-imdb-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simple%20theory%20of%20IMDB%20bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbYYyfCgWNcnqJaYGz%2Fsimple-theory-of-imdb-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbYYyfCgWNcnqJaYGz%2Fsimple-theory-of-imdb-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p><a href=\"http://www.imdb.com/chart/top\">IMDB top 250 list</a> is dominated by old movies, which conflicts with my perception (shared by majority of people as far as I can tell) that new movies are far better than old movies (comparing either top with top or average with average).</p>\n<p>I have a simple theory why IMDB is wrong:</p>\n<p>\n<ul>\n<li>For new movies, very wide population have seen it, many not fans of the genre. They vote on IMDB soon after watching.</li>\n<li>For old movies, only narrow population of fans have seen it recently. The only people who vote on IMDB are those who've seen it recently (atypical fans), or have particularly good memories of it (atypical fans again). People who watched an old movie ages ago but don't remember much about it are very unlikely to vote on IMDB.</li>\n<li>Therefore it's much more difficult for a new movie to get a good IMDB score than it is for an old movie.</li>\n<li>Therefore a new movie with identical IMDB store is likely much better than an old movie with identical score.</li>\n</ul>\n<div>The \"correct\" procedure would of course be gathering random sample of people, showing them random movies, and asking for ratings just after the movie. For practical reasons this cannot really be done, so the next best thing we can do is ignoring old movies with unreasonably high IMDB scores.<br /></div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bYYyfCgWNcnqJaYGz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -7, "extendedScore": null, "score": 8.255153881497261e-07, "legacy": true, "legacyId": "11872", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-03T13:04:06.747Z", "modifiedAt": null, "url": null, "title": "Meetup : First Sydney 2012 meetup.", "slug": "meetup-first-sydney-2012-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:29.277Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QZcxEyKdxf9ow4tEC/meetup-first-sydney-2012-meetup", "pageUrlRelative": "/posts/QZcxEyKdxf9ow4tEC/meetup-first-sydney-2012-meetup", "linkUrl": "https://www.lesswrong.com/posts/QZcxEyKdxf9ow4tEC/meetup-first-sydney-2012-meetup", "postedAtFormatted": "Tuesday, January 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Sydney%202012%20meetup.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Sydney%202012%20meetup.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZcxEyKdxf9ow4tEC%2Fmeetup-first-sydney-2012-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Sydney%202012%20meetup.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZcxEyKdxf9ow4tEC%2Fmeetup-first-sydney-2012-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZcxEyKdxf9ow4tEC%2Fmeetup-first-sydney-2012-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5s'>First Sydney 2012 meetup.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 January 2012 06:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">22 The Promenade,, Sydney NSW 2000 (James Squire Brewhouse)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[All advice for improving location/time would be appreciated, especially location, which has not worked out amazingly well before] Hello Less Wrongers of Sydney! Assuming the local interest has not died down, seems only right that we should try and set up another meet-up.</p>\n\n<p>Given my (fairly well supported, I imagine) belief that peer pressure is one of the best methods to change behavior, I figure that the lowest hanging fruit we could discuss is trying to control Akrasia. I'm prepared to bring a structured plan and research (if there is interest) in order to achieve the following:</p>\n\n<ol>\n<li><p>Review standing literature on causation.</p></li>\n<li><p>Discuss current Akrasia control methods used by attendants, less wrong, life hacker, reddit extc. (all of which, of course, are noted for their effect AGAINST Akrasia control. Sudden thought - is there procrastination negative search engine?)</p></li>\n<li><p>Try and assign tasks to willing participants, in order to test methods and combinations thereof. I've a mind that if this goes well we could contribute as a group to the less wrong community some good old fashioned data. Plus, it's always good to measure interest in a more regular meet-up!</p></li>\n</ol></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5s'>First Sydney 2012 meetup.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QZcxEyKdxf9ow4tEC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.255766403301977e-07, "legacy": true, "legacyId": "11873", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Sydney_2012_meetup_\">Discussion article for the meetup : <a href=\"/meetups/5s\">First Sydney 2012 meetup.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 January 2012 06:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">22 The Promenade,, Sydney NSW 2000 (James Squire Brewhouse)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>[All advice for improving location/time would be appreciated, especially location, which has not worked out amazingly well before] Hello Less Wrongers of Sydney! Assuming the local interest has not died down, seems only right that we should try and set up another meet-up.</p>\n\n<p>Given my (fairly well supported, I imagine) belief that peer pressure is one of the best methods to change behavior, I figure that the lowest hanging fruit we could discuss is trying to control Akrasia. I'm prepared to bring a structured plan and research (if there is interest) in order to achieve the following:</p>\n\n<ol>\n<li><p>Review standing literature on causation.</p></li>\n<li><p>Discuss current Akrasia control methods used by attendants, less wrong, life hacker, reddit extc. (all of which, of course, are noted for their effect AGAINST Akrasia control. Sudden thought - is there procrastination negative search engine?)</p></li>\n<li><p>Try and assign tasks to willing participants, in order to test methods and combinations thereof. I've a mind that if this goes well we could contribute as a group to the less wrong community some good old fashioned data. Plus, it's always good to measure interest in a more regular meet-up!</p></li>\n</ol></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Sydney_2012_meetup_1\">Discussion article for the meetup : <a href=\"/meetups/5s\">First Sydney 2012 meetup.</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Sydney 2012 meetup.", "anchor": "Discussion_article_for_the_meetup___First_Sydney_2012_meetup_", "level": 1}, {"title": "Discussion article for the meetup : First Sydney 2012 meetup.", "anchor": "Discussion_article_for_the_meetup___First_Sydney_2012_meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-03T18:37:38.021Z", "modifiedAt": null, "url": null, "title": "Describe your personal Mount Stupid", "slug": "describe-your-personal-mount-stupid", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8XZPFGLJJvftnc3bm/describe-your-personal-mount-stupid", "pageUrlRelative": "/posts/8XZPFGLJJvftnc3bm/describe-your-personal-mount-stupid", "linkUrl": "https://www.lesswrong.com/posts/8XZPFGLJJvftnc3bm/describe-your-personal-mount-stupid", "postedAtFormatted": "Tuesday, January 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Describe%20your%20personal%20Mount%20Stupid&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADescribe%20your%20personal%20Mount%20Stupid%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8XZPFGLJJvftnc3bm%2Fdescribe-your-personal-mount-stupid%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Describe%20your%20personal%20Mount%20Stupid%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8XZPFGLJJvftnc3bm%2Fdescribe-your-personal-mount-stupid", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8XZPFGLJJvftnc3bm%2Fdescribe-your-personal-mount-stupid", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>A little knowledge is a&nbsp;<a title=\"SMBC comic\" href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2475#comic\">dangerous thing</a>, not sure what the official name for this particular cognitive bias is (feel free to enlighten me). Probably most of us can recognize that feeling of enlightenment after learning a bit of something new and exciting, and not realizing yet how far it is from the mastery of the subject. I suspect that learning the LW brand of rationality is one of those. (Incidentally, if the words \"LW brand of rationality\" irked you, because you think that there is only one true rationality, consider how close you might be to that particular summit of Mt. Stupid.) See also the last bullet point in the linked comic strip.</p>\n<p>As an exercise in rationality, I suggest people post personal accounts of successfully traversing Mt.Stupid, or maybe getting stuck there forever, never to be heard from again. Did you find any of the techniques described in the sequences useful to overcome this bias, beyond the obvious of continuing to learn more about the topic in question? Did you manage to avoid turning Mt.Stupid into the Loggerhead range?</p>\n<p>My example: I thought I was great at programming fresh out of college, and ready to dispense my newly found wisdom. Boy, oh boy, was I ever wrong. And then it happened again when I learned some more of the subject on the job...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8XZPFGLJJvftnc3bm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 14, "extendedScore": null, "score": 8.257014687980632e-07, "legacy": true, "legacyId": "11874", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-03T19:46:46.587Z", "modifiedAt": null, "url": null, "title": "[META] Trackbacks", "slug": "meta-trackbacks", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pEKup37dSCWLfyZo8/meta-trackbacks", "pageUrlRelative": "/posts/pEKup37dSCWLfyZo8/meta-trackbacks", "linkUrl": "https://www.lesswrong.com/posts/pEKup37dSCWLfyZo8/meta-trackbacks", "postedAtFormatted": "Tuesday, January 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Trackbacks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Trackbacks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpEKup37dSCWLfyZo8%2Fmeta-trackbacks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Trackbacks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpEKup37dSCWLfyZo8%2Fmeta-trackbacks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpEKup37dSCWLfyZo8%2Fmeta-trackbacks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p>When I was going through the sequences, I often found that reading about a fallacy in passing, like when it was hyperlinked in the middle of a sentence, was more really helped me <a href=\"http://www.google.com/url?q=http://lesswrong.com/lw/8a5/seq_rerun_truly_part_of_you/&amp;sa=U&amp;ei=gVoDT_qiL8iggwf0q-CQAg&amp;ved=0CAYQFjAB&amp;client=internal-uds-cse&amp;usg=AFQjCNG-EOFC_pei1xdA0XtrDKALNYGSfw\">get the idea.&nbsp;</a>&nbsp;<br /><br />I know that on the wiki, there is a feature were you can see all the <a href=\"http://wiki.lesswrong.com/wiki/Special:WhatLinksHere/LessWrong_Wiki\">trackbacks to a page.&nbsp;</a>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Is there a way to do this for non-wiki pages? This could be useful even for non-LW pages.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pEKup37dSCWLfyZo8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.257273513301869e-07, "legacy": true, "legacyId": "11876", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-03T21:16:33.626Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-5", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:20.867Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PgQRqscgxHiddFqp6/meetup-austin-tx-5", "pageUrlRelative": "/posts/PgQRqscgxHiddFqp6/meetup-austin-tx-5", "linkUrl": "https://www.lesswrong.com/posts/PgQRqscgxHiddFqp6/meetup-austin-tx-5", "postedAtFormatted": "Tuesday, January 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPgQRqscgxHiddFqp6%2Fmeetup-austin-tx-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPgQRqscgxHiddFqp6%2Fmeetup-austin-tx-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPgQRqscgxHiddFqp6%2Fmeetup-austin-tx-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5t'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 January 2012 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Welcome back! The Austin meetup returns to Caffe Medici after a brief winter hiatus. We sit on the second floor to the left, near (or often on) the stage. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5t'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PgQRqscgxHiddFqp6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.257609627385918e-07, "legacy": true, "legacyId": "11877", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/5t\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 January 2012 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Welcome back! The Austin meetup returns to Caffe Medici after a brief winter hiatus. We sit on the second floor to the left, near (or often on) the stage. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/5t\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-03T22:54:45.081Z", "modifiedAt": null, "url": null, "title": "Utilitarians probably wasting time on recreation", "slug": "utilitarians-probably-wasting-time-on-recreation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:26.922Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nebulous", "createdAt": "2011-11-23T06:28:39.231Z", "isAdmin": false, "displayName": "nebulous"}, "userId": "nqrJWFKPS8dLDDjmi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/47FLmN6KdfaY5MRZd/utilitarians-probably-wasting-time-on-recreation", "pageUrlRelative": "/posts/47FLmN6KdfaY5MRZd/utilitarians-probably-wasting-time-on-recreation", "linkUrl": "https://www.lesswrong.com/posts/47FLmN6KdfaY5MRZd/utilitarians-probably-wasting-time-on-recreation", "postedAtFormatted": "Tuesday, January 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utilitarians%20probably%20wasting%20time%20on%20recreation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtilitarians%20probably%20wasting%20time%20on%20recreation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47FLmN6KdfaY5MRZd%2Futilitarians-probably-wasting-time-on-recreation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utilitarians%20probably%20wasting%20time%20on%20recreation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47FLmN6KdfaY5MRZd%2Futilitarians-probably-wasting-time-on-recreation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47FLmN6KdfaY5MRZd%2Futilitarians-probably-wasting-time-on-recreation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 528, "htmlBody": "<p>[Post edited to use life expectancy data from estimated time of birth rather than from 2012 and avoid extra significant digits.]</p>\n<p>[Edited again to make the title more to the point and less abrasive, change the math since I found that Uganda is not one of their top four countries aided, include an accurate figure for the average age of an AMF beneficiary, link to sources on life expectancy and mosquito net distribution data, and improve some wording.]</p>\n<p>&nbsp;</p>\n<p>This post argues that working a job and donating the resultant money to the Against Malaria Foundation (AMF) is more beneficial than recreation from a utilitarian standpoint.<br /><br />AMF, GiveWell's current top rated charity, distributes mosquito nets to people at high risk of contracting and dying from malaria. To find the amount of life saved by donating a dollar to AMF, I use the following formula: (average life expectancy in aided country - average age of beneficiary) / dollars AMF needs to save one life.<br /><br />According to an email from AMF representative Rob Mather, the average age of an AMF beneficiary is 25-30. I'll pick the age 28 to be conservative on the amount of life saved per donation. I made a weighted average by nets distributed of the life expectancies of the top three countries that AMF has worked in (Zambia, Malawi, and Tanzania) to estimate the average life expectancy in a typical AMF-aided country.</p>\n<p>Zambia has 332,660 nets distributed, Malawi has 355,400 nets distributed, and Tanzania has 131,293 nets distributed, for a total of 819,353. Zambia has ~41 percent of nets distributed among the top three, while Malawi has ~43 percent and Tanzania has ~16 percent. Zambia's life expectancy for the average 28-year-old beneficiary is 51.56 years, Malawi's is 51.08 years, and Tanzania's is 45.75 years. The average life expectancy for an AMF beneficiary in the top three aided countries multiplies and adds up to ~50.42 years. (<a href=\"http://www.google.com/publicdata/explore?ds=d5bncppjof8f9_&amp;met_y=sp_dyn_le00_in&amp;idim=country:ZMB&amp;dl=en&amp;hl=en&amp;q=zambia+life+expectancy#ctype=l&amp;strail=false&amp;bcs=d&amp;nselm=h&amp;met_y=sp_dyn_le00_in&amp;scale_y=lin&amp;ind_y=false&amp;rdim=country&amp;idim=country:ZMB:TZA:MWI&amp;ifdim=country&amp;hl=en&amp;dl=en\">Source on life expectancy.</a> <a href=\"http://www.againstmalaria.com/Distributions_TopLevel.aspx\">Source on net distribution.</a>)</p>\n<p>This means that the time saved per life saved is ~22 years. According to GiveWell, AMF needs just under two thousand dollars to save a life. 22 divided by two thousand is ~0.011 years saved per dollar, or ~4.0 days saved per dollar. Suppose that you gave up some recreation time and instead worked some part-time job such as filling out online surveys for five dollars an hour. If each dollar was donated to AMF, that would save ~20 days per hour, or ~480 hours per hour. If the highest-paying job you could work in your recreation time pays five dollars an hour, then to justify your spending time on recreation rather than on working and donating the money to AMF within an altruistic morality, your recreation time would need to be ~480 times as valuable as an equivalent amount of time in a third world person's life. Your recreation time would need to be even more valuable if a higher-paying job was available. Just multiply the available hourly salary by the amount of life AMF can save per dollar to find how much life you can save per hour.<br /><br />If anyone has more accurate figures, please post them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "47FLmN6KdfaY5MRZd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -10, "extendedScore": null, "score": 8.257977241336703e-07, "legacy": true, "legacyId": "11878", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T00:31:38.377Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta, GA", "slug": "meetup-atlanta-ga", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dFfHuiZDZyF6GwTKo/meetup-atlanta-ga", "pageUrlRelative": "/posts/dFfHuiZDZyF6GwTKo/meetup-atlanta-ga", "linkUrl": "https://www.lesswrong.com/posts/dFfHuiZDZyF6GwTKo/meetup-atlanta-ga", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%2C%20GA&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%2C%20GA%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdFfHuiZDZyF6GwTKo%2Fmeetup-atlanta-ga%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%2C%20GA%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdFfHuiZDZyF6GwTKo%2Fmeetup-atlanta-ga", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdFfHuiZDZyF6GwTKo%2Fmeetup-atlanta-ga", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/5u\">Atlanta Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 January 2012 06:30:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The next meetup is Saturday, January 7th at 6:30pm at Chocolate Coffee in Decatur:</p>\n<div class=\"md\"><a rel=\"nofollow\" href=\"http://www.mychocolatecoffee.com/\">http://www.mychocolatecoffee.com/</a><br /> 2094 North Decatur Road, Decatur, GA 30033-5367<br /> (404) 982-0790</div>\n<div class=\"md\"><br /></div>\n<p>Here is the official agenda:</p>\n<p><a rel=\"nofollow\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions</a> <br />1.6 Focus Your Uncertainty <br />1.7 The Virtue of Narrowness <br />1.8 Your Strength As A Rationalist <br />1.9 Absence of Evidence is Evidence of Absence <br />1.10 Conservation of Expected Evidence</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/5u\">Atlanta Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dFfHuiZDZyF6GwTKo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.258340006592298e-07, "legacy": true, "legacyId": "11880", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Meetup\">Discussion article for the meetup : <a href=\"/meetups/5u\">Atlanta Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 January 2012 06:30:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The next meetup is Saturday, January 7th at 6:30pm at Chocolate Coffee in Decatur:</p>\n<div class=\"md\"><a rel=\"nofollow\" href=\"http://www.mychocolatecoffee.com/\">http://www.mychocolatecoffee.com/</a><br> 2094 North Decatur Road, Decatur, GA 30033-5367<br> (404) 982-0790</div>\n<div class=\"md\"><br></div>\n<p>Here is the official agenda:</p>\n<p><a rel=\"nofollow\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions</a> <br>1.6 Focus Your Uncertainty <br>1.7 The Virtue of Narrowness <br>1.8 Your Strength As A Rationalist <br>1.9 Absence of Evidence is Evidence of Absence <br>1.10 Conservation of Expected Evidence</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/5u\">Atlanta Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Meetup", "anchor": "Discussion_article_for_the_meetup___Atlanta_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Meetup", "anchor": "Discussion_article_for_the_meetup___Atlanta_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T01:20:36.415Z", "modifiedAt": null, "url": null, "title": "[link] Anger as antidote to Confirmation Bias ", "slug": "link-anger-as-antidote-to-confirmation-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:34.054Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A9d7rkedHqPktGgyP/link-anger-as-antidote-to-confirmation-bias", "pageUrlRelative": "/posts/A9d7rkedHqPktGgyP/link-anger-as-antidote-to-confirmation-bias", "linkUrl": "https://www.lesswrong.com/posts/A9d7rkedHqPktGgyP/link-anger-as-antidote-to-confirmation-bias", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Anger%20as%20antidote%20to%20Confirmation%20Bias%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Anger%20as%20antidote%20to%20Confirmation%20Bias%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9d7rkedHqPktGgyP%2Flink-anger-as-antidote-to-confirmation-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Anger%20as%20antidote%20to%20Confirmation%20Bias%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9d7rkedHqPktGgyP%2Flink-anger-as-antidote-to-confirmation-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9d7rkedHqPktGgyP%2Flink-anger-as-antidote-to-confirmation-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p><a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1488605 \">Abstract</a>:</p>\n<blockquote>The current research explores the effect of anger on hypothesis confirmation &mdash; the propensity to seek information that confirms rather than disconfirms one&rsquo;s opinion. We argue that the moving against action tendency associated with anger leads angry individuals to seek out disconfirming evidence, attenuating the confirmation bias. We test this hypothesis in two studies of experimentally-primed anger and sadness on the selective exposure to hypothesis confirming and disconfirming information. In Study 1, participants in the angry condition were more likely to choose disconfirming information than those in the sad or neutral condition when given the opportunity to read about a controversial social issue. Study 2 measured participants&rsquo; opinions and information selection about the 2008 Presidential Election and the desire to &lsquo;move against&rsquo; a person or object. Participants in the angry condition reported a greater tendency to oppose a person or object, and this tendency led them to select more disconfirming information.</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A9d7rkedHqPktGgyP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 8.258523358486129e-07, "legacy": true, "legacyId": "11884", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T04:53:11.481Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup", "slug": "meetup-fort-collins-colorado-meetup-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.088Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FoboDpPK5c44RycZa/meetup-fort-collins-colorado-meetup-3", "pageUrlRelative": "/posts/FoboDpPK5c44RycZa/meetup-fort-collins-colorado-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/FoboDpPK5c44RycZa/meetup-fort-collins-colorado-meetup-3", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoboDpPK5c44RycZa%2Fmeetup-fort-collins-colorado-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoboDpPK5c44RycZa%2Fmeetup-fort-collins-colorado-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFoboDpPK5c44RycZa%2Fmeetup-fort-collins-colorado-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5v'>Fort Collins, Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 January 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're planning a field trip to a local 3D printer shop.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5v'>Fort Collins, Colorado Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FoboDpPK5c44RycZa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.25931943828781e-07, "legacy": true, "legacyId": "11893", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup\">Discussion article for the meetup : <a href=\"/meetups/5v\">Fort Collins, Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 January 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're planning a field trip to a local 3D printer shop.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/5v\">Fort Collins, Colorado Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T05:18:58.159Z", "modifiedAt": null, "url": null, "title": "Visualizing effect sizes", "slug": "visualizing-effect-sizes", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NG4G7vFdD68NbpSJj/visualizing-effect-sizes", "pageUrlRelative": "/posts/NG4G7vFdD68NbpSJj/visualizing-effect-sizes", "linkUrl": "https://www.lesswrong.com/posts/NG4G7vFdD68NbpSJj/visualizing-effect-sizes", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Visualizing%20effect%20sizes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVisualizing%20effect%20sizes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNG4G7vFdD68NbpSJj%2Fvisualizing-effect-sizes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Visualizing%20effect%20sizes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNG4G7vFdD68NbpSJj%2Fvisualizing-effect-sizes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNG4G7vFdD68NbpSJj%2Fvisualizing-effect-sizes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>http://healthyinfluence.com/wordpress/steves-primer-of-practical-persuasion-3-0/intro/windowpane/</p>\n<p>\"The point of this demonstration is to show that you can think with  numbers in a practical and efficient way without having a statistician  in the room.&nbsp; Anyone can handle the windowpane approach with numbers.&nbsp;  Just have a clear definition of Changed? (Yes or No) and a clear  definition of the Group (Treatment or Control).&nbsp; Then just count and  look for percentage differences.&nbsp; A 10% difference is small, 30% is  moderate, and 50% is large.&nbsp; And, realize that while &ldquo;small&rdquo; may be hard  to detect, it can definitely make big practical effect.</p>\n<p>Now whether you conceptualize Effect Sizes as windowpanes or jars  with marbles, you now understand what the idea, Difference, means.&nbsp; You  can count or see No, Small, Medium, or Large Differences and interpret  those complex statistical arguments you encounter all the time.&nbsp; Realize  again, that this approach is not Statistics for Dummies, Idiots, or  Fools, but is a standard and mathematically correct way to present  quantitative information.\"</p>\n<p>http://www.psychologicalscience.org/journals/pspi/pspi_8_2_article.pdf</p>\n<p>tldr; Natural frequencies (ratios of counts of subjects) rather than Conditional probabilities, are easier for people to comprehend.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NG4G7vFdD68NbpSJj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 8.259415979881982e-07, "legacy": true, "legacyId": "11896", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T11:15:04.756Z", "modifiedAt": null, "url": null, "title": "HPMoR.com", "slug": "hpmor-com", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QhwBg9TwFBtaBJ2Qo/hpmor-com", "pageUrlRelative": "/posts/QhwBg9TwFBtaBJ2Qo/hpmor-com", "linkUrl": "https://www.lesswrong.com/posts/QhwBg9TwFBtaBJ2Qo/hpmor-com", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20HPMoR.com&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHPMoR.com%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhwBg9TwFBtaBJ2Qo%2Fhpmor-com%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=HPMoR.com%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhwBg9TwFBtaBJ2Qo%2Fhpmor-com", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhwBg9TwFBtaBJ2Qo%2Fhpmor-com", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p><a href=\"http://www.elsewhere.org/journal/about/\">Josh's</a> mirror of <em>Harry Potter and the Methods of Rationality</em>&nbsp;has been redesigned by <a href=\"/user/Lightwave/\">Lightwave</a> (who also did <a href=\"http://intelligenceexplosion.com/\">IntelligenceExplosion.com</a>, <a href=\"http://friendly-ai.com/\">Friendly-AI.com</a>, and <a href=\"http://lukeprog.com/\">lukeprog.com</a>), and it is now located at a simpler URL:&nbsp;<a href=\"http://hpmor.com/\">HPMoR.com</a>. Thanks also to <a href=\"/user/Louie\">Louie</a> who put together this \"facelift\" project.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QhwBg9TwFBtaBJ2Qo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 24, "extendedScore": null, "score": 8.26074986279444e-07, "legacy": true, "legacyId": "11901", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T14:39:52.079Z", "modifiedAt": null, "url": null, "title": "[Link] Duolingo", "slug": "link-duolingo", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bradm", "createdAt": "2011-11-09T17:56:00.702Z", "isAdmin": false, "displayName": "bradm"}, "userId": "Lgua9P4XifSEuXkud", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uzeJNEw8eHut3HnEQ/link-duolingo", "pageUrlRelative": "/posts/uzeJNEw8eHut3HnEQ/link-duolingo", "linkUrl": "https://www.lesswrong.com/posts/uzeJNEw8eHut3HnEQ/link-duolingo", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Duolingo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Duolingo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuzeJNEw8eHut3HnEQ%2Flink-duolingo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Duolingo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuzeJNEw8eHut3HnEQ%2Flink-duolingo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuzeJNEw8eHut3HnEQ%2Flink-duolingo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<p>Yesterday I heard about an interesting new project called <a href=\"http://duolingo.com/\">Duolingo</a>. &nbsp;For some background, see <a href=\"http://youtu.be/cQl6jUjFjp4\">this TEDx Talk</a> by one of the creators,&nbsp;Luis von Ahn. &nbsp;It is a crowdsourcing approach to language translation, where the users learn a foreign language while translating. &nbsp;I've signed up but I haven't received an invitation yet to start. &nbsp;Once I start using it, I will provide updates on its effectiveness.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uzeJNEw8eHut3HnEQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 11, "extendedScore": null, "score": 8.261517113776517e-07, "legacy": true, "legacyId": "11802", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T18:58:49.233Z", "modifiedAt": null, "url": null, "title": "[META] 'Rational' vs 'Optimized'", "slug": "meta-rational-vs-optimized", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TheOtherDave", "createdAt": "2010-10-21T20:41:01.109Z", "isAdmin": false, "displayName": "TheOtherDave"}, "userId": "PTt4TYCrNnWYzd6Ky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SEzE9sFQ54MNpKw6f/meta-rational-vs-optimized", "pageUrlRelative": "/posts/SEzE9sFQ54MNpKw6f/meta-rational-vs-optimized", "linkUrl": "https://www.lesswrong.com/posts/SEzE9sFQ54MNpKw6f/meta-rational-vs-optimized", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20'Rational'%20vs%20'Optimized'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20'Rational'%20vs%20'Optimized'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEzE9sFQ54MNpKw6f%2Fmeta-rational-vs-optimized%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20'Rational'%20vs%20'Optimized'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEzE9sFQ54MNpKw6f%2Fmeta-rational-vs-optimized", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEzE9sFQ54MNpKw6f%2Fmeta-rational-vs-optimized", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 405, "htmlBody": "<p>A new arrival, <a href=\"/user/Kouran\">Kouran</a>, recently challenged our conventional use of the label \"rational\" to describe various systems. The full thread is <a href=\"/lw/90l/welcome_to_less_wrong_2012/5j57\">here</a>, and it doesn't summarize neatly, but he observes that we often use \"rational\" in the context of non-intellectual, non-cognitive, etc. systems, and that this is an unconventional use of the word.<br /><br />Unsurprisingly, this led to Standard Conversation Number 12 about how we don't really use \"rational\" to mean what the rest of the world means by it, and about instrumental rationality, and etc. and etc. In the course of that discussion I made the observation a couple of times (<a href=\"/lw/90l/welcome_to_less_wrong_2012/5lry\">here</a> and <a href=\"/lw/90l/welcome_to_less_wrong_2012/5lrx\">here</a>) that we could probably substitute some form of \"optimal\" for \"rational\" wherever it appears without losing any information.</p>\n<p>Of course, status quo bias being what it is, I promptly added that we wouldn't actually want to do that, because, y'know, it would be work and involve changing stuff.<br /><br />But the more I think about it, the more it seems like I <em>ought </em>to endorse that lexical shift. We do spend a not-inconsiderable amount of time and attention on alleviating undesirable side-effects of the word 'rational,' such as the Spock effect, and our occasional annoying tendency to talk about the 'rational' choice of shoe-polish when we really mean the optimal choice, and our occasional tendency to tie ourselves in knots around \"rationalists should <em>win</em>\". (That optimized systems do better than non-optimized systems is pretty much the <em>definition </em>of \"optimized,\" after all. If we say that rational systems generally do better than irrational systems, we're saying that rational systems are generally optimal, which is a non-empty statement. But if we define \"rational\" to mean the thing that wins, which we sometimes do, it seems simpler to talk about optimized systems in the first place.)</p>\n<p>There's precedent for this... a while ago I started getting out of the habit of talking about \"artificial intelligences\" when I really wanted to talk about superhuman optimizing systems instead, and I continue to endorse that change. So, I'm going to stop using \"rational\" when I actually mean optimal. I encourage others to do so as well. (Or, conversely, to tell me why I shouldn't.)<br /><br />This should go without saying, but in case it doesn't: I'm not proposing recoding anything or rewriting anything or doing any work here beyond changing my use of language as it's convenient for me to do so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "p8nXWqwPH7mPSZf6p": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SEzE9sFQ54MNpKw6f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 44, "extendedScore": null, "score": 8.26248747269722e-07, "legacy": true, "legacyId": "11903", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T19:25:58.947Z", "modifiedAt": null, "url": null, "title": "Anger To Warm Fuzzies With DNS", "slug": "anger-to-warm-fuzzies-with-dns", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:21.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Db6GzMK24ozFArE2r", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TbLFkT38EPzHL3n4g/anger-to-warm-fuzzies-with-dns", "pageUrlRelative": "/posts/TbLFkT38EPzHL3n4g/anger-to-warm-fuzzies-with-dns", "linkUrl": "https://www.lesswrong.com/posts/TbLFkT38EPzHL3n4g/anger-to-warm-fuzzies-with-dns", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anger%20To%20Warm%20Fuzzies%20With%20DNS&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnger%20To%20Warm%20Fuzzies%20With%20DNS%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTbLFkT38EPzHL3n4g%2Fanger-to-warm-fuzzies-with-dns%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anger%20To%20Warm%20Fuzzies%20With%20DNS%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTbLFkT38EPzHL3n4g%2Fanger-to-warm-fuzzies-with-dns", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTbLFkT38EPzHL3n4g%2Fanger-to-warm-fuzzies-with-dns", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<p>We have a third-party DNS provider at my work that blocks certain sites. When a site is blocked I get redirected to a web page filled with ads to support the provider. I do not want to support the provider. I can't use something like Goolge DNS since our internal server names resolve through the same provider. Also, for various reasons, it would be a pain to maintain host file entries for the internal servers. So, considering myself stuck with the blocking, I initially made that URL redirect to a custom web page that just said \"Blocked\". Then I thought, instead of just seeing that message and getting angry I can <a title=\"Purchase Fuzzies and Utilons Separately\" href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\" target=\"_blank\">purchase some fuzzies</a>&nbsp;and&nbsp;set out to find a site that deserved the ad revenue.&nbsp;Now, I'm not entirely sure ad impression revenue from&nbsp;<a href=\"http://www.causes.com/causes/7416-singularity-institute-for-artificial-intelligence?recruiter_id=141717962\">http://www.causes.com/causes/7416-singularity-institute-for-artificial-intelligence?recruiter_id=141717962</a>&nbsp;goes to SI and I wouldn't mind if it went to causes.com, but it was the only SI related site I could find with ads. If anyone thinks any site for any charity may be <a title=\"Efficient Charity: Do Unto Others...\" href=\"/lw/3gj/efficient_charity_do_unto_others/\" target=\"_blank\">more&nbsp;efficient</a>&nbsp;(I know this idea isn't extremely efficient in the first place) I'd like to consider it.</p>\n<p>I'm not sure what the average level of computer knowledge is on LW. For now, since I'm lazy, I'll assume it's high and just say, if you'd like to to do something like this <em>and you have admin rights on your machine</em>, just add a host file entry to redirect the \"blocked site\" URL to localhost and create a webpage that just redirects to your preferred charity site with ads. If you'd like more detailed help for Windows or Linux (and maybe Mac), I may be able to help. Also, if anyone thinks of a less involved way of accomplishing the same results I'd like to hear about it, +1 if it works for all browsers without individual configuration, +2 if it doesn't require admin rights, +bleem if both.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TbLFkT38EPzHL3n4g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -4, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "11902", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF", "pC47ZTsPNAkjavkXs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-04T21:31:33.387Z", "modifiedAt": null, "url": null, "title": "[LINK] Antidepressants: Bad Drugs... Or Bad Patients?", "slug": "link-antidepressants-bad-drugs-or-bad-patients", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.220Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wallowinmaya", "createdAt": "2011-03-21T00:39:18.855Z", "isAdmin": false, "displayName": "David Althaus"}, "userId": "xY8DDzk6TyvRroJEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5yZeqewqDcQGkuToq/link-antidepressants-bad-drugs-or-bad-patients", "pageUrlRelative": "/posts/5yZeqewqDcQGkuToq/link-antidepressants-bad-drugs-or-bad-patients", "linkUrl": "https://www.lesswrong.com/posts/5yZeqewqDcQGkuToq/link-antidepressants-bad-drugs-or-bad-patients", "postedAtFormatted": "Wednesday, January 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Antidepressants%3A%20Bad%20Drugs...%20Or%20Bad%20Patients%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Antidepressants%3A%20Bad%20Drugs...%20Or%20Bad%20Patients%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yZeqewqDcQGkuToq%2Flink-antidepressants-bad-drugs-or-bad-patients%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Antidepressants%3A%20Bad%20Drugs...%20Or%20Bad%20Patients%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yZeqewqDcQGkuToq%2Flink-antidepressants-bad-drugs-or-bad-patients", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yZeqewqDcQGkuToq%2Flink-antidepressants-bad-drugs-or-bad-patients", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 299, "htmlBody": "<p><a href=\"http://neuroskeptic.blogspot.com/2012/01/antidepressants-bad-drugs-or-bad.html\">Illuminating post</a> on <em>Neuroskeptic</em>:</p>\n<p>Some Quotes:</p>\n<blockquote>\n<p>Why is it that modern trials of antidepressant drugs <em>increasingly show no benefit of the drugs over placebo</em>?.....</p>\n</blockquote>\n<blockquote>\n<p>They suggest that maybe it's the patients fault: Participation that is induced by cash payments may lead subjects to exaggerate their symptoms [i.e. in order to get included into the trial]... Another contributing factor to high placebo response rates may be the extent to which the volunteers in antidepressant trials are really generalizable to patients in clinical practice.</p>\n</blockquote>\n<blockquote>\n<p><span style=\"color: #000000;\">Since the initial antidepressant trials in the 1960s, participants have gone from being patients who were recruited primarily from inpatient psychiatric populations to outpatient volunteers who are often recruited by advertisements. At times, these symptomatic volunteers have participated in other trials. When we contact potential participants to schedule screening, they often ask to be reminded which trial we are screening for or mistake our research trial for a different protocol in which they recently participated.</span></p>\n</blockquote>\n<blockquote>\n<p>A few years ago I was running a study recruiting people who'd recovered from psychiatric illness. The main source of volunteers was online adverts..... We recruited about 20 people. No fewer than 3 turned out to have enrolled in other studies and lied about it. After I realized this I Googled the offender's names and two of them turned up in the court pages of the local newspaper pleading guilty to various petty crimes.</p>\n</blockquote>\n<blockquote>\n<p>In my view, the authors miss out on the real problem with recruiting depressed people through adverts:&nbsp; depressed people don't tend to respond to adverts, because depressed people don't do anything. That's why they call it depression.</p>\n</blockquote>\n<blockquote>\n<p>So while you wouldn't go looking for aquaphobic people in a swimming  pool, I'm not sure we should be looking for depressed people through  adverts.</p>\n</blockquote>\n<p>Could similar mechanisms hold true for other drugs?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5yZeqewqDcQGkuToq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 8.263059906191869e-07, "legacy": true, "legacyId": "11904", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T00:20:55.708Z", "modifiedAt": null, "url": null, "title": "Professional Patients: Fraud that ruins studies", "slug": "professional-patients-fraud-that-ruins-studies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QMAv7ir5KbmFb3Xwg/professional-patients-fraud-that-ruins-studies", "pageUrlRelative": "/posts/QMAv7ir5KbmFb3Xwg/professional-patients-fraud-that-ruins-studies", "linkUrl": "https://www.lesswrong.com/posts/QMAv7ir5KbmFb3Xwg/professional-patients-fraud-that-ruins-studies", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Professional%20Patients%3A%20Fraud%20that%20ruins%20studies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProfessional%20Patients%3A%20Fraud%20that%20ruins%20studies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQMAv7ir5KbmFb3Xwg%2Fprofessional-patients-fraud-that-ruins-studies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Professional%20Patients%3A%20Fraud%20that%20ruins%20studies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQMAv7ir5KbmFb3Xwg%2Fprofessional-patients-fraud-that-ruins-studies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQMAv7ir5KbmFb3Xwg%2Fprofessional-patients-fraud-that-ruins-studies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 631, "htmlBody": "<p>I just read&nbsp;<a href=\"http://neuroskeptic.blogspot.com/2012/01/antidepressants-bad-drugs-or-bad.html\">Antidepressants: Bad Drugs... Or Bad Patients</a>,&nbsp;linked by wallowinmaya in a <a href=\"/r/discussion/lw/96o/link_antidepressants_bad_drugs_or_bad_patients/\">discussion post</a>&nbsp;and based on the journal articles&nbsp;<a href=\"http://ajp.psychiatryonline.org/article.aspx?articleid=181045\">Antidepressant Clinical Trials and Subject Recruitment: Just Who Are Symptomatic Volunteers?</a>&nbsp;and <a href=\"http://article.psychiatrist.com/dao_1-login.asp?ID=10007575&amp;RSID=65065571361575\">Failure Rate and \"Professional Subjects\" in Clinical Trials of Major Depressive Disorder</a>&nbsp;(paywalled). The authors of the latter paper \"were told anonymously by trial sponsors that duplicate subjects in some protocols have been as high as 5%\", and write \"we believe that failure rates are rising due to the increase in \"professional subjects,\" who go from site to site, learning inclusion and exclusion criteria and collecting stipends.\"</p>\n<p>Aha! <em>No wonder so much antidepressant research is crap</em>. How many people do you suppose there are, who sign up for lots of trials at once? They'd have to defraud the researchers, of course; no one would allow a patient like that into their study knowingly. What do you suppose that sort of person would do, and how would it be reflected in the data? What other types of studies are affected? And how would we find out? (Dietary studies look vulnerable, and their results have been conspicuously unreliable. On the other hand, lots of people want to lose weight, so legitimate subjects are probably plentiful and drive down the percentage).</p>\n<p><a id=\"more\"></a>Let's start with some speculative modeling. First, to be clear: we're talking about people who participate in multiple studies, and lie about it. That means defrauding researchers, and they know it. If they're doing so rationally, then a lot of their responses are going to be lies, designed to maximize the chance they get paid, and minimize the chance they get caught.&nbsp;Now,&nbsp;if you were defrauding researchers anyways, there's no reason to actually take all the drugs, and good reasons not to (other drugs they denied taking could interact, for example). Instead, you'd start by figuring out whether you were in the placebo group or not. This isn't very hard; you can either try the pills and see if there's any effect at all (real drugs have perceptible effects, even if they're only side effects), or break one open and taste it, or even do a proper chemical test. If in the placebo group, you'd answer all questions in a manner consistent with getting no effect: no side effects, no change in status, etc. This could be even more placebo-like than a real placebo; normal subjects on placebo sometimes report unrelated things as side-effects, but fraudsters probably wouldn't. If in the experimental group, on the other hand, you'd try to match the experimenters' expectations, to avoid attention. This would mean claiming that it helped. So in a drug trial with a goal that's unverifiable, you'd expect people who got into the study fraudulently to systematically bias the study towards showing effectiveness.</p>\n<p>One of the papers repeated an anonymous claim of finding up to 5% duplicate subjects in some studies.&nbsp;Those are subjects who used the same name each time, but there could be more, who use aliases. Since one fraudster can participate in many studies, a small number of them can have a big impact. Since many studies end with small effect sizes, but we count significance rather than size, their effect is magnified further.</p>\n<p>The main obstacle to preventing this kind of fraud is confidentiality; medical records are supposed to be kept secret, which means it's hard to get at other studies' patient lists to cross-check. But it's certainly not impossible, and I think some auditing is called for.&nbsp;Another strategy is to conduct a&nbsp;\"study on study fraud\" - advertise it like an antidepressant study, conduct it like one, pay well, and give everyone placebos in a special bottle that logs the time and the weight of its contents whenever it's opened. A similar strategy for dietary studies is to include a food with distinctive metabolites, and test for those metabolites.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QMAv7ir5KbmFb3Xwg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 22, "extendedScore": null, "score": 8.263693058303636e-07, "legacy": true, "legacyId": "11906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5yZeqewqDcQGkuToq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T00:36:57.211Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:21.929Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rTxDtTucL9ayoy7iQ/", "pageUrlRelative": "/posts/rTxDtTucL9ayoy7iQ/", "linkUrl": "https://www.lesswrong.com/posts/rTxDtTucL9ayoy7iQ/", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTxDtTucL9ayoy7iQ%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTxDtTucL9ayoy7iQ%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTxDtTucL9ayoy7iQ%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rTxDtTucL9ayoy7iQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 8.263754842755941e-07, "legacy": true, "legacyId": "11915", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T03:23:18.515Z", "modifiedAt": null, "url": null, "title": "East Coast Megameetup II: Request for Talks", "slug": "east-coast-megameetup-ii-request-for-talks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:35.252Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ibfYZkanHfucXiP4g/east-coast-megameetup-ii-request-for-talks", "pageUrlRelative": "/posts/ibfYZkanHfucXiP4g/east-coast-megameetup-ii-request-for-talks", "linkUrl": "https://www.lesswrong.com/posts/ibfYZkanHfucXiP4g/east-coast-megameetup-ii-request-for-talks", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20East%20Coast%20Megameetup%20II%3A%20Request%20for%20Talks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEast%20Coast%20Megameetup%20II%3A%20Request%20for%20Talks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibfYZkanHfucXiP4g%2Feast-coast-megameetup-ii-request-for-talks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=East%20Coast%20Megameetup%20II%3A%20Request%20for%20Talks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibfYZkanHfucXiP4g%2Feast-coast-megameetup-ii-request-for-talks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibfYZkanHfucXiP4g%2Feast-coast-megameetup-ii-request-for-talks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p>One thing that I'd like to see at the next megameetup is a few informal focused discussions on various topics related to the community, like meditation or diet.</p>\n<p>People should prepare for them, but I don't think that say, powerpoints would be necessary. Just trying to encourage knowledge transfer.</p>\n<p>Anyone on the East Coast interested in giving one? Any things you'd like to see?</p>\n<p>Schedule Update: The Megameetup will be held on on the weekend of January 27th or February 4th.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ibfYZkanHfucXiP4g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "11921", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T04:51:03.415Z", "modifiedAt": null, "url": null, "title": "[Link] ASPERGERS/PUA official university research project", "slug": "link-aspergers-pua-official-university-research-project", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:21.757Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Karmakaiser", "createdAt": "2011-08-19T20:23:06.809Z", "isAdmin": false, "displayName": "Karmakaiser"}, "userId": "35k6aTWG8i43xjoqR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7rWtZTADBPEfTQcsF/link-aspergers-pua-official-university-research-project", "pageUrlRelative": "/posts/7rWtZTADBPEfTQcsF/link-aspergers-pua-official-university-research-project", "linkUrl": "https://www.lesswrong.com/posts/7rWtZTADBPEfTQcsF/link-aspergers-pua-official-university-research-project", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20ASPERGERS%2FPUA%20official%20university%20research%20project&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20ASPERGERS%2FPUA%20official%20university%20research%20project%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7rWtZTADBPEfTQcsF%2Flink-aspergers-pua-official-university-research-project%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20ASPERGERS%2FPUA%20official%20university%20research%20project%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7rWtZTADBPEfTQcsF%2Flink-aspergers-pua-official-university-research-project", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7rWtZTADBPEfTQcsF%2Flink-aspergers-pua-official-university-research-project", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 444, "htmlBody": "<p>I know some wrongians have studied pickup. In case any are formally diagnosed on the Autism spectrum:&nbsp;</p>\n<p><a href=\"http://www.reddit.com/r/seduction/comments/ncov4/aspergerspua_official_university_research_project/\">http://www.reddit.com/r/seduction/comments/ncov4/aspergerspua_official_university_research_project/</a></p>\n<blockquote>\n<p><span style=\"background-color: #fafafa; font-family: verdana, arial, helvetica, sans-serif;\">Hey guys!</span></p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">Have you improved a lot socially after studying the game? (even if your overall game isn't rockstar level?)</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">Would your family say you are easier to get along with now?</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">Is your stress level better?</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">We are four formally diagnosed aspies who have found major improvement via the game. We've investigated this thoroughly and now a clinical professor in autism at a major university is going to research this in depth. This is likely to draw major funding as a formal research project.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">Yes, that means the changes you have felt may enter the academic system and be available to help aspies all over the world.. not just the few of us who are able to study social dynamics.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">But we need your help!</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">We need more guys willing to go through an interview process with the clinical professor, to talk about their experiences and how your life has changed. (we're doing it too!). This is something very very special and the cornerstone of some big things. You must be formally diagnosed on the autism spectrum (which includes PDD NOS).</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">This is not about promoting the game. This is about extracting the key pieces out of the game with the goal of helping everyone on the autism spectrum lead happier lives.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">We get asked if this is real. Yes, it absolutely is. The reality is that for a small percentage of aspies who get into it, the game has a profound impact on their lives, far beyond anything else available today. The rest aren't advanced enough socially to be able to handle the game. We're doing this to help them because we understand how difficult their lives are. Everyone deserves quality relationships in their lives.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">If you're interested, please email us. Of course everything will be 1000% confidential -- we fully understand that both the autism and game aspects of this need to be quiet. And thats why this research is so important, because it hasn't been done before for this reason.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">Please, contact us.. even if you're not sure.. we'll show you what we have.. I think you'll be blown away.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">Daniel&nbsp;<span class=\" keyNavAnnotation\">[1]&nbsp;</span><span style=\"color: #336699;\">project.aspergers@gmail.com</span>&nbsp;(we're using a throwaway email address for the initial research. but yes, this is 100% real and you will work directly with us.)</p>\n</blockquote>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">&nbsp;</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">&nbsp;</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">I just finished skyping with the organizer of this shindig and I can confirm its legit. If you are formally diagnosed on the autism spectrum and seen improvement after studying pickup I strongly encourage you to send an email.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7rWtZTADBPEfTQcsF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -4, "extendedScore": null, "score": -1e-05, "legacy": true, "legacyId": "11922", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T05:34:03.363Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Parable of the Dagger", "slug": "seq-rerun-the-parable-of-the-dagger", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.080Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mME3tP9HkaZtNnnNw/seq-rerun-the-parable-of-the-dagger", "pageUrlRelative": "/posts/mME3tP9HkaZtNnnNw/seq-rerun-the-parable-of-the-dagger", "linkUrl": "https://www.lesswrong.com/posts/mME3tP9HkaZtNnnNw/seq-rerun-the-parable-of-the-dagger", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Parable%20of%20the%20Dagger&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Parable%20of%20the%20Dagger%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmME3tP9HkaZtNnnNw%2Fseq-rerun-the-parable-of-the-dagger%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Parable%20of%20the%20Dagger%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmME3tP9HkaZtNnnNw%2Fseq-rerun-the-parable-of-the-dagger", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmME3tP9HkaZtNnnNw%2Fseq-rerun-the-parable-of-the-dagger", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Today's post, <a href=\"/lw/ne/the_parable_of_the_dagger/\">The Parable of the Dagger</a> was originally published on 01 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A parable adapted from Raymond Smullyan, showing a failure of logic to reach a desired goal.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/95r/seq_rerun_newcombs_problem_and_regret_of/\">Newcomb's Problem and Regret of Rationality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mME3tP9HkaZtNnnNw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.264868707848232e-07, "legacy": true, "legacyId": "11928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hQxYBfu2LPc9Ydo6w", "654rxsFBmku6YDpdM", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T06:22:06.326Z", "modifiedAt": null, "url": null, "title": "Quixey is hiring a writer", "slug": "quixey-is-hiring-a-writer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.308Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Liron", "createdAt": "2009-02-27T04:43:11.294Z", "isAdmin": false, "displayName": "Liron"}, "userId": "AyzRrs8hNm54QptLi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BMFhDcDiFvQud9mmD/quixey-is-hiring-a-writer", "pageUrlRelative": "/posts/BMFhDcDiFvQud9mmD/quixey-is-hiring-a-writer", "linkUrl": "https://www.lesswrong.com/posts/BMFhDcDiFvQud9mmD/quixey-is-hiring-a-writer", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quixey%20is%20hiring%20a%20writer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuixey%20is%20hiring%20a%20writer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBMFhDcDiFvQud9mmD%2Fquixey-is-hiring-a-writer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quixey%20is%20hiring%20a%20writer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBMFhDcDiFvQud9mmD%2Fquixey-is-hiring-a-writer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBMFhDcDiFvQud9mmD%2Fquixey-is-hiring-a-writer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<p><img src=\"http://www.quixeychallenge.com/static/images/quixeylogowithtagline.png\" alt=\"\" width=\"187\" height=\"100\" /></p>\n<p>We've posted about jobs at&nbsp;<a href=\"http://www.quixey.com\">Quixey</a> before:</p>\n<p><a style=\"font-weight: bold; \" href=\"/lw/7u8/quixey_startup_applying_lwstyle_rationality/\">Quixey - startup applying LW-style rationality - hiring engineers</a></p>\n<p>Since then we've hired LessWrong user&nbsp;<a href=\"/user/cata\">cata</a>. And it occurred to us that&nbsp;the LessWrong community is not only full of software engineers, it's also full of unusually strong writers.</p>\n<p><a id=\"more\"></a></p>\n<h2>Job Description</h2>\n<p>Help write and edit content to professional standards. For example:</p>\n<ul>\n<li>Copy for <a href=\"http://www.quixey.com\">quixey.com</a></li>\n<li>Posts on the&nbsp;<a href=\"http://blog.quixey.com\">Quixey Blog</a></li>\n<li>Documents and slides for pitches to potential partners</li>\n<li>White papers</li>\n<li>Employee handbook</li>\n<li>Internal style guide</li>\n<li>Video scripts</li>\n<li>Twitter messages</li>\n</ul>\n<div><strong>Requirements:</strong></div>\n<ul>\n<li>You have really great writing skills</li>\n<li>You love marketing and telling a clear story</li>\n</ul>\n<p>Quixey is a great place to work with top notch people, and a great opportunity to advance your career as part of a fast-growing startup. For more info about Quixey, see&nbsp;<a href=\"/lw/7u8/quixey_startup_applying_lwstyle_rationality/\">this post</a>.</p>\n<p>This is a full-time position in our Palo Alto, California office with competitive compensation and benefits.</p>\n<p>To apply, email some Bayesian evidence that you're a good match to&nbsp;<a href=\"mailto:jobs@quixey.com\">jobs@quixey.com</a>, such&nbsp;as your LessWrong user profile. (It's hard to imagine evidence that wouldn't be screened off by a writing sample.)</p>\n<p><strong>Added:</strong>&nbsp;We're also interested to test a contract writer. Please email us if you want to do that, too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BMFhDcDiFvQud9mmD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 16, "extendedScore": null, "score": 8.265048874617258e-07, "legacy": true, "legacyId": "11927", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JnLSM7zmZmd7NCbB5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T09:28:09.184Z", "modifiedAt": null, "url": null, "title": "Quantified Health Prize Deadline Extended", "slug": "quantified-health-prize-deadline-extended", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:29.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ke334cYJW9BJFoREN/quantified-health-prize-deadline-extended", "pageUrlRelative": "/posts/ke334cYJW9BJFoREN/quantified-health-prize-deadline-extended", "linkUrl": "https://www.lesswrong.com/posts/ke334cYJW9BJFoREN/quantified-health-prize-deadline-extended", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantified%20Health%20Prize%20Deadline%20Extended&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantified%20Health%20Prize%20Deadline%20Extended%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fke334cYJW9BJFoREN%2Fquantified-health-prize-deadline-extended%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantified%20Health%20Prize%20Deadline%20Extended%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fke334cYJW9BJFoREN%2Fquantified-health-prize-deadline-extended", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fke334cYJW9BJFoREN%2Fquantified-health-prize-deadline-extended", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>(Original Post: <a href=\"/lw/8nx/announcing_the_quantified_health_prize/\">Announcing the Quantified Health Prize</a>)<a href=\"/lw/8nx/announcing_the_quantified_health_prize/\"><br /></a></p>\n<p>I've recently been hired by <a href=\"http://www.medicineispersonal.com\">Personalized Medicine,</a> a new research company trying to bring Less Wrongian rationality to the medical world. We're giving away a $5000 prize for well-researched, well-reasoned presentations that answer the following question: What are the best recommendations for what quantities adults (ages 20-60) should take the important dietary minerals in, and what are the costs and benefits of various amounts?</p>\n<p>Entries are now <strong>due by January 15th, 2012</strong>. This is an update from the original date of December 31st, 2011. However, we will <strong>not</strong> change this deadline again, and it will be strictly enforced. If you submit your entry on January 16 at 12:01 AM Pacific time, we will not read it.</p>\n<p>Why enter the contest? <span class=\"author-g-xz122zpsidcpt2artm0e\">If you have an excellent entry, even if you don&rsquo;t win the grand prize, you can still win one of four additional cash prizes, you&rsquo;ll be under consideration for a job as a researcher with our company Personalized Medicine, and you&rsquo;ll get a leg up in the larger contest we plan to run after this one. You also get to help people get better nutrition and stay healthier. <br /></span></p>\n<p>More info about the contest, and instructions for submitting entries, can be found at the contest website at <a href=\"http://www.medicineispersonal.com/contest/home\">http://www.medicineispersonal.com/contest/home</a>. Good luck!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ke334cYJW9BJFoREN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 8.265746547455218e-07, "legacy": true, "legacyId": "11931", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Y9iXchCMdYLsPNzZM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T13:54:55.387Z", "modifiedAt": null, "url": null, "title": "Giving: is money better than options?", "slug": "giving-is-money-better-than-options", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AkpaCg57Sdutrjcso/giving-is-money-better-than-options", "pageUrlRelative": "/posts/AkpaCg57Sdutrjcso/giving-is-money-better-than-options", "linkUrl": "https://www.lesswrong.com/posts/AkpaCg57Sdutrjcso/giving-is-money-better-than-options", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Giving%3A%20is%20money%20better%20than%20options%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGiving%3A%20is%20money%20better%20than%20options%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAkpaCg57Sdutrjcso%2Fgiving-is-money-better-than-options%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Giving%3A%20is%20money%20better%20than%20options%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAkpaCg57Sdutrjcso%2Fgiving-is-money-better-than-options", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAkpaCg57Sdutrjcso%2Fgiving-is-money-better-than-options", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 372, "htmlBody": "<p>When I started my new job at a startup about a year ago, I started getting about a third [1] of my pay in stock options. Being risk neutral in donating, I <a href=\"http://www.jefftk.com/news/2010-12-07.html\">decided</a> to spend my salary on me and donate any proceeds from my stock options. This let me increase the fraction of my pay I was giving away without decreasing what I keep. Mathematically and economically, considering just what I can give, I think this was the right decision.</p>\n<p>The problem is, much of my potential impact is from convincing others to give, and if I have to talk about stock options, expected value, and money that I intend to give away it's confusing and distracting. Things were much simpler when we could just <a href=\"http://boldergiving.org/stories.php?story=Julia_Wise_97\">say</a> \"we lived on about $22,000 and gave about $45,000\". Should I switch back to giving money?</p>\n<p>It would not be an easy switch. Options represent a small chance of a lot of money, and they're not very valuable to me personally. (Each additional dollar is worth less than the last). If I were to start giving a third of my compensation away as cash, that would be about 2/3 of my paycheque [2]. Which would be pretty hard. Maybe I should donate some combination of cash and options?&nbsp; Making this more complicated, I had negotiated more options in exchange for a $10K lower salary, figuring that for money I was giving away this was the right thing to do.&nbsp; Suggestions?</p>\n<p><br /> [1] You might say \"how can you say 'about a third' when you have no idea whether your stock options will even be worth something ever?\" What I did was estimate how likely I thought Cogo Labs was to be worth $X in about ten brackets ($0, $10M, $50M, ...), and then calculate an expected value as <tt>$0*P_1 + $10M*P_2 + $50M*P_3</tt>... Then I multiplied by the fraction of the company whose options would vest to me each quarter, and got something about half my quarterly salary. So: one third of compensation.</p>\n<p>[2] It's 1/2 my salary (the last third is options) but 2/3 of takehome pay because 1/6 of my pay goes to taxes and other paycheque deductions.</p>\n<p>(I also put this up as a <a href=\"http://www.jefftk.com/news/2012-01-05.html\">blog post</a>)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AkpaCg57Sdutrjcso", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 8.266747109329826e-07, "legacy": true, "legacyId": "11932", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T18:49:46.457Z", "modifiedAt": null, "url": null, "title": "[Link] Evaluating experts on expertise", "slug": "link-evaluating-experts-on-expertise", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.121Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5HQkexWDdzMMMNMZ4/link-evaluating-experts-on-expertise", "pageUrlRelative": "/posts/5HQkexWDdzMMMNMZ4/link-evaluating-experts-on-expertise", "linkUrl": "https://www.lesswrong.com/posts/5HQkexWDdzMMMNMZ4/link-evaluating-experts-on-expertise", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Evaluating%20experts%20on%20expertise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Evaluating%20experts%20on%20expertise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5HQkexWDdzMMMNMZ4%2Flink-evaluating-experts-on-expertise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Evaluating%20experts%20on%20expertise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5HQkexWDdzMMMNMZ4%2Flink-evaluating-experts-on-expertise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5HQkexWDdzMMMNMZ4%2Flink-evaluating-experts-on-expertise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 489, "htmlBody": "<p><a href=\"https://ignoranceanduncertainty.wordpress.com/2011/08/11/expertise-on-expertise/\">https://ignoranceanduncertainty.wordpress.com/2011/08/11/expertise-on-expertise/</a></p>\n<p>Nice article on meta-expertise, ie. the skill of figuring out which experts are actually experts. The author notes that there are domains in which can't really be mastered, and then lays out some useful-seeming tests for distinguishing them:</p>\n<blockquote>\n<p>Cognitive biases and styles aside, another contributing set of  factors may be the characteristics of the complex, deep domains  themselves that render deep expertise very difficult to attain. Here is a  list of tests you can apply to such domains by way of evaluating their  potential for the development of genuine expertise:</p>\n<ol>\n<li>Stationarity? Is the domain stable enough for generalizable methods  to be derived? In chaotic systems long-range prediction is impossible  because of initial-condition sensitivity. In human history, politics and  culture, the underlying processes may not be stationary at all.</li>\n<li>Rarity? When it comes to prediction, rare phenomena simply are difficult to predict (see my <a href=\"http://www.bestthinking.com/thinkers/science/social_sciences/psychology/michael-smithson?tab=blog&amp;blogpostid=10588\">post </a>on making the wrong decisions most of the time for the right reasons).</li>\n<li>Observability? Can the outcomes of predictions or decisions be  directly or immediately observed? For example in psychology, direct  observation of mental states is nearly impossible, and in climatology  the consequences of human interventions will take a very long time to  unfold.</li>\n<li>Objective or even impartial criteria? For instance, what is &ldquo;good,&rdquo;  &ldquo;beautiful,&rdquo; or even &ldquo;acceptable&rdquo; in domains such as music, dance or the  visual arts? Are such domains irreducibly subjective and culture-bound?</li>\n<li>Testability? Are there clear criteria for when an expert has  succeeded or failed? Or is there too much &ldquo;wiggle-room&rdquo; to be able to  tell?</li>\n</ol>\n<p>Finally, here are a few tests that can be used to evaluate the &ldquo;experts&rdquo; in your life:</p>\n<ol>\n<li>Credentials: Does the expert possess credentials that have involved testable criteria for demonstrating proficiency?</li>\n<li>Walking the walk: Is the expert an active practitioner in their domain (versus being a critic or a commentator)?</li>\n<li>Overconfidence: Ask your expert to make yes-no predictions in their  domain of expertise, and before any of these predictions can be tested  ask them to estimate the percentage of time they&rsquo;re going to be correct.  Compare that estimate with the resulting percentage correct. If their  estimate was too high then your expert may suffer from over-confidence.</li>\n<li>Confirmation bias: We&rsquo;re all prone to this, but some more so than  others. Is your expert reasonably open to evidence or viewpoints  contrary to their own views?</li>\n<li>Hedgehog-Fox test: Tetlock found that Foxes were better-calibrated  and more able to entertain self-disconfirming counterfactuals than  hedgehogs, but allowed that hedgehogs can occasionally be &ldquo;stunningly  right&rdquo; in a way that foxes cannot. Is your expert a fox or a hedgehog?</li>\n<li>Willingness to own up to error: Bad luck is a far more popular  explanation for being wrong than good luck is for being right. Is your  expert balanced, i.e., equally critical, when assessing their own  successes and failures?</li>\n</ol></blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5HQkexWDdzMMMNMZ4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 8.267853243003286e-07, "legacy": true, "legacyId": "11933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T18:51:10.606Z", "modifiedAt": null, "url": null, "title": "Baconmas: The holiday for the sciences", "slug": "baconmas-the-holiday-for-the-sciences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hPTwCjif6whRjo6jL/baconmas-the-holiday-for-the-sciences", "pageUrlRelative": "/posts/hPTwCjif6whRjo6jL/baconmas-the-holiday-for-the-sciences", "linkUrl": "https://www.lesswrong.com/posts/hPTwCjif6whRjo6jL/baconmas-the-holiday-for-the-sciences", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Baconmas%3A%20The%20holiday%20for%20the%20sciences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABaconmas%3A%20The%20holiday%20for%20the%20sciences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPTwCjif6whRjo6jL%2Fbaconmas-the-holiday-for-the-sciences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Baconmas%3A%20The%20holiday%20for%20the%20sciences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPTwCjif6whRjo6jL%2Fbaconmas-the-holiday-for-the-sciences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPTwCjif6whRjo6jL%2Fbaconmas-the-holiday-for-the-sciences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 450, "htmlBody": "<p><strong>Summary:</strong> Sir Francis Bacon's birthday (Jan. 22) is a holiday devoted to the sciences, with a side order of bacon. Check out <a href=\"http://baconmas.com/\">the website</a> and share the Baconmas cheer with everyone!</p>\n<h2>What is Baconmas?</h2>\n<p>For the past few years, I've been celebrating the birthday of Sir Francis Bacon (Jan. 22) as a holiday, hosting parties with both science experiments and bacon dishes. It's been excellent enough that I want to share it with everyone else, so I made <a href=\"http://baconmas.com/\">a website devoted to Baconmas</a> and I'd like you to check it out (and share it if you like it).</p>\n<p>It goes without saying that holidays devoted to the sciences can be a force for good as well as a lot of fun (if you haven't, you should see the <a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">writeup of the Solstice Celebration</a> for an awesome example). I thought it would be especially powerful to have a holiday that was (1) explicitly about science, (2) fun to celebrate, with a \"hook\" like bacon, and (3) positive and open to everyone.</p>\n<p>The <a href=\"http://baconmas.com/2011/12/26/the-fundamental-tradition/\">main \"tradition\" of Baconmas</a> is simply to try something new with each celebration, and to record how it went. Everything else is just a suggestion. I think it's clear that the <a href=\"http://xkcd.com/397/\">Zombie Feynman school of science</a> is a powerful and good meme. Science isn't only about the things that are shiny and fun, but it <em>should</em> be shiny and fun whenever possible. So I'm <a href=\"http://baconmas.com/category/experiments-2/\">linking a bunch of fun, easy experiments</a> (that have actual content for both novices and the scientifically literate).</p>\n<h2>How can I help Baconmas grow?</h2>\n<p>Are you as excited about Baconmas as I am? Great! There are some things you can do to really help!</p>\n<ul>\n<li>Tell all your friends! Share the <a href=\"http://baconmas.com/\">website</a>, the <a href=\"https://twitter.com/#!/baconmas\">Twitter feed</a>, and the <a href=\"http://www.facebook.com/events/274412302613679/\">Facebook event page</a>. If you're part of a relevant forum (anything from a subreddit to the XKCD forums), share it there too!</li>\n<li>Host a Baconmas party, and then send me (happybaconmas at gmail) pictures/video/testimonials that I can add to the site! These will help Baconmas 2013 grow even faster than 2012.</li>\n<li>Create a local Baconmas Meetup group, and send me a link that I can post.</li>\n<li>Find or invent any of the following: experiments, recipes, traditions, carols, guest blog posts; then send them to me for the Baconmas site!</li>\n<li>One thing that would be totally incredible: a funny video biography of Sir Francis Bacon, done with puppets/costume/animation/whatever works for you! I'd love to do this myself, but may not have the time.</li>\n<li>Give me feedback and ideas for the website and everything else!</li>\n</ul>\n<p><strong>P.S.</strong> Thanks for all the comments on my <a href=\"/lw/951/advice_request_baconmas_website/\">advice request post</a>! I made a few key additions based on your input.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hPTwCjif6whRjo6jL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "11934", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Summary:</strong> Sir Francis Bacon's birthday (Jan. 22) is a holiday devoted to the sciences, with a side order of bacon. Check out <a href=\"http://baconmas.com/\">the website</a> and share the Baconmas cheer with everyone!</p>\n<h2 id=\"What_is_Baconmas_\">What is Baconmas?</h2>\n<p>For the past few years, I've been celebrating the birthday of Sir Francis Bacon (Jan. 22) as a holiday, hosting parties with both science experiments and bacon dishes. It's been excellent enough that I want to share it with everyone else, so I made <a href=\"http://baconmas.com/\">a website devoted to Baconmas</a> and I'd like you to check it out (and share it if you like it).</p>\n<p>It goes without saying that holidays devoted to the sciences can be a force for good as well as a lot of fun (if you haven't, you should see the <a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">writeup of the Solstice Celebration</a> for an awesome example). I thought it would be especially powerful to have a holiday that was (1) explicitly about science, (2) fun to celebrate, with a \"hook\" like bacon, and (3) positive and open to everyone.</p>\n<p>The <a href=\"http://baconmas.com/2011/12/26/the-fundamental-tradition/\">main \"tradition\" of Baconmas</a> is simply to try something new with each celebration, and to record how it went. Everything else is just a suggestion. I think it's clear that the <a href=\"http://xkcd.com/397/\">Zombie Feynman school of science</a> is a powerful and good meme. Science isn't only about the things that are shiny and fun, but it <em>should</em> be shiny and fun whenever possible. So I'm <a href=\"http://baconmas.com/category/experiments-2/\">linking a bunch of fun, easy experiments</a> (that have actual content for both novices and the scientifically literate).</p>\n<h2 id=\"How_can_I_help_Baconmas_grow_\">How can I help Baconmas grow?</h2>\n<p>Are you as excited about Baconmas as I am? Great! There are some things you can do to really help!</p>\n<ul>\n<li>Tell all your friends! Share the <a href=\"http://baconmas.com/\">website</a>, the <a href=\"https://twitter.com/#!/baconmas\">Twitter feed</a>, and the <a href=\"http://www.facebook.com/events/274412302613679/\">Facebook event page</a>. If you're part of a relevant forum (anything from a subreddit to the XKCD forums), share it there too!</li>\n<li>Host a Baconmas party, and then send me (happybaconmas at gmail) pictures/video/testimonials that I can add to the site! These will help Baconmas 2013 grow even faster than 2012.</li>\n<li>Create a local Baconmas Meetup group, and send me a link that I can post.</li>\n<li>Find or invent any of the following: experiments, recipes, traditions, carols, guest blog posts; then send them to me for the Baconmas site!</li>\n<li>One thing that would be totally incredible: a funny video biography of Sir Francis Bacon, done with puppets/costume/animation/whatever works for you! I'd love to do this myself, but may not have the time.</li>\n<li>Give me feedback and ideas for the website and everything else!</li>\n</ul>\n<p><strong>P.S.</strong> Thanks for all the comments on my <a href=\"/lw/951/advice_request_baconmas_website/\">advice request post</a>! I made a few key additions based on your input.</p>", "sections": [{"title": "What is Baconmas?", "anchor": "What_is_Baconmas_", "level": 1}, {"title": "How can I help Baconmas grow?", "anchor": "How_can_I_help_Baconmas_grow_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jES7mcPvKpfmzMTgC", "h3xDc3FYTwRFJW2Y4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T21:03:59.547Z", "modifiedAt": null, "url": null, "title": "Meetup : Seattle Board Games", "slug": "meetup-seattle-board-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.289Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GuySrinivasan", "createdAt": "2009-02-27T21:00:32.986Z", "isAdmin": false, "displayName": "GuySrinivasan"}, "userId": "HMnfd9HdRCfuRcdBG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vnZ8EpA88pQbETuKc/meetup-seattle-board-games", "pageUrlRelative": "/posts/vnZ8EpA88pQbETuKc/meetup-seattle-board-games", "linkUrl": "https://www.lesswrong.com/posts/vnZ8EpA88pQbETuKc/meetup-seattle-board-games", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Seattle%20Board%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Seattle%20Board%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvnZ8EpA88pQbETuKc%2Fmeetup-seattle-board-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Seattle%20Board%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvnZ8EpA88pQbETuKc%2Fmeetup-seattle-board-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvnZ8EpA88pQbETuKc%2Fmeetup-seattle-board-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5w'>Seattle Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 January 2012 01:01:30PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">15207 NE 72nd St, Redmond, WA 98052, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come play board games with fellow LWers and friends in Redmond!</p>\n\n<p>Go to <a href=\"http://groups.google.com/group/lw-seattle\" rel=\"nofollow\">http://groups.google.com/group/lw-seattle</a> to arrange carpooling.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5w'>Seattle Board Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vnZ8EpA88pQbETuKc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.268356848715395e-07, "legacy": true, "legacyId": "11935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Seattle_Board_Games\">Discussion article for the meetup : <a href=\"/meetups/5w\">Seattle Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 January 2012 01:01:30PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">15207 NE 72nd St, Redmond, WA 98052, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come play board games with fellow LWers and friends in Redmond!</p>\n\n<p>Go to <a href=\"http://groups.google.com/group/lw-seattle\" rel=\"nofollow\">http://groups.google.com/group/lw-seattle</a> to arrange carpooling.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Seattle_Board_Games1\">Discussion article for the meetup : <a href=\"/meetups/5w\">Seattle Board Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Seattle Board Games", "anchor": "Discussion_article_for_the_meetup___Seattle_Board_Games", "level": 1}, {"title": "Discussion article for the meetup : Seattle Board Games", "anchor": "Discussion_article_for_the_meetup___Seattle_Board_Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T21:14:09.865Z", "modifiedAt": null, "url": null, "title": "Request for advice- Reading on politics", "slug": "request-for-advice-reading-on-politics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6CnHpC56t8jpC7xDr/request-for-advice-reading-on-politics", "pageUrlRelative": "/posts/6CnHpC56t8jpC7xDr/request-for-advice-reading-on-politics", "linkUrl": "https://www.lesswrong.com/posts/6CnHpC56t8jpC7xDr/request-for-advice-reading-on-politics", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%20for%20advice-%20Reading%20on%20politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%20for%20advice-%20Reading%20on%20politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CnHpC56t8jpC7xDr%2Frequest-for-advice-reading-on-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%20for%20advice-%20Reading%20on%20politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CnHpC56t8jpC7xDr%2Frequest-for-advice-reading-on-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6CnHpC56t8jpC7xDr%2Frequest-for-advice-reading-on-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 609, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal.dotm</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>532</o:Words> <o:Characters>3035</o:Characters> <o:Company>North Central High School</o:Company> <o:Lines>25</o:Lines> <o:Paragraphs>6</o:Paragraphs> <o:CharactersWithSpaces>3727</o:CharactersWithSpaces> <o:Version>12.256</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:TrackMoves>false</w:TrackMoves> <w:TrackFormatting /> <w:PunctuationKerning /> <w:DrawingGridHorizontalSpacing>18 pt</w:DrawingGridHorizontalSpacing> <w:DrawingGridVerticalSpacing>18 pt</w:DrawingGridVerticalSpacing> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:DontGrowAutofit /> <w:DontAutofitConstrainedTables /> <w:DontVertAlignInTxbx /> </w:Compatibility> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"276\"> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--></p>\n<p style=\"margin-top:.1pt;margin-right:0in;margin-bottom:.1pt;margin-left:0in; background:white\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal.dotm</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>535</o:Words> <o:Characters>3054</o:Characters> <o:Company>North Central High School</o:Company> <o:Lines>25</o:Lines> <o:Paragraphs>6</o:Paragraphs> <o:CharactersWithSpaces>3750</o:CharactersWithSpaces> <o:Version>12.256</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:TrackMoves>false</w:TrackMoves> <w:TrackFormatting /> <w:PunctuationKerning /> <w:DrawingGridHorizontalSpacing>18 pt</w:DrawingGridHorizontalSpacing> <w:DrawingGridVerticalSpacing>18 pt</w:DrawingGridVerticalSpacing> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:DontGrowAutofit /> <w:DontAutofitConstrainedTables /> <w:DontVertAlignInTxbx /> </w:Compatibility> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"276\"> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal.dotm</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>533</o:Words> <o:Characters>3041</o:Characters> <o:Company>North Central High School</o:Company> <o:Lines>25</o:Lines> <o:Paragraphs>6</o:Paragraphs> <o:CharactersWithSpaces>3734</o:CharactersWithSpaces> <o:Version>12.256</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:TrackMoves>false</w:TrackMoves> <w:TrackFormatting /> <w:PunctuationKerning /> <w:DrawingGridHorizontalSpacing>18 pt</w:DrawingGridHorizontalSpacing> <w:DrawingGridVerticalSpacing>18 pt</w:DrawingGridVerticalSpacing> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:DontGrowAutofit /> <w:DontAutofitConstrainedTables /> <w:DontVertAlignInTxbx /> </w:Compatibility> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"276\"> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">I've become adept at navigating the bureaucracy of my public high school. I've dropped environmental science as an AP (because it was painfully slow and replete with busywork) and am now taking an \"independent study\" in government. I'm going to be using this mainly as a way to study environmental science at my own pace, but I also have to read and write some about standard political issues. the requirements of the independent study are pretty vague. In order to get approved, I've got to BS some reason why I should be granted an independent study.&nbsp;<a href=\"/lw/jc/rationality_and_the_english_language/\"><span style=\"color:blue\">I'm obviously not going to speak plainly.</span></a>&nbsp;I'll probably say something about my interests in seasteading, environmentalism, and education reform.&nbsp;&nbsp;What books do you recommend on the politics of these subjects&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\"><span style=\"color:blue\">given that it is the mindkiller</span></a>? Also, the main focus is on environmentalism, not on education or seasteading. &nbsp;I've done a bit of research regarding seasteading, but there's not much that I know about&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">I was particularly interested in this point brought up in the&nbsp;<a href=\"http://seasteading.org/book_beta/section_index.html\"><span style=\"color:blue\">seasteading book:</span></a></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">&nbsp;</p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"margin-top:.1pt;margin-right:0in;margin-bottom:.1pt; margin-left:0in;mso-para-margin-top:.01gd;mso-para-margin-right:0in;mso-para-margin-bottom: .01gd;mso-para-margin-left:0in;background:white\">Let&rsquo;s consider several different levels on which we could discuss politics:</p>\n<p class=\"MsoNormal\" style=\"margin-top:.1pt;margin-right:24.0pt;margin-bottom: .1pt;margin-left:24.0pt;mso-para-margin-top:.01gd;mso-para-margin-right:24.0pt; mso-para-margin-bottom:.01gd;mso-para-margin-left:24.0pt;text-indent:-.25in; mso-list:l0 level1 lfo1;tab-stops:list .5in;background:white\"><!--[if !supportLists]--><span style=\"font-size: 10pt; font-family: Symbol; \">&middot;<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]-->Policy. For example, a debate about whether to criminalize drug use, attempt to reduce the harm of use, or completely legalize it. What are the effects of each specific policy? Which does the most net good? Who is hurt, and who is helped?</p>\n<p class=\"MsoNormal\" style=\"margin-top:.1pt;margin-right:24.0pt;margin-bottom: .1pt;margin-left:24.0pt;mso-para-margin-top:.01gd;mso-para-margin-right:24.0pt; mso-para-margin-bottom:.01gd;mso-para-margin-left:24.0pt;text-indent:-.25in; mso-list:l0 level1 lfo1;tab-stops:list .5in;background:white\"><!--[if !supportLists]--><span style=\"font-size: 10pt; font-family: Symbol; \">&middot;<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]-->System. What types of policies does a specific political system tend to generate? For example, in a democracy, a special interest group can easily coordinate to influence legislation which benefits them, but costs everyone a little bit. If every consumer loses a dollar a year from a policy, it just isn&rsquo;t worth anyone&rsquo;s time to fight it. Hence we expect democracies to frequently produce policies which steal small amounts from many and give them to a few. And indeed, tariffs, farm subsidies, and bailouts, just to name a few, fit this model quite well. This type of argument is at a level of generality above any specific policy, and it can offer enormous insight at consistent errors made by current governments. But to fix those problems, we need to rise further yet.</p>\n<p class=\"MsoNormal\" style=\"margin-top:.1pt;margin-right:24.0pt;margin-bottom: .1pt;margin-left:24.0pt;mso-para-margin-top:.01gd;mso-para-margin-right:24.0pt; mso-para-margin-bottom:.01gd;mso-para-margin-left:24.0pt;text-indent:-.25in; mso-list:l0 level1 lfo1;tab-stops:list .5in;background:white\"><!--[if !supportLists]--><span style=\"font-size: 10pt; font-family: Symbol; \">&middot;<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]-->Meta-system. At the level we want, we think about the entire industry of government. What types of systems does it produce? How can it be changed to produce better systems (that is, systems which produce better policies)? What influences how well the governments of the world serve their citizens? How can we increase competition between governments? This level is the most abstract and the most complex, which can make it difficult to get a handle on, but if we can grasp that handle, it gives us the most leverage to change the world.&nbsp;</p>\n</blockquote>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">They also recommend a reading list:</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">&nbsp;</p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">Machinery of Freedom (David Friedman)</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">Game Theory and the Social Contract (Ken Binmore)</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">Mancur Olson - stuff</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">Myth of the Rational Voter (Bryan Caplan)</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">Economics In One Lesson (Henry Hazlitt) ?</p>\n</blockquote>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\">In regards to environmentalism, I was thinking about focusing on the relationships between government funding for green businesses as green entrepreneurship is of interest to me. I'd probably have to talk about the Solyndra scandal at some point.&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0in;margin-bottom:.0001pt;background: white\"><br /> As a side note, if the requirements aren't too stringent and I can just write about whatever I feel like so long as it vaguely relates to politics (like in my independent study in psychology), I may just go meta and write about <a href=\"http://www.americanselect.org/\">Americans Elect</a>.&nbsp;</p>\n<!--EndFragment-->\n<p>&nbsp;</p>\n<p>Edit:&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\">&nbsp;I do think that there is a difference between descriptive politics ( e.g.describing the workings of the EPA or a standard civics class) and and normative (woo liberatarians!). I'm more interested in descriptive politics.&nbsp;</span></p>\n<div><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 16px; text-align: justify;\"><br /></span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6CnHpC56t8jpC7xDr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 3, "extendedScore": null, "score": 8.268395017559259e-07, "legacy": true, "legacyId": "11936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Lz64L3yJEtYGkzMzu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-05T22:13:31.511Z", "modifiedAt": null, "url": null, "title": "[POLL] Wisdom of the Crowd experiment", "slug": "poll-wisdom-of-the-crowd-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.172Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tgb", "createdAt": "2011-11-22T01:42:56.795Z", "isAdmin": false, "displayName": "tgb"}, "userId": "ZSMRTvQtfA4eieBPk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D2Rzwu6a8syB9tHZT/poll-wisdom-of-the-crowd-experiment", "pageUrlRelative": "/posts/D2Rzwu6a8syB9tHZT/poll-wisdom-of-the-crowd-experiment", "linkUrl": "https://www.lesswrong.com/posts/D2Rzwu6a8syB9tHZT/poll-wisdom-of-the-crowd-experiment", "postedAtFormatted": "Thursday, January 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPOLL%5D%20Wisdom%20of%20the%20Crowd%20experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPOLL%5D%20Wisdom%20of%20the%20Crowd%20experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2Rzwu6a8syB9tHZT%2Fpoll-wisdom-of-the-crowd-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPOLL%5D%20Wisdom%20of%20the%20Crowd%20experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2Rzwu6a8syB9tHZT%2Fpoll-wisdom-of-the-crowd-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2Rzwu6a8syB9tHZT%2Fpoll-wisdom-of-the-crowd-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p>Many of you will be familiar with the \"<a href=\"http://en.wikipedia.org/wiki/Wisdom_of_the_crowd\">Wisdom of the Crowd</a>\" - a phenomenon where the average result of a large poll of people's estimates tends to be very accurate, even when most people make poor estimates. I've written a short poll to test a small variant of this setup which I would like to test.&nbsp;</p>\n<p><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHhVX3c1ZU1BZ2pEZENvVXdvY3BvdWc6MQ\">Please fill out this short poll.</a></p>\n<p>Specifically, I want to see how the weighted average of the results performs when the question is posed as \"is the value in question closer to A or B?\" This change is inspired by your usual two-party election where people choose between two extreme values, when many voters have opinions in the middle of the two.</p>\n<p>Thank you for helping!</p>\n<p>&nbsp;</p>\n<p>I'm a little worried about anchoring in this survey. Suggestions for how to improve it would be appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D2Rzwu6a8syB9tHZT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 8.268617766705096e-07, "legacy": true, "legacyId": "11937", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T02:23:12.427Z", "modifiedAt": null, "url": null, "title": "[link] Post-doc position available at FHI", "slug": "link-post-doc-position-available-at-fhi", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ve4DWym7unuYJQT3u/link-post-doc-position-available-at-fhi", "pageUrlRelative": "/posts/ve4DWym7unuYJQT3u/link-post-doc-position-available-at-fhi", "linkUrl": "https://www.lesswrong.com/posts/ve4DWym7unuYJQT3u/link-post-doc-position-available-at-fhi", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Post-doc%20position%20available%20at%20FHI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Post-doc%20position%20available%20at%20FHI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fve4DWym7unuYJQT3u%2Flink-post-doc-position-available-at-fhi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Post-doc%20position%20available%20at%20FHI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fve4DWym7unuYJQT3u%2Flink-post-doc-position-available-at-fhi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fve4DWym7unuYJQT3u%2Flink-post-doc-position-available-at-fhi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7, "htmlBody": "<p><a href=\"http://www.fhi.ox.ac.uk/news/2011/academic_vacancy_at_the_fhi\">Here</a>. The application deadline is January 23rd.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ve4DWym7unuYJQT3u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 6, "extendedScore": null, "score": 8.269554804364227e-07, "legacy": true, "legacyId": "11950", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T03:40:12.909Z", "modifiedAt": null, "url": null, "title": "Singularity Institute Executive Director Q&A #2", "slug": "singularity-institute-executive-director-q-and-a-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.277Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DWDA33qGTMX9ZJejp/singularity-institute-executive-director-q-and-a-2", "pageUrlRelative": "/posts/DWDA33qGTMX9ZJejp/singularity-institute-executive-director-q-and-a-2", "linkUrl": "https://www.lesswrong.com/posts/DWDA33qGTMX9ZJejp/singularity-institute-executive-director-q-and-a-2", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Institute%20Executive%20Director%20Q%26A%20%232&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Institute%20Executive%20Director%20Q%26A%20%232%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWDA33qGTMX9ZJejp%2Fsingularity-institute-executive-director-q-and-a-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Institute%20Executive%20Director%20Q%26A%20%232%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWDA33qGTMX9ZJejp%2Fsingularity-institute-executive-director-q-and-a-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWDA33qGTMX9ZJejp%2Fsingularity-institute-executive-director-q-and-a-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1253, "htmlBody": "<p><small>Previously: <a href=\"http://intelligence.org/blog/2011/09/15/interview-with-new-singularity-institute-research-fellow-luke-muehlhuaser-september-2011/\">Interview as a researcher</a>, <a href=\"/lw/8s6/video_qa_with_singularity_institute_executive/\">Q&amp;A #1</a></small></p>\n<p>This is my second Q&amp;A as Executive Director of the Singularity Institute. I'll skip the video this time.</p>\n<p>&nbsp;</p>\n<h4>Singularity Institute Activities</h4>\n<p>Bugmaster asks:</p>\n<blockquote>\n<p>...what does the SIAI actually <em>do</em>? You don't submit your work to rigorous scrutiny by your peers in the field... you either aren't doing any AGI research, or are keeping it so secret that no one knows about it... and you aren't developing any practical applications of AI, either... So, what is it that you are actually working on, other than growing the SIAI itself ?</p>\n</blockquote>\n<p>It's a good question, and my own biggest concern right now. Donors would like to know: Where is the visible return on investment? How can I see that I'm buying existential risk reduction when I donate to the Singularity Institute?</p>\n<p>SI has a problem, here, because it has done so much invisible work lately. Our researchers have done a ton of work that hasn't been written up and published yet; Eliezer has been writing his rationality books that aren't yet published; Anna and Eliezer have been developing a new rationality curriculum for the future \"Rationality Org\" that will be spun off from the Singularity Institute; Carl has been doing a lot of mostly invisible work in the optimal philanthropy community; and so on. I believe this is all valuable x-risk-reducing work, but of course not all of our supporters are willing to just take our word for it that we're doing valuable work. Our supporters want to see tangible results, and all they see is the Singularity Summit, a few papers a year, some web pages and Less Wrong posts, and a couple rationality training camps. That's good, but not good enough!</p>\n<p>I agree with this concern, which is why I'm focused on doing things that happen to be both x-risk-reducing and visible.&nbsp;</p>\n<p>First, we've been working on visible \"meta\" work that makes the Singularity Institute more transparent and effective in general: a strategic plan, a donor database (\"visible\" to donors in the form of thank-yous), a new website (forthcoming), and an annual report (forthcoming).</p>\n<p>Second, we're pushing to publish more research results this year. We have three chapters forthcoming in <em>The Singularity Hypothesis</em>, one chapter forthcoming in <em>The Cambridge Handbook of Artificial Intelligence</em>, one forthcoming article on the difficulty of AI, and several other articles and working papers we're planning to publish in 2012. I've also begun writing the first comprehensive <a href=\"http://lukeprog.com/SaveTheWorld.html\">outline of open problems in Singularity research</a>, so that interested researchers from around the world can participate in solving the world's most important problems.</p>\n<p>Third, there is visible rationality work forthcoming. One of Eliezer's books is now being shopped to agents and publishers, and we're field-testing different versions of rationality curriculum material for use in Less Wrong meetups and classes.</p>\n<p>Fourth, we're expanding the Singularity Summit brand, an important platform for spreading the memes of x-risk reduction and AI safety.</p>\n<p>So my answer is to the question is: \"Yes, visible return on investment has been a problem lately due to our choice of projects. Even before I was made Executive Director, it was one of my top concerns to help correct that situation, and this is still the case today.\"</p>\n<p>&nbsp;</p>\n<h4>What if?</h4>\n<p>XiXiDu asks:</p>\n<blockquote>\n<p>What would SI do if it became apparent that AGI is at most 10 years away?</p>\n</blockquote>\n<p>This would be a serious problem because by default, AGI will be extremely destructive, and we don't yet know how to make AGI not be destructive.</p>\n<p>What would we do if we thought AGI was at most 10 years away?</p>\n<p>This depends on whether it's apparent to a wider public that AGI is at most 10 years away, or a conclusion based only on a nonpublic analysis.</p>\n<p>If it becomes apparent to a wide variety of folks that AGI is close, then it should be much easier to get people and support for Friendly AI work, so a big intensification of effort would be a good move. If the analysis that AGI is 10 years away leads to hundreds of well-staffed and well-funded AGI research programs and a rich public literature, then trying to outrace the rest with a Friendly AI project becomes much harder. After an intensified Friendly AI effort, one could try to build up knowledge in Friendly AI theory and practice that could be applied (somewhat less effectively) to systems not designed from the ground up for Friendliness. This knowledge could then be distributed widely to increase the odds of a project pulling through, calling in real Friendliness experts, etc. But in general, a widespread belief that AGI is only 10 years away would be a much hairier situation than the one we're in now.</p>\n<p>But if the basis for thinking AI was 10 years away was nonpublic (but nonetheless persuasive to supporters who have lots of resources), then it could be used to differentially attract support to a Friendly AI project, hopefully without provoking dozens of AGI teams to intensify their efforts. So if we had a convincing case that AGI was only 10 years away, we might not publicize this but would instead make the case to individual supporters that we needed to immediately intensify our efforts toward a theory of Friendly AI in a way that only much greater funding can allow.</p>\n<p>&nbsp;</p>\n<h4>Budget</h4>\n<p>MileyCyrus asks:</p>\n<blockquote>\n<p>What kind of budget would be required to solve the friendly AI problem?</p>\n</blockquote>\n<p>Large research projects always come with large uncertainties concerning how difficult they will be, especially ones that require fundamental breakthroughs in mathematics and philosophy like Friendly AI does.</p>\n<p>Even a small, 10-person team of top-level Friendly AI researchers taking academic-level salaries for a decade would require tens of millions of dollars. And even getting to the point where you can raise that kind of money requires a slow \"ramping up\" of researcher recruitment and output. We need enough money to attract the kinds of mathematicians who are also being recruited by hedge funds, Google, and the NSA, and have a funded \"chair\" for each of them such that they can be prepared to dedicate their careers to the problem. That part alone requires tens of millions of dollars for just a few researchers.</p>\n<p>Other efforts like the Summit, Less Wrong, outreach work, and early publications cost money, and they work toward having the community and infrastructure required to start funding chairs for top-level mathematicians to be career Friendly AI researchers. This kind of work costs between $500,000 and $3 million per year, with more money per year of course producing more progress.</p>\n<p>&nbsp;</p>\n<h4>Predictions</h4>\n<p>Wix asks:</p>\n<blockquote>\n<p>How much do members' predictions of when the singularity will happen differ within the Singularity Institute?</p>\n</blockquote>\n<p>I asked some Singularity Institute staff members to answer a slightly different question, one pulled from the Future of Humanity Institute's <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0015/21516/MI_survey.pdf\">2011 machine intelligence survey</a>:</p>\n<blockquote>\n<p>Assuming no global catastrophe halts progress, by what year would you assign a&nbsp;10%/50%/90% chance of the development of human-level machine intelligence? Feel free to answer &lsquo;never&rsquo; if you believe such a milestone will never be reached.</p>\n</blockquote>\n<p>In short, the survey participants' median estimates (excepting 5 outliers) for 10%/50%/90% were:</p>\n<p style=\"padding-left: 30px; \">2028 / 2050 / 2150</p>\n<p>Here are five of the Singularity Institute's staff members' responses, names unattached, for the years by which they would assign a 10%/50%/90% chance of HLAI creation, conditioning on no global catastrophe halting scientific progress:</p>\n<p>&nbsp;</p>\n<ul>\n<li>2025 / 2073 / 2168</li>\n<li>2030 / 2060 / 2200</li>\n<li>2027 / 2055 / 2160</li>\n<li>2025 / 2045 / 2100&nbsp;</li>\n<li>2040 / 2080 / 2200</li>\n</ul>\n<p>&nbsp;</p>\n<p>Those are all the answers I had time to prepare in this round; I hope they are helpful!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DWDA33qGTMX9ZJejp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 30, "extendedScore": null, "score": 8.269843847940368e-07, "legacy": true, "legacyId": "11952", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Previously: <a href=\"http://intelligence.org/blog/2011/09/15/interview-with-new-singularity-institute-research-fellow-luke-muehlhuaser-september-2011/\">Interview as a researcher</a>, <a href=\"/lw/8s6/video_qa_with_singularity_institute_executive/\">Q&amp;A #1</a></small></p>\n<p>This is my second Q&amp;A as Executive Director of the Singularity Institute. I'll skip the video this time.</p>\n<p>&nbsp;</p>\n<h4 id=\"Singularity_Institute_Activities\">Singularity Institute Activities</h4>\n<p>Bugmaster asks:</p>\n<blockquote>\n<p>...what does the SIAI actually <em>do</em>? You don't submit your work to rigorous scrutiny by your peers in the field... you either aren't doing any AGI research, or are keeping it so secret that no one knows about it... and you aren't developing any practical applications of AI, either... So, what is it that you are actually working on, other than growing the SIAI itself ?</p>\n</blockquote>\n<p>It's a good question, and my own biggest concern right now. Donors would like to know: Where is the visible return on investment? How can I see that I'm buying existential risk reduction when I donate to the Singularity Institute?</p>\n<p>SI has a problem, here, because it has done so much invisible work lately. Our researchers have done a ton of work that hasn't been written up and published yet; Eliezer has been writing his rationality books that aren't yet published; Anna and Eliezer have been developing a new rationality curriculum for the future \"Rationality Org\" that will be spun off from the Singularity Institute; Carl has been doing a lot of mostly invisible work in the optimal philanthropy community; and so on. I believe this is all valuable x-risk-reducing work, but of course not all of our supporters are willing to just take our word for it that we're doing valuable work. Our supporters want to see tangible results, and all they see is the Singularity Summit, a few papers a year, some web pages and Less Wrong posts, and a couple rationality training camps. That's good, but not good enough!</p>\n<p>I agree with this concern, which is why I'm focused on doing things that happen to be both x-risk-reducing and visible.&nbsp;</p>\n<p>First, we've been working on visible \"meta\" work that makes the Singularity Institute more transparent and effective in general: a strategic plan, a donor database (\"visible\" to donors in the form of thank-yous), a new website (forthcoming), and an annual report (forthcoming).</p>\n<p>Second, we're pushing to publish more research results this year. We have three chapters forthcoming in <em>The Singularity Hypothesis</em>, one chapter forthcoming in <em>The Cambridge Handbook of Artificial Intelligence</em>, one forthcoming article on the difficulty of AI, and several other articles and working papers we're planning to publish in 2012. I've also begun writing the first comprehensive <a href=\"http://lukeprog.com/SaveTheWorld.html\">outline of open problems in Singularity research</a>, so that interested researchers from around the world can participate in solving the world's most important problems.</p>\n<p>Third, there is visible rationality work forthcoming. One of Eliezer's books is now being shopped to agents and publishers, and we're field-testing different versions of rationality curriculum material for use in Less Wrong meetups and classes.</p>\n<p>Fourth, we're expanding the Singularity Summit brand, an important platform for spreading the memes of x-risk reduction and AI safety.</p>\n<p>So my answer is to the question is: \"Yes, visible return on investment has been a problem lately due to our choice of projects. Even before I was made Executive Director, it was one of my top concerns to help correct that situation, and this is still the case today.\"</p>\n<p>&nbsp;</p>\n<h4 id=\"What_if_\">What if?</h4>\n<p>XiXiDu asks:</p>\n<blockquote>\n<p>What would SI do if it became apparent that AGI is at most 10 years away?</p>\n</blockquote>\n<p>This would be a serious problem because by default, AGI will be extremely destructive, and we don't yet know how to make AGI not be destructive.</p>\n<p>What would we do if we thought AGI was at most 10 years away?</p>\n<p>This depends on whether it's apparent to a wider public that AGI is at most 10 years away, or a conclusion based only on a nonpublic analysis.</p>\n<p>If it becomes apparent to a wide variety of folks that AGI is close, then it should be much easier to get people and support for Friendly AI work, so a big intensification of effort would be a good move. If the analysis that AGI is 10 years away leads to hundreds of well-staffed and well-funded AGI research programs and a rich public literature, then trying to outrace the rest with a Friendly AI project becomes much harder. After an intensified Friendly AI effort, one could try to build up knowledge in Friendly AI theory and practice that could be applied (somewhat less effectively) to systems not designed from the ground up for Friendliness. This knowledge could then be distributed widely to increase the odds of a project pulling through, calling in real Friendliness experts, etc. But in general, a widespread belief that AGI is only 10 years away would be a much hairier situation than the one we're in now.</p>\n<p>But if the basis for thinking AI was 10 years away was nonpublic (but nonetheless persuasive to supporters who have lots of resources), then it could be used to differentially attract support to a Friendly AI project, hopefully without provoking dozens of AGI teams to intensify their efforts. So if we had a convincing case that AGI was only 10 years away, we might not publicize this but would instead make the case to individual supporters that we needed to immediately intensify our efforts toward a theory of Friendly AI in a way that only much greater funding can allow.</p>\n<p>&nbsp;</p>\n<h4 id=\"Budget\">Budget</h4>\n<p>MileyCyrus asks:</p>\n<blockquote>\n<p>What kind of budget would be required to solve the friendly AI problem?</p>\n</blockquote>\n<p>Large research projects always come with large uncertainties concerning how difficult they will be, especially ones that require fundamental breakthroughs in mathematics and philosophy like Friendly AI does.</p>\n<p>Even a small, 10-person team of top-level Friendly AI researchers taking academic-level salaries for a decade would require tens of millions of dollars. And even getting to the point where you can raise that kind of money requires a slow \"ramping up\" of researcher recruitment and output. We need enough money to attract the kinds of mathematicians who are also being recruited by hedge funds, Google, and the NSA, and have a funded \"chair\" for each of them such that they can be prepared to dedicate their careers to the problem. That part alone requires tens of millions of dollars for just a few researchers.</p>\n<p>Other efforts like the Summit, Less Wrong, outreach work, and early publications cost money, and they work toward having the community and infrastructure required to start funding chairs for top-level mathematicians to be career Friendly AI researchers. This kind of work costs between $500,000 and $3 million per year, with more money per year of course producing more progress.</p>\n<p>&nbsp;</p>\n<h4 id=\"Predictions\">Predictions</h4>\n<p>Wix asks:</p>\n<blockquote>\n<p>How much do members' predictions of when the singularity will happen differ within the Singularity Institute?</p>\n</blockquote>\n<p>I asked some Singularity Institute staff members to answer a slightly different question, one pulled from the Future of Humanity Institute's <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0015/21516/MI_survey.pdf\">2011 machine intelligence survey</a>:</p>\n<blockquote>\n<p>Assuming no global catastrophe halts progress, by what year would you assign a&nbsp;10%/50%/90% chance of the development of human-level machine intelligence? Feel free to answer \u2018never\u2019 if you believe such a milestone will never be reached.</p>\n</blockquote>\n<p>In short, the survey participants' median estimates (excepting 5 outliers) for 10%/50%/90% were:</p>\n<p style=\"padding-left: 30px; \">2028 / 2050 / 2150</p>\n<p>Here are five of the Singularity Institute's staff members' responses, names unattached, for the years by which they would assign a 10%/50%/90% chance of HLAI creation, conditioning on no global catastrophe halting scientific progress:</p>\n<p>&nbsp;</p>\n<ul>\n<li>2025 / 2073 / 2168</li>\n<li>2030 / 2060 / 2200</li>\n<li>2027 / 2055 / 2160</li>\n<li>2025 / 2045 / 2100&nbsp;</li>\n<li>2040 / 2080 / 2200</li>\n</ul>\n<p>&nbsp;</p>\n<p>Those are all the answers I had time to prepare in this round; I hope they are helpful!</p>", "sections": [{"title": "Singularity Institute Activities", "anchor": "Singularity_Institute_Activities", "level": 1}, {"title": "What if?", "anchor": "What_if_", "level": 1}, {"title": "Budget", "anchor": "Budget", "level": 1}, {"title": "Predictions", "anchor": "Predictions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "39 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yGZHQYqWkLMbXy3z7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T05:09:36.511Z", "modifiedAt": null, "url": null, "title": "Roger Williams (Author of Metamorphosis of Prime Intellect) on Singularity", "slug": "roger-williams-author-of-metamorphosis-of-prime-intellect-on", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.355Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/irJSDFjW3hef9N93T/roger-williams-author-of-metamorphosis-of-prime-intellect-on", "pageUrlRelative": "/posts/irJSDFjW3hef9N93T/roger-williams-author-of-metamorphosis-of-prime-intellect-on", "linkUrl": "https://www.lesswrong.com/posts/irJSDFjW3hef9N93T/roger-williams-author-of-metamorphosis-of-prime-intellect-on", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Roger%20Williams%20(Author%20of%20Metamorphosis%20of%20Prime%20Intellect)%20on%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARoger%20Williams%20(Author%20of%20Metamorphosis%20of%20Prime%20Intellect)%20on%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirJSDFjW3hef9N93T%2Froger-williams-author-of-metamorphosis-of-prime-intellect-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Roger%20Williams%20(Author%20of%20Metamorphosis%20of%20Prime%20Intellect)%20on%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirJSDFjW3hef9N93T%2Froger-williams-author-of-metamorphosis-of-prime-intellect-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirJSDFjW3hef9N93T%2Froger-williams-author-of-metamorphosis-of-prime-intellect-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p>This is Roger's article on Singularity issues, connecting his MOPI novel to how things might/should happen.</p>\n<p><a href=\"http://localroger.com/prime-intellect/mopising.html\">http://localroger.com/prime-intellect/mopising.html</a></p>\n<p>(Roger is clearly aware of SIAI ideas and has referenced Eliezer in some of his posts)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "irJSDFjW3hef9N93T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 9, "extendedScore": null, "score": 8.270179401481555e-07, "legacy": true, "legacyId": "11953", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T05:55:35.061Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Parable of Hemlock", "slug": "seq-rerun-the-parable-of-hemlock", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6cATNYkMLndYeZ7sS/seq-rerun-the-parable-of-hemlock", "pageUrlRelative": "/posts/6cATNYkMLndYeZ7sS/seq-rerun-the-parable-of-hemlock", "linkUrl": "https://www.lesswrong.com/posts/6cATNYkMLndYeZ7sS/seq-rerun-the-parable-of-hemlock", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Parable%20of%20Hemlock&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Parable%20of%20Hemlock%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6cATNYkMLndYeZ7sS%2Fseq-rerun-the-parable-of-hemlock%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Parable%20of%20Hemlock%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6cATNYkMLndYeZ7sS%2Fseq-rerun-the-parable-of-hemlock", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6cATNYkMLndYeZ7sS%2Fseq-rerun-the-parable-of-hemlock", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>Today's post, <a href=\"/lw/nf/the_parable_of_hemlock/\">The Parable of Hemlock</a> was originally published on 03 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Socrates is a human, and humans, by definition, are mortal. So if you defined humans to not be mortal, would Socrates live forever?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/97c/seq_rerun_the_parable_of_the_dagger/\">The Parable of the Dagger</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6cATNYkMLndYeZ7sS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 8.270351988490277e-07, "legacy": true, "legacyId": "11957", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bcM5ft8jvsffsZZ4Y", "mME3tP9HkaZtNnnNw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T14:54:51.900Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Fort Collins and Melbourne", "slug": "weekly-lw-meetups-fort-collins-and-melbourne", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2fyxA7kHikfaeaEbz/weekly-lw-meetups-fort-collins-and-melbourne", "pageUrlRelative": "/posts/2fyxA7kHikfaeaEbz/weekly-lw-meetups-fort-collins-and-melbourne", "linkUrl": "https://www.lesswrong.com/posts/2fyxA7kHikfaeaEbz/weekly-lw-meetups-fort-collins-and-melbourne", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Fort%20Collins%20and%20Melbourne&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Fort%20Collins%20and%20Melbourne%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2fyxA7kHikfaeaEbz%2Fweekly-lw-meetups-fort-collins-and-melbourne%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Fort%20Collins%20and%20Melbourne%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2fyxA7kHikfaeaEbz%2Fweekly-lw-meetups-fort-collins-and-melbourne", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2fyxA7kHikfaeaEbz%2Fweekly-lw-meetups-fort-collins-and-melbourne", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 361, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/5m\">Fort Collins, Colorado Meetup:&nbsp;<span class=\"date\">04 January 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/5r\">First Philadelphia Meetup of 2012:&nbsp;<span class=\"date\">08 January 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/5p\">[TENTATIVE] Portland Meetup?:&nbsp;<span class=\"date\">14 January 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/5q\">San Diego experimental meetup:&nbsp;<span class=\"date\">15 January 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/5j\">Salt Lake City, Late January 2012</a></li>\n<li><a href=\"/meetups/5n\">Columbus or Cincinnati Meetup:&nbsp;<span class=\"date\">22 January 2012 05:00PM</span></a><a href=\"/meetups/5n\"></a></li>\n<li><a href=\"/meetups/5f\">First Brussels meetup:&nbsp;<span class=\"date\">11 February 2012 11:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/5o\">Melbourne practical rationality meetup:&nbsp;<span class=\"date\">06 January 2012 07:00AM</span></a></li>\n</ul>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2fyxA7kHikfaeaEbz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.272376858706748e-07, "legacy": true, "legacyId": "11800", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T17:23:02.978Z", "modifiedAt": null, "url": null, "title": "Explained: G\u00f6del's theorem and the Banach-Tarski Paradox", "slug": "explained-goedel-s-theorem-and-the-banach-tarski-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.066Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B6i6XuW7kuxmjC4Zr/explained-goedel-s-theorem-and-the-banach-tarski-paradox", "pageUrlRelative": "/posts/B6i6XuW7kuxmjC4Zr/explained-goedel-s-theorem-and-the-banach-tarski-paradox", "linkUrl": "https://www.lesswrong.com/posts/B6i6XuW7kuxmjC4Zr/explained-goedel-s-theorem-and-the-banach-tarski-paradox", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Explained%3A%20G%C3%B6del's%20theorem%20and%20the%20Banach-Tarski%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExplained%3A%20G%C3%B6del's%20theorem%20and%20the%20Banach-Tarski%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB6i6XuW7kuxmjC4Zr%2Fexplained-goedel-s-theorem-and-the-banach-tarski-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Explained%3A%20G%C3%B6del's%20theorem%20and%20the%20Banach-Tarski%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB6i6XuW7kuxmjC4Zr%2Fexplained-goedel-s-theorem-and-the-banach-tarski-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB6i6XuW7kuxmjC4Zr%2Fexplained-goedel-s-theorem-and-the-banach-tarski-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 750, "htmlBody": "<p>I want to share the following explanations that I came across recently and which I enjoyed very much. I can't tell and don't suspect that they come close to an understanding of the original concepts but that they are so easy to grasp that it is worth the time if you don't already studied the extended formal versions of those concepts. In other words, by reading the following explanations your grasp of the matter will be <em>less</em> wrong than before but not necessarily correct.</p>\n<h3>World's shortest explanation of G&ouml;del's theorem</h3>\n<p><em>by Raymond Smullyan, '5000 BC and Other Philosophical Fantasies'</em><em> via <a href=\"http://blog.plover.com/math/Gdl-Smullyan.html\">Mark Dominus</a> </em>(ask me for the PDF of the book)</p>\n<blockquote>\n<p>We have some sort of machine that prints out statements in some sort of language. It needn't be a statement-printing machine exactly; it could be some sort of technique for taking statements and deciding if they are true. But let's think of it as a machine that prints out statements.</p>\n<p>In particular, some of the statements that the machine might (or might not) print look like these:</p>\n<table border=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td align=\"right\">P*x</td>\n<td>(which means that</td>\n<td>the machine will print x)</td>\n</tr>\n<tr>\n<td align=\"right\">NP*x</td>\n<td>(which means that</td>\n<td>the machine will never print x)</td>\n</tr>\n<tr>\n<td align=\"right\">PR*x</td>\n<td>(which means that</td>\n<td>the machine will print xx)</td>\n</tr>\n<tr>\n<td align=\"right\">NPR*x</td>\n<td>(which means that</td>\n<td>the machine will never print xx)</td>\n</tr>\n</tbody>\n</table>\n<p>For example, NPR*FOO means that the machine will never print FOOFOO. NP*FOOFOO means the same thing. So far, so good.</p>\n<p>Now, let's consider the statement NPR*NPR*. This statement asserts that the machine will never print NPR*NPR*.</p>\n<p>Either the machine prints NPR*NPR*, or it never prints NPR*NPR*.</p>\n<p>If the machine prints NPR*NPR*, it has printed a false statement. But if the machine never prints NPR*NPR*, then NPR*NPR* is a true statement that the machine never prints.</p>\n<p>So either the machine sometimes prints false statements, or there are true statements that it never prints.</p>\n<p>So any machine that prints only true statements must fail to print some true statements.</p>\n<p>Or conversely, any machine that prints every possible true statement must print some false statements too.</p>\n</blockquote>\n<p>Mark Dominus further writes,</p>\n<blockquote>\n<p>The proof of G&ouml;del's theorem shows that there are statements of pure arithmetic that essentially express NPR*NPR*; the trick is to find some way to express NPR*NPR* as a statement about arithmetic, and most of the technical details (and cleverness!) of G&ouml;del's theorem are concerned with this trick. But once the trick is done, the argument can be applied to any machine or other method for producing statements about arithmetic.</p>\n<p>The conclusion then translates directly: any machine or method that produces statements about arithmetic either sometimes produces false statements, or else there are true statements about arithmetic that it never produces. Because if it produces something like NPR*NPR* then it is wrong, but if it fails to produce NPR*NPR*, then that is a true statement that it has failed to produce.</p>\n<p>So any machine or other method that produces only true statements about arithmetic must fail to produce some true statements.</p>\n</blockquote>\n<h3>The Banach-Tarski Paradox</h3>\n<p><em>by <a href=\"http://scientopia.org/blogs/goodmath/2012/01/06/the-banach-tarski-non-paradox/\">MarkCC</a></em></p>\n<blockquote>\n<p>Suppose you have a sphere. You can take that sphere, and slice it into a <em>finite</em> number of pieces. Then you can take those pieces, and re-assemble them so that, without any gaps, you now have <em>two</em> spheres of the exact same size as the original.</p>\n<p>[...]</p>\n<p>How about this? Take the set of all natural numbers. Divide it into two sets: the set of even naturals, and the set of odd naturals. Now you have two infinite sets, the set {0, 2, 4, 6, 8, ...}, and the set {1, 3, 5, 7, 9, ...}. The size of both of those sets is the &omega; - which is also the size of the original set you started with.</p>\n<p>Now take the set of even numbers, and map it so that for any given value i, f(i) = i/2. Now you've got a copy of the set of natural numbers. Take the set of odd naturals, and map them with g(i) = (i-1)/2. Now you've got a <em>second</em> copy of the set of natural numbers. So you've created two identical copies of the set of natural numbers out of the original set of natural numbers.</p>\n<p>[...] math doesn't have to follow conservation of mass [...]. A sphere doesn't <em>have</em> a mass. It's just an uncountably infinite set of points with a particular collection of topological relationship and geometric relationships.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B6i6XuW7kuxmjC4Zr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "11962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I want to share the following explanations that I came across recently and which I enjoyed very much. I can't tell and don't suspect that they come close to an understanding of the original concepts but that they are so easy to grasp that it is worth the time if you don't already studied the extended formal versions of those concepts. In other words, by reading the following explanations your grasp of the matter will be <em>less</em> wrong than before but not necessarily correct.</p>\n<h3 id=\"World_s_shortest_explanation_of_G_del_s_theorem\">World's shortest explanation of G\u00f6del's theorem</h3>\n<p><em>by Raymond Smullyan, '5000 BC and Other Philosophical Fantasies'</em><em> via <a href=\"http://blog.plover.com/math/Gdl-Smullyan.html\">Mark Dominus</a> </em>(ask me for the PDF of the book)</p>\n<blockquote>\n<p>We have some sort of machine that prints out statements in some sort of language. It needn't be a statement-printing machine exactly; it could be some sort of technique for taking statements and deciding if they are true. But let's think of it as a machine that prints out statements.</p>\n<p>In particular, some of the statements that the machine might (or might not) print look like these:</p>\n<table border=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td align=\"right\">P*x</td>\n<td>(which means that</td>\n<td>the machine will print x)</td>\n</tr>\n<tr>\n<td align=\"right\">NP*x</td>\n<td>(which means that</td>\n<td>the machine will never print x)</td>\n</tr>\n<tr>\n<td align=\"right\">PR*x</td>\n<td>(which means that</td>\n<td>the machine will print xx)</td>\n</tr>\n<tr>\n<td align=\"right\">NPR*x</td>\n<td>(which means that</td>\n<td>the machine will never print xx)</td>\n</tr>\n</tbody>\n</table>\n<p>For example, NPR*FOO means that the machine will never print FOOFOO. NP*FOOFOO means the same thing. So far, so good.</p>\n<p>Now, let's consider the statement NPR*NPR*. This statement asserts that the machine will never print NPR*NPR*.</p>\n<p>Either the machine prints NPR*NPR*, or it never prints NPR*NPR*.</p>\n<p>If the machine prints NPR*NPR*, it has printed a false statement. But if the machine never prints NPR*NPR*, then NPR*NPR* is a true statement that the machine never prints.</p>\n<p>So either the machine sometimes prints false statements, or there are true statements that it never prints.</p>\n<p>So any machine that prints only true statements must fail to print some true statements.</p>\n<p>Or conversely, any machine that prints every possible true statement must print some false statements too.</p>\n</blockquote>\n<p>Mark Dominus further writes,</p>\n<blockquote>\n<p>The proof of G\u00f6del's theorem shows that there are statements of pure arithmetic that essentially express NPR*NPR*; the trick is to find some way to express NPR*NPR* as a statement about arithmetic, and most of the technical details (and cleverness!) of G\u00f6del's theorem are concerned with this trick. But once the trick is done, the argument can be applied to any machine or other method for producing statements about arithmetic.</p>\n<p>The conclusion then translates directly: any machine or method that produces statements about arithmetic either sometimes produces false statements, or else there are true statements about arithmetic that it never produces. Because if it produces something like NPR*NPR* then it is wrong, but if it fails to produce NPR*NPR*, then that is a true statement that it has failed to produce.</p>\n<p>So any machine or other method that produces only true statements about arithmetic must fail to produce some true statements.</p>\n</blockquote>\n<h3 id=\"The_Banach_Tarski_Paradox\">The Banach-Tarski Paradox</h3>\n<p><em>by <a href=\"http://scientopia.org/blogs/goodmath/2012/01/06/the-banach-tarski-non-paradox/\">MarkCC</a></em></p>\n<blockquote>\n<p>Suppose you have a sphere. You can take that sphere, and slice it into a <em>finite</em> number of pieces. Then you can take those pieces, and re-assemble them so that, without any gaps, you now have <em>two</em> spheres of the exact same size as the original.</p>\n<p>[...]</p>\n<p>How about this? Take the set of all natural numbers. Divide it into two sets: the set of even naturals, and the set of odd naturals. Now you have two infinite sets, the set {0, 2, 4, 6, 8, ...}, and the set {1, 3, 5, 7, 9, ...}. The size of both of those sets is the \u03c9 - which is also the size of the original set you started with.</p>\n<p>Now take the set of even numbers, and map it so that for any given value i, f(i) = i/2. Now you've got a copy of the set of natural numbers. Take the set of odd naturals, and map them with g(i) = (i-1)/2. Now you've got a <em>second</em> copy of the set of natural numbers. So you've created two identical copies of the set of natural numbers out of the original set of natural numbers.</p>\n<p>[...] math doesn't have to follow conservation of mass [...]. A sphere doesn't <em>have</em> a mass. It's just an uncountably infinite set of points with a particular collection of topological relationship and geometric relationships.</p>\n</blockquote>", "sections": [{"title": "World's shortest explanation of G\u00f6del's theorem", "anchor": "World_s_shortest_explanation_of_G_del_s_theorem", "level": 1}, {"title": "The Banach-Tarski Paradox", "anchor": "The_Banach_Tarski_Paradox", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "40 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T18:10:39.264Z", "modifiedAt": null, "url": null, "title": "More intuitive explanations!", "slug": "more-intuitive-explanations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.974Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hDk2reWmo5GuadS3e/more-intuitive-explanations", "pageUrlRelative": "/posts/hDk2reWmo5GuadS3e/more-intuitive-explanations", "linkUrl": "https://www.lesswrong.com/posts/hDk2reWmo5GuadS3e/more-intuitive-explanations", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20intuitive%20explanations!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20intuitive%20explanations!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDk2reWmo5GuadS3e%2Fmore-intuitive-explanations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20intuitive%20explanations!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDk2reWmo5GuadS3e%2Fmore-intuitive-explanations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDk2reWmo5GuadS3e%2Fmore-intuitive-explanations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 477, "htmlBody": "<p>The post on two easy to grasp explanations on <a href=\"/r/discussion/lw/98a/explained_g%C3%B6dels_theorem_and_the_banachtarski/\">G&ouml;del's theorem and the Banach-Tarski paradox</a> made me think of other explanations that I found easy or insightful and that I could share them as well.</p>\n<p>1) Here is a nice <a href=\"http://www.cut-the-knot.org/pythagoras/index.shtml\"><strong>proof of the Pythagorean theorem</strong></a>:</p>\n<p><img src=\"http://kruel.co/img/pt.gif\" alt=\"\" width=\"251\" height=\"231\" /></p>\n<p>2) An easy and concise <strong><a href=\"http://facingthesingularity.com/2011/the-laws-of-thought/\">explanation of expected utility calculations</a></strong> by Luke Muehlhauser:</p>\n<blockquote>\n<p>Decision theory is about choosing among possible actions based on how much you desire the possible outcomes of those actions.</p>\n<p>How does this work? We can describe what you want with something called a <em>utility function</em>, which assigns a number that expresses how much you desire each possible outcome (or &ldquo;description of an entire possible future&rdquo;). Perhaps a single scoop of ice cream has 40 &ldquo;utils&rdquo; for you, the death of your daughter has -\u2060274,000 utils for you, and so on. This numerical representation of everything you care about is your utility function.</p>\n<p>We can combine your probabilistic beliefs and your utility function to calculate the <em>expected utility</em>&nbsp;for any action under consideration. The expected utility of an action is the average utility of the action&rsquo;s possible outcomes, weighted by the probability that each outcome occurs.</p>\n<p>Suppose you&rsquo;re walking along a freeway with your young daughter. You see an ice cream stand across the freeway, but you recently injured your leg and wouldn&rsquo;t be able to move quickly across the freeway. Given what you know, if you send your daughter across the freeway to get you some ice cream, there&rsquo;s a 60% chance you&rsquo;ll get some ice cream, a 5% your child will be killed by speeding cars, and other probabilities for other outcomes.</p>\n<p>To calculate the expected utility of sending your daughter across the freeway for ice cream, we multiply the utility of the first outcome by its probability: 0.6&nbsp;&times; 40 = 24. Then, we add to this the product of the next outcome&rsquo;s utility and its probability: 24 + (0.05&nbsp;&times; -\u2060274,000) = -\u206013,676. And suppose the sum of the products of the utilities and probabilities for other possible outcomes was 0. The expected utility of sending your daughter across the freeway for ice cream is thus <em>very low</em>&nbsp;(as we would expect from common sense). You should probably take one of the <em>other</em> actions available to you, for example the action of <em>not</em>&nbsp;sending your daughter across the freeway for ice cream &mdash; or, some action with even <em>higher</em>&nbsp;expected utility.</p>\n<p>A rational agent aims to maximize its expected utility, because an agent that does so will on average get the <em>most possible</em> of what it wants, given its beliefs and desires.</p>\n</blockquote>\n<p>3) <a href=\"http://kruel.co/micro-macroevolution.jpg\"><strong>Micro- and macroevolution</strong></a> visualized.</p>\n<p>4) <strong><a href=\"http://mathforum.org/library/drmath/view/54476.html\">Slopes of Perpendicular Lines</a></strong>.</p>\n<p>5) <a href=\"http://en.wikipedia.org/wiki/Euler%27s_formula#Using_power_series\"><strong>Proof of Euler's formula</strong></a> using power series expansions.</p>\n<p>6) <strong><a href=\"http://kruel.co/math/chainrule.pdf\">Proof of the Chain Rule</a></strong>.</p>\n<p>7) <strong><a href=\"http://www.mathsisfun.com/multiplying-negatives.html\">Multiplying Negatives Makes A Positive</a></strong>.</p>\n<p>8) <a href=\"http://www.mathsisfun.com/algebra/completing-square.html\"><strong>Completing the Square</strong></a> and <strong><a href=\"http://www.mathsisfun.com/algebra/quadratic-equation-derivation.html\">Derivation of Quadratic Formula</a></strong>.</p>\n<p>9) <strong><a href=\"http://en.wikipedia.org/wiki/Quadratic_equation#Quadratic_factorization\">Quadratic factorization</a></strong>.</p>\n<p>10) <strong><a href=\"http://www.mathsisfun.com/algebra/polynomials-remainder-factor.html\">Remainder Theorem and Factor Theorem</a></strong>.</p>\n<p>11) <a href=\"http://www.scribd.com/doc/36270691/Combinations-With-Repetitions\"><strong>Combinations with repetitions</strong></a>.</p>\n<p>12) <strong><a href=\"http://www.tark.org/proceedings/tark_mar19_86/p341-smullyan.pdf\">L&ouml;b's theorem</a></strong>.</p>\n<p><a class=\"l vst\" href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCEQFjAA&amp;url=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D22652&amp;ei=czgHT8qPDMnysgaEveCCDw&amp;usg=AFQjCNHTe0OAFbSspZNgP9XRquCPI6gUBA&amp;sig2=1-I_kYyoN6rgSj5-6SHk8w\"><em><br /></em></a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"z95PGFXtPpwakqkTA": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hDk2reWmo5GuadS3e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 37, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "11963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B6i6XuW7kuxmjC4Zr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T18:40:06.238Z", "modifiedAt": null, "url": null, "title": "Less wrong has a fitocracy group (invites)", "slug": "less-wrong-has-a-fitocracy-group-invites", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.868Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uiv6zEEBBgB2Sj8p8/less-wrong-has-a-fitocracy-group-invites", "pageUrlRelative": "/posts/uiv6zEEBBgB2Sj8p8/less-wrong-has-a-fitocracy-group-invites", "linkUrl": "https://www.lesswrong.com/posts/uiv6zEEBBgB2Sj8p8/less-wrong-has-a-fitocracy-group-invites", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20wrong%20has%20a%20fitocracy%20group%20(invites)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20wrong%20has%20a%20fitocracy%20group%20(invites)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuiv6zEEBBgB2Sj8p8%2Fless-wrong-has-a-fitocracy-group-invites%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20wrong%20has%20a%20fitocracy%20group%20(invites)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuiv6zEEBBgB2Sj8p8%2Fless-wrong-has-a-fitocracy-group-invites", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuiv6zEEBBgB2Sj8p8%2Fless-wrong-has-a-fitocracy-group-invites", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>Fitocracy is a fitness gamification social site for logging and tracking workouts. &nbsp;It has an RPG like system with quests and&nbsp;leveling&nbsp;up. &nbsp;It makes it very easy to make public commitments to a workout plan and provide each other with motivation and advice.</p>\n<p>I shouldn't really have to go over why you need to be working out if you plan on living&nbsp;until the singularity :)</p>\n<p><a style=\"border-image: initial; outline-width: 0px; outline-style: initial; outline-color: initial; vertical-align: baseline; color: #3760b5; text-decoration: none; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 12px; line-height: 24px; background-color: #f0f3f5; padding: 0px; margin: 0px; border: 0px initial initial;\" href=\"http://ftcy.co/vMOYfA\">http://ftcy.co/vMOYfA</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uiv6zEEBBgB2Sj8p8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 8.273222836615159e-07, "legacy": true, "legacyId": "11964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T18:52:09.991Z", "modifiedAt": null, "url": null, "title": "Nick Bostrom TED talk on world's biggest problems", "slug": "nick-bostrom-ted-talk-on-world-s-biggest-problems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/igsjidZSc2dq37KB3/nick-bostrom-ted-talk-on-world-s-biggest-problems", "pageUrlRelative": "/posts/igsjidZSc2dq37KB3/nick-bostrom-ted-talk-on-world-s-biggest-problems", "linkUrl": "https://www.lesswrong.com/posts/igsjidZSc2dq37KB3/nick-bostrom-ted-talk-on-world-s-biggest-problems", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nick%20Bostrom%20TED%20talk%20on%20world's%20biggest%20problems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANick%20Bostrom%20TED%20talk%20on%20world's%20biggest%20problems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigsjidZSc2dq37KB3%2Fnick-bostrom-ted-talk-on-world-s-biggest-problems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nick%20Bostrom%20TED%20talk%20on%20world's%20biggest%20problems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigsjidZSc2dq37KB3%2Fnick-bostrom-ted-talk-on-world-s-biggest-problems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigsjidZSc2dq37KB3%2Fnick-bostrom-ted-talk-on-world-s-biggest-problems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.ted.com/talks/nick_bostrom_on_our_biggest_problems.html\">http://www.ted.com/talks/nick_bostrom_on_our_biggest_problems.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "igsjidZSc2dq37KB3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 23, "extendedScore": null, "score": 8.273268146786271e-07, "legacy": true, "legacyId": "11965", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-06T21:28:00.179Z", "modifiedAt": null, "url": null, "title": "What Curiosity Looks Like", "slug": "what-curiosity-looks-like", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:08.864Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3oYaLja5h8qL5adDn/what-curiosity-looks-like", "pageUrlRelative": "/posts/3oYaLja5h8qL5adDn/what-curiosity-looks-like", "linkUrl": "https://www.lesswrong.com/posts/3oYaLja5h8qL5adDn/what-curiosity-looks-like", "postedAtFormatted": "Friday, January 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Curiosity%20Looks%20Like&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Curiosity%20Looks%20Like%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oYaLja5h8qL5adDn%2Fwhat-curiosity-looks-like%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Curiosity%20Looks%20Like%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oYaLja5h8qL5adDn%2Fwhat-curiosity-looks-like", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oYaLja5h8qL5adDn%2Fwhat-curiosity-looks-like", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 488, "htmlBody": "<p><small>See also: <a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a>, <a href=\"/lw/jz/the_meditation_on_curiosity/\">The Meditation on Curiosity</a>, <a href=\"/lw/4ku/use_curiosity/\">Use Curiosity</a></small></p>\n<p>What would it look like if someone was&nbsp;<em>truly curious</em>&nbsp;&mdash;&nbsp;if they <em>actually wanted true beliefs</em>? Not someone who wanted to <em>feel</em>&nbsp;like they sought the truth, or to&nbsp;<em>feel</em>&nbsp;their beliefs were justified. Not someone who wanted to <a href=\"http://www.overcomingbias.com/tag/signaling\">signal</a> a desire for true beliefs.&nbsp;No: someone who <em>really</em>&nbsp;wanted true beliefs. What would that look like?</p>\n<p>A truly curious person would seek to understand the world as broadly and deeply as possible. <a href=\"http://en.wikipedia.org/wiki/Singular_they\">They</a> would study the humanities but especially math and the sciences. They would study logic, probability theory, argument, scientific method, and other core tools of truth-seeking. They would inquire into <a href=\"http://plato.stanford.edu/entries/epistemology/\">epistemology</a>, the study of knowing. They would <a href=\"http://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/\">study artificial intelligence</a> to learn the algorithms, the <em>math</em>, the <a href=\"http://facingthesingularity.com/2011/the-laws-of-thought/\">laws</a> of how an <em>ideal</em>&nbsp;agent would acquire true beliefs. They would study modern psychology and neuroscience to learn <a href=\"http://facingthesingularity.com/2011/the-crazy-robots-rebellion/\">how their brain</a>&nbsp;acquires beliefs, and how those processes <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">depart</a> from ideal truth-seeking processes. And they would study&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">how to minimize their thinking errors</a>.</p>\n<p>They would practice truth-seeking skills as a musician practices playing her instrument. They would practice \"debiasing\"&nbsp;<a href=\"/lw/5x8/teachable_rationality_skills/\">techniques</a> for reducing common thinking errors. They would seek out <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">contexts</a> known to make truth-seeking more successful. They would <a href=\"/lw/8ir/where_do_i_most_obviously_still_need_to_say_oops/\">ask others</a> to help them on their journey. They would ask to be <a href=\"/r/discussion/lw/95u/describe_your_personal_mount_stupid/\">held accountable</a>.</p>\n<p>They would cultivate <a href=\"http://yudkowsky.net/rational/virtues\">that burning itch to <em>know</em></a>. They would admit their ignorance but seek to destroy it.</p>\n<p>They would be <a href=\"/lw/ic/the_virtue_of_narrowness/\">precise</a>, not vague. They would be clear, not <a href=\"http://richarddawkins.net/articles/824-postmodernism-disrobed\">obscurantist</a>.</p>\n<p>They would not <a href=\"http://facingthesingularity.com/2011/dont-flinch-away/\">flinch away</a> from experiences that might destroy their beliefs. They would train their emotions to fit the facts.</p>\n<p>They would update their beliefs quickly. They would resist the human impulse to&nbsp;<a href=\"/lw/ju/rationalization/\">rationalize</a>.</p>\n<p>But even <em>all this</em> could merely be a signaling game to increase their status in a group that rewards the appearance of curiosity. Thus, the final test for genuine curiosity is <em>behavioral change</em>. You would find a genuinely curious person studying and learning. You would find them practicing the skills of truth-seeking. You wouldn't merely find them saying, \"Okay, I'm updating my belief about that\" &mdash; you would also find them making decisions consistent with their new belief and inconsistent with their former belief.</p>\n<p>Every week I talk to people who say they are trying to figure out the truth about something. When I ask them a few questions about it, I often learn that they know almost nothing of logic, probability theory, argument, scientific method, epistemology, artificial intelligence, human cognitive science, or debiasing techniques. They do not regularly practice the skills of truth-seeking. They don't seem to <a href=\"/lw/i9/the_importance_of_saying_oops/\">say \"oops\"</a> very often, and they change their behavior even <em>less</em> often. I conclude that they probably want to <em>feel</em>&nbsp;they are truth-seeking, or they want to <em>signal</em> a desire for truth-seeking, or they might even self-deceivingly \"believe\" that they place a high value on knowing the truth. But their actions show that they aren't&nbsp;<a href=\"/lw/ui/use_the_try_harder_luke/\">trying very hard</a> to have true beliefs.</p>\n<p>Dare I say it? Few people <em>look</em>&nbsp;like they really want true beliefs.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"moeYqrcakMgXnQNyF": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3oYaLja5h8qL5adDn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 67, "baseScore": 43, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "11899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 286, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3nZMgRTfFEfHp34Gb", "WrSe4aB8sWBy3Nphm", "xLm9mgJRPvmPGpo7Q", "f4CZNEHirweN3XEjs", "Q5CjE8pRiACqTvhRM", "BgpnbaJMthXjDeHcE", "8XZPFGLJJvftnc3bm", "yDfxTj9TKYsYiWH5o", "SFZoEBpLo9frSJGkc", "wCqfCLs8z5Qw4GbKS", "fhEPnveFhb9tmd7Pe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-07T03:08:46.687Z", "modifiedAt": null, "url": null, "title": "Active AGI and/or FAI researchers", "slug": "active-agi-and-or-fai-researchers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2odNJqsz2BchS3CKP/active-agi-and-or-fai-researchers", "pageUrlRelative": "/posts/2odNJqsz2BchS3CKP/active-agi-and-or-fai-researchers", "linkUrl": "https://www.lesswrong.com/posts/2odNJqsz2BchS3CKP/active-agi-and-or-fai-researchers", "postedAtFormatted": "Saturday, January 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Active%20AGI%20and%2For%20FAI%20researchers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AActive%20AGI%20and%2For%20FAI%20researchers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2odNJqsz2BchS3CKP%2Factive-agi-and-or-fai-researchers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Active%20AGI%20and%2For%20FAI%20researchers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2odNJqsz2BchS3CKP%2Factive-agi-and-or-fai-researchers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2odNJqsz2BchS3CKP%2Factive-agi-and-or-fai-researchers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>I am curious in general about who here, if anyone, is actively researching the AGI and/or FAI problems directly in a full-time capacity (or soon will be). So if that's you, please say hello! Or if you know another website/mailing-list/etc this question would be more appropriate to ask, please let me know.</p>\n<p>If you are interested in saying more about who you are and what you're doing, I've included some additional questions below.&nbsp;Feel free to provide as much or as little information as you'd like - but the more the better!</p>\n<ul>\n<li>Are you working for a particular organization or a known AGI project? If so, which? Link? If not, are you working on these issues independently, or can you otherwise explain your situation?</li>\n<li>What is your overall theory/philosophy on FAI/AGI? How are you similar to and how are you different from Eliezer Yudkowsky in this respect?</li>\n<li>What has been your overall approach to study for this line of research / what specific curriculum and what specific books/papers/etc would you recommend? I would be interested in as much detail as you can provide here.</li>\n<li>Do you have any published material&nbsp;(even informal/in-progress information, documentation, discussion, blogs, etc)? Links?</li>\n<li>What are you working on now and what's coming next in your work? Are you solving some interesting problem, creating some interesting new idea, bringing together a grand theory, actually building a working FAI/AGI or similar, or approaching some big milestone along any of these paths or others? What do your plans and timeline for the future look like?</li>\n</ul>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2odNJqsz2BchS3CKP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 14, "extendedScore": null, "score": 8.275133934262744e-07, "legacy": true, "legacyId": "11981", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-07T03:30:33.879Z", "modifiedAt": null, "url": null, "title": "The Third Annual Young Cryonicists Gathering (2012)", "slug": "the-third-annual-young-cryonicists-gathering-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.381Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vYgFy9Zv3nwYqNMd7/the-third-annual-young-cryonicists-gathering-2012", "pageUrlRelative": "/posts/vYgFy9Zv3nwYqNMd7/the-third-annual-young-cryonicists-gathering-2012", "linkUrl": "https://www.lesswrong.com/posts/vYgFy9Zv3nwYqNMd7/the-third-annual-young-cryonicists-gathering-2012", "postedAtFormatted": "Saturday, January 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Third%20Annual%20Young%20Cryonicists%20Gathering%20(2012)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Third%20Annual%20Young%20Cryonicists%20Gathering%20(2012)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYgFy9Zv3nwYqNMd7%2Fthe-third-annual-young-cryonicists-gathering-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Third%20Annual%20Young%20Cryonicists%20Gathering%20(2012)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYgFy9Zv3nwYqNMd7%2Fthe-third-annual-young-cryonicists-gathering-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYgFy9Zv3nwYqNMd7%2Fthe-third-annual-young-cryonicists-gathering-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>I received notification of this today from Alcor, my cryonics provider. I also received notification of this event last year but was unable to attend at that time. Does anyone plan on attending this year? Has anyone attended this type of event in the past, and if so was it worth going / what was your experience?</p>\n<p>The Third Annual Young Cryonicists Gathering<br /> Teens &amp; Twenties 2012: &nbsp;Getting to Know You -&nbsp;You Getting to Know Each Other<br /> Friday-Sunday; April 27-29, 2012 &nbsp; Deerfield Beach &nbsp;FL &nbsp;Host: Bill Faloo</p>\n<p>I'm sorry I don't have more information at this time. Here is a link to a discussion post regarding the one last year:&nbsp;<a href=\"/lw/5ob/young_cryonicists_conference_2011/\">http://lesswrong.com/lw/5ob/young_cryonicists_conference_2011/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vYgFy9Zv3nwYqNMd7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 5, "extendedScore": null, "score": 8.275215803952637e-07, "legacy": true, "legacyId": "11983", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RABNzhGkrvav527nF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-07T05:30:41.437Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Words as Hidden Inferences", "slug": "seq-rerun-words-as-hidden-inferences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:30.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gs3i4QBwrG64DgGzh/seq-rerun-words-as-hidden-inferences", "pageUrlRelative": "/posts/gs3i4QBwrG64DgGzh/seq-rerun-words-as-hidden-inferences", "linkUrl": "https://www.lesswrong.com/posts/gs3i4QBwrG64DgGzh/seq-rerun-words-as-hidden-inferences", "postedAtFormatted": "Saturday, January 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Words%20as%20Hidden%20Inferences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Words%20as%20Hidden%20Inferences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgs3i4QBwrG64DgGzh%2Fseq-rerun-words-as-hidden-inferences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Words%20as%20Hidden%20Inferences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgs3i4QBwrG64DgGzh%2Fseq-rerun-words-as-hidden-inferences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgs3i4QBwrG64DgGzh%2Fseq-rerun-words-as-hidden-inferences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/ng/words_as_hidden_inferences/\">Words as Hidden Inferences</a> was originally published on 03 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The mere presence of words can influence thinking, sometimes misleading it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/985/seq_rerun_the_parable_of_hemlock/\">The Parable of Hemlock</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gs3i4QBwrG64DgGzh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.275667241707165e-07, "legacy": true, "legacyId": "11984", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3nxs2WYDGzJbzcLMp", "6cATNYkMLndYeZ7sS", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-07T09:07:53.775Z", "modifiedAt": null, "url": null, "title": "An argument that animals don't really suffer", "slug": "an-argument-that-animals-don-t-really-suffer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.049Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bAWXeFjfRZGbNH6BD/an-argument-that-animals-don-t-really-suffer", "pageUrlRelative": "/posts/bAWXeFjfRZGbNH6BD/an-argument-that-animals-don-t-really-suffer", "linkUrl": "https://www.lesswrong.com/posts/bAWXeFjfRZGbNH6BD/an-argument-that-animals-don-t-really-suffer", "postedAtFormatted": "Saturday, January 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20argument%20that%20animals%20don't%20really%20suffer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20argument%20that%20animals%20don't%20really%20suffer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbAWXeFjfRZGbNH6BD%2Fan-argument-that-animals-don-t-really-suffer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20argument%20that%20animals%20don't%20really%20suffer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbAWXeFjfRZGbNH6BD%2Fan-argument-that-animals-don-t-really-suffer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbAWXeFjfRZGbNH6BD%2Fan-argument-that-animals-don-t-really-suffer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 869, "htmlBody": "<p>I ended up reading <a href=\"http://www.reasonablefaith.org/site/News2?page=NewsArticle&amp;id=9229\">this article about animal suffering</a> by this Christian apologist called William Craig. Forgive the source, please.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<blockquote>\n<p style=\"font-size: 13px; color: #000066; line-height: 16px; text-align: justify;\">In his book&nbsp;<em>Nature Red in Tooth and Claw</em>, Michael Murray explains on the basis of neurological studies that there is an ascending three-fold hierarchy of pain awareness in nature:<a id=\"_ednref1\" style=\"color: #4c79ba; text-decoration: none;\" name=\"_ednref1\" href=\"http://www.reasonablefaith.org/site/News2?page=NewsArticle&amp;id=9229#_edn1\">i</a></p>\n<div class=\"artDoubleInset\" style=\"padding-top: 0px; padding-right: 40px; padding-bottom: 0px; padding-left: 40px; color: #000066; font-size: 13px; line-height: 16px; text-align: justify;\">\n<p style=\"font-size: 10pt;\">Level 3: Awareness that one is oneself in pain<br />Level 2: Mental states of pain<br />Level 1: Aversive reaction to noxious stimuli</p>\n</div>\n<p style=\"font-size: 13px; color: #000066; line-height: 16px; text-align: justify;\">Organisms which are not sentient, that is, have no mental life, display at most Level 1 reactions. Insects, worms, and other invertebrates react to noxious stimuli but lack the neurological capacity to feel pain. Their avoidance behavior obviously has a selective advantage in the struggle for survival and so is built into them by natural selection. The experience of pain is thus not necessary for an organism to exhibit aversive behavior to contact that may be injurious. Thus when your friend asks, &ldquo;If you beat an animal, wouldn't it try to avoid the source of pain so that way 'it' wouldn't suffer? Isn't that a form of 'self-awareness?',\" you can see that such aversive behavior doesn&rsquo;t even imply second order pain awareness, much less third order awareness. Avoidance behavior doesn&rsquo;t require pain awareness, and the neurological capacities of primitive organisms aren&rsquo;t sufficient to support Level 2 mental states.</p>\n<p style=\"font-size: 13px; color: #000066; line-height: 16px; text-align: justify;\">Level 2 awareness arrives on the scene with the vertebrates. Their nervous systems are sufficiently developed to have associated with certain brain states mental states of pain. So when we see an animal like a dog, cat, or horse thrashing about or screaming when injured, it is irresistible to ascribe to them second order mental states of pain. It is this experience of animal pain that forms the basis of the objection to God&rsquo;s goodness from animal suffering. But notice that an experience of Level 2 pain awareness does not imply a Level 3 awareness. Indeed, the biological evidence indicates that very few animals have an awareness that they are themselves in pain.</p>\n<p style=\"font-size: 13px; color: #000066; line-height: 16px; text-align: justify;\">Level 3 is a higher-order awareness that one is oneself experiencing a Level 2 state. Your friend asks, &ldquo;How could an animal not be aware of their suffering if they're yelping/screaming out of pain?\" Brain studies supply the remarkable answer. Neurological research indicates that there are two independent neural pathways associated with the experience of pain. The one pathway is involved in producing Level 2 mental states of being in pain. But there is an independent neural pathway that is associated with being aware that one is oneself in a Level 2 state. And this second neural pathway is apparently a very late evolutionary development which only emerges in the higher primates, including man. Other animals lack the neural pathways for having the experience of Level 3 pain awareness. So even though animals like zebras and giraffes, for example, experience pain when attacked by a lion, they really aren&rsquo;t aware of it.</p>\n<p style=\"font-size: 13px; color: #000066; line-height: 16px; text-align: justify;\">To help understand this, consider an astonishing analogous phenomenon in human experience known as blind sight. The experience of sight is also associated biologically with two independent neural pathways in the brain. The one pathway conveys visual stimuli about what external objects are presented to the viewer. The other pathway is associated with an awareness of the visual states. Incredibly, certain persons, who have experienced impairment to the second neural pathway but whose first neural pathway is functioning normally, exhibit what is called blind sight. That is to say, these people are effectively blind because they are not aware that they can see anything. But in fact, they do &ldquo;see&rdquo; in the sense that they correctly register visual stimuli conveyed by the first neural pathway. If you toss a ball to such a person he will catch it because he does see it. But he isn&rsquo;t aware that he sees it! Phenomenologically, he is like a person who is utterly blind, who doesn&rsquo;t receive any visual stimuli. Obviously, as Michael Murray says, it would be a pointless undertaking to invite a blind sighted person to spend an afternoon at the art gallery. For even though he, in a sense, sees the paintings on the walls, he isn&rsquo;t aware that he sees them and so has no experience of the paintings.</p>\n<p style=\"font-size: 13px; color: #000066; line-height: 16px; text-align: justify;\">Now neurobiology indicates a similar situation with respect to animal pain awareness. All animals but the great apes and man lack the neural pathways associated with Level 3 pain awareness. Being a very late evolutionary development, this pathway is not present throughout the animal world. What that implies is that throughout almost the entirety of the long history of evolutionary development, no creature was ever aware of being in pain.</p>\n</blockquote>\n<p style=\"font-size: 13px; color: #000066; line-height: 16px; text-align: justify;\"><a href=\"http://www.reasonablefaith.org/site/News2?page=NewsArticle&amp;id=9245\">He continues the argument here.</a></p>\n<p style=\"font-size: 13px; color: #000066; line-height: 16px; text-align: justify;\"><span style=\"color: #000000; line-height: normal; text-align: -webkit-auto; font-size: small;\">How decent do you think this argument is? I don't know where to look to evaluate the core claim, as I know very little neuroscience myself. I'm quite concerned about animal suffering, and choose to be vegetarian largely on the basis of that concern. How much should my decision on that be affected by this argument?</span></p>\n<p>EDIT: David_Gerard wins by doing the basic Google search that I neglected. It seems that the argument is flawed. Particularly, animals apart from primates have pre-frontal cortexes.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 2, "Q9ASuEEoJWxT3RLMT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bAWXeFjfRZGbNH6BD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 5, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "11990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-07T19:55:00.726Z", "modifiedAt": null, "url": null, "title": "\"Personal Identity and Uploading\", by Mark Walker", "slug": "personal-identity-and-uploading-by-mark-walker", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.245Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G3hxkSeDCMBucYsp6/personal-identity-and-uploading-by-mark-walker", "pageUrlRelative": "/posts/G3hxkSeDCMBucYsp6/personal-identity-and-uploading-by-mark-walker", "linkUrl": "https://www.lesswrong.com/posts/G3hxkSeDCMBucYsp6/personal-identity-and-uploading-by-mark-walker", "postedAtFormatted": "Saturday, January 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Personal%20Identity%20and%20Uploading%22%2C%20by%20Mark%20Walker&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Personal%20Identity%20and%20Uploading%22%2C%20by%20Mark%20Walker%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG3hxkSeDCMBucYsp6%2Fpersonal-identity-and-uploading-by-mark-walker%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Personal%20Identity%20and%20Uploading%22%2C%20by%20Mark%20Walker%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG3hxkSeDCMBucYsp6%2Fpersonal-identity-and-uploading-by-mark-walker", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG3hxkSeDCMBucYsp6%2Fpersonal-identity-and-uploading-by-mark-walker", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4664, "htmlBody": "<p><a href=\"http://jetpress.org/v22/walker.htm\">&ldquo;Personal Identity and Uploading&rdquo;</a>, <a href=\"http://www.nmsu.edu/~philos/mark-walkers-home-page.html\">Mark Walker</a> is the next JET paper. Abstract:</p>\n<blockquote>\n<p>Objections to uploading may be parsed into substrate issues, dealing with the computer platform of upload and personal identity. This paper argues that the personal identity issues of uploading are no more or less challenging than those of bodily transfer often discussed in the philosophical literature. It is argued that what is important in personal identity involves both token and type identity. While uploading does not preserve token identity, it does save type identity; and even qua token, one may have good reason to think that the preservation of the type is worth the cost.</p>\n</blockquote>\n<h1 id=\"uploading-prospects-and-perils\"><a id=\"more\"></a></h1>\n<h1><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Uploading: prospects and perils</a></h1>\n<blockquote>\n<p>If, like me, you think that uploading is possible (at least in principle), and so you hold that the first interpretation of these events is correct, then you must hold true the following three theses:</p>\n<blockquote>\n<p>1. <em>Computers are capable of supporting the important properties constitutive of personal identity, e.g., thought and consciousness.</em></p>\n</blockquote>\n<p>It is clear that uploading will not preserve all properties we associate with <em>Homo sapiens</em>, e.g., basic facts about the human digestive system are not likely to be preserved in uploading to a robotic body. But these facts are not typically thought to be important for personal identity. Candidates for important properties include thought, consciousness, emotions, creativity, aesthetic experience, sensory experience, empathy and so on. For the most part, the question of which properties are important is not as serious as it may first seem, since uploading promises to preserve the essential aspects of the brain and nervous system, which overlap with the usual lists of important properties for identity.</p>\n<p>&hellip;suffice it to say that if Searle is correct, then #1 may be false. For Searle thinks that a computer can never consciously think merely in virtue of instantiating a computer program, and the uploading process seems to be one of merely instantiating a computer program (Agar 2010, 2011).</p>\n<blockquote>\n<p>2. <em>It is possible to capture the information necessary to emulate the important properties of individual humans.</em></p>\n</blockquote>\n<p>&hellip;If we slice off layers of your neurons, and record the information of each layer, the lower layers will change (due to trauma or death).<sup>1</sup> If we flash freeze your brain, we may destroy some essential information. Philosophical questions arise as to whether the information encoded in the brain is sufficient to account for all the relevant properties. For example, consider a dualist who believes that we have souls in addition to brains, and much of what is morally important (e.g., conscious thought) resides in the soul. If the dualist is right, then scanning your brain could never be sufficient, for it would be necessary to scan your soul to access at least some of what is important. If it is unlikely that we will be able to scan souls, there will be an insurmountable obstacle to uploading. Notice how theses #1 and #2 may differ on this point: a dualist could consistently hold that a computer might have a soul; it is just that if computers have souls, it is not because we obtained the soul-building information from humans. (Perhaps God implants souls in humans and computers.)</p>\n<blockquote>\n<p>3. <em>It is possible to survive the uploading process.</em></p>\n</blockquote>\n<p>To see how #3 differs from #1 and #2, imagine that at some point in the future we have created computers of sufficient complexity that it is agreed that they have the same morally relevant properties as humans: these advanced computers think and are conscious, they are accorded rights, and the scanning problem has been solved so that we are able to scan the brain in such a way that we are not worried about loss of information. None of this answers the question of whether you have been preserved during uploading or whether uploading merely makes a very good copy of you&hellip;If an individual can be uploaded once, then it seems the same individual could be uploaded twice into separate computers, and indeed, billions of the same individual all embodied in separate robotic bodies could be created.</p>\n<p>To make the discussion manageable, I will focus on thesis #3, and assume without argument that #1 and #2 have been resolved in favor of uploading. So our question is this: assuming that computers can be conscious, have memories, and (robotic) bodies, and assuming that it is possible to scan and capture all the information of a human brain, does uploading preserve personal identity? I will argue that uploading does preserve personal identity; at least identity of a certain sort.</p>\n</blockquote>\n<h1 id=\"the-equivalency-thesis\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> The equivalency thesis</a></h1>\n<blockquote>\n<p>The fact that we are assuming that computers are capable of embodying all the same type of properties necessary for personal identity means that we can make use of the equivalency thesis:</p>\n<blockquote>\n<p><em>Equivalency thesis</em>: If it is possible for an individual to survive migration from a carbon to a carbon body, then it is possible for individuals to survive migration from a carbon to a silicon body.</p>\n</blockquote>\n<p>&hellip;There are a couple of reasons for invoking the equivalency thesis. The first is so that we are not misled by a new form of racism: substratism (Walker 2006). Substratism is the view that one&rsquo;s substrate is inherently superior to that of other substrates along the lines that racists think their race is inherently superior to some other race. In the present case, it would suggest the idea that carbon-based humans are inherently more morally worthy than silicon based beings. Consider the fact that we would not accept this argument: it is not possible for persons to migrate from one body to another because then it would be possible for people of skin color X to move to bodies of skin color Y, and Y skin color is morally inferior. We want to avoid the same bad argument in considering moving from one substrate to another. Notice that this does not beg the issue at hand, since it is possible to say that having a certain substrate (or even skin color) is constitutive of my identity; it merely prohibits saying that this property in itself makes for moral superiority.</p>\n<p>The second is that it makes directly relevant an enormous amount of philosophical effort that has gone into exploring the possibility of carbon-to-carbon transfers. The question of carbon-to-silicon transfers thus may piggyback on this effort.</p>\n</blockquote>\n<p>The thesis disturbed me the first time I saw it; it seemed to me that it either begged the basic philosophical question at point or it did not do any work. So I read on to see how it was used. It seems to be the latter case: the thesis is barely used and not really germane to the examples that criticize somaticism and argue for a type-token kind of personal identity. This is good because it seems like used in any kind of strong sense, it&rsquo;s easy to criticize the thesis.</p>\n<p>(Implicitly, it seems to scope over all individuals - that we could rewrite it as, &lsquo;for all individuals that survive any carbon-&gt;carbon transition, there is a carbon-&gt;silicon transition they survive&rsquo;. But this seems false: a book is made out of carbon, survives minute to minute or copy to copy, and can be satisfactorily uploaded, but can a squishy human brain? Can a bowl of water? If I take a stick of carbon and light it on fire, how do I upload the burning stick? What does an uploaded diamond <em>do</em>? One might say the physics of the constituent atoms can be uploaded and this is a correct emulation with any necessary properties like emergence, but then we&rsquo;re back to the question-begging.)</p>\n<h1 id=\"personal-identity-psychological-and-somatic-accounts\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> Personal identity: psychological and somatic accounts</a></h1>\n<blockquote>\n<p>Historically, there are two main schools of thought about what is required for personal survival; the psychological and somatic approaches (Olson 2002).</p>\n<p>&hellip;the psychological account says that what is essential for survival is continuity of psychological states such as memory, beliefs, desires and personality. John Locke, an early proponent of this view, famously described personal identity in terms of psychological continuity, within an analysis of personhood as consisting in existence as &ldquo;a thinking intelligent being, that has reason and reflection, and can consider itself as itself, the same thinking thing, in different times and places&hellip;&rdquo; (Locke 1975). The person on Mars who awakens will claim to remember being Derek Parfit, and to have memories and a personality that are psychologically indistinguishable from the person on earth whose body was destroyed. Locke, then, would say that Parfit survived teletransportation.</p>\n<p>Somaticist accounts suggest the survival of a particular body is critical for personal identity over time. Since the body on Earth is destroyed during the scanning process, Parfit ceases to be. A different person will awake on Mars. This person will of course have psychologically indistinguishable memories and personality to those of the late Parfit, but this person will not be Parfit. The new person will be but an infant in terms of chronological age: only a few minutes old. We will think of &ldquo;somaticism&rdquo; as the view that continuity of one&rsquo;s body is necessary for personal identity from one time to the next.<sup>2</sup> There are two ways that one might be a somaticist: one can believe that bodily continuity is necessary but not sufficient, or that it is necessary and sufficient. One easy case to distinguish these two is as follows: a piano falls on your head, and causes you to go into a permanent vegetative state. Your relatives discuss whether to &ldquo;pull the plug.&rdquo; Those who think that bodily continuity is necessary but not sufficient may say that you no longer exist, but your body continues to exist. Those who think that bodily continuity is necessary and sufficient will say that you continue to exist, albeit your cognitive capacities are non-existent. Both views qualify as &ldquo;somaticism&rdquo; in our sense.</p>\n</blockquote>\n<h2 id=\"against-somaticism-the-big-stroke\"><a href=\"#TOC\"><span class=\"header-section-number\">3.1</span> Against somaticism: the big stroke</a></h2>\n<blockquote>\n<p>The Vorlons,<sup>5</sup> a mysterious and intellectually advanced alien species, make this offer: you can have an original undiscovered play by Shakespeare written in his hand, or a copy of the play made by one of his lackeys. You salivate at the joy this will bring to the world (not to mention the fame and fortune it will bring you personally). Since you can have only one, the choice, it seems, is a no-brainer. You should opt for the one written by the bard&rsquo;s hand. But now consider this variant: the Vorlons tell you that the text written in Shakespeare&rsquo;s hand is missing the last two pages, while they assure you the copy written by the lackey is a perfectly faithful reproduction of all the words in the original. While it would be great to have both, you reason that the most important thing is the play itself be preserved, not Shakespeare&rsquo;s handwriting. The copy here is in some sense better than the original because the original has been damaged.</p>\n<p>&hellip;The Vorlons, with their ability to see into the future, say the news is grim. In less than twelve hours you will have a massive stroke that will cause you to lose many of your memories and some mobility, and impair your intelligence. Your stroke will not be as bad as some: the damage from the stroke will not leave you completely cognitively impaired, but you will no longer be able to work as an academic. You will have to find some relatively mindless job befitting your new level of intelligence, perhaps in academic administration&hellip;there is nothing the Vorlons can do to prevent the stroke. They provide a radical alternative: creating a perfect replica of you &ndash; down to the molecular level &ndash; with the exception that the problems with the arteries to your brain will be fixed in the body replica. They insist, however, that only one body can survive. You must choose tonight whether the replica or your current body survives.</p>\n<p>&hellip;Obviously, this example is structurally similar to Parfit&rsquo;s [suicidal teletransporter], but with one big exception: what is gained by having the replica survive is much more significant in this case than in the Teletransporter to Mars case. Parfit offers the incentive of avoiding three weeks of space travel. (We might not even sacrifice our original wedding ring for an exact replica if the benefit is merely avoiding three weeks in a spaceship). Here the incentive is the possibility of not having one&rsquo;s life radically altered by the stroke. The attachment to one&rsquo;s body does not seem worth the cost in this case.</p>\n<p>&hellip;It is worth noting that not all somaticists are likely to be convinced by this example.<sup>6</sup> But it should convince a few, and points out one of the heavy costs of somaticism.</p>\n</blockquote>\n<h2 id=\"against-somaticism-retrospective-replicas\"><a href=\"#TOC\"><span class=\"header-section-number\">3.2</span> Against somaticism: retrospective replicas</a></h2>\n<blockquote>\n<p>&hellip;perhaps it is this fear of the unknown, rather than a commitment to somaticism, that explains the reluctance to use the Teletransporter. We can test this thought by considering a retrospective rather than a prospective version of a replication scenario.</p>\n<p>Suppose that every night when people sleep their bodies (including their brains) are scanned by a swarm of nanobots and a molecule for molecule identical body is beamed from a hidden alien spaceship in orbit; the old body is vaporized in a manner that is undetectable by the human eye. Scientists discovered this fortuitously: physicists noticed a spike in neutrino levels every time psychologists in the adjoining lab conducted sleep experiments. Intrigued, scientists built a chamber to isolate subjects from neutrino influences and then had test subjects sleep in the chamber. Once the experiment was initiated, a hologram of a Vorlon appeared in the lab and spoke thusly:</p>\n<blockquote>\n<p>We are an ancient race known as the &ldquo;Vorlons.&rdquo; We battled another species, the &ldquo;Shadows,&rdquo; just as your species was beginning to evolve on this planet. One of the toxic effects of our war was a type of radiation that kills all higher intelligences within three days. We have no way of eliminating the radiation, but we have left advanced technology to recreate your bodies from different molecules every day so that the radiation will not harm you. We left the galaxy eons ago. You are hearing this message now because you have advanced technologically to the point where you can detect our technology. If you interfere with our replicator technology, you will quickly die of radiation poisoning.</p>\n</blockquote>\n<p>What should we make of this? It is clear that dismantling it is out of the question since all humans will die within three days. If you are a somaticist, you must conclude that you have been alive only for a very short while. In fact, you have existed only since last night. After all, the physical continuity of one&rsquo;s body has lasted only this length of time. However, most of us, I think, would conclude the opposite. That is, that we have existed for years: that we do not cease to exist every night and a new person comes into being.</p>\n<p>&hellip;the somaticist must now explain how so many people could be mistaken about their own identity retrospective case; after all, it seems very likely that, upon learning about the Vorlons&rsquo; technology, most would conduct their lives as if they hadn&rsquo;t just come into existence that day. Who is going to say such things as: &ldquo;I do not have to look after these children you call mine: how can I have children if I myself was born today. I can&rsquo;t use this driver&rsquo;s license, it is someone else&rsquo;s &ndash; I was just born today. I&rsquo;m not qualified to teach any classes: a postgraduate degree is required, which takes years to earn, and I was just born today?&rdquo;</p>\n</blockquote>\n<p>This example reminds me strongly of Nick Bostrom&rsquo;s <a href=\"http://www.nickbostrom.com/ethics/statusquo.pdf\">reversal test for the status quo bias</a>; an example would be a drug that increases IQ 10 points may be feared and rejected, but would it be accepted if scientists discovered new pollution will reduce IQ 10 points and that drug would compensate? I like his reversal test, and I like this example as well.</p>\n<h2 id=\"against-somaticism-practical-ethics\"><a href=\"#TOC\"><span class=\"header-section-number\">3.3</span> Against somaticism: practical ethics</a></h2>\n<blockquote>\n<p>&hellip;To emphasize, let us suppose that despite the fact that the preponderance of reasons seem to be against somaticism, imagine that the metaphysical reasons for and against somaticism are exactly balanced. Does this imply that we should be neutral on the issue? I think not. It may be that there is a further court of appeal to decide the issue, specifically, practical ethics. That is, the suggestion is that if our metaphysical arguments and intuitions cannot decide the metaphysical issue of personal identity, it is permissible to decide the issue on non-metaphysical grounds.</p>\n<p>Imagine two persons, McCoy and Hatfield, who want to kill one another. They are co-inventors of the first replicating machine. McCoy thinks it should be used on humans, Hatfield believes that it never should be so employed. McCoy believes in the psychological continuity thesis of personal identity, whereas Hatfield believes in somaticism. How should we reason about personal identity in terms of what is good for society? We can imagine two possibilities: society adopts for, social and legal purposes (its &ldquo;public norm&rdquo; for short), somaticism or psychological continuity. Which is better for society?</p>\n<p>Consider first using somaticism as the public norm. McCoy could kill Hatfield and then hop in the replicating machine. We would be forced to say, because we have adopted somaticism as our public norm, that McCoy is dead and the replica of McCoy (call this person &ldquo;McCoyson&rdquo;) is a different person. Since McCoyson was born after the crime, McCoyson cannot be responsible for the crime. (We have long abandoned the idea that one can inherit personal responsibility for the sins of one&rsquo;s ancestors). This crime would be ruled a murder-suicide in a somaticist jurisdiction. Of course McCoy then has every reason to commit the crime, as he does not believe in somaticism.</p>\n<p>&hellip;If psychological continuity is the public norm, then neither Hatfield nor McCoy will have reason to commit the crime based on replication. As before, Hatfield will not because he will consider this equivalent to suicide. McCoy will not because the public norm says that McCoy will survive the replication and be subject to criminal sanctions. Since a public norm of somaticism is more likely to lead to negative social consequences, this gives us some reason to reject somaticism.</p>\n</blockquote>\n<h1 id=\"no-branching\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> No branching</a></h1>\n<blockquote>\n<p>Debates about identity preservation and uploading invariably get hung up on the &ldquo;branching&rdquo; problem, and this probably provides the strongest support for somaticism. The problem is that it seems there is only one of me. But uploading seems to allow the possibility that there could be hundreds, if not millions, of &ldquo;me.&rdquo;</p>\n<p>Using the equivalence thesis we can see how this is exactly the same problem as the problem of branching that philosophers discuss in connection with carbon-to-carbon transfers. Parfit extends his Teletransporter case in exactly this way:</p>\n<blockquote>\n<p>Several years pass, during which I am often Teletransported. I am now back in the cubicle, ready for another trip to Mars. But this time, when I press the green button, I do not lose consciousness. There is a whirring sound, then silence. I do not lose consciousness. I leave the cubicle, and say to the attendant: &ldquo;It&rsquo;s not working. What did I do wrong?&rdquo; &ldquo;It&rsquo;s working,&rdquo; he replies, handing me a printed card. This reads: &ldquo;The New Scanner records your blueprint without destroying your brain and your body. We hope that you will welcome the opportunities which this technical advance offers.&rdquo; (Parfit 1987, 199)</p>\n</blockquote>\n<p>Of course there is no reason to stop at one replica. Using Parfit&rsquo;s Teletransporter thousands of organic molecule-for-molecule identical persons could awaken in the same instant, all claiming to be Mark Walker.</p>\n<p>&hellip;What does the psychological account have to say about multiple replicas? Here opinions differ. On the one hand, it seems that if there are multiple replicas, and they are all psychologically indistinguishable from the original, then each of them has as good a claim to be me, and so they are all me. The contrary &ldquo;no-branching&rdquo; view is that at most one replica is me, for there can be only one me (Shorter 1962).</p>\n<p>The question then is whether there can be &ldquo;branching&rdquo;: more than one of me. I will argue that both sides of the debate are correct; there is a sense in which there can&rsquo;t be more than one of me, and a sense in which there can be multiple versions of me.</p>\n<blockquote>\n<p>The No-branching Argument:</p>\n<ul>\n<li>P1: Multiple replicas X, Y, Z&hellip;. of an individual O (the original) are numerically non-identical with each other, that is, X is not identical with Y or Z, Y is not identical with X or Z, and so on.</li>\n<li>P2: Preservation of personal identity requires preservation of numerical identity.</li>\n<li>C: Therefore, not all replicas X, Y, Z&hellip; preserve personal identity of O.7</li>\n</ul>\n</blockquote>\n<p>&hellip;I want to suggest that the problem with the no-branching argument is that there is a critical ambiguity. To explain the ambiguity it will be helpful to review the type/token distinction.</p>\n</blockquote>\n<p>(For those not familiar with the literature, &lsquo;numerical&rsquo; here is being used in a sense of complete identity - there being complete logical equivalence. So for example, everyone reading this is numerically identical with themselves, and numerically not identical with the pope.)</p>\n<h1 id=\"types-and-tokens\"><a href=\"#TOC\"><span class=\"header-section-number\">5</span> Types and tokens</a></h1>\n<p>Walker invokes the <a href=\"http://en.wikipedia.org/wiki/Type%E2%80%93token_distinction\">type-token distinction</a>:</p>\n<blockquote>\n<p>There are twenty tokens of the word <em>the</em>, but a single type of the word <em>the</em>. The argument to be canvassed is that if we think of personal identity as ambiguous between types and tokens, then the no-branching argument may be rejected.</p>\n</blockquote>\n<p>for 2 false anti-replication personal-identity arguments:</p>\n<blockquote>\n<p>No-branching Token Argument</p>\n<ul>\n<li>P1&rsquo;: Multiple replicas X, Y, Z&hellip;. of an individual O (the original <em>Hamlet</em> penned in Shakespeare&rsquo;s hand) are numerically not (token) identical with each other, that is, X is not (token) identical with Y or Z, Y is not (token) identical with X or Z, and so on.</li>\n<li>P2&rsquo;: Preservation of play-identity requires preservation of (token) numerical identity.</li>\n<li>C&rsquo;: Therefore, not all replicas X, Y, Z&hellip; preserve play-identity of O.</li>\n</ul>\n<p>No-branching Type Argument</p>\n<ul>\n<li>P1&rsquo;&rsquo;: Multiple replicas X, Y, Z&hellip;. of an individual O (the original <em>Hamlet</em> penned in Shakespeare&rsquo;s hand) are numerically not (type) identical with each other, that is, X is not (type) identical with Y or Z, Y is not (type) identical with X or Z, and so on.</li>\n<li>P2&rsquo;&rsquo;: Preservation of play-identity requires preservation of (type) numerical identity.</li>\n<li>C&rdquo;: Therefore, not all replicas X, Y, Z&hellip; preserve play-identity of O.</li>\n</ul>\n</blockquote>\n<p>The reader can guess what comes next: he&rsquo;ll make the move of saying personal identity is the &lsquo;type&rsquo; and any upload or copy is the &lsquo;token&rsquo;. We accept that while the original <em>Hamlet</em> is valuable in many respects, <em>Hamlet</em> survives the destruction of the original if an appropriately faithful copy is made.</p>\n<h1 id=\"the-typetoken-solution-to-personal-identity\"><a href=\"#TOC\"><span class=\"header-section-number\">6</span> The type/token solution to personal identity</a></h1>\n<blockquote>\n<p>The ontological status of abstract entities is a perplexing and contested issue (Wetzel 2009), but there is no reason to think that it is more perplexing in the case of persons rather than literature, and we are committed to types in the case of literature.<sup>11</sup></p>\n<blockquote>\n<p>The No-branching Argument in terms of Tokens</p>\n<ul>\n<li>P1&rsquo;&rsquo;&rsquo;: Multiple replicas X, Y, Z&hellip;. of an individual O (the original) are numerically [token] non-identical with each other.</li>\n<li>P2&rsquo;&rsquo;&rsquo;: Preservation of personal identity requires preservation of numerical [token] identity.</li>\n<li>C&rsquo;&rdquo;: Therefore, not all replicas X, Y, Z&hellip; preserve personal identity of O.</li>\n</ul>\n</blockquote>\n<p>There are two problems with this argument. First, it is question begging. The entire issue is whether personal identity can be explained in terms of preservation of type identity, and so P2&rsquo;&rsquo;&rsquo; prejudges the issue.<sup>12</sup></p>\n<p>The other problem is that it is difficult to see how one can insist on non-branching without collapsing into somaticism. To see this, consider the case where the original Mark Walker&rsquo;s body, O, is destroyed when three replicas X, Y, and Z are created. Either O is not identical with any of X, Y, Z, or O is identical with one of X, Y, Z. If the former, then non-branching is simply somaticism in disguise. If it is asserted that O is identical with exactly one of X, Y, Z, then any choice would be arbitrary in the sense that choosing one among the thousand to be The Mark Walker would not be choosing based on any intrinsic differences. We could, for example, have all the replicas draw a number out of a hat and designate the winner of the lottery The Mark Walker. But an appeal to a lottery shows that precisely no intrinsic properties are used to individuate: it is the process (the lottery) that does the individuating. We could do the same for Hamlet. We could assign a number to every extant copy of Hamlet and have a lottery to find out which is The Hamlet, and which are mere copies. But, of course, no one would be impressed by this.<sup>13</sup></p>\n</blockquote>\n<p>Points in favor of the type-token:</p>\n<ol style=\"list-style-type: decimal\">\n<li>explains reluctance to sacrifice a rare token for small gain in Mars case (destructive)</li>\n<li>explains why people would sacrifice rare token for large gains, in first Vorlon case</li>\n<li>explains our lack of concern, in second Vorlon case</li>\n<li>explains precedence of Earth original over Mars copy, in second Mars case (non-destructive)</li>\n<li>not too paradoxical in cases of multiple copies and no surviving original</li>\n</ol>\n<h1 id=\"should-i-upload\"><a href=\"#TOC\"><span class=\"header-section-number\">7</span> Should I upload?</a></h1>\n<blockquote>\n<p>Still, it may look as though this is tantamount to an argument against uploading: if there is any loss in uploading, even if it is only token identity, why would anyone want to sacrifice some identity?</p>\n</blockquote>\n<p>(This hearkens back to a previous JET paper I covered, <a href=\"/lw/8of/ray_kurzweil_and_uploading_just_say_no_nick_agar/\">&ldquo;Ray Kurzweil and Uploading: Just Say No!&rdquo;, Nick Agar</a>. Agar is not cited for this part of the paper.)</p>\n<blockquote>\n<p>The answer is that there are considerable advantages (or at least purported advantages) to being uploaded, including immortality and enhancement. Except for the completely reckless, forgetful or lazy (ahem), everyone backs up his or her valuable computer files. But once we see that people too can be backed-up, it appears that virtual immortality is assured. For so long as there are operating computers, one can simply transfer the files that comprise oneself from computer to computer. If the hardware on one computer fails, you simply move to another computer&hellip;As for enhancement, one possibility is that our senses could be radically enhanced: robots presently make use of a sensory apparatus that detects light in parts of the spectrum not available to (unaided) human vision (e.g., infrared, x-rays, etc.), sounds that are beyond normal human auditory range, and so on. In terms of enhancing cognition consider that it is a relatively routine matter to add memory or computing power to today&rsquo;s computers. If one is uploaded to a computer, then it seems that it would be a relatively routine matter to enhance one&rsquo;s memory or cognition: just add more computer memory or processing power. The sky is literally the limit here</p>\n<p>&hellip;It is beyond the scope of this paper to argue that these purported benefits of uploading really are benefits, but, if they are, the temptation to upload is clear. And just like in the stroke case, it is clear why it might be rational to forgo token identity survival for these advantages.</p>\n</blockquote>\n<h1 id=\"further-reading\"><a href=\"#TOC\"><span class=\"header-section-number\">8</span> Further reading</a></h1>\n<p>There doesn&rsquo;t seem to be any discussion of this paper online. My own views on personal identity tend to the psychological pattern, which does not seem to be very different from a type-token theory of personal identity, if there is any meaningful difference at all, so this was a less challenging paper to read than the others, the equivalency thesis aside. The examples may be worth remembering.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 2, "jQytxyauJ7kPhhGj3": 2, "x6evH6MyPK3nxsoff": 2, "5f5c37ee1b5cdee568cfb2fa": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G3hxkSeDCMBucYsp6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 8.27891667802089e-07, "legacy": true, "legacyId": "11992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://jetpress.org/v22/walker.htm\">\u201cPersonal Identity and Uploading\u201d</a>, <a href=\"http://www.nmsu.edu/~philos/mark-walkers-home-page.html\">Mark Walker</a> is the next JET paper. Abstract:</p>\n<blockquote>\n<p>Objections to uploading may be parsed into substrate issues, dealing with the computer platform of upload and personal identity. This paper argues that the personal identity issues of uploading are no more or less challenging than those of bodily transfer often discussed in the philosophical literature. It is argued that what is important in personal identity involves both token and type identity. While uploading does not preserve token identity, it does save type identity; and even qua token, one may have good reason to think that the preservation of the type is worth the cost.</p>\n</blockquote>\n<h1 id=\"uploading-prospects-and-perils\"><a id=\"more\"></a></h1>\n<h1 id=\"1_Uploading__prospects_and_perils\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Uploading: prospects and perils</a></h1>\n<blockquote>\n<p>If, like me, you think that uploading is possible (at least in principle), and so you hold that the first interpretation of these events is correct, then you must hold true the following three theses:</p>\n<blockquote>\n<p>1. <em>Computers are capable of supporting the important properties constitutive of personal identity, e.g., thought and consciousness.</em></p>\n</blockquote>\n<p>It is clear that uploading will not preserve all properties we associate with <em>Homo sapiens</em>, e.g., basic facts about the human digestive system are not likely to be preserved in uploading to a robotic body. But these facts are not typically thought to be important for personal identity. Candidates for important properties include thought, consciousness, emotions, creativity, aesthetic experience, sensory experience, empathy and so on. For the most part, the question of which properties are important is not as serious as it may first seem, since uploading promises to preserve the essential aspects of the brain and nervous system, which overlap with the usual lists of important properties for identity.</p>\n<p>\u2026suffice it to say that if Searle is correct, then #1 may be false. For Searle thinks that a computer can never consciously think merely in virtue of instantiating a computer program, and the uploading process seems to be one of merely instantiating a computer program (Agar 2010, 2011).</p>\n<blockquote>\n<p>2. <em>It is possible to capture the information necessary to emulate the important properties of individual humans.</em></p>\n</blockquote>\n<p>\u2026If we slice off layers of your neurons, and record the information of each layer, the lower layers will change (due to trauma or death).<sup>1</sup> If we flash freeze your brain, we may destroy some essential information. Philosophical questions arise as to whether the information encoded in the brain is sufficient to account for all the relevant properties. For example, consider a dualist who believes that we have souls in addition to brains, and much of what is morally important (e.g., conscious thought) resides in the soul. If the dualist is right, then scanning your brain could never be sufficient, for it would be necessary to scan your soul to access at least some of what is important. If it is unlikely that we will be able to scan souls, there will be an insurmountable obstacle to uploading. Notice how theses #1 and #2 may differ on this point: a dualist could consistently hold that a computer might have a soul; it is just that if computers have souls, it is not because we obtained the soul-building information from humans. (Perhaps God implants souls in humans and computers.)</p>\n<blockquote>\n<p>3. <em>It is possible to survive the uploading process.</em></p>\n</blockquote>\n<p>To see how #3 differs from #1 and #2, imagine that at some point in the future we have created computers of sufficient complexity that it is agreed that they have the same morally relevant properties as humans: these advanced computers think and are conscious, they are accorded rights, and the scanning problem has been solved so that we are able to scan the brain in such a way that we are not worried about loss of information. None of this answers the question of whether you have been preserved during uploading or whether uploading merely makes a very good copy of you\u2026If an individual can be uploaded once, then it seems the same individual could be uploaded twice into separate computers, and indeed, billions of the same individual all embodied in separate robotic bodies could be created.</p>\n<p>To make the discussion manageable, I will focus on thesis #3, and assume without argument that #1 and #2 have been resolved in favor of uploading. So our question is this: assuming that computers can be conscious, have memories, and (robotic) bodies, and assuming that it is possible to scan and capture all the information of a human brain, does uploading preserve personal identity? I will argue that uploading does preserve personal identity; at least identity of a certain sort.</p>\n</blockquote>\n<h1 id=\"2_The_equivalency_thesis\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> The equivalency thesis</a></h1>\n<blockquote>\n<p>The fact that we are assuming that computers are capable of embodying all the same type of properties necessary for personal identity means that we can make use of the equivalency thesis:</p>\n<blockquote>\n<p><em>Equivalency thesis</em>: If it is possible for an individual to survive migration from a carbon to a carbon body, then it is possible for individuals to survive migration from a carbon to a silicon body.</p>\n</blockquote>\n<p>\u2026There are a couple of reasons for invoking the equivalency thesis. The first is so that we are not misled by a new form of racism: substratism (Walker 2006). Substratism is the view that one\u2019s substrate is inherently superior to that of other substrates along the lines that racists think their race is inherently superior to some other race. In the present case, it would suggest the idea that carbon-based humans are inherently more morally worthy than silicon based beings. Consider the fact that we would not accept this argument: it is not possible for persons to migrate from one body to another because then it would be possible for people of skin color X to move to bodies of skin color Y, and Y skin color is morally inferior. We want to avoid the same bad argument in considering moving from one substrate to another. Notice that this does not beg the issue at hand, since it is possible to say that having a certain substrate (or even skin color) is constitutive of my identity; it merely prohibits saying that this property in itself makes for moral superiority.</p>\n<p>The second is that it makes directly relevant an enormous amount of philosophical effort that has gone into exploring the possibility of carbon-to-carbon transfers. The question of carbon-to-silicon transfers thus may piggyback on this effort.</p>\n</blockquote>\n<p>The thesis disturbed me the first time I saw it; it seemed to me that it either begged the basic philosophical question at point or it did not do any work. So I read on to see how it was used. It seems to be the latter case: the thesis is barely used and not really germane to the examples that criticize somaticism and argue for a type-token kind of personal identity. This is good because it seems like used in any kind of strong sense, it\u2019s easy to criticize the thesis.</p>\n<p>(Implicitly, it seems to scope over all individuals - that we could rewrite it as, \u2018for all individuals that survive any carbon-&gt;carbon transition, there is a carbon-&gt;silicon transition they survive\u2019. But this seems false: a book is made out of carbon, survives minute to minute or copy to copy, and can be satisfactorily uploaded, but can a squishy human brain? Can a bowl of water? If I take a stick of carbon and light it on fire, how do I upload the burning stick? What does an uploaded diamond <em>do</em>? One might say the physics of the constituent atoms can be uploaded and this is a correct emulation with any necessary properties like emergence, but then we\u2019re back to the question-begging.)</p>\n<h1 id=\"3_Personal_identity__psychological_and_somatic_accounts\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> Personal identity: psychological and somatic accounts</a></h1>\n<blockquote>\n<p>Historically, there are two main schools of thought about what is required for personal survival; the psychological and somatic approaches (Olson 2002).</p>\n<p>\u2026the psychological account says that what is essential for survival is continuity of psychological states such as memory, beliefs, desires and personality. John Locke, an early proponent of this view, famously described personal identity in terms of psychological continuity, within an analysis of personhood as consisting in existence as \u201ca thinking intelligent being, that has reason and reflection, and can consider itself as itself, the same thinking thing, in different times and places\u2026\u201d (Locke 1975). The person on Mars who awakens will claim to remember being Derek Parfit, and to have memories and a personality that are psychologically indistinguishable from the person on earth whose body was destroyed. Locke, then, would say that Parfit survived teletransportation.</p>\n<p>Somaticist accounts suggest the survival of a particular body is critical for personal identity over time. Since the body on Earth is destroyed during the scanning process, Parfit ceases to be. A different person will awake on Mars. This person will of course have psychologically indistinguishable memories and personality to those of the late Parfit, but this person will not be Parfit. The new person will be but an infant in terms of chronological age: only a few minutes old. We will think of \u201csomaticism\u201d as the view that continuity of one\u2019s body is necessary for personal identity from one time to the next.<sup>2</sup> There are two ways that one might be a somaticist: one can believe that bodily continuity is necessary but not sufficient, or that it is necessary and sufficient. One easy case to distinguish these two is as follows: a piano falls on your head, and causes you to go into a permanent vegetative state. Your relatives discuss whether to \u201cpull the plug.\u201d Those who think that bodily continuity is necessary but not sufficient may say that you no longer exist, but your body continues to exist. Those who think that bodily continuity is necessary and sufficient will say that you continue to exist, albeit your cognitive capacities are non-existent. Both views qualify as \u201csomaticism\u201d in our sense.</p>\n</blockquote>\n<h2 id=\"3_1_Against_somaticism__the_big_stroke\"><a href=\"#TOC\"><span class=\"header-section-number\">3.1</span> Against somaticism: the big stroke</a></h2>\n<blockquote>\n<p>The Vorlons,<sup>5</sup> a mysterious and intellectually advanced alien species, make this offer: you can have an original undiscovered play by Shakespeare written in his hand, or a copy of the play made by one of his lackeys. You salivate at the joy this will bring to the world (not to mention the fame and fortune it will bring you personally). Since you can have only one, the choice, it seems, is a no-brainer. You should opt for the one written by the bard\u2019s hand. But now consider this variant: the Vorlons tell you that the text written in Shakespeare\u2019s hand is missing the last two pages, while they assure you the copy written by the lackey is a perfectly faithful reproduction of all the words in the original. While it would be great to have both, you reason that the most important thing is the play itself be preserved, not Shakespeare\u2019s handwriting. The copy here is in some sense better than the original because the original has been damaged.</p>\n<p>\u2026The Vorlons, with their ability to see into the future, say the news is grim. In less than twelve hours you will have a massive stroke that will cause you to lose many of your memories and some mobility, and impair your intelligence. Your stroke will not be as bad as some: the damage from the stroke will not leave you completely cognitively impaired, but you will no longer be able to work as an academic. You will have to find some relatively mindless job befitting your new level of intelligence, perhaps in academic administration\u2026there is nothing the Vorlons can do to prevent the stroke. They provide a radical alternative: creating a perfect replica of you \u2013 down to the molecular level \u2013 with the exception that the problems with the arteries to your brain will be fixed in the body replica. They insist, however, that only one body can survive. You must choose tonight whether the replica or your current body survives.</p>\n<p>\u2026Obviously, this example is structurally similar to Parfit\u2019s [suicidal teletransporter], but with one big exception: what is gained by having the replica survive is much more significant in this case than in the Teletransporter to Mars case. Parfit offers the incentive of avoiding three weeks of space travel. (We might not even sacrifice our original wedding ring for an exact replica if the benefit is merely avoiding three weeks in a spaceship). Here the incentive is the possibility of not having one\u2019s life radically altered by the stroke. The attachment to one\u2019s body does not seem worth the cost in this case.</p>\n<p>\u2026It is worth noting that not all somaticists are likely to be convinced by this example.<sup>6</sup> But it should convince a few, and points out one of the heavy costs of somaticism.</p>\n</blockquote>\n<h2 id=\"3_2_Against_somaticism__retrospective_replicas\"><a href=\"#TOC\"><span class=\"header-section-number\">3.2</span> Against somaticism: retrospective replicas</a></h2>\n<blockquote>\n<p>\u2026perhaps it is this fear of the unknown, rather than a commitment to somaticism, that explains the reluctance to use the Teletransporter. We can test this thought by considering a retrospective rather than a prospective version of a replication scenario.</p>\n<p>Suppose that every night when people sleep their bodies (including their brains) are scanned by a swarm of nanobots and a molecule for molecule identical body is beamed from a hidden alien spaceship in orbit; the old body is vaporized in a manner that is undetectable by the human eye. Scientists discovered this fortuitously: physicists noticed a spike in neutrino levels every time psychologists in the adjoining lab conducted sleep experiments. Intrigued, scientists built a chamber to isolate subjects from neutrino influences and then had test subjects sleep in the chamber. Once the experiment was initiated, a hologram of a Vorlon appeared in the lab and spoke thusly:</p>\n<blockquote>\n<p>We are an ancient race known as the \u201cVorlons.\u201d We battled another species, the \u201cShadows,\u201d just as your species was beginning to evolve on this planet. One of the toxic effects of our war was a type of radiation that kills all higher intelligences within three days. We have no way of eliminating the radiation, but we have left advanced technology to recreate your bodies from different molecules every day so that the radiation will not harm you. We left the galaxy eons ago. You are hearing this message now because you have advanced technologically to the point where you can detect our technology. If you interfere with our replicator technology, you will quickly die of radiation poisoning.</p>\n</blockquote>\n<p>What should we make of this? It is clear that dismantling it is out of the question since all humans will die within three days. If you are a somaticist, you must conclude that you have been alive only for a very short while. In fact, you have existed only since last night. After all, the physical continuity of one\u2019s body has lasted only this length of time. However, most of us, I think, would conclude the opposite. That is, that we have existed for years: that we do not cease to exist every night and a new person comes into being.</p>\n<p>\u2026the somaticist must now explain how so many people could be mistaken about their own identity retrospective case; after all, it seems very likely that, upon learning about the Vorlons\u2019 technology, most would conduct their lives as if they hadn\u2019t just come into existence that day. Who is going to say such things as: \u201cI do not have to look after these children you call mine: how can I have children if I myself was born today. I can\u2019t use this driver\u2019s license, it is someone else\u2019s \u2013 I was just born today. I\u2019m not qualified to teach any classes: a postgraduate degree is required, which takes years to earn, and I was just born today?\u201d</p>\n</blockquote>\n<p>This example reminds me strongly of Nick Bostrom\u2019s <a href=\"http://www.nickbostrom.com/ethics/statusquo.pdf\">reversal test for the status quo bias</a>; an example would be a drug that increases IQ 10 points may be feared and rejected, but would it be accepted if scientists discovered new pollution will reduce IQ 10 points and that drug would compensate? I like his reversal test, and I like this example as well.</p>\n<h2 id=\"3_3_Against_somaticism__practical_ethics\"><a href=\"#TOC\"><span class=\"header-section-number\">3.3</span> Against somaticism: practical ethics</a></h2>\n<blockquote>\n<p>\u2026To emphasize, let us suppose that despite the fact that the preponderance of reasons seem to be against somaticism, imagine that the metaphysical reasons for and against somaticism are exactly balanced. Does this imply that we should be neutral on the issue? I think not. It may be that there is a further court of appeal to decide the issue, specifically, practical ethics. That is, the suggestion is that if our metaphysical arguments and intuitions cannot decide the metaphysical issue of personal identity, it is permissible to decide the issue on non-metaphysical grounds.</p>\n<p>Imagine two persons, McCoy and Hatfield, who want to kill one another. They are co-inventors of the first replicating machine. McCoy thinks it should be used on humans, Hatfield believes that it never should be so employed. McCoy believes in the psychological continuity thesis of personal identity, whereas Hatfield believes in somaticism. How should we reason about personal identity in terms of what is good for society? We can imagine two possibilities: society adopts for, social and legal purposes (its \u201cpublic norm\u201d for short), somaticism or psychological continuity. Which is better for society?</p>\n<p>Consider first using somaticism as the public norm. McCoy could kill Hatfield and then hop in the replicating machine. We would be forced to say, because we have adopted somaticism as our public norm, that McCoy is dead and the replica of McCoy (call this person \u201cMcCoyson\u201d) is a different person. Since McCoyson was born after the crime, McCoyson cannot be responsible for the crime. (We have long abandoned the idea that one can inherit personal responsibility for the sins of one\u2019s ancestors). This crime would be ruled a murder-suicide in a somaticist jurisdiction. Of course McCoy then has every reason to commit the crime, as he does not believe in somaticism.</p>\n<p>\u2026If psychological continuity is the public norm, then neither Hatfield nor McCoy will have reason to commit the crime based on replication. As before, Hatfield will not because he will consider this equivalent to suicide. McCoy will not because the public norm says that McCoy will survive the replication and be subject to criminal sanctions. Since a public norm of somaticism is more likely to lead to negative social consequences, this gives us some reason to reject somaticism.</p>\n</blockquote>\n<h1 id=\"4_No_branching\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> No branching</a></h1>\n<blockquote>\n<p>Debates about identity preservation and uploading invariably get hung up on the \u201cbranching\u201d problem, and this probably provides the strongest support for somaticism. The problem is that it seems there is only one of me. But uploading seems to allow the possibility that there could be hundreds, if not millions, of \u201cme.\u201d</p>\n<p>Using the equivalence thesis we can see how this is exactly the same problem as the problem of branching that philosophers discuss in connection with carbon-to-carbon transfers. Parfit extends his Teletransporter case in exactly this way:</p>\n<blockquote>\n<p>Several years pass, during which I am often Teletransported. I am now back in the cubicle, ready for another trip to Mars. But this time, when I press the green button, I do not lose consciousness. There is a whirring sound, then silence. I do not lose consciousness. I leave the cubicle, and say to the attendant: \u201cIt\u2019s not working. What did I do wrong?\u201d \u201cIt\u2019s working,\u201d he replies, handing me a printed card. This reads: \u201cThe New Scanner records your blueprint without destroying your brain and your body. We hope that you will welcome the opportunities which this technical advance offers.\u201d (Parfit 1987, 199)</p>\n</blockquote>\n<p>Of course there is no reason to stop at one replica. Using Parfit\u2019s Teletransporter thousands of organic molecule-for-molecule identical persons could awaken in the same instant, all claiming to be Mark Walker.</p>\n<p>\u2026What does the psychological account have to say about multiple replicas? Here opinions differ. On the one hand, it seems that if there are multiple replicas, and they are all psychologically indistinguishable from the original, then each of them has as good a claim to be me, and so they are all me. The contrary \u201cno-branching\u201d view is that at most one replica is me, for there can be only one me (Shorter 1962).</p>\n<p>The question then is whether there can be \u201cbranching\u201d: more than one of me. I will argue that both sides of the debate are correct; there is a sense in which there can\u2019t be more than one of me, and a sense in which there can be multiple versions of me.</p>\n<blockquote>\n<p>The No-branching Argument:</p>\n<ul>\n<li>P1: Multiple replicas X, Y, Z\u2026. of an individual O (the original) are numerically non-identical with each other, that is, X is not identical with Y or Z, Y is not identical with X or Z, and so on.</li>\n<li>P2: Preservation of personal identity requires preservation of numerical identity.</li>\n<li>C: Therefore, not all replicas X, Y, Z\u2026 preserve personal identity of O.7</li>\n</ul>\n</blockquote>\n<p>\u2026I want to suggest that the problem with the no-branching argument is that there is a critical ambiguity. To explain the ambiguity it will be helpful to review the type/token distinction.</p>\n</blockquote>\n<p>(For those not familiar with the literature, \u2018numerical\u2019 here is being used in a sense of complete identity - there being complete logical equivalence. So for example, everyone reading this is numerically identical with themselves, and numerically not identical with the pope.)</p>\n<h1 id=\"5_Types_and_tokens\"><a href=\"#TOC\"><span class=\"header-section-number\">5</span> Types and tokens</a></h1>\n<p>Walker invokes the <a href=\"http://en.wikipedia.org/wiki/Type%E2%80%93token_distinction\">type-token distinction</a>:</p>\n<blockquote>\n<p>There are twenty tokens of the word <em>the</em>, but a single type of the word <em>the</em>. The argument to be canvassed is that if we think of personal identity as ambiguous between types and tokens, then the no-branching argument may be rejected.</p>\n</blockquote>\n<p>for 2 false anti-replication personal-identity arguments:</p>\n<blockquote>\n<p>No-branching Token Argument</p>\n<ul>\n<li>P1\u2019: Multiple replicas X, Y, Z\u2026. of an individual O (the original <em>Hamlet</em> penned in Shakespeare\u2019s hand) are numerically not (token) identical with each other, that is, X is not (token) identical with Y or Z, Y is not (token) identical with X or Z, and so on.</li>\n<li>P2\u2019: Preservation of play-identity requires preservation of (token) numerical identity.</li>\n<li>C\u2019: Therefore, not all replicas X, Y, Z\u2026 preserve play-identity of O.</li>\n</ul>\n<p>No-branching Type Argument</p>\n<ul>\n<li>P1\u2019\u2019: Multiple replicas X, Y, Z\u2026. of an individual O (the original <em>Hamlet</em> penned in Shakespeare\u2019s hand) are numerically not (type) identical with each other, that is, X is not (type) identical with Y or Z, Y is not (type) identical with X or Z, and so on.</li>\n<li>P2\u2019\u2019: Preservation of play-identity requires preservation of (type) numerical identity.</li>\n<li>C\u201d: Therefore, not all replicas X, Y, Z\u2026 preserve play-identity of O.</li>\n</ul>\n</blockquote>\n<p>The reader can guess what comes next: he\u2019ll make the move of saying personal identity is the \u2018type\u2019 and any upload or copy is the \u2018token\u2019. We accept that while the original <em>Hamlet</em> is valuable in many respects, <em>Hamlet</em> survives the destruction of the original if an appropriately faithful copy is made.</p>\n<h1 id=\"6_The_type_token_solution_to_personal_identity\"><a href=\"#TOC\"><span class=\"header-section-number\">6</span> The type/token solution to personal identity</a></h1>\n<blockquote>\n<p>The ontological status of abstract entities is a perplexing and contested issue (Wetzel 2009), but there is no reason to think that it is more perplexing in the case of persons rather than literature, and we are committed to types in the case of literature.<sup>11</sup></p>\n<blockquote>\n<p>The No-branching Argument in terms of Tokens</p>\n<ul>\n<li>P1\u2019\u2019\u2019: Multiple replicas X, Y, Z\u2026. of an individual O (the original) are numerically [token] non-identical with each other.</li>\n<li>P2\u2019\u2019\u2019: Preservation of personal identity requires preservation of numerical [token] identity.</li>\n<li>C\u2019\u201d: Therefore, not all replicas X, Y, Z\u2026 preserve personal identity of O.</li>\n</ul>\n</blockquote>\n<p>There are two problems with this argument. First, it is question begging. The entire issue is whether personal identity can be explained in terms of preservation of type identity, and so P2\u2019\u2019\u2019 prejudges the issue.<sup>12</sup></p>\n<p>The other problem is that it is difficult to see how one can insist on non-branching without collapsing into somaticism. To see this, consider the case where the original Mark Walker\u2019s body, O, is destroyed when three replicas X, Y, and Z are created. Either O is not identical with any of X, Y, Z, or O is identical with one of X, Y, Z. If the former, then non-branching is simply somaticism in disguise. If it is asserted that O is identical with exactly one of X, Y, Z, then any choice would be arbitrary in the sense that choosing one among the thousand to be The Mark Walker would not be choosing based on any intrinsic differences. We could, for example, have all the replicas draw a number out of a hat and designate the winner of the lottery The Mark Walker. But an appeal to a lottery shows that precisely no intrinsic properties are used to individuate: it is the process (the lottery) that does the individuating. We could do the same for Hamlet. We could assign a number to every extant copy of Hamlet and have a lottery to find out which is The Hamlet, and which are mere copies. But, of course, no one would be impressed by this.<sup>13</sup></p>\n</blockquote>\n<p>Points in favor of the type-token:</p>\n<ol style=\"list-style-type: decimal\">\n<li>explains reluctance to sacrifice a rare token for small gain in Mars case (destructive)</li>\n<li>explains why people would sacrifice rare token for large gains, in first Vorlon case</li>\n<li>explains our lack of concern, in second Vorlon case</li>\n<li>explains precedence of Earth original over Mars copy, in second Mars case (non-destructive)</li>\n<li>not too paradoxical in cases of multiple copies and no surviving original</li>\n</ol>\n<h1 id=\"7_Should_I_upload_\"><a href=\"#TOC\"><span class=\"header-section-number\">7</span> Should I upload?</a></h1>\n<blockquote>\n<p>Still, it may look as though this is tantamount to an argument against uploading: if there is any loss in uploading, even if it is only token identity, why would anyone want to sacrifice some identity?</p>\n</blockquote>\n<p>(This hearkens back to a previous JET paper I covered, <a href=\"/lw/8of/ray_kurzweil_and_uploading_just_say_no_nick_agar/\">\u201cRay Kurzweil and Uploading: Just Say No!\u201d, Nick Agar</a>. Agar is not cited for this part of the paper.)</p>\n<blockquote>\n<p>The answer is that there are considerable advantages (or at least purported advantages) to being uploaded, including immortality and enhancement. Except for the completely reckless, forgetful or lazy (ahem), everyone backs up his or her valuable computer files. But once we see that people too can be backed-up, it appears that virtual immortality is assured. For so long as there are operating computers, one can simply transfer the files that comprise oneself from computer to computer. If the hardware on one computer fails, you simply move to another computer\u2026As for enhancement, one possibility is that our senses could be radically enhanced: robots presently make use of a sensory apparatus that detects light in parts of the spectrum not available to (unaided) human vision (e.g., infrared, x-rays, etc.), sounds that are beyond normal human auditory range, and so on. In terms of enhancing cognition consider that it is a relatively routine matter to add memory or computing power to today\u2019s computers. If one is uploaded to a computer, then it seems that it would be a relatively routine matter to enhance one\u2019s memory or cognition: just add more computer memory or processing power. The sky is literally the limit here</p>\n<p>\u2026It is beyond the scope of this paper to argue that these purported benefits of uploading really are benefits, but, if they are, the temptation to upload is clear. And just like in the stroke case, it is clear why it might be rational to forgo token identity survival for these advantages.</p>\n</blockquote>\n<h1 id=\"8_Further_reading\"><a href=\"#TOC\"><span class=\"header-section-number\">8</span> Further reading</a></h1>\n<p>There doesn\u2019t seem to be any discussion of this paper online. My own views on personal identity tend to the psychological pattern, which does not seem to be very different from a type-token theory of personal identity, if there is any meaningful difference at all, so this was a less challenging paper to read than the others, the equivalency thesis aside. The examples may be worth remembering.</p>", "sections": [{"title": "1 Uploading: prospects and perils", "anchor": "1_Uploading__prospects_and_perils", "level": 1}, {"title": "2 The equivalency thesis", "anchor": "2_The_equivalency_thesis", "level": 1}, {"title": "3 Personal identity: psychological and somatic accounts", "anchor": "3_Personal_identity__psychological_and_somatic_accounts", "level": 1}, {"title": "3.1 Against somaticism: the big stroke", "anchor": "3_1_Against_somaticism__the_big_stroke", "level": 2}, {"title": "3.2 Against somaticism: retrospective replicas", "anchor": "3_2_Against_somaticism__retrospective_replicas", "level": 2}, {"title": "3.3 Against somaticism: practical ethics", "anchor": "3_3_Against_somaticism__practical_ethics", "level": 2}, {"title": "4 No branching", "anchor": "4_No_branching", "level": 1}, {"title": "5 Types and tokens", "anchor": "5_Types_and_tokens", "level": 1}, {"title": "6 The type/token solution to personal identity", "anchor": "6_The_type_token_solution_to_personal_identity", "level": 1}, {"title": "7 Should I upload?", "anchor": "7_Should_I_upload_", "level": 1}, {"title": "8 Further reading", "anchor": "8_Further_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["agbSrvyL3tDh3ZP7h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-08T06:48:53.468Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Extensions and Intensions", "slug": "seq-rerun-extensions-and-intensions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.073Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7fmaoCyP6SNQXqSyj/seq-rerun-extensions-and-intensions", "pageUrlRelative": "/posts/7fmaoCyP6SNQXqSyj/seq-rerun-extensions-and-intensions", "linkUrl": "https://www.lesswrong.com/posts/7fmaoCyP6SNQXqSyj/seq-rerun-extensions-and-intensions", "postedAtFormatted": "Sunday, January 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Extensions%20and%20Intensions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Extensions%20and%20Intensions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7fmaoCyP6SNQXqSyj%2Fseq-rerun-extensions-and-intensions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Extensions%20and%20Intensions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7fmaoCyP6SNQXqSyj%2Fseq-rerun-extensions-and-intensions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7fmaoCyP6SNQXqSyj%2Fseq-rerun-extensions-and-intensions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>Today's post, <a href=\"/lw/nh/extensions_and_intensions/\">Extensions and Intensions</a> was originally published on 04 February 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You try to define a word using words, in turn defined with ever-more-abstract words, without being able to point to an example. \"What is red?\" \"Red is a color.\" \"What's a color?\" \"It's a property of a thing?\" \"What's a thing? What's a property?\" It never occurs to you to point to a stop sign and an apple.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/98w/seq_rerun_words_as_hidden_inferences/\">Words as Hidden Inferences</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7fmaoCyP6SNQXqSyj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.281376451965288e-07, "legacy": true, "legacyId": "11993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HsznWM9A7NiuGsp28", "gs3i4QBwrG64DgGzh", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-08T07:54:12.190Z", "modifiedAt": null, "url": null, "title": "Non-theist cinema?", "slug": "non-theist-cinema", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:30.663Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jay_Schweikert", "createdAt": "2011-04-29T03:53:40.789Z", "isAdmin": false, "displayName": "Jay_Schweikert"}, "userId": "6niJdEi2bPMT4Bd5t", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HBBKH5jNt6PWb3ZsD/non-theist-cinema", "pageUrlRelative": "/posts/HBBKH5jNt6PWb3ZsD/non-theist-cinema", "linkUrl": "https://www.lesswrong.com/posts/HBBKH5jNt6PWb3ZsD/non-theist-cinema", "postedAtFormatted": "Sunday, January 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Non-theist%20cinema%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANon-theist%20cinema%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBBKH5jNt6PWb3ZsD%2Fnon-theist-cinema%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Non-theist%20cinema%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBBKH5jNt6PWb3ZsD%2Fnon-theist-cinema", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBBKH5jNt6PWb3ZsD%2Fnon-theist-cinema", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 629, "htmlBody": "<p>There isn't much in the way of explicitly atheist cinema* -- that is, movies that contain the explicit or implicit message that religion is nothing but superstition, and where this point itself is a central part of the story. The only popular films that jump to mind here are&nbsp;<a href=\"http://www.imdb.com/title/tt1058017/\">The Invention of Lying</a>, and to a lesser extent <a href=\"http://www.imdb.com/title/tt0756683/\">The Man from Earth</a> (overall a phenomenal movie, but far less well known). Sure, there are lots of popular movies that make fun of <em>organized</em>&nbsp;religion, or what some people might call religious \"fanaticism\" (e.g., <a href=\"http://www.imdb.com/title/tt0120655/\">Dogma</a>, <a href=\"http://www.imdb.com/title/tt0332375/\">Saved</a>, <a href=\"http://www.imdb.com/title/tt0079470/\">The Life of Brian</a>, <a href=\"http://www.imdb.com/title/tt0486358/\">Jesus Camp</a>). But pretty much all of these come away with the message that it's fine to be \"spiritual\" or whatever, so long as you don't hurt other people, and don't get <em>too</em>&nbsp;crazy about what you believe. As much as some \"<a href=\"/lw/gw/politics_is_the_mindkiller\">conservative</a>\" pundits love to accuse Hollywood \"<a href=\"/lw/gw/politics_is_the_mindkiller\">liberals</a>\" of being godless, there sure aren't many movies where godlessness is really taken seriously.</p>\n<p>And that's unfortunate, in my view, as movies are probably the most prevalent and influential art form for the general public, and because many people will form their views on abstract concepts based on the percepts that movies provide (related to the issue of <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">generalizing from fictional evidence</a>). One need only glance over the examples on the tvtropes page \"<a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/HollywoodAtheist\">Hollywood Atheist</a>\" to see that movies and television aren't exactly putting the best foot forward for our kind.</p>\n<p>But perhaps there's a bit more hope in the way of&nbsp;<em>non</em>-theist cinema, as opposed to overt atheist cinema. Of course, any story without gods is a non-theist story, and there are plenty of movies that don't touch on gods or religion at all. But what I'm talking about are movies where one would normally <em>expect</em>&nbsp;to find religion, but where no religion is to be found -- in other words, movies that seem to be depicting the alternate world where humanity never fell prey to this particular superstition, and where the concepts of god and religion simply don't exist.</p>\n<p>The movie that inspired this particular thought was <a href=\"http://www.imdb.com/title/tt1306980/\">50/50</a>, the recent comedy-drama where Joseph Gordon-Levitt plays a man dealing with potentially fatal cancer. It's a great movie, but what struck me afterwards is how completely absent any mention of god, religion, the afterlife, etc. was in a movie about a man, along with his friends and family, potentially facing his own death. There are lots of characters, lots of conflicts, lots of different perspectives on what he's going through, but nothing at all from anyone amounting to a \"spiritual\" response to the situation (at least that I recall).</p>\n<p>And it got me thinking, what other sorts of issues are there where we would normally expect religion to pop up, such that a story without it would be <em>decidedly</em>&nbsp;non-theist, as opposed to incidentally non-theist? And are there other major movies that you think tell such a story?&nbsp;I ask both because I'm always eager to hear about new movies I might enjoy (or old movies I might appreciate more), but also because I think this sort of non-theist cinema might be a good bridge to people who would instinctively rebel against anything openly atheist. In other words, show people that a \"godless\" world really isn't all that crazy, that people get by just fine and find ways to face conflicts, etc. Anyway, just thought I'd poll the membership and see what people thought about this idea. Looking forward to seeing the responses!</p>\n<p>*I'm well aware that there's quite a bit of atheist and non-theist art in other mediums -- sf literature most prominently. But I'm focusing on movies (and perhaps to a lesser extent, television) because those are the main forms of \"public art\" in our culture, and the mediums most likely to influence how the public at large views these concepts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HBBKH5jNt6PWb3ZsD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 13, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "11994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "rHBdcHGLJ7KvLJQPk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-08T08:13:01.850Z", "modifiedAt": null, "url": null, "title": "A variant on the trolley problem and babies as unit of currency", "slug": "a-variant-on-the-trolley-problem-and-babies-as-unit-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:34.571Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Solvent", "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bpJqNQh7E4JzHZd4Z/a-variant-on-the-trolley-problem-and-babies-as-unit-of", "pageUrlRelative": "/posts/bpJqNQh7E4JzHZd4Z/a-variant-on-the-trolley-problem-and-babies-as-unit-of", "linkUrl": "https://www.lesswrong.com/posts/bpJqNQh7E4JzHZd4Z/a-variant-on-the-trolley-problem-and-babies-as-unit-of", "postedAtFormatted": "Sunday, January 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20variant%20on%20the%20trolley%20problem%20and%20babies%20as%20unit%20of%20currency&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20variant%20on%20the%20trolley%20problem%20and%20babies%20as%20unit%20of%20currency%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbpJqNQh7E4JzHZd4Z%2Fa-variant-on-the-trolley-problem-and-babies-as-unit-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20variant%20on%20the%20trolley%20problem%20and%20babies%20as%20unit%20of%20currency%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbpJqNQh7E4JzHZd4Z%2Fa-variant-on-the-trolley-problem-and-babies-as-unit-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbpJqNQh7E4JzHZd4Z%2Fa-variant-on-the-trolley-problem-and-babies-as-unit-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 387, "htmlBody": "<p>I was discussing utilitarianism and charitable giving and similar ideas with someone today, and I came up with this hybrid version of the <a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">trolley problem</a>, particularly the <a href=\"http://en.wikipedia.org/wiki/Trolley_problem#The_fat_man\">fat man variation</a>, and the article by Scott Alexander/Yvain about using <a href=\"http://www.raikoth.net/deadchild.html\">dead children as a unit of currency</a>. It's not extremely original, and I'd be surprised if no-one on LW had thought of it before.</p>\n<p>You are offered a magical box. If you press the button on the box, one person somewhere in the world will die, you get $6000, and $4,000 is donated to one of the&nbsp;top rated charities on GiveWell.org.&nbsp;According to the $800 per life saved figure, this charity gift would save five lives, which is a net gain of four lives and $6,000 to you. Is it moral to press the button?</p>\n<p>All of the usual responses to the trolley problem apply. To wit: It's good to have heuristics like \"don't kill.\" There's arguments about establishing Schelling points with regards to not killing people. (This Schelling point argument doesn't work as well in a case like this, with anonymity and privacy and randomization of the person who gets killed.) Eliezer argued that for a human, being in the trolley problem is extraordinarily unlikely, and he would be willing to acknowledge that killing the fat man would be appropriate for an AI in the situation to do, but not a human.</p>\n<p>There's also lots of arguments against giving to charity, too. <a href=\"/lw/2b/so_you_say_youre_an_altruist/\">See here</a> for some discussion of this on LessWrong.</p>\n<p>I feel that the advantage of my dilemma is that in the original extreme altruism faces a whole lot of motivated cognition against it, because it implies that you should be giving much of your income to charity. In this dilemma, you want the $6,000, and so are inclined to be less skeptical of the charity's effectiveness.</p>\n<p>Possible use: Present this first, then argue for extreme altruism. This would annoy people, but as far as I can tell, pretty much everyone gets defensive and comes up with a rationalization for their selfishness when you bring up altruism anyway.</p>\n<p>What would you people do?</p>\n<p>EDIT: This $800 figure is probably out of date. $2000 is probably more accurate. However, it's easy to simply increase the amount of money at stake in the thought experiment.</p>\n<p>Edit 2: I fixed some swapped-around values, as kindly pointed out by Vaniver.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LMFBzsJaCRADQqw3F": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bpJqNQh7E4JzHZd4Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 8.281693063884561e-07, "legacy": true, "legacyId": "11995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Dc3bwCjM9HzZcq9M8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-08T11:46:15.378Z", "modifiedAt": null, "url": null, "title": "Q&A with experts on risks from AI #1", "slug": "q-and-a-with-experts-on-risks-from-ai-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:26.605Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/okmpRuKjhG9dvDh3Z/q-and-a-with-experts-on-risks-from-ai-1", "pageUrlRelative": "/posts/okmpRuKjhG9dvDh3Z/q-and-a-with-experts-on-risks-from-ai-1", "linkUrl": "https://www.lesswrong.com/posts/okmpRuKjhG9dvDh3Z/q-and-a-with-experts-on-risks-from-ai-1", "postedAtFormatted": "Sunday, January 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20experts%20on%20risks%20from%20AI%20%231&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20experts%20on%20risks%20from%20AI%20%231%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokmpRuKjhG9dvDh3Z%2Fq-and-a-with-experts-on-risks-from-ai-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20experts%20on%20risks%20from%20AI%20%231%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokmpRuKjhG9dvDh3Z%2Fq-and-a-with-experts-on-risks-from-ai-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FokmpRuKjhG9dvDh3Z%2Fq-and-a-with-experts-on-risks-from-ai-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2561, "htmlBody": "<p><strong>[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<h3><strong>Brandon Rohrer </strong></h3>\n<p>Sandia National Laboratories<br />Cited by 536</p>\n<p><strong>Education</strong></p>\n<p>PhD, Mechanical Engineering, Massachusetts Institute of Technology, 2002.<br /> Neville Hogan, Advisor and Thesis Committee Chair.</p>\n<p>MS, Mechanical Engineering, Massachusetts Institute of Technology, 1999.<br /> National Science Foundation Fellowship</p>\n<p>BS cum laude, Mechanical Engineering, Brigham Young University, 1997.<br /> Ezra Taft Benson (BYU's Presidential) Scholarship<br /> National Merit Scholarship</p>\n<p><strong>Experience</strong></p>\n<p>Sandia National Laboratories, Albuquerque, NM.<br /> Principal Member of the Technical Staff, 2006 - present<br /> Senior Member of the Technical Staff, 2002 - 2006 <br /><br /> University of New Mexico, Albuquerque, NM.<br /> Adjunct Assistant Professor, <br /> Department of Electrical and Computer Engineering, 2007 - present</p>\n<p><strong>Homepage:</strong> <a href=\"http://www.sandia.gov/~brrohre/\">sandia.gov/~brrohre/</a></p>\n<p><strong>Papers:</strong> <a href=\"http://www.sandia.gov/rohrer/papers.html\">sandia.gov/rohrer/papers.html</a></p>\n<p><strong>Google Scholar:</strong> <a href=\"http://scholar.google.com/scholar?q=Brandon+Rohrer\">scholar.google.com/scholar?q=Brandon+Rohrer</a></p>\n<h3>Tim Finin</h3>\n<p>Professor of Computer Science and Electrical Engineering, University of Maryland<br />Cited by 20832</p>\n<blockquote>\n<p>Tim Finin is a Professor of Computer Science and Electrical Engineering at the University of Maryland, Baltimore County (UMBC). He has over 30 years of experience in applications of Artificial Intelligence to problems in information systems and language understanding. His current research is focused on the Semantic Web, mobile computing, analyzing and extracting information from text and online social media, and on enhancing security and privacy in information systems.</p>\n<p>Finin received an S.B. degree in Electrical Engineering from MIT and a Ph.D. degree in Computer Science from the University of Illinois at Urbana-Champaign. He has held full-time positions at UMBC, Unisys, the University of Pennsylvania, and the MIT AI Laboratory. He is the author of over 300 refereed publications and has received research grants and contracts from a variety of sources. He participated in the DARPA/NSF Knowledge Sharing Effort and helped lead the development of the KQML agent communication language and was a member of the W3C Web Ontology Working Group that standardized the OWL Semantic Web language.</p>\n<p>Finin has chaired of the UMBC Computer Science Department, served on the board of directors of the Computing Research Association, been a AAAI councilor, and chaired several major research conferences. He is currently an editor-in-chief of the Elsevier Journal of Web Semantics.</p>\n</blockquote>\n<p><strong>Homepage:</strong> <a href=\"http://www.csee.umbc.edu/~finin/\">csee.umbc.edu/~finin/</a></p>\n<p><strong>Google Scholar:</strong> <a href=\"http://scholar.google.com/scholar?q=Tim+Finin\">scholar.google.com/scholar?q=Tim+Finin</a></p>\n<h3>Pat Hayes</h3>\n<blockquote>\n<p>Pat Hayes has a BA in mathematics from Cambridge University and a PhD in Artificial Intelligence from Edinburgh. He has been a professor of computer science at the University of Essex and philosophy at the University of Illinois, and the Luce Professor of cognitive science at the University of Rochester. He has been a visiting scholar at Universite de Geneve and the Center for Advanced Study in the Behavioral Studies at Stanford, and has directed applied AI research at Xerox-PARC, SRI and Schlumberger, Inc.. At various times, Pat has been secretary of <a href=\"http://www.aisb.org.uk/\">AISB</a>, chairman and trustee of <a href=\"http://ijcai.org/\">IJCAI</a>, associate editor of <a href=\"http://www.elsevier.nl/locate/artint\">Artificial Intelligence</a>, a governor of the <a href=\"http://www.cognitivesciencesociety.org/\">Cognitive Science Society</a> and president of <a href=\"http://aaai.org/\">AAAI</a>.</p>\n<p>Pat's research interests include knowledge representation and automatic reasoning, especially the representation of space and time; the semantic web; ontology design; image description and the philosophical foundations of AI and computer science. During the past decade Pat has been active in the Semantic Web initiative, largely as an invited member of the <a href=\"http://www.w3.org/\">W3C</a> Working Groups responsible for the <a href=\"http://www.w3.org/RDF/\">RDF</a>, <a href=\"http://www.w3.org/TR/owl-features/\">OWL</a> and <a href=\"http://www.w3.org/2001/sw/DataAccess/homepage-20080115\">SPARQL</a> standards. Pat is a member of the <a href=\"http://webscience.org/people.html\">Web Science Trust</a> and of <a href=\"http://www.oasis-open.org/home/index.php\">OASIS</a>, where he works on the development of ontology standards.</p>\n<p>In his spare time, Pat restores antique mechanical clocks and remodels old houses. He is also a practicing artist, with works exhibited in local competitions and international collections. Pat is a charter Fellow of AAAI and of the Cognitive Science Society, and has professional competence in domestic plumbing, carpentry and electrical work.</p>\n</blockquote>\n<p><strong>Homepage:</strong> <a href=\"http://www.ihmc.us/groups/phayes/\">ihmc.us/groups/phayes/</a></p>\n<p><strong>Selected research:</strong> <a href=\"http://www.ihmc.us/groups/phayes/wiki/a3817/Pat_Hayes_Selected_Research.html\">ihmc.us/groups/phayes/wiki/a3817/Pat_Hayes_Selected_Research.html</a></p>\n<h3><strong>The Interview</strong>:<strong></strong></h3>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer: </strong>This is an entertaining survey. I appreciate the specificity with which you've worded some of the questions. I don't have a defensible or scientific answer to any of the questions, but I've included some answers below that are wild-ass guesses. You got some good and thoughtful responses. I've been enjoying reading them. Thanks for compiling them.</p>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em><br /> <br /> <em>Explanatory remark to Q1:</em><br /> <br /> <em>P(human-level AI by (year) | no wars &and; no disasters &and; beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> 2032/2052/2072</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> 20/100/200 years</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes:</strong> I do not consider this question to be answerable, as I do not accpet this (common) notion of \"human-level intelligence\" as meaningful. Artificially intelligent artifacts are in some ways superhuman, and have been for many years now; but in other ways, they are sub-human, or perhaps it would be better to say, non-human. They simply differ from human intelligences, and it is inappropriate to speak of \"levels\" of intelligence in this way. Intelligence is too complex and multifacetted a topic to be spoken of as though it were something like sea level that can be calibrated on a simple linear scale.<br /> <br /> If by 'human-level' you mean, the AI will be an accurate simalcrum of a human being, or perhaps a human personality (as is often envisioned in science fiction, eg HAL from \"2001\") my answer would be, never. We will never create such a machine intelligence, because it is probably technically close to impossible, and not technically useful (note that HAL failed in its mission through being TOO \"human\": it had a nervous breakdown. Bad engineering.) But mostly because we have absolutely no need to do so. Human beings are not in such short supply at resent that it makes sense to try to make artificial ones at great cost. And actual AI work, as opposed to the fantasies often woven around it by journalists and futurists, is not aiming to create such things. A self-driving car is not an artificial human, but it is likely to be a far better driver than any human, because it will not be limited by human-level attention spans and human-level response times. It will be, in these areas, super-human, just as present computers are superhuman at calculation and keeping track of large numbers of complex patterns, etc.. .</p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of badly done AI?</em><br /> <br /> <em>Explanatory remark to Q2:</em><br /> <br /> <em>P(human extinction | badly done AI) = ?</em></p>\n<p><em>(Where 'badly done' = AGI capable of self-modification that is <strong>not </strong>provably non-dangerous.)</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> &lt; 1%</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> 0.001</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>Zero. The whole idea is ludicrous.</p>\n<p><strong>Q3: </strong><em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </em><br /> <br /> <em>Explanatory remark to Q3:</em></p>\n<p><em>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> &lt; 1%</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> 0.0001/0.0001/0.01</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>Again, zero. Self-modification in any useful sense has never been technically demonstrated. Machine learning is possible and indeed is a widely used technique (no longer only in AI) but a learning engine is the same thing after it has learnt something as it was before., just as biological learners are. When we learn, we get more informed, but not more intelligent: similarly with machines.</p>\n<p><strong>Q4: </strong><em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em><br /> <br /> <em>Explanatory remark to Q4:</em></p>\n<p><em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> No more.</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> No.</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>No. There is no reason to suppose that any manufactured system will have any emotional stance towards us of any kind, friendly or unfriendly. In fact, even if the idea of \"human-level\" made sense, we could have a more-than-human-level super-intelligent machine, and still have it bear no emotional stance towards other entities whatsoever. Nor need it have any lust for power or political ambitions, unless we set out to construct such a thing (which AFAIK, nobody is doing.) Think of an unworldly boffin who just wants to be left alone to think, and does not care a whit for changing the world for better or for worse, and has no intentions or desires, but simply answers questions that are put to it and thinks about htings that it is asked to think about. It has no ambition and in any case no means to achieve any far-reaching changes even if it \"wanted\" to do so. It seems to me that this is what a super-intelligent question-answering system would be like. I see no inherent, even slight, danger arising from the presence of such a device.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?<br /><br /></em><em>Explanatory remark to Q5:</em></p>\n<p><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> Evolved variants of currently existing biological viruses and bacteria.</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> No.</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>No. Nanotechnology has the potential to make far-reaching changes to the actual physical environment. AI poses no such threat. Indeed, I do not see that AI itself (that is, actual AI work being done, rather than the somewhat uninformed fantasies that some authors, such as Ray Kurtzwiel, have invented) poses any serious threat to anyone.</p>\n<p style=\"padding-left: 30px;\">I would say that any human-extinction type event is likely to make a serious dent in my personal goals. (But of course I am being sarcastic, as the question as posed seems to me to be ridiculous.)<br /> <br /> When I think of the next century, say, the risk I amost concerned about is global warming and the resulting disruption to the biosphere and human society. I do not think that humans will become extinct, but I think that our current global civilization might not survive.</p>\n<p><strong>Q6: </strong><em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> High.</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> About right.</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>The actual risks are negligible: the perceived risks (thanks to the popularization of such nonsensical ideas as the \"singularity\") are much greater.</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human\u2010level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> No, but the demonstrated ability of a robot to learn from its experience in a complex and unstructured environment is likely to be a milestone on that path, perhaps signalling HLI is 20 years away.</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> Passing a well constructed, open ended turing test.</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>No. There are no 'milestones' in AI. Progress is slow but steady, and there are no magic bullets.</p>\n<h3>Anonymous</h3>\n<p>The following are replies from experts who either did not answer the questions for various reasons or didn't want them to be published.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 1:</strong> Sorry, I don't want to do an email interview - it is too hard to qualify comments.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 2: </strong>Thanks for your inquiry - but as you note I am a roboticist and not a futurist, so I generally try to avoid speculation.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 3:</strong> my firmest belief about the timeline for human-level AI is that we can't estimate it usefully. partly this is because i don't think \"human level AI\" will prove to be a single thing (or event) that we can point to and say \"aha there it is!\". instead i think there will be a series of human level abilities that are achieved. in fact some already have (though many more haven't).</p>\n<p style=\"padding-left: 30px;\">(on the other hand, i think shooting for human-level AI is a good long term research goal. it doesn't need to be one thing in the end to be a good focus of work.)</p>\n<p style=\"padding-left: 30px;\">another important catch, with respect to the \"risk from human level AI\" equation, is that i don't think human level AI immediately leads to super-human level AI. we have had many human-level human's working on AI for a long time, and haven't added up to even a single human. i don't think it's is necessarily (or even likely) the case that a human level AI would have much more luck at making itself smarter than we have been.... <strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Expert 4:</strong> Thanks for this - fascinating questions, and I am a great supporter of probability elicitation, but only from people who are well-informed about the subject-matter! &nbsp;And I am afraid this does not include me - I am sure I should know more about this, but I don't, and so am unwilling to express publicly any firm opinion.<br /> <br /> Of course in private in a bar I may be more forthcoming!</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 5:</strong> Interesting questions, I'll enjoy seeing your published results! &nbsp;Unfortunately, now that I work at ****** (through the acquisition of one of my companies, ******), there are policies in place that prohibit me from participating in this kind of exercise. <strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Expert 6:</strong> I don't think I can answer your questions in a meaningful way... <strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Expert 7:</strong> Thanks for your interest. I feel that this is not in the area of my primary expertise. &nbsp;However, I'd refer you to ****** ( a colleague, and co-chair of the *******) who I think might be in a better position to give you current and informed answers.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 8:</strong> Unfortunately, most of these questions do not have a simple answer, in my opinion, so I can't just say \"five years\" or whatever -- I would have to write a little essay in order to give an answer that reflects what I really believe. &nbsp;For example, the concept of \"roughly human-level intelligence\" is a complicated one, and any simple answer would be misleading. &nbsp;By some measures we're already there; by other measures, the goal is still far in the future. &nbsp;And I think that the idea of a \"provably friendly\" system is just meaningless.</p>\n<p style=\"padding-left: 30px;\">Anyway, good luck with your survey. &nbsp;I'm sure you'll get simple answers from some people, but I suspect that you will find them confusing or confused.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 9:</strong> Thank you for your email. I do not feel comfortable answering your questions for a public audience.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 10:</strong> sorry no reply for such questions</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 11:</strong> I regard speculation about AI as a waste of time.&nbsp; We are at an impasse: none of our current techniques seems likely to provide truly human-like intelligence.&nbsp; I think what's needed is a conceptual breakthrough from someone comparable to Newton or Einstein. Until that happens, we're going to remain stuck, although there will be lots of useful technology coming along.&nbsp; It won't be \"intelligent\" or \"conscious\" the way humans are, but it might do a really good job of guessing what movies we want to watch or what news stories interest us the most. <br /><br />Given our current state of ignorance, I feel that speculating about either the timeline or the impact of AI is best left to science fiction writers.</p>\n<p>More interviews forthcoming (hopefully). At least one person told me that the questions are extremely important and that he would work out some answers over the next few days.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 1, "ZFrgTgzwEfStg26JL": 1, "DigEmY3RrF3XL5cwe": 1, "9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "okmpRuKjhG9dvDh3Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 45, "extendedScore": null, "score": 0.000102, "legacy": true, "legacyId": "11997", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Click_here_to_see_a_list_of_all_interviews_\">[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<h3 id=\"Brandon_Rohrer_\"><strong>Brandon Rohrer </strong></h3>\n<p>Sandia National Laboratories<br>Cited by 536</p>\n<p><strong id=\"Education\">Education</strong></p>\n<p>PhD, Mechanical Engineering, Massachusetts Institute of Technology, 2002.<br> Neville Hogan, Advisor and Thesis Committee Chair.</p>\n<p>MS, Mechanical Engineering, Massachusetts Institute of Technology, 1999.<br> National Science Foundation Fellowship</p>\n<p>BS cum laude, Mechanical Engineering, Brigham Young University, 1997.<br> Ezra Taft Benson (BYU's Presidential) Scholarship<br> National Merit Scholarship</p>\n<p><strong id=\"Experience\">Experience</strong></p>\n<p>Sandia National Laboratories, Albuquerque, NM.<br> Principal Member of the Technical Staff, 2006 - present<br> Senior Member of the Technical Staff, 2002 - 2006 <br><br> University of New Mexico, Albuquerque, NM.<br> Adjunct Assistant Professor, <br> Department of Electrical and Computer Engineering, 2007 - present</p>\n<p><strong>Homepage:</strong> <a href=\"http://www.sandia.gov/~brrohre/\">sandia.gov/~brrohre/</a></p>\n<p><strong>Papers:</strong> <a href=\"http://www.sandia.gov/rohrer/papers.html\">sandia.gov/rohrer/papers.html</a></p>\n<p><strong>Google Scholar:</strong> <a href=\"http://scholar.google.com/scholar?q=Brandon+Rohrer\">scholar.google.com/scholar?q=Brandon+Rohrer</a></p>\n<h3 id=\"Tim_Finin\">Tim Finin</h3>\n<p>Professor of Computer Science and Electrical Engineering, University of Maryland<br>Cited by 20832</p>\n<blockquote>\n<p>Tim Finin is a Professor of Computer Science and Electrical Engineering at the University of Maryland, Baltimore County (UMBC). He has over 30 years of experience in applications of Artificial Intelligence to problems in information systems and language understanding. His current research is focused on the Semantic Web, mobile computing, analyzing and extracting information from text and online social media, and on enhancing security and privacy in information systems.</p>\n<p>Finin received an S.B. degree in Electrical Engineering from MIT and a Ph.D. degree in Computer Science from the University of Illinois at Urbana-Champaign. He has held full-time positions at UMBC, Unisys, the University of Pennsylvania, and the MIT AI Laboratory. He is the author of over 300 refereed publications and has received research grants and contracts from a variety of sources. He participated in the DARPA/NSF Knowledge Sharing Effort and helped lead the development of the KQML agent communication language and was a member of the W3C Web Ontology Working Group that standardized the OWL Semantic Web language.</p>\n<p>Finin has chaired of the UMBC Computer Science Department, served on the board of directors of the Computing Research Association, been a AAAI councilor, and chaired several major research conferences. He is currently an editor-in-chief of the Elsevier Journal of Web Semantics.</p>\n</blockquote>\n<p><strong>Homepage:</strong> <a href=\"http://www.csee.umbc.edu/~finin/\">csee.umbc.edu/~finin/</a></p>\n<p><strong>Google Scholar:</strong> <a href=\"http://scholar.google.com/scholar?q=Tim+Finin\">scholar.google.com/scholar?q=Tim+Finin</a></p>\n<h3 id=\"Pat_Hayes\">Pat Hayes</h3>\n<blockquote>\n<p>Pat Hayes has a BA in mathematics from Cambridge University and a PhD in Artificial Intelligence from Edinburgh. He has been a professor of computer science at the University of Essex and philosophy at the University of Illinois, and the Luce Professor of cognitive science at the University of Rochester. He has been a visiting scholar at Universite de Geneve and the Center for Advanced Study in the Behavioral Studies at Stanford, and has directed applied AI research at Xerox-PARC, SRI and Schlumberger, Inc.. At various times, Pat has been secretary of <a href=\"http://www.aisb.org.uk/\">AISB</a>, chairman and trustee of <a href=\"http://ijcai.org/\">IJCAI</a>, associate editor of <a href=\"http://www.elsevier.nl/locate/artint\">Artificial Intelligence</a>, a governor of the <a href=\"http://www.cognitivesciencesociety.org/\">Cognitive Science Society</a> and president of <a href=\"http://aaai.org/\">AAAI</a>.</p>\n<p>Pat's research interests include knowledge representation and automatic reasoning, especially the representation of space and time; the semantic web; ontology design; image description and the philosophical foundations of AI and computer science. During the past decade Pat has been active in the Semantic Web initiative, largely as an invited member of the <a href=\"http://www.w3.org/\">W3C</a> Working Groups responsible for the <a href=\"http://www.w3.org/RDF/\">RDF</a>, <a href=\"http://www.w3.org/TR/owl-features/\">OWL</a> and <a href=\"http://www.w3.org/2001/sw/DataAccess/homepage-20080115\">SPARQL</a> standards. Pat is a member of the <a href=\"http://webscience.org/people.html\">Web Science Trust</a> and of <a href=\"http://www.oasis-open.org/home/index.php\">OASIS</a>, where he works on the development of ontology standards.</p>\n<p>In his spare time, Pat restores antique mechanical clocks and remodels old houses. He is also a practicing artist, with works exhibited in local competitions and international collections. Pat is a charter Fellow of AAAI and of the Cognitive Science Society, and has professional competence in domestic plumbing, carpentry and electrical work.</p>\n</blockquote>\n<p><strong>Homepage:</strong> <a href=\"http://www.ihmc.us/groups/phayes/\">ihmc.us/groups/phayes/</a></p>\n<p><strong>Selected research:</strong> <a href=\"http://www.ihmc.us/groups/phayes/wiki/a3817/Pat_Hayes_Selected_Research.html\">ihmc.us/groups/phayes/wiki/a3817/Pat_Hayes_Selected_Research.html</a></p>\n<h3 id=\"The_Interview_\"><strong>The Interview</strong>:<strong></strong></h3>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer: </strong>This is an entertaining survey. I appreciate the specificity with which you've worded some of the questions. I don't have a defensible or scientific answer to any of the questions, but I've included some answers below that are wild-ass guesses. You got some good and thoughtful responses. I've been enjoying reading them. Thanks for compiling them.</p>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em><br> <br> <em>Explanatory remark to Q1:</em><br> <br> <em>P(human-level AI by (year) | no wars \u2227 no disasters \u2227 beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> 2032/2052/2072</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> 20/100/200 years</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes:</strong> I do not consider this question to be answerable, as I do not accpet this (common) notion of \"human-level intelligence\" as meaningful. Artificially intelligent artifacts are in some ways superhuman, and have been for many years now; but in other ways, they are sub-human, or perhaps it would be better to say, non-human. They simply differ from human intelligences, and it is inappropriate to speak of \"levels\" of intelligence in this way. Intelligence is too complex and multifacetted a topic to be spoken of as though it were something like sea level that can be calibrated on a simple linear scale.<br> <br> If by 'human-level' you mean, the AI will be an accurate simalcrum of a human being, or perhaps a human personality (as is often envisioned in science fiction, eg HAL from \"2001\") my answer would be, never. We will never create such a machine intelligence, because it is probably technically close to impossible, and not technically useful (note that HAL failed in its mission through being TOO \"human\": it had a nervous breakdown. Bad engineering.) But mostly because we have absolutely no need to do so. Human beings are not in such short supply at resent that it makes sense to try to make artificial ones at great cost. And actual AI work, as opposed to the fantasies often woven around it by journalists and futurists, is not aiming to create such things. A self-driving car is not an artificial human, but it is likely to be a far better driver than any human, because it will not be limited by human-level attention spans and human-level response times. It will be, in these areas, super-human, just as present computers are superhuman at calculation and keeping track of large numbers of complex patterns, etc.. .</p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of badly done AI?</em><br> <br> <em>Explanatory remark to Q2:</em><br> <br> <em>P(human extinction | badly done AI) = ?</em></p>\n<p><em>(Where 'badly done' = AGI capable of self-modification that is <strong>not </strong>provably non-dangerous.)</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> &lt; 1%</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> 0.001</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>Zero. The whole idea is ludicrous.</p>\n<p><strong>Q3: </strong><em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </em><br> <br> <em>Explanatory remark to Q3:</em></p>\n<p><em>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> &lt; 1%</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> 0.0001/0.0001/0.01</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>Again, zero. Self-modification in any useful sense has never been technically demonstrated. Machine learning is possible and indeed is a widely used technique (no longer only in AI) but a learning engine is the same thing after it has learnt something as it was before., just as biological learners are. When we learn, we get more informed, but not more intelligent: similarly with machines.</p>\n<p><strong>Q4: </strong><em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em><br> <br> <em>Explanatory remark to Q4:</em></p>\n<p><em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> No more.</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> No.</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>No. There is no reason to suppose that any manufactured system will have any emotional stance towards us of any kind, friendly or unfriendly. In fact, even if the idea of \"human-level\" made sense, we could have a more-than-human-level super-intelligent machine, and still have it bear no emotional stance towards other entities whatsoever. Nor need it have any lust for power or political ambitions, unless we set out to construct such a thing (which AFAIK, nobody is doing.) Think of an unworldly boffin who just wants to be left alone to think, and does not care a whit for changing the world for better or for worse, and has no intentions or desires, but simply answers questions that are put to it and thinks about htings that it is asked to think about. It has no ambition and in any case no means to achieve any far-reaching changes even if it \"wanted\" to do so. It seems to me that this is what a super-intelligent question-answering system would be like. I see no inherent, even slight, danger arising from the presence of such a device.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?<br><br></em><em>Explanatory remark to Q5:</em></p>\n<p><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> Evolved variants of currently existing biological viruses and bacteria.</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> No.</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>No. Nanotechnology has the potential to make far-reaching changes to the actual physical environment. AI poses no such threat. Indeed, I do not see that AI itself (that is, actual AI work being done, rather than the somewhat uninformed fantasies that some authors, such as Ray Kurtzwiel, have invented) poses any serious threat to anyone.</p>\n<p style=\"padding-left: 30px;\">I would say that any human-extinction type event is likely to make a serious dent in my personal goals. (But of course I am being sarcastic, as the question as posed seems to me to be ridiculous.)<br> <br> When I think of the next century, say, the risk I amost concerned about is global warming and the resulting disruption to the biosphere and human society. I do not think that humans will become extinct, but I think that our current global civilization might not survive.</p>\n<p><strong>Q6: </strong><em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> High.</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> About right.</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>The actual risks are negligible: the perceived risks (thanks to the popularization of such nonsensical ideas as the \"singularity\") are much greater.</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human\u2010level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Brandon Rohrer:</strong> No, but the demonstrated ability of a robot to learn from its experience in a complex and unstructured environment is likely to be a milestone on that path, perhaps signalling HLI is 20 years away.</p>\n<p style=\"padding-left: 30px;\"><strong>Tim Finin:</strong> Passing a well constructed, open ended turing test.</p>\n<p style=\"padding-left: 30px;\"><strong>Pat Hayes: </strong>No. There are no 'milestones' in AI. Progress is slow but steady, and there are no magic bullets.</p>\n<h3 id=\"Anonymous\">Anonymous</h3>\n<p>The following are replies from experts who either did not answer the questions for various reasons or didn't want them to be published.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 1:</strong> Sorry, I don't want to do an email interview - it is too hard to qualify comments.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 2: </strong>Thanks for your inquiry - but as you note I am a roboticist and not a futurist, so I generally try to avoid speculation.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 3:</strong> my firmest belief about the timeline for human-level AI is that we can't estimate it usefully. partly this is because i don't think \"human level AI\" will prove to be a single thing (or event) that we can point to and say \"aha there it is!\". instead i think there will be a series of human level abilities that are achieved. in fact some already have (though many more haven't).</p>\n<p style=\"padding-left: 30px;\">(on the other hand, i think shooting for human-level AI is a good long term research goal. it doesn't need to be one thing in the end to be a good focus of work.)</p>\n<p style=\"padding-left: 30px;\">another important catch, with respect to the \"risk from human level AI\" equation, is that i don't think human level AI immediately leads to super-human level AI. we have had many human-level human's working on AI for a long time, and haven't added up to even a single human. i don't think it's is necessarily (or even likely) the case that a human level AI would have much more luck at making itself smarter than we have been.... <strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Expert 4:</strong> Thanks for this - fascinating questions, and I am a great supporter of probability elicitation, but only from people who are well-informed about the subject-matter! &nbsp;And I am afraid this does not include me - I am sure I should know more about this, but I don't, and so am unwilling to express publicly any firm opinion.<br> <br> Of course in private in a bar I may be more forthcoming!</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 5:</strong> Interesting questions, I'll enjoy seeing your published results! &nbsp;Unfortunately, now that I work at ****** (through the acquisition of one of my companies, ******), there are policies in place that prohibit me from participating in this kind of exercise. <strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Expert 6:</strong> I don't think I can answer your questions in a meaningful way... <strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Expert 7:</strong> Thanks for your interest. I feel that this is not in the area of my primary expertise. &nbsp;However, I'd refer you to ****** ( a colleague, and co-chair of the *******) who I think might be in a better position to give you current and informed answers.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 8:</strong> Unfortunately, most of these questions do not have a simple answer, in my opinion, so I can't just say \"five years\" or whatever -- I would have to write a little essay in order to give an answer that reflects what I really believe. &nbsp;For example, the concept of \"roughly human-level intelligence\" is a complicated one, and any simple answer would be misleading. &nbsp;By some measures we're already there; by other measures, the goal is still far in the future. &nbsp;And I think that the idea of a \"provably friendly\" system is just meaningless.</p>\n<p style=\"padding-left: 30px;\">Anyway, good luck with your survey. &nbsp;I'm sure you'll get simple answers from some people, but I suspect that you will find them confusing or confused.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 9:</strong> Thank you for your email. I do not feel comfortable answering your questions for a public audience.</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 10:</strong> sorry no reply for such questions</p>\n<p style=\"padding-left: 30px;\"><strong>Expert 11:</strong> I regard speculation about AI as a waste of time.&nbsp; We are at an impasse: none of our current techniques seems likely to provide truly human-like intelligence.&nbsp; I think what's needed is a conceptual breakthrough from someone comparable to Newton or Einstein. Until that happens, we're going to remain stuck, although there will be lots of useful technology coming along.&nbsp; It won't be \"intelligent\" or \"conscious\" the way humans are, but it might do a really good job of guessing what movies we want to watch or what news stories interest us the most. <br><br>Given our current state of ignorance, I feel that speculating about either the timeline or the impact of AI is best left to science fiction writers.</p>\n<p>More interviews forthcoming (hopefully). At least one person told me that the questions are extremely important and that he would work out some answers over the next few days.</p>", "sections": [{"title": "[Click here to see a list of all interviews]", "anchor": "_Click_here_to_see_a_list_of_all_interviews_", "level": 2}, {"title": "Brandon Rohrer ", "anchor": "Brandon_Rohrer_", "level": 1}, {"title": "Education", "anchor": "Education", "level": 2}, {"title": "Experience", "anchor": "Experience", "level": 2}, {"title": "Tim Finin", "anchor": "Tim_Finin", "level": 1}, {"title": "Pat Hayes", "anchor": "Pat_Hayes", "level": 1}, {"title": "The Interview:", "anchor": "The_Interview_", "level": 1}, {"title": "Anonymous", "anchor": "Anonymous", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "67 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-08T18:24:33.344Z", "modifiedAt": null, "url": null, "title": "Alternative uses of paperclips", "slug": "alternative-uses-of-paperclips", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.060Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JyoFQFdPAuBzW9yHn/alternative-uses-of-paperclips", "pageUrlRelative": "/posts/JyoFQFdPAuBzW9yHn/alternative-uses-of-paperclips", "linkUrl": "https://www.lesswrong.com/posts/JyoFQFdPAuBzW9yHn/alternative-uses-of-paperclips", "postedAtFormatted": "Sunday, January 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alternative%20uses%20of%20paperclips&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlternative%20uses%20of%20paperclips%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyoFQFdPAuBzW9yHn%2Falternative-uses-of-paperclips%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alternative%20uses%20of%20paperclips%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyoFQFdPAuBzW9yHn%2Falternative-uses-of-paperclips", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJyoFQFdPAuBzW9yHn%2Falternative-uses-of-paperclips", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 13, "htmlBody": "<p><a href=\"http://i.imgur.com/4f1FF.jpg\"><img src=\"http://i.imgur.com/4f1FF.jpg\" alt=\"\" width=\"980\" height=\"735\" /></a></p>\n<p><a href=\"http://i.imgur.com/4f1FF.jpg\">\n<p>Somewhere in the distant galaxy, Clippys post pictures of alternative uses of humans.</p>\n</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"QH4LhvnyR4QkW9MG8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JyoFQFdPAuBzW9yHn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 18, "extendedScore": null, "score": 8.283994835123843e-07, "legacy": true, "legacyId": "11999", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-08T19:01:11.825Z", "modifiedAt": null, "url": null, "title": "[Transcript] Richard Feynman on Why Questions", "slug": "transcript-richard-feynman-on-why-questions", "viewCount": null, "lastCommentedAt": "2019-07-22T10:48:38.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Grognor", "createdAt": "2011-01-31T02:54:34.463Z", "isAdmin": false, "displayName": "Grognor"}, "userId": "LoykQRMTxJFxwwdPy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W9rJv26sxs4g2B9bL/transcript-richard-feynman-on-why-questions", "pageUrlRelative": "/posts/W9rJv26sxs4g2B9bL/transcript-richard-feynman-on-why-questions", "linkUrl": "https://www.lesswrong.com/posts/W9rJv26sxs4g2B9bL/transcript-richard-feynman-on-why-questions", "postedAtFormatted": "Sunday, January 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BTranscript%5D%20Richard%20Feynman%20on%20Why%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BTranscript%5D%20Richard%20Feynman%20on%20Why%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW9rJv26sxs4g2B9bL%2Ftranscript-richard-feynman-on-why-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BTranscript%5D%20Richard%20Feynman%20on%20Why%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW9rJv26sxs4g2B9bL%2Ftranscript-richard-feynman-on-why-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW9rJv26sxs4g2B9bL%2Ftranscript-richard-feynman-on-why-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1399, "htmlBody": "<p>I thought <a href=\"http://www.youtube.com/watch?v=wMFPe-DwULM\">this video</a> was a <em>really</em> good <a href=\"/lw/of/dissolving_the_question/\">question dissolving</a> by Richard Feynman. But it's in 240p! Nobody likes watching 240p videos. So I transcribed it. (<em>Edit</em>: That was in jest. The real reasons are because I thought I could get more exposure this way, and because a lot of people appreciate transcripts. Also, Paul Graham <a href=\"http://www.paulgraham.com/speak.html\">speculates</a> that the written word is universally superior than the spoken word for the purpose of ideas.) I was going to post it as a rationality quote, but the transcript was sufficiently long that I think it warrants a discussion post instead.</p>\n<p>Here you go:</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>Interviewer: If you get hold of two magnets, and you push them, you can feel this pushing between them. Turn them around the other way, and they slam together. Now, what is it, the feeling between those two magnets?</p>\n<p>Feynman: What do you mean, \"What's the feeling between the two magnets?\"</p>\n<p>Interviewer: There's something there, isn't there? The sensation is that there's something there when you push these two magnets together.</p>\n<p>Feynman: Listen to my question. What is the meaning when you say that there's a feeling? Of course you feel it. Now what do you want to know?</p>\n<p>Interviewer: What I want to know is what's going on between these two bits of metal?</p>\n<p>Feynman: They repel each other.</p>\n<p>Interviewer: What does that mean, or why are they doing that, or how are they doing that? I think that's a perfectly reasonable question.</p>\n<p>Feynman: Of course, it's an excellent question. But the problem, you see, when you ask <em>why</em> something happens, how does a person answer why something happens? For example, Aunt Minnie is in the hospital. Why? Because she went out, slipped on the ice, and broke her hip. That satisfies people. It satisfies, but it wouldn't satisfy someone who came from another planet and who knew nothing about why when you break your hip do you go to the hospital. How do you get to the hospital when the hip is broken? Well, because her husband, seeing that her hip was broken, called the hospital up and sent somebody to get her. All that is understood by people. And when you explain a <em>why</em>, you have to be in some framework that you allow something to be true. Otherwise, you're perpetually asking why. Why did the husband call up the hospital? Because the husband is interested in his wife's welfare. Not always, some husbands aren't interested in their wives' welfare when they're drunk, and they're angry.</p>\n<p>And you begin to get a very interesting understanding of the world and all its complications. If you try to follow anything up, you go deeper and deeper in various directions. For example, if you go, \"Why did she slip on the ice?\" Well, ice is slippery. Everybody knows that, no problem. But you ask <em>why is ice slippery?</em> That's kinda curious. Ice is extremely slippery. It's very interesting. You say, how does it work? You could either say, \"I'm satisfied that you've answered me. Ice is slippery; that explains it,\" or you could go on and say, \"Why is ice slippery?\" and then you're involved with something, because there aren't many things as slippery as ice. It's very hard to get greasy stuff, but that's sort of wet and slimy. But a solid that's so slippery? Because it is, in the case of ice, when you stand on it (they say) momentarily the pressure melts the ice a little bit so you get a sort of instantaneous water surface on which you're slipping. Why on ice and not on other things? Because water expands when it freezes, so the pressure tries to undo the expansion and melts it. It's capable of melting, but other substances get cracked when they're freezing, and when you push them they're satisfied to be solid.</p>\n<p>Why does water expand when it freezes and other substances don't? I'm not answering your question, but I'm telling you how difficult the <em>why </em>question is. You have to know what it is that you're permitted to understand and allow to be understood and known, and what it is you're not. You'll notice, in this example, that the more I ask why, the deeper a thing is, the more interesting it gets. We could even go further and say, \"Why did she fall down when she slipped?\" It has to do with gravity, involves all the planets and everything else. Nevermind! It goes on and on. And when you're asked, for example, why two magnets repel, there are many different levels. It depends on whether you're a student of physics, or an ordinary person who doesn't know anything. If you're somebody who doesn't know anything at all about it, all I can say is the magnetic force makes them repel, and that you're feeling that force.</p>\n<p>You say, \"That's very strange, because I don't feel kind of force like that in other circumstances.\" When you turn them the other way, they attract. There's a very analogous force, electrical force, which is the same kind of a question, that's also very weird. But you're not at all disturbed by the fact that when you put your hand on a chair, it pushes you back. But we found out by looking at it that that's the same force, as a matter of fact (an electrical force, not magnetic exactly, in that case). But it's the same electric repulsions that are involved in keeping your finger away from the chair because it's electrical forces in minor and microscopic details. There's other forces involved, connected to electrical forces. It turns out that the magnetic and electrical force with which I wish to explain this repulsion in the first place is what ultimately is the deeper thing that we have to start with to explain many other things that everybody would just accept. You know you can't put your hand through the chair; that's taken for granted. But that you can't put your hand through the chair, when looked at more closely, <em>why</em>, involves the same repulsive forces that appear in magnets. The situation you then have to explain is why, in magnets, it goes over a bigger distance than ordinarily. There it has to do with the fact that in iron all the electrons are spinning in the same direction, they all get lined up, and they magnify the effect of the force 'til it's large enough, at a distance, that you can feel it. But it's a force which is present all the time and very common and is a basic force of almost - I mean, I could go a little further back if I went more technical - but on an early level I've just got to tell you that's going to be one of the things you'll just have to take as an element of the world: the existence of magnetic repulsion, or electrical attraction, magnetic attraction.</p>\n<p>I can't explain that attraction in terms of anything else that's familiar to you. For example, if we said the magnets attract like if rubber bands, I would be cheating you. Because they're not connected by rubber bands. I'd soon be in trouble. And secondly, if you were curious enough, you'd ask me why rubber bands tend to pull back together again, and I would end up explaining that in terms of electrical forces, which are the very things that I'm trying to use the rubber bands to explain. So I have cheated very badly, you see. So I am not going to be able to give you an answer to why magnets attract each other except to tell you that they do. And to tell you that that's one of the elements in the world - there are electrical forces, magnetic forces, gravitational forces, and others, and those are some of the parts. If you were a student, I could go further. I could tell you that the&nbsp;magnetic forces are related to the electrical forces very intimately, that the relationship between the gravity forces and electrical forces remains unknown, and so on. But I really can't do a good job, any job, of explaining magnetic force in terms of something else you're more familiar with, because I don't understand it in terms of anything else that you're more familiar with.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BhfefamXXee6c2CH8": 1, "9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W9rJv26sxs4g2B9bL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 115, "extendedScore": null, "score": 0.000229, "legacy": true, "legacyId": "12000", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 115, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T03:56:11.977Z", "modifiedAt": null, "url": null, "title": "Meetup : Monthly San Francisco Bay Area meetup", "slug": "meetup-monthly-san-francisco-bay-area-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v9tuD9fSAr2nN9SQt/meetup-monthly-san-francisco-bay-area-meetup", "pageUrlRelative": "/posts/v9tuD9fSAr2nN9SQt/meetup-monthly-san-francisco-bay-area-meetup", "linkUrl": "https://www.lesswrong.com/posts/v9tuD9fSAr2nN9SQt/meetup-monthly-san-francisco-bay-area-meetup", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monthly%20San%20Francisco%20Bay%20Area%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monthly%20San%20Francisco%20Bay%20Area%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9tuD9fSAr2nN9SQt%2Fmeetup-monthly-san-francisco-bay-area-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monthly%20San%20Francisco%20Bay%20Area%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9tuD9fSAr2nN9SQt%2Fmeetup-monthly-san-francisco-bay-area-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9tuD9fSAr2nN9SQt%2Fmeetup-monthly-san-francisco-bay-area-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5x'>Monthly San Francisco Bay Area meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 January 2012 07:49:14PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2128 Oxford St., Berkley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first monthly meetup of the new year will be Saturday, January 14. We'll meet at 7pm in the Starbucks on Oxford Street and then migrate to a restaurant, probably Biryani House.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5x'>Monthly San Francisco Bay Area meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v9tuD9fSAr2nN9SQt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "12016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monthly_San_Francisco_Bay_Area_meetup\">Discussion article for the meetup : <a href=\"/meetups/5x\">Monthly San Francisco Bay Area meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 January 2012 07:49:14PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2128 Oxford St., Berkley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first monthly meetup of the new year will be Saturday, January 14. We'll meet at 7pm in the Starbucks on Oxford Street and then migrate to a restaurant, probably Biryani House.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monthly_San_Francisco_Bay_Area_meetup1\">Discussion article for the meetup : <a href=\"/meetups/5x\">Monthly San Francisco Bay Area meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monthly San Francisco Bay Area meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_San_Francisco_Bay_Area_meetup", "level": 1}, {"title": "Discussion article for the meetup : Monthly San Francisco Bay Area meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_San_Francisco_Bay_Area_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T05:45:50.070Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Buy Now or Forever Hold Your Peace", "slug": "seq-rerun-buy-now-or-forever-hold-your-peace", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.364Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SYCx4Y3D2LxfPXuY7/seq-rerun-buy-now-or-forever-hold-your-peace", "pageUrlRelative": "/posts/SYCx4Y3D2LxfPXuY7/seq-rerun-buy-now-or-forever-hold-your-peace", "linkUrl": "https://www.lesswrong.com/posts/SYCx4Y3D2LxfPXuY7/seq-rerun-buy-now-or-forever-hold-your-peace", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Buy%20Now%20or%20Forever%20Hold%20Your%20Peace&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Buy%20Now%20or%20Forever%20Hold%20Your%20Peace%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYCx4Y3D2LxfPXuY7%2Fseq-rerun-buy-now-or-forever-hold-your-peace%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Buy%20Now%20or%20Forever%20Hold%20Your%20Peace%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYCx4Y3D2LxfPXuY7%2Fseq-rerun-buy-now-or-forever-hold-your-peace", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYCx4Y3D2LxfPXuY7%2Fseq-rerun-buy-now-or-forever-hold-your-peace", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Today's post, <a href=\"/lw/ni/buy_now_or_forever_hold_your_peace/\">Buy Now Or Forever Hold Your Peace</a> was originally published on 04 February 2008 .  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you really think that your reasoning is superior to that of prediction markets, there is free money available to you right now. If you aren't picking it up, you clearly don't really believe that you can beat the markets.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/995/seq_rerun_extensions_and_intensions/\">http://lesswrong.com/r/discussion/lw/995/seq_rerun_extensions_and_intensions/</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SYCx4Y3D2LxfPXuY7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 13, "extendedScore": null, "score": 8.286560491189017e-07, "legacy": true, "legacyId": "12018", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QDHcpmTXMqPrrsddr", "7fmaoCyP6SNQXqSyj", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T12:07:15.339Z", "modifiedAt": null, "url": null, "title": "The Ethical Status of Non-human Animals", "slug": "the-ethical-status-of-non-human-animals", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "syllogism", "createdAt": "2010-12-09T02:25:23.672Z", "isAdmin": false, "displayName": "syllogism"}, "userId": "aHznJxGf4ZbruWkNa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ikQArh7yRE47bsokR/the-ethical-status-of-non-human-animals", "pageUrlRelative": "/posts/ikQArh7yRE47bsokR/the-ethical-status-of-non-human-animals", "linkUrl": "https://www.lesswrong.com/posts/ikQArh7yRE47bsokR/the-ethical-status-of-non-human-animals", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Ethical%20Status%20of%20Non-human%20Animals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Ethical%20Status%20of%20Non-human%20Animals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FikQArh7yRE47bsokR%2Fthe-ethical-status-of-non-human-animals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Ethical%20Status%20of%20Non-human%20Animals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FikQArh7yRE47bsokR%2Fthe-ethical-status-of-non-human-animals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FikQArh7yRE47bsokR%2Fthe-ethical-status-of-non-human-animals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1732, "htmlBody": "<p>There's been some discussion on this site about vegetarianism previously, although less than I expected. It's a complicated topic, so I want to focus on a critical sub-issue: within a consequentialist/utilitarian framework, what should be the status of non-human animals? Do only humans matter? If non-human animals matter only a little, just how much do they matter?</p>\n<p>I argue that species-specific weighting factors have no place in our moral calculus. If two minds experience the same sort of stimulus, the species of those minds shouldn't affect how good or bad we believe that to be. I owe the line of argument I'll be sketching to Peter Singer's work. His book Practical Ethics is the best statement of the case that I'm aware of.</p>\n<p><a id=\"more\"></a></p>\n<h2><span style=\"font-size: small;\">\n<p style=\"font-weight: normal;\">Front-loaded definitions and summary:</p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; \">\n<li style=\"font-weight: normal;\"><strong>Self-aware: </strong><span style=\"font-weight: normal;\">A self-aware mind is one that understands that it exists and that it persists through time.</span></li>\n<li style=\"font-weight: normal;\"><strong>Sentience</strong><strong style=\"font-weight: normal; \">: </strong><span style=\"font-weight: normal;\">A sentient mind is one that&nbsp;has subjective experiences, such as pleasure and pain. I assume that self-awareness subsumes sentience (i.e. all self-aware minds are also sentient, but not vice versa).</span></li>\n<li style=\"font-weight: normal;\">Person<span style=\"font-weight: normal;\">:</span><span style=\"font-weight: normal;\"> <span style=\"font-weight: normal;\">A self-aware mind.</span></span></li>\n<li style=\"font-weight: normal;\">A human may be alive but non-sentient, due to injury or birth defects.</li>\n<li style=\"font-weight: normal;\">Humans may be sentient but not self-aware, due to injury, birth defect or infancy.</li>\n<li style=\"font-weight: normal;\">Non-human persons are possible: hypothetically, aliens and AIs; controversially, non-human great apes.</li>\n<li style=\"font-weight: normal;\">Many non-human animals are sentient, many are not.</li>\n<li><span style=\"font-weight: normal;\">Utilitarian ethics involve </span>moral calculus<span style=\"font-weight: normal;\">: summing the impacts of an action (or some proxy for them, such as preferences) on all minds.</span></li>\n<li><span style=\"font-weight: normal;\">When performing this calculus, do sentient (but non-self aware) minds count at all? If so, do they count as much as persons?</span></li>\n<li><span style=\"font-weight: normal;\">If they count for zero, there's no ethical problem with secretly torturing puppies, just for fun.</span></li>\n<li><span style=\"font-weight: normal;\">We're tempted to believe that sentient minds count for something, but less than persons.</span></li>\n<li><span style=\"font-weight: normal;\">I think this is just a cover for what we're really tempted to believe: humans count for more than non-humans, not because of the character of our minds, but simply because of the species we belong to.</span></li>\n<li><span style=\"font-weight: normal;\">Historically, allowing your ethical system to arbitrarily promote the interests of those similar to you has led to very bad results.</span></li>\n</ul>\n<div><span style=\"font-weight: normal;\"> \n<hr />\n</span></div>\n</span></h2>\n<h2>Personhood and Sentience</h2>\n<p>Cognitively healthy mature humans have minds that differ in many ways from the other species on Earth. The most striking is probably the level of abstraction we are able to think at. A related ability is that we are able to form detailed plans far into the future. We also have a sense of self that persists through time.</p>\n<p>Let's call a mind that is fully self-aware a <em>person</em>. Now, whether or not there are any non-human persons on Earth today, non-human persons are certainly possible. They might include aliens, artificial intelligences, or extinct ancestral species. There are also humans that are not persons; due to brain damage, birth defects, or perhaps simply infancy[1]. Minds that are not self-aware in this way, but are able to have subjective experiences, let's call <em>sentient</em>.</p>\n<h2>Consequentialism/Utilitarianism</h2>\n<p><em>This is an abridged summary of consequentialism/utilitarianism, included for completeness. It's designed to tell you what I'm on about if you've never heard of this before. For a full argument in support of this framework, see elsewhere.</em></p>\n<p>A consequentialist ethical framework is one in which the ethical status of an action is judged by the \"goodness\" of the possible worlds it creates, weighted by the probability of those outcomes[2]. Nailing down a \"goodness function\" (usually called a utility function) that returns an answer [0,1] for the desirability of a possible world is understandably difficult. But the parts that are most difficult also seldom matter. The basics are easy to agree upon. Many of our subjective experiences are either sharply good or sharply bad. Roughly, a world in which minds experience lots of good things and few bad things should be preferable to a world in which minds have lots of negative experiences and few positive experiences.</p>\n<p>In particular, it's obvious that pain is bad, all else being equal. A little pain can be a worthwhile price for good experiences later, but it's considered a&nbsp;<em style=\"font-style: italic; \">price</em>&nbsp;precisely because we'd prefer not to pay it. It's a negative on the ledger. So, an action which reduces the amount of pain in the world, without doing sufficient other harms to balance it out, would be judged \"ethical\".</p>\n<p>The question is: should we only consider the minds of persons -- self-conscious minds that understand they are a mind with a past, present, and future? Or should we also consider merely <em>sentient</em> minds? And if we do consider sentient minds, should we down-weight them in our utility calculation?</p>\n<p>Do the experiences of merely sentient minds receive a weight of 0, 1, or somewhere in between?</p>\n<h2>How much do sentient non-persons count?</h2>\n<p>Be careful before answering \"0\". This implies that a person can never treat a merely sentient mind unethically, except in violation of the preferences of other persons. Torturing puppies for passing amusement would be ethically A-OK, so long as you keep it quiet in front of other persons who might mind. I'm not a moral realist -- I don't believe that when I say \"X is unethical\", I'm describing a property of objective reality. I think it's more like deduction given axioms. So if your utility function really is such that you ascribe 0 weight to the suffering of merely sentient minds, I can't say you're objectively correct or incorrect. I doubt many people can honestly claim this, though.</p>\n<p>Is a 1.0 weight not equally ridiculous, though? Let's take a simple negative stimulus, pain. Imagine you had to choose between possible worlds in which either a cognitively normal adult human or a cognitively normal pig received a small shallow cut that crossed a section of skin connected to approximately the same number of nerves. The wound will be delivered with a sterile instrument and promptly cleaned and covered, so the only relevant thing here is the pain. The pig will also feel some fear, but let's ignore that.</p>\n<p>You might claim that a utility function that didn't prefer that the pig feel the pain was hopelessly broken. But remember that the weight we're talking about applies to <em>kinds of minds</em>, not <em>members of species</em>. If you had to decide between a cognitively normal adult human, and a human that had experienced some brain damage such that they were merely sentient, would the decision be so easy? How about if you had to decide between a cognitively normal adult human, and a human infant?</p>\n<h2>The problem with speciesism</h2>\n<p>If you want to claim that causing the pig pain is preferable to causing a sentient but not self-aware human pain, you're going to have to make your utility function species-sensitive. You're going to have to claim that humans deserve special moral consideration, and not because of any characteristics of their minds. Simply <em>because they're human</em>.</p>\n<p>It's easy to go wild with hypotheticals here. What about an alien race that was (for some unimaginable reason) just like us? What about humanoid robots with minds indistinguishable from ours?</p>\n<p>To me it's quite obvious that species-membership, by itself, shouldn't be morally relevant. But it's plain that this idea is unintuitive, and I don't think it's a huge mystery why.</p>\n<p>We have an emotional knee-jerk reaction to consider harm done to beings similar to ourselves as much worse than harm done to beings different from us. That's why the idea that a pig's pain might matter just as much as a human's makes you twitch. But you mustn't let that twitch be the deciding factor.</p>\n<p>Well, that's not precisely correct: again, there's no ethical realism. There's nothing in observable reality that says that one utility function is better than another. So you could just throw in a weighting for non-human animals, satisfy your emotional knee-jerk reaction, and be done with it. However, that similarity metric once made people twitch at the idea that the pain of a person with a different skin pigmentation mattered as much as theirs.</p>\n<p>If you listen to that twitch, that instinct that those similar to you matter more, you're following an ethical algorithm that would have led you to the wrong answer on most of the major ethical questions through history. Or at least, the ones we've since changed our minds about.</p>\n<p>If I'm happy to arbitrarily weight non-human animals lower, just because I don't like the implications of considering their interests equal, I would have been free to do the same when considering how much the experiences of out-group persons should matter. When deciding my values, I want to be using an algorithm that would've gotten the right answer on slavery, even given 19th century inputs.</p>\n<p>Now, having said that the experiences of merely sentient minds matter, I should reiterate that there are lots of kinds of joys and sufferings not relevant to them. Because a rabbit doesn't understand its continued existence, it's not wrong to kill it suddenly and painlessly, out of sight/smell/earshot of other rabbits. There are no circumstances in which killing a <em>person</em> doesn't involve serious negative utility. Persons have plans and aspirations. When I consider what would be bad about being murdered, the momentary fear and pain barely rank. Similarly, I think it's possible to please a person more deeply than a merely sentient mind. But when it comes to a simple stimulus like pain, which both minds feel similarly, it's just as bad for both of them.</p>\n<p>When I changed my mind about this, I hadn't yet decided to particularly care about how ethical I was. This kept me from having to say \"well, I'm not allowed to believe this, because then I'd have to be vegetarian, and hell no!\". I later did decide to be more ethical, but doing it in two stages like that seemed to make changing my mind less traumatic.&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>[1] I haven't really studied the evidence about infant cognition. It's possible infants are fully self-conscious (as in, have an understanding that they are a mind plus a body that persists through time), but it seems unlikely to me.</p>\n<p>[2] Actually I seldom see it stated probabilistically like this. I think this is surely just an oversight? If you have to choose between pushing a button that will save a life with probability 0.99, and cost a life with probability 0.01, surely it's not unethical after the fact if you got unlucky.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ikQArh7yRE47bsokR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 10, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "9259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>There's been some discussion on this site about vegetarianism previously, although less than I expected. It's a complicated topic, so I want to focus on a critical sub-issue: within a consequentialist/utilitarian framework, what should be the status of non-human animals? Do only humans matter? If non-human animals matter only a little, just how much do they matter?</p>\n<p>I argue that species-specific weighting factors have no place in our moral calculus. If two minds experience the same sort of stimulus, the species of those minds shouldn't affect how good or bad we believe that to be. I owe the line of argument I'll be sketching to Peter Singer's work. His book Practical Ethics is the best statement of the case that I'm aware of.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"_Front_loaded_definitions_and_summary___Self_aware__A_self_aware_mind_is_one_that_understands_that_it_exists_and_that_it_persists_through_time__Sentience__A_sentient_mind_is_one_that_has_subjective_experiences__such_as_pleasure_and_pain__I_assume_that_self_awareness_subsumes_sentience__i_e__all_self_aware_minds_are_also_sentient__but_not_vice_versa___Person__A_self_aware_mind__A_human_may_be_alive_but_non_sentient__due_to_injury_or_birth_defects__Humans_may_be_sentient_but_not_self_aware__due_to_injury__birth_defect_or_infancy__Non_human_persons_are_possible__hypothetically__aliens_and_AIs__controversially__non_human_great_apes__Many_non_human_animals_are_sentient__many_are_not__Utilitarian_ethics_involve_moral_calculus__summing_the_impacts_of_an_action__or_some_proxy_for_them__such_as_preferences__on_all_minds__When_performing_this_calculus__do_sentient__but_non_self_aware__minds_count_at_all__If_so__do_they_count_as_much_as_persons__If_they_count_for_zero__there_s_no_ethical_problem_with_secretly_torturing_puppies__just_for_fun__We_re_tempted_to_believe_that_sentient_minds_count_for_something__but_less_than_persons__I_think_this_is_just_a_cover_for_what_we_re_really_tempted_to_believe__humans_count_for_more_than_non_humans__not_because_of_the_character_of_our_minds__but_simply_because_of_the_species_we_belong_to__Historically__allowing_your_ethical_system_to_arbitrarily_promote_the_interests_of_those_similar_to_you_has_led_to_very_bad_results_______\"><span style=\"font-size: small;\">\n<p style=\"font-weight: normal;\">Front-loaded definitions and summary:</p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; \">\n<li style=\"font-weight: normal;\"><strong>Self-aware: </strong><span style=\"font-weight: normal;\">A self-aware mind is one that understands that it exists and that it persists through time.</span></li>\n<li style=\"font-weight: normal;\"><strong>Sentience</strong><strong style=\"font-weight: normal; \">: </strong><span style=\"font-weight: normal;\">A sentient mind is one that&nbsp;has subjective experiences, such as pleasure and pain. I assume that self-awareness subsumes sentience (i.e. all self-aware minds are also sentient, but not vice versa).</span></li>\n<li style=\"font-weight: normal;\">Person<span style=\"font-weight: normal;\">:</span><span style=\"font-weight: normal;\"> <span style=\"font-weight: normal;\">A self-aware mind.</span></span></li>\n<li style=\"font-weight: normal;\">A human may be alive but non-sentient, due to injury or birth defects.</li>\n<li style=\"font-weight: normal;\">Humans may be sentient but not self-aware, due to injury, birth defect or infancy.</li>\n<li style=\"font-weight: normal;\">Non-human persons are possible: hypothetically, aliens and AIs; controversially, non-human great apes.</li>\n<li style=\"font-weight: normal;\">Many non-human animals are sentient, many are not.</li>\n<li><span style=\"font-weight: normal;\">Utilitarian ethics involve </span>moral calculus<span style=\"font-weight: normal;\">: summing the impacts of an action (or some proxy for them, such as preferences) on all minds.</span></li>\n<li><span style=\"font-weight: normal;\">When performing this calculus, do sentient (but non-self aware) minds count at all? If so, do they count as much as persons?</span></li>\n<li><span style=\"font-weight: normal;\">If they count for zero, there's no ethical problem with secretly torturing puppies, just for fun.</span></li>\n<li><span style=\"font-weight: normal;\">We're tempted to believe that sentient minds count for something, but less than persons.</span></li>\n<li><span style=\"font-weight: normal;\">I think this is just a cover for what we're really tempted to believe: humans count for more than non-humans, not because of the character of our minds, but simply because of the species we belong to.</span></li>\n<li><span style=\"font-weight: normal;\">Historically, allowing your ethical system to arbitrarily promote the interests of those similar to you has led to very bad results.</span></li>\n</ul>\n<div><span style=\"font-weight: normal;\"> \n<hr>\n</span></div>\n</span></h2>\n<h2 id=\"Personhood_and_Sentience\">Personhood and Sentience</h2>\n<p>Cognitively healthy mature humans have minds that differ in many ways from the other species on Earth. The most striking is probably the level of abstraction we are able to think at. A related ability is that we are able to form detailed plans far into the future. We also have a sense of self that persists through time.</p>\n<p>Let's call a mind that is fully self-aware a <em>person</em>. Now, whether or not there are any non-human persons on Earth today, non-human persons are certainly possible. They might include aliens, artificial intelligences, or extinct ancestral species. There are also humans that are not persons; due to brain damage, birth defects, or perhaps simply infancy[1]. Minds that are not self-aware in this way, but are able to have subjective experiences, let's call <em>sentient</em>.</p>\n<h2 id=\"Consequentialism_Utilitarianism\">Consequentialism/Utilitarianism</h2>\n<p><em>This is an abridged summary of consequentialism/utilitarianism, included for completeness. It's designed to tell you what I'm on about if you've never heard of this before. For a full argument in support of this framework, see elsewhere.</em></p>\n<p>A consequentialist ethical framework is one in which the ethical status of an action is judged by the \"goodness\" of the possible worlds it creates, weighted by the probability of those outcomes[2]. Nailing down a \"goodness function\" (usually called a utility function) that returns an answer [0,1] for the desirability of a possible world is understandably difficult. But the parts that are most difficult also seldom matter. The basics are easy to agree upon. Many of our subjective experiences are either sharply good or sharply bad. Roughly, a world in which minds experience lots of good things and few bad things should be preferable to a world in which minds have lots of negative experiences and few positive experiences.</p>\n<p>In particular, it's obvious that pain is bad, all else being equal. A little pain can be a worthwhile price for good experiences later, but it's considered a&nbsp;<em style=\"font-style: italic; \">price</em>&nbsp;precisely because we'd prefer not to pay it. It's a negative on the ledger. So, an action which reduces the amount of pain in the world, without doing sufficient other harms to balance it out, would be judged \"ethical\".</p>\n<p>The question is: should we only consider the minds of persons -- self-conscious minds that understand they are a mind with a past, present, and future? Or should we also consider merely <em>sentient</em> minds? And if we do consider sentient minds, should we down-weight them in our utility calculation?</p>\n<p>Do the experiences of merely sentient minds receive a weight of 0, 1, or somewhere in between?</p>\n<h2 id=\"How_much_do_sentient_non_persons_count_\">How much do sentient non-persons count?</h2>\n<p>Be careful before answering \"0\". This implies that a person can never treat a merely sentient mind unethically, except in violation of the preferences of other persons. Torturing puppies for passing amusement would be ethically A-OK, so long as you keep it quiet in front of other persons who might mind. I'm not a moral realist -- I don't believe that when I say \"X is unethical\", I'm describing a property of objective reality. I think it's more like deduction given axioms. So if your utility function really is such that you ascribe 0 weight to the suffering of merely sentient minds, I can't say you're objectively correct or incorrect. I doubt many people can honestly claim this, though.</p>\n<p>Is a 1.0 weight not equally ridiculous, though? Let's take a simple negative stimulus, pain. Imagine you had to choose between possible worlds in which either a cognitively normal adult human or a cognitively normal pig received a small shallow cut that crossed a section of skin connected to approximately the same number of nerves. The wound will be delivered with a sterile instrument and promptly cleaned and covered, so the only relevant thing here is the pain. The pig will also feel some fear, but let's ignore that.</p>\n<p>You might claim that a utility function that didn't prefer that the pig feel the pain was hopelessly broken. But remember that the weight we're talking about applies to <em>kinds of minds</em>, not <em>members of species</em>. If you had to decide between a cognitively normal adult human, and a human that had experienced some brain damage such that they were merely sentient, would the decision be so easy? How about if you had to decide between a cognitively normal adult human, and a human infant?</p>\n<h2 id=\"The_problem_with_speciesism\">The problem with speciesism</h2>\n<p>If you want to claim that causing the pig pain is preferable to causing a sentient but not self-aware human pain, you're going to have to make your utility function species-sensitive. You're going to have to claim that humans deserve special moral consideration, and not because of any characteristics of their minds. Simply <em>because they're human</em>.</p>\n<p>It's easy to go wild with hypotheticals here. What about an alien race that was (for some unimaginable reason) just like us? What about humanoid robots with minds indistinguishable from ours?</p>\n<p>To me it's quite obvious that species-membership, by itself, shouldn't be morally relevant. But it's plain that this idea is unintuitive, and I don't think it's a huge mystery why.</p>\n<p>We have an emotional knee-jerk reaction to consider harm done to beings similar to ourselves as much worse than harm done to beings different from us. That's why the idea that a pig's pain might matter just as much as a human's makes you twitch. But you mustn't let that twitch be the deciding factor.</p>\n<p>Well, that's not precisely correct: again, there's no ethical realism. There's nothing in observable reality that says that one utility function is better than another. So you could just throw in a weighting for non-human animals, satisfy your emotional knee-jerk reaction, and be done with it. However, that similarity metric once made people twitch at the idea that the pain of a person with a different skin pigmentation mattered as much as theirs.</p>\n<p>If you listen to that twitch, that instinct that those similar to you matter more, you're following an ethical algorithm that would have led you to the wrong answer on most of the major ethical questions through history. Or at least, the ones we've since changed our minds about.</p>\n<p>If I'm happy to arbitrarily weight non-human animals lower, just because I don't like the implications of considering their interests equal, I would have been free to do the same when considering how much the experiences of out-group persons should matter. When deciding my values, I want to be using an algorithm that would've gotten the right answer on slavery, even given 19th century inputs.</p>\n<p>Now, having said that the experiences of merely sentient minds matter, I should reiterate that there are lots of kinds of joys and sufferings not relevant to them. Because a rabbit doesn't understand its continued existence, it's not wrong to kill it suddenly and painlessly, out of sight/smell/earshot of other rabbits. There are no circumstances in which killing a <em>person</em> doesn't involve serious negative utility. Persons have plans and aspirations. When I consider what would be bad about being murdered, the momentary fear and pain barely rank. Similarly, I think it's possible to please a person more deeply than a merely sentient mind. But when it comes to a simple stimulus like pain, which both minds feel similarly, it's just as bad for both of them.</p>\n<p>When I changed my mind about this, I hadn't yet decided to particularly care about how ethical I was. This kept me from having to say \"well, I'm not allowed to believe this, because then I'd have to be vegetarian, and hell no!\". I later did decide to be more ethical, but doing it in two stages like that seemed to make changing my mind less traumatic.&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>[1] I haven't really studied the evidence about infant cognition. It's possible infants are fully self-conscious (as in, have an understanding that they are a mind plus a body that persists through time), but it seems unlikely to me.</p>\n<p>[2] Actually I seldom see it stated probabilistically like this. I think this is surely just an oversight? If you have to choose between pushing a button that will save a life with probability 0.99, and cost a life with probability 0.01, surely it's not unethical after the fact if you got unlucky.</p>\n<p>&nbsp;</p>", "sections": [{"title": "\nFront-loaded definitions and summary:\n\nSelf-aware: A self-aware mind is one that understands that it exists and that it persists through time.\nSentience: A sentient mind is one that\u00a0has subjective experiences, such as pleasure and pain. I assume that self-awareness subsumes sentience (i.e. all self-aware minds are also sentient, but not vice versa).\nPerson: A self-aware mind.\nA human may be alive but non-sentient, due to injury or birth defects.\nHumans may be sentient but not self-aware, due to injury, birth defect or infancy.\nNon-human persons are possible: hypothetically, aliens and AIs; controversially, non-human great apes.\nMany non-human animals are sentient, many are not.\nUtilitarian ethics involve moral calculus: summing the impacts of an action (or some proxy for them, such as preferences) on all minds.\nWhen performing this calculus, do sentient (but non-self aware) minds count at all? If so, do they count as much as persons?\nIf they count for zero, there's no ethical problem with secretly torturing puppies, just for fun.\nWe're tempted to believe that sentient minds count for something, but less than persons.\nI think this is just a cover for what we're really tempted to believe: humans count for more than non-humans, not because of the character of our minds, but simply because of the species we belong to.\nHistorically, allowing your ethical system to arbitrarily promote the interests of those similar to you has led to very bad results.\n\n \n\n\n", "anchor": "_Front_loaded_definitions_and_summary___Self_aware__A_self_aware_mind_is_one_that_understands_that_it_exists_and_that_it_persists_through_time__Sentience__A_sentient_mind_is_one_that_has_subjective_experiences__such_as_pleasure_and_pain__I_assume_that_self_awareness_subsumes_sentience__i_e__all_self_aware_minds_are_also_sentient__but_not_vice_versa___Person__A_self_aware_mind__A_human_may_be_alive_but_non_sentient__due_to_injury_or_birth_defects__Humans_may_be_sentient_but_not_self_aware__due_to_injury__birth_defect_or_infancy__Non_human_persons_are_possible__hypothetically__aliens_and_AIs__controversially__non_human_great_apes__Many_non_human_animals_are_sentient__many_are_not__Utilitarian_ethics_involve_moral_calculus__summing_the_impacts_of_an_action__or_some_proxy_for_them__such_as_preferences__on_all_minds__When_performing_this_calculus__do_sentient__but_non_self_aware__minds_count_at_all__If_so__do_they_count_as_much_as_persons__If_they_count_for_zero__there_s_no_ethical_problem_with_secretly_torturing_puppies__just_for_fun__We_re_tempted_to_believe_that_sentient_minds_count_for_something__but_less_than_persons__I_think_this_is_just_a_cover_for_what_we_re_really_tempted_to_believe__humans_count_for_more_than_non_humans__not_because_of_the_character_of_our_minds__but_simply_because_of_the_species_we_belong_to__Historically__allowing_your_ethical_system_to_arbitrarily_promote_the_interests_of_those_similar_to_you_has_led_to_very_bad_results_______", "level": 1}, {"title": "Personhood and Sentience", "anchor": "Personhood_and_Sentience", "level": 1}, {"title": "Consequentialism/Utilitarianism", "anchor": "Consequentialism_Utilitarianism", "level": 1}, {"title": "How much do sentient non-persons count?", "anchor": "How_much_do_sentient_non_persons_count_", "level": 1}, {"title": "The problem with speciesism", "anchor": "The_problem_with_speciesism", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "88 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T14:43:32.714Z", "modifiedAt": null, "url": null, "title": "\"Talking with God\", a transhumanist short story", "slug": "talking-with-god-a-transhumanist-short-story", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:26.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QcADhHvAo4hsxv38p/talking-with-god-a-transhumanist-short-story", "pageUrlRelative": "/posts/QcADhHvAo4hsxv38p/talking-with-god-a-transhumanist-short-story", "linkUrl": "https://www.lesswrong.com/posts/QcADhHvAo4hsxv38p/talking-with-god-a-transhumanist-short-story", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Talking%20with%20God%22%2C%20a%20transhumanist%20short%20story&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Talking%20with%20God%22%2C%20a%20transhumanist%20short%20story%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcADhHvAo4hsxv38p%2Ftalking-with-god-a-transhumanist-short-story%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Talking%20with%20God%22%2C%20a%20transhumanist%20short%20story%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcADhHvAo4hsxv38p%2Ftalking-with-god-a-transhumanist-short-story", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcADhHvAo4hsxv38p%2Ftalking-with-god-a-transhumanist-short-story", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p><a href=\"http://www.fullmoon.nu/articles/art.php?id=tal\">Talking with God</a> is a pleasant and inspiring transhumanist short story. I've got some quibbles, but I'll save them for the comments because I think the story is better without spoilers.</p>\n<p>There's a <a href=\"http://www.fullmoon.nu/rtpforum/phpBB2/viewforum.php?f=4\">discussion forum</a> at the story's site.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QcADhHvAo4hsxv38p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 6, "extendedScore": null, "score": 8.28858475394266e-07, "legacy": true, "legacyId": "12022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T16:44:55.503Z", "modifiedAt": null, "url": null, "title": "[Link] A Bayes' Theorem Visualization", "slug": "link-a-bayes-theorem-visualization", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/caFY2EGfhdDANuiri/link-a-bayes-theorem-visualization", "pageUrlRelative": "/posts/caFY2EGfhdDANuiri/link-a-bayes-theorem-visualization", "linkUrl": "https://www.lesswrong.com/posts/caFY2EGfhdDANuiri/link-a-bayes-theorem-visualization", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20A%20Bayes'%20Theorem%20Visualization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20A%20Bayes'%20Theorem%20Visualization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcaFY2EGfhdDANuiri%2Flink-a-bayes-theorem-visualization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20A%20Bayes'%20Theorem%20Visualization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcaFY2EGfhdDANuiri%2Flink-a-bayes-theorem-visualization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcaFY2EGfhdDANuiri%2Flink-a-bayes-theorem-visualization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>A while ago when Bret Victor's amazing article&nbsp;<a href=\"/lw/828/link_awesome_interactive_visualization_article/\">Up and Down the Ladder of Abstraction</a>&nbsp;was being discussed, someone mentioned that they'd like to see one made for Bayes' Theorem. I've just completed version 1.0 of my \"Bayes' Theorem Ladder of Abstraction\", and it can be found here:&nbsp;<a href=\"http://www.coarsegra.in/?p=111\">http://www.coarsegra.in/?p=111</a></p>\n<p>(It uses the Canvas html5 element, so won't work with older versions of IE).</p>\n<p>There's a few bugs in it, and it leaves out many things that I'd like to (eventually) include, but I'm reasonably satisfied with it as a first attempt. Any feedback for what works and what doesn't work, or what you think should be added, would be greatly appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "caFY2EGfhdDANuiri", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 23, "extendedScore": null, "score": 8.289043925658416e-07, "legacy": true, "legacyId": "12023", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wJbTM5rNgqBTQrgsr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T18:46:50.722Z", "modifiedAt": null, "url": null, "title": "Dead Child Currency", "slug": "dead-child-currency", "viewCount": null, "lastCommentedAt": "2021-05-19T09:59:25.490Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6cRhG6PKeASdNHqxD/dead-child-currency", "pageUrlRelative": "/posts/6cRhG6PKeASdNHqxD/dead-child-currency", "linkUrl": "https://www.lesswrong.com/posts/6cRhG6PKeASdNHqxD/dead-child-currency", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dead%20Child%20Currency&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADead%20Child%20Currency%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6cRhG6PKeASdNHqxD%2Fdead-child-currency%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dead%20Child%20Currency%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6cRhG6PKeASdNHqxD%2Fdead-child-currency", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6cRhG6PKeASdNHqxD%2Fdead-child-currency", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 463, "htmlBody": "<p>From Yvain's 'proposal' to <a href=\"http://www.raikoth.net/deadchild.html\">measure money in dead children</a>:</p>\n<blockquote>According to Population Services International, a respected charity research group, it costs <a href=\"http://www.givewell.net/PSI\">between $650 and $1000</a> [1] to save one child's life through charity. You've probably heard lower numbers like twenty cents somewhere. The lower numbers are wrong. Yes, maybe an anti-measles vaccine for a kid in Africa only costs twenty cents, and measles can be fatal. But there's a lot of overhead, and you have to immunize a lot of people before you get the one kid otherwise destined to die of measles. I find the $650-$1000 figure much more believable. Let's round it off to $800.\n<p>So one dead child = eight hundred dollars. If you spend eight hundred dollars on a laptop, that's one African kid who died because you didn't give it to charity. Distasteful but true. Now that we know that, we can get down to the details of designing the currency itself. It should be a big gold coin, with a picture of a smiling Burmese child on the front, and a tombstone on the back. The abbreviation can be DC.</p>\n</blockquote>\n<p>This makes sense to me, to a limited extent. You can spend money for your own benefit or to help others elsewhere, and there really are people who wouldn't have to die if you would forgo some luxuries. Making this tradeoff more explicit (\"we're looking for an apartment costing no more than six dead children annually\") might lead some people to greater generosity. It's a way of <a href=\"http://www.jefftk.com/news/2010-12-05.html\">abstracting compassion</a>.</p>\n<p>Two things worry me, though. The first is that there's a big focus on spending here [2], but increasing earnings deserves more focus: getting a raise or a new job that added $10K to my salary would let me keep more children from dying than would <a href=\"http://www.jefftk.com/news/2010-07-06.html\">reducing</a> my spending on myself to zero. [3] The second is that thinking of all your purchases in terms of dead children is likely to make you miserable. Not just that, but miserable to little gain: you still probably spend almost as much money on yourself, you just feel more guilty about it. Much better, I think, is to pick a rule for <a href=\"http://www.jefftk.com/news/2011-08-09.html\">how much to give</a> and then apply it to money <a href=\"http://www.jefftk.com/news/2010-07-19.html\">as it comes in</a>. That way each purchase has no effect on the number of deaths you're averting.</p>\n<p>(Note: I also posted this on <a href=\"http://www.jefftk.com/news/2012-01-09.html\">my blog</a>)</p>\n<p><br /> [1] The current number is probably <a href=\"http://givewell.org/international/top-charities/AMF\">closer to $2K</a>.</p>\n<p>[2] Maybe this is because it sounds weird to talk about salary in terms of dead children? (\"I wonder what job earns me the most dead children?\") Perhaps for earning the unit should be the \"<a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/UndeadChild\">undead child</a>\"?</p>\n<p>[3] In 2011 Julia and I lived on $18K for the two of us, not including taxes or health insurance.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6cRhG6PKeASdNHqxD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 18, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "12024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T19:40:26.776Z", "modifiedAt": null, "url": null, "title": "Q&A with experts on risks from AI #2", "slug": "q-and-a-with-experts-on-risks-from-ai-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.520Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xoxZdRtpyRnXmhher/q-and-a-with-experts-on-risks-from-ai-2", "pageUrlRelative": "/posts/xoxZdRtpyRnXmhher/q-and-a-with-experts-on-risks-from-ai-2", "linkUrl": "https://www.lesswrong.com/posts/xoxZdRtpyRnXmhher/q-and-a-with-experts-on-risks-from-ai-2", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20experts%20on%20risks%20from%20AI%20%232&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20experts%20on%20risks%20from%20AI%20%232%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxoxZdRtpyRnXmhher%2Fq-and-a-with-experts-on-risks-from-ai-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20experts%20on%20risks%20from%20AI%20%232%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxoxZdRtpyRnXmhher%2Fq-and-a-with-experts-on-risks-from-ai-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxoxZdRtpyRnXmhher%2Fq-and-a-with-experts-on-risks-from-ai-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2213, "htmlBody": "<p><strong>[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI.</p>\n<p>(Note: I am also asking <em>Reinforcement Learning / Universal AI</em> researchers, teams like the one that build IBM Watson, organisations like DARPA and various companies. Some haven't replied yet while I still have to write others.)</p>\n<p><strong>Nils John Nilsson</strong> is one of the founding researchers in the discipline of Artificial intelligence. He is the Kumagai Professor of Engineering, Emeritus in Computer Science at Stanford University. He is particularly famous for his contributions to search, planning, knowledge representation, and robotics. [<a href=\"http://en.wikipedia.org/wiki/Nils_Nilsson_%28researcher%29\">Wikipedia</a>] [<a href=\"http://ai.stanford.edu/~nilsson/\">Homepage</a>] [<a href=\"http://scholar.google.com/scholar?q=Nils+Nilsson\">Google Scholar</a>]</p>\n<p><strong>Peter J. Bentley</strong>, a British author and computer scientist based at University College London. [<a href=\"http://en.wikipedia.org/wiki/Peter_J._Bentley\">Wikipedia</a>] [<a href=\"http://www.peterjbentley.com/\">Homepage</a>]</p>\n<p><strong>David Alan Plaisted</strong> is a computer science professor at the University of North Carolina at Chapel Hill. [<a href=\"http://en.wikipedia.org/wiki/David_Plaisted\">Wikipedia</a>] [<a href=\"http://www.cs.unc.edu/~plaisted/\">Homepage</a>]</p>\n<p><strong>Hector Levesque</strong> is a Canadian academic and researcher in artificial intelligence. He does research in the area of knowledge representation and reasoning in artificial intelligence. [<a href=\"http://en.wikipedia.org/wiki/Hector_Levesque\">Wikipedia</a>] [<a href=\"http://www.cs.toronto.edu/~hector/\">Homepage</a>]</p>\n<h3><strong>The Interview</strong>:</h3>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> I did look at the lesswrong.com Web page.&nbsp; Its goals are extremely important!&nbsp; One problem about blogs like these is that I think they are read mainly by those who agree with what they say -- the already converted.&nbsp; We need to find ways to get the Fox News viewers to understand the importance of what these blogs are saying.&nbsp; How do we get to them?<br /><br />Before proceeding to deal with your questions, you might be interested in the following:<br /><br />1. An article called \"Rationally-Shaped Artificial Intelligence\" by Steve Omohundro about robot \"morality.\"&nbsp;&nbsp; It's at:<br /><br /><a href=\"http://selfawaresystems.com/2011/10/07/rationally-shaped-artificial-intelligence/\">http://selfawaresystems.com/2011/10/07/rationally-shaped-artificial-intelligence/</a><br /><br />2.&nbsp; I have a draft book on \"beliefs\" that deals with (among other things) how to evaluate them. See:<br /><br /><a href=\"http://ai.stanford.edu/~nilsson/Beliefs.html\">http://ai.stanford.edu/~nilsson/Beliefs.html</a><br /><br />(Comments welcome!)<br /><br />3. Of course, you must know already about David Kahneman's excellent book, \"Thinking, Fast and Slow.\"</p>\n<p style=\"padding-left: 30px;\">Here are some (maybe hasty!) answers/responses to your questions.</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted: </strong>Your questions are very interesting and I've had such questions for a long time, actually.&nbsp; I've been surprised that people are not more concerned about such things.<br /><br />However, right now the problems with AI seem so difficult that I'm not worried about these issues.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>: </strong>I think intelligence is extremely hard to define, even harder to measure, and human-level intelligence is a largely meaningless phrase. All life on Earth has human-level intelligence in one sense for we have all evolved for the same amount of time and we are equally able to survive in our appropriate niches and solve problems relevant to us in highly effective ways.<br /><br />There is no danger from clever AI - only from stupid AI that is so bad that it kills us by accident. I wish we were on track to create something as clever as a frog. We have a long, long way to go. I agree with Pat Hayes on this subject.<br /><br />I actually find the discussion a little silly. We are *much* more likely to all become a bunch of cyborgs completely reliant on integrated tech (including some clever computers) in the next 100 years. Computers won't be external entities, they will be a part of us. Many of us already can't live our modern lives without being plugged into their gadgets for most of their waking lives. Worry about that instead :)</p>\n<p style=\"padding-left: 30px;\">I think your questions are very hard to answer in any rigorous sense, for they involve prediction of future events so far ahead that anything I say is likely to be quite inaccurate. I will try to answer some below, but these are really just my educated guesses.</p>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em><br /> <br /> <em>Explanatory remark to Q1:</em><br /> <br /> <em>P(human-level AI by (year) | no wars &and; no disasters &and; beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> Because human intelligence is so multi-faceted, your question really should be divided into each of the many components of intelligence. For example, on language translation, AI probably already exceeds the performance of many translators.&nbsp; On integrating symbolic expressions in calculus, AI (or computer science generally) is already much better than humans.&nbsp; AI does better on many planning and scheduling tasks.&nbsp; On chess, same!&nbsp; On the Jeopardy! quiz show, same!<br /><br />A while back I wrote an essay about a replacement for the Turing test. It was called the \"Employment Test.\"&nbsp; (See:&nbsp; <a href=\"http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General_Essays/AIMag26-04-HLAI.pdf\">http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General_Essays/AIMag26-04-HLAI.pdf</a>)&nbsp; How many of the many, many jobs that humans do can be done by machines?&nbsp; I'll rephrase your question to be: When will AI be able to perform around 80% of these jobs as well or better than humans perform?<br /><br />10% chance:&nbsp; 2030<br />50% chance:&nbsp; 2050<br />90% chance: 2100</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> It seems that the development of human level intelligence is always later than people think it will be.&nbsp; I don't have an idea how long this might take.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong> That depends on what you mean by human level intelligence and how it is measured. Computers can already surpass us at basic arithmetic. Some machine learning methods can equal us in recognition of patterns in images. Most other forms of \"AI\" are tremendously bad at tasks we perform well. The human brain is the result of a several billion years of evolution at molecular scales to macro scales. Our evolutionary history spans unimaginable numbers of generations, challenges, environments, predators, etc. For an artificial brain to resemble ours, it must necessarily go through a very similar evolutionary history. Otherwise it may be a clever machine, but its intelligence will be in areas that do not necessarily resemble human intelligence.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>No idea. There's a lot of factors beyond wars etc mentioned. It's tough to make these kind of predictions.</p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of badly done AI?</em><br /> <br /> <em>Explanatory remark to Q2:</em><br /> <br /> <em>P(human extinction | badly done AI) = ?</em></p>\n<p><em>(Where 'badly done' = AGI capable of self-modification that is <strong>not </strong>provably non-dangerous.)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> 0.01% probability during the current century.&nbsp; Beyond that, who knows?</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> I think people will be so concerned about the misuse of intelligent computers that they will take safeguards to prevent such problems.&nbsp; To me it seems more likely that disaster will come on the human race from nuclear or biological weapons, or possibly some natural disaster.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong> </strong>If this were ever to happen, it is most likely to be because the AI was too stupid and we relied on it too much. It is *extremely* unlikely for any AI to become \"self aware\" and take over the world as they like to show in the movies. It's more likely that your pot plant will take over the world.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>Low. The probability of human extinction by other means (e.g. climate problems, micro biology etc) is sufficiently higher that if we were to survive all of them, surviving the result of AI work would be comparatively easy.</p>\n<p style=\"padding-left: 30px;\"><strong></strong><strong> </strong></p>\n<p><strong>Q3: </strong><em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </em><br /> <br /> <em>Explanatory remark to Q3:</em></p>\n<p><em>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> I'll assume that you mean sometime during this century, and that my \"employment test\" is the measure of superhuman intelligence.<br /><br />hours:&nbsp; 5%<br />days:&nbsp;&nbsp;&nbsp; 50%<br />&lt;5 years:&nbsp; 90%</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> This would require a lot in terms of robots being able to build hardware devices or modify their own hardware.&nbsp; I suppose they could also modify their software to do this, but right now it seems like a far out possibility.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong> </strong>It won't happen. Has nothing to do with internet connections or speeds. The question is rather silly.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>Good. An automated human level intelligence is achieved, it ought to be able to learn what humans know more quickly.</p>\n<p><strong>Q4: </strong><em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em><br /> <br /> <em>Explanatory remark to Q4:</em></p>\n<p><em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> Work on this problem should be ongoing, I think, with the work on AGI.&nbsp; We should start, now, with \"little more,\" and gradually expand through the \"much\" and \"vastly\" as we get closer to AGI.</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> Yes, some kind of ethical system should be built into robots, but then one has to understand their functioning well enough to be sure that they would not get around it somehow.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong> Humans are the ultimate in killers. We have taken over the planet like a plague and wiped out a large number of existing species. \"Intelligent\" computers would be very very stupid if they tried to get in our way. If they have any intelligence at all, they will be very friendly. We are the dangerous ones, not them.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>It's always important to watch for risks with any technology. AI technology is no different.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?<br /><br /></em><em>Explanatory remark to Q5:</em></p>\n<p><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> I think the risk of terrorists getting nuclear weapons is a greater risk than AI will be during this century. They would certainly use them if they had them -- they would be doing the work of Allah in destroying the Great Satan.&nbsp; Other than that, I think global warming and other environmental problems will have a greater negative impact than AI will have during this century.&nbsp; I believe technology can save us from the risks associated with new viruses.&nbsp; Bill Joy worries about nano-dust, but I don't know enough about that field to assess its possible negative impacts.&nbsp; Then, of course, there's the odd meteor.&nbsp; Probably technology will save us from that.</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> There are risks with any technology, even computers as we have them now.&nbsp; It depends on the form of government and the nature of those in power whether technology is used for good or evil more than on the nature of the technology itself.&nbsp; Even military technology can be used for repression and persecution.&nbsp; Look at some countries today that use technology to keep their people in subjection.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong> </strong>No.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>See above Q2. I think AI risks are smaller than others.</p>\n<p><strong>Q6: </strong><em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> Not as high as it should be. Some, like Steven Omohundro, Wendell Wallach and Colin Allen (\"Moral Machines: Teaching Robots Right from Wrong\"), Patrick Lin (\"Robot Ethics\"), and Ronald Arkin (\"Governing Lethal Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot Architecture\") are among those thinking and writing about these problems.&nbsp; You probably know of several others.</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> Probably not as high as it should be.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong> </strong>The whole idea is blown up out of all proportion. There is no real risk and will not be for a very long time. We are also well aware of the potential risks.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>Low. Technology in the area is well behind what was predicted in the past, and so concern for risks is correspondingly low.</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human-level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> Because human intelligence involves so many different abilities, I think AGI will require many different technologies with many different milestones. I don't think there is a single one.&nbsp; I do think, though, that the work that Andrew Ng, Geoff Hinton, and (more popularly) Jeff Hawkins and colleagues are doing on modeling learning in the neo-cortex using deep Bayes networks is on the right track.<br /><br />Thanks for giving me the opportunity to think about your questions, and I hope to stay in touch with your work!</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> I think it depends on the interaction of many different capabilities.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong>&nbsp;</strong><strong></strong>Too many advances are needed to describe here...</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>Reading comprehension at the level of a 10-year old.</p>\n<p><strong>Q8:</strong> <em>Are you familiar with formal concepts of optimal AI design which relate to searches over complete spaces of computable hypotheses or computational strategies, such as Solomonoff induction, Levin search, Hutter's algorithm M, AIXI, or G&ouml;del machines?</em></p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> My research does not specifically relate to those kinds of questions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "ZFrgTgzwEfStg26JL": 1, "DigEmY3RrF3XL5cwe": 1, "9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xoxZdRtpyRnXmhher", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 22, "extendedScore": null, "score": 8.289705506855957e-07, "legacy": true, "legacyId": "12025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Click_here_to_see_a_list_of_all_interviews_\">[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI.</p>\n<p>(Note: I am also asking <em>Reinforcement Learning / Universal AI</em> researchers, teams like the one that build IBM Watson, organisations like DARPA and various companies. Some haven't replied yet while I still have to write others.)</p>\n<p><strong>Nils John Nilsson</strong> is one of the founding researchers in the discipline of Artificial intelligence. He is the Kumagai Professor of Engineering, Emeritus in Computer Science at Stanford University. He is particularly famous for his contributions to search, planning, knowledge representation, and robotics. [<a href=\"http://en.wikipedia.org/wiki/Nils_Nilsson_%28researcher%29\">Wikipedia</a>] [<a href=\"http://ai.stanford.edu/~nilsson/\">Homepage</a>] [<a href=\"http://scholar.google.com/scholar?q=Nils+Nilsson\">Google Scholar</a>]</p>\n<p><strong>Peter J. Bentley</strong>, a British author and computer scientist based at University College London. [<a href=\"http://en.wikipedia.org/wiki/Peter_J._Bentley\">Wikipedia</a>] [<a href=\"http://www.peterjbentley.com/\">Homepage</a>]</p>\n<p><strong>David Alan Plaisted</strong> is a computer science professor at the University of North Carolina at Chapel Hill. [<a href=\"http://en.wikipedia.org/wiki/David_Plaisted\">Wikipedia</a>] [<a href=\"http://www.cs.unc.edu/~plaisted/\">Homepage</a>]</p>\n<p><strong>Hector Levesque</strong> is a Canadian academic and researcher in artificial intelligence. He does research in the area of knowledge representation and reasoning in artificial intelligence. [<a href=\"http://en.wikipedia.org/wiki/Hector_Levesque\">Wikipedia</a>] [<a href=\"http://www.cs.toronto.edu/~hector/\">Homepage</a>]</p>\n<h3 id=\"The_Interview_\"><strong>The Interview</strong>:</h3>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> I did look at the lesswrong.com Web page.&nbsp; Its goals are extremely important!&nbsp; One problem about blogs like these is that I think they are read mainly by those who agree with what they say -- the already converted.&nbsp; We need to find ways to get the Fox News viewers to understand the importance of what these blogs are saying.&nbsp; How do we get to them?<br><br>Before proceeding to deal with your questions, you might be interested in the following:<br><br>1. An article called \"Rationally-Shaped Artificial Intelligence\" by Steve Omohundro about robot \"morality.\"&nbsp;&nbsp; It's at:<br><br><a href=\"http://selfawaresystems.com/2011/10/07/rationally-shaped-artificial-intelligence/\">http://selfawaresystems.com/2011/10/07/rationally-shaped-artificial-intelligence/</a><br><br>2.&nbsp; I have a draft book on \"beliefs\" that deals with (among other things) how to evaluate them. See:<br><br><a href=\"http://ai.stanford.edu/~nilsson/Beliefs.html\">http://ai.stanford.edu/~nilsson/Beliefs.html</a><br><br>(Comments welcome!)<br><br>3. Of course, you must know already about David Kahneman's excellent book, \"Thinking, Fast and Slow.\"</p>\n<p style=\"padding-left: 30px;\">Here are some (maybe hasty!) answers/responses to your questions.</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted: </strong>Your questions are very interesting and I've had such questions for a long time, actually.&nbsp; I've been surprised that people are not more concerned about such things.<br><br>However, right now the problems with AI seem so difficult that I'm not worried about these issues.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>: </strong>I think intelligence is extremely hard to define, even harder to measure, and human-level intelligence is a largely meaningless phrase. All life on Earth has human-level intelligence in one sense for we have all evolved for the same amount of time and we are equally able to survive in our appropriate niches and solve problems relevant to us in highly effective ways.<br><br>There is no danger from clever AI - only from stupid AI that is so bad that it kills us by accident. I wish we were on track to create something as clever as a frog. We have a long, long way to go. I agree with Pat Hayes on this subject.<br><br>I actually find the discussion a little silly. We are *much* more likely to all become a bunch of cyborgs completely reliant on integrated tech (including some clever computers) in the next 100 years. Computers won't be external entities, they will be a part of us. Many of us already can't live our modern lives without being plugged into their gadgets for most of their waking lives. Worry about that instead :)</p>\n<p style=\"padding-left: 30px;\">I think your questions are very hard to answer in any rigorous sense, for they involve prediction of future events so far ahead that anything I say is likely to be quite inaccurate. I will try to answer some below, but these are really just my educated guesses.</p>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em><br> <br> <em>Explanatory remark to Q1:</em><br> <br> <em>P(human-level AI by (year) | no wars \u2227 no disasters \u2227 beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> Because human intelligence is so multi-faceted, your question really should be divided into each of the many components of intelligence. For example, on language translation, AI probably already exceeds the performance of many translators.&nbsp; On integrating symbolic expressions in calculus, AI (or computer science generally) is already much better than humans.&nbsp; AI does better on many planning and scheduling tasks.&nbsp; On chess, same!&nbsp; On the Jeopardy! quiz show, same!<br><br>A while back I wrote an essay about a replacement for the Turing test. It was called the \"Employment Test.\"&nbsp; (See:&nbsp; <a href=\"http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General_Essays/AIMag26-04-HLAI.pdf\">http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General_Essays/AIMag26-04-HLAI.pdf</a>)&nbsp; How many of the many, many jobs that humans do can be done by machines?&nbsp; I'll rephrase your question to be: When will AI be able to perform around 80% of these jobs as well or better than humans perform?<br><br>10% chance:&nbsp; 2030<br>50% chance:&nbsp; 2050<br>90% chance: 2100</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> It seems that the development of human level intelligence is always later than people think it will be.&nbsp; I don't have an idea how long this might take.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong> That depends on what you mean by human level intelligence and how it is measured. Computers can already surpass us at basic arithmetic. Some machine learning methods can equal us in recognition of patterns in images. Most other forms of \"AI\" are tremendously bad at tasks we perform well. The human brain is the result of a several billion years of evolution at molecular scales to macro scales. Our evolutionary history spans unimaginable numbers of generations, challenges, environments, predators, etc. For an artificial brain to resemble ours, it must necessarily go through a very similar evolutionary history. Otherwise it may be a clever machine, but its intelligence will be in areas that do not necessarily resemble human intelligence.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>No idea. There's a lot of factors beyond wars etc mentioned. It's tough to make these kind of predictions.</p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of badly done AI?</em><br> <br> <em>Explanatory remark to Q2:</em><br> <br> <em>P(human extinction | badly done AI) = ?</em></p>\n<p><em>(Where 'badly done' = AGI capable of self-modification that is <strong>not </strong>provably non-dangerous.)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> 0.01% probability during the current century.&nbsp; Beyond that, who knows?</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> I think people will be so concerned about the misuse of intelligent computers that they will take safeguards to prevent such problems.&nbsp; To me it seems more likely that disaster will come on the human race from nuclear or biological weapons, or possibly some natural disaster.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong> </strong>If this were ever to happen, it is most likely to be because the AI was too stupid and we relied on it too much. It is *extremely* unlikely for any AI to become \"self aware\" and take over the world as they like to show in the movies. It's more likely that your pot plant will take over the world.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>Low. The probability of human extinction by other means (e.g. climate problems, micro biology etc) is sufficiently higher that if we were to survive all of them, surviving the result of AI work would be comparatively easy.</p>\n<p style=\"padding-left: 30px;\"><strong></strong><strong> </strong></p>\n<p><strong>Q3: </strong><em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </em><br> <br> <em>Explanatory remark to Q3:</em></p>\n<p><em>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> I'll assume that you mean sometime during this century, and that my \"employment test\" is the measure of superhuman intelligence.<br><br>hours:&nbsp; 5%<br>days:&nbsp;&nbsp;&nbsp; 50%<br>&lt;5 years:&nbsp; 90%</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> This would require a lot in terms of robots being able to build hardware devices or modify their own hardware.&nbsp; I suppose they could also modify their software to do this, but right now it seems like a far out possibility.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong> </strong>It won't happen. Has nothing to do with internet connections or speeds. The question is rather silly.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>Good. An automated human level intelligence is achieved, it ought to be able to learn what humans know more quickly.</p>\n<p><strong>Q4: </strong><em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em><br> <br> <em>Explanatory remark to Q4:</em></p>\n<p><em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> Work on this problem should be ongoing, I think, with the work on AGI.&nbsp; We should start, now, with \"little more,\" and gradually expand through the \"much\" and \"vastly\" as we get closer to AGI.</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> Yes, some kind of ethical system should be built into robots, but then one has to understand their functioning well enough to be sure that they would not get around it somehow.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong> Humans are the ultimate in killers. We have taken over the planet like a plague and wiped out a large number of existing species. \"Intelligent\" computers would be very very stupid if they tried to get in our way. If they have any intelligence at all, they will be very friendly. We are the dangerous ones, not them.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>It's always important to watch for risks with any technology. AI technology is no different.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?<br><br></em><em>Explanatory remark to Q5:</em></p>\n<p><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> I think the risk of terrorists getting nuclear weapons is a greater risk than AI will be during this century. They would certainly use them if they had them -- they would be doing the work of Allah in destroying the Great Satan.&nbsp; Other than that, I think global warming and other environmental problems will have a greater negative impact than AI will have during this century.&nbsp; I believe technology can save us from the risks associated with new viruses.&nbsp; Bill Joy worries about nano-dust, but I don't know enough about that field to assess its possible negative impacts.&nbsp; Then, of course, there's the odd meteor.&nbsp; Probably technology will save us from that.</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> There are risks with any technology, even computers as we have them now.&nbsp; It depends on the form of government and the nature of those in power whether technology is used for good or evil more than on the nature of the technology itself.&nbsp; Even military technology can be used for repression and persecution.&nbsp; Look at some countries today that use technology to keep their people in subjection.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong> </strong>No.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>See above Q2. I think AI risks are smaller than others.</p>\n<p><strong>Q6: </strong><em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> Not as high as it should be. Some, like Steven Omohundro, Wendell Wallach and Colin Allen (\"Moral Machines: Teaching Robots Right from Wrong\"), Patrick Lin (\"Robot Ethics\"), and Ronald Arkin (\"Governing Lethal Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot Architecture\") are among those thinking and writing about these problems.&nbsp; You probably know of several others.</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> Probably not as high as it should be.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong> </strong>The whole idea is blown up out of all proportion. There is no real risk and will not be for a very long time. We are also well aware of the potential risks.</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>Low. Technology in the area is well behind what was predicted in the past, and so concern for risks is correspondingly low.</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human-level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Nils Nilsson:</strong> Because human intelligence involves so many different abilities, I think AGI will require many different technologies with many different milestones. I don't think there is a single one.&nbsp; I do think, though, that the work that Andrew Ng, Geoff Hinton, and (more popularly) Jeff Hawkins and colleagues are doing on modeling learning in the neo-cortex using deep Bayes networks is on the right track.<br><br>Thanks for giving me the opportunity to think about your questions, and I hope to stay in touch with your work!</p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> I think it depends on the interaction of many different capabilities.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter J. Bentley</strong><strong>:</strong><strong>&nbsp;</strong><strong></strong>Too many advances are needed to describe here...</p>\n<p style=\"padding-left: 30px;\"><strong>Hector Levesque: </strong>Reading comprehension at the level of a 10-year old.</p>\n<p><strong>Q8:</strong> <em>Are you familiar with formal concepts of optimal AI design which relate to searches over complete spaces of computable hypotheses or computational strategies, such as Solomonoff induction, Levin search, Hutter's algorithm M, AIXI, or G\u00f6del machines?</em></p>\n<p style=\"padding-left: 30px;\"><strong>David Plaisted:</strong> My research does not specifically relate to those kinds of questions.</p>", "sections": [{"title": "[Click here to see a list of all interviews]", "anchor": "_Click_here_to_see_a_list_of_all_interviews_", "level": 2}, {"title": "The Interview:", "anchor": "The_Interview_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T19:47:00.843Z", "modifiedAt": null, "url": null, "title": "Introducing Leverage Research", "slug": "introducing-leverage-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:36.778Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/969wcdD3weuCscvoJ/introducing-leverage-research", "pageUrlRelative": "/posts/969wcdD3weuCscvoJ/introducing-leverage-research", "linkUrl": "https://www.lesswrong.com/posts/969wcdD3weuCscvoJ/introducing-leverage-research", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introducing%20Leverage%20Research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroducing%20Leverage%20Research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F969wcdD3weuCscvoJ%2Fintroducing-leverage-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introducing%20Leverage%20Research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F969wcdD3weuCscvoJ%2Fintroducing-leverage-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F969wcdD3weuCscvoJ%2Fintroducing-leverage-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 576, "htmlBody": "<p>Geoff Anders asked me to post this introduction to Leverage Research. Several friends of the Singularity Institute are now with Leverage Research, and we have overlapping goals.</p>\n<p>&nbsp;</p>\n<blockquote>\n<p class=\"MsoNormal\">Hello Less Wrong! I'm Geoff Anders, founder of <a href=\"http://www.leverageresearch.org\" target=\"_blank\">Leverage Research</a>. Many Less Wrong readers are already familiar with Leverage. But many are not, and because of our ties to the Less Wrong community and our deep interest in rationality, I thought it would be good to formally introduce ourselves.</p>\n<p class=\"MsoNormal\">I founded Leverage at the beginning of 2011. At that time we had six members. Now we have <a href=\"http://www.leverageresearch.org/tiki-index.php?page=Our+Team\" target=\"_blank\">a team</a>&nbsp;of more than twenty. Over half of our people come from the Less Wrong / Singularity Institute community. One of our members is Jasen Murray, the leader of the Singularity Institute's recent Rationality Boot Camp. Another is Justin Shovelain, a two-year Visiting Fellow at SIAI and the former leader of their intelligence amplification research. A third is Adam Widmer, a former co-organizer of the New York Less Wrong group.</p>\n<p class=\"MsoNormal\">Our goal at Leverage is to make the world a much better place, using the most effective means we can. So far, our conclusion has been that the most effective way to change the world is by means of high-value projects, projects that will have extremely positive effects if they succeed and that have at least a fair probability of success.</p>\n<p class=\"MsoNormal\">One of our projects is existential risk reduction. We have <a href=\"http://www.leverageresearch.org/tiki-index.php?page=AGI+Threat+Persuasion+Study\" target=\"_blank\">conducted a study</a>&nbsp;of the efficacy of methods for persuading people to take the risks of artificial general intelligence (AGI) seriously. We have begun a detailed analysis of AGI catastrophe scenarios. We are working with risk analysts inside and outside of academia. Ultimately, we intend to achieve a comprehensive understanding of AGI and other global risks, develop response plans, and then enact those plans.</p>\n<p class=\"MsoNormal\">A second project is intelligence amplification. We have reviewed the existing research and analyzed current approaches. We then created an initial list of research priorities, ranking techniques by likelihood of success, likely size of effect, safety, cost and so on. We plan to start testing novel techniques soon.</p>\n<p class=\"MsoNormal\">These are just two of <a href=\"http://www.leverageresearch.org/tiki-index.php?page=Current+Research\" target=\"_blank\">our projects</a>. We have several others, including the development of <a href=\"http://www.leverageresearch.org/tiki-index.php?page=Training\" target=\"_blank\">rationality training program</a>, the construction and testing of <a href=\"http://www.leverageresearch.org/tiki-index.php?page=Connection+Theory+Research\" target=\"_blank\">theories of the human mind</a>&nbsp;and an investigation of the laws of idea propagation.</p>\n<p class=\"MsoNormal\">Changing the world is a complex task. Thus we have <a href=\"http://www.leverageresearch.org/tiki-index.php?page=Our+Current+Plan\" target=\"_blank\">a plan</a>&nbsp;that guides our efforts. We know that to succeed, we need to become better than we are. So we take training and self-improvement very seriously. Finally, we know that to succeed, <a href=\"http://www.leverageresearch.org/tiki-index.php?page=Recruitment\" target=\"_blank\">we need more talented people</a>.&nbsp;If you want to significantly improve the world, are serious about self-improvement and believe that changing the world means we need to work together, contact us. We're looking for people who are interested in our current projects or who have ideas of their own.</p>\n<p class=\"MsoNormal\">We've been around for just over a year. In that time we've gotten many of our projects underway. We doubled once in our first six months and again in our second six months. And we have just set up our first physical location, in New York City.</p>\n<p class=\"MsoNormal\">If you want to learn more, visit <a href=\"http://www.leverageresearch.org\" target=\"_blank\">our website</a>.&nbsp;If you want to get involved, want to send a word of encouragement, or if you have suggestions for how we can improve, <a href=\"http://www.leverageresearch.org/tiki-index.php?page=Contact+Us\" target=\"_blank\">write to us</a>.</p>\n<p class=\"MsoNormal\">With hope for the future,</p>\n<p class=\"MsoNormal\">Geoff Anders, on behalf of the Leverage Team</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jZF2jwLnPKBv6m3Ag": 1, "sYbszETv5rKst6gxD": 11}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "969wcdD3weuCscvoJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 41, "extendedScore": null, "score": 0.000112, "legacy": true, "legacyId": "12026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T22:05:28.287Z", "modifiedAt": null, "url": null, "title": "Rational Justice", "slug": "rational-justice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.363Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "geebee2", "createdAt": "2012-01-05T23:38:33.657Z", "isAdmin": false, "displayName": "geebee2"}, "userId": "gxYewzRqePc7rbQid", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CDtF7H3cH3AKwwHK3/rational-justice", "pageUrlRelative": "/posts/CDtF7H3cH3AKwwHK3/rational-justice", "linkUrl": "https://www.lesswrong.com/posts/CDtF7H3cH3AKwwHK3/rational-justice", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Justice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Justice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDtF7H3cH3AKwwHK3%2Frational-justice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Justice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDtF7H3cH3AKwwHK3%2Frational-justice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCDtF7H3cH3AKwwHK3%2Frational-justice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 310, "htmlBody": "<p>I'm interested in how courts and juries might use rational techniques to arrive at correct decisions on guilt.</p>\n<p>In a complex case, it would seem to sensible to assess each component of the prosecution and defence case, and estimate the relative likelihood. If the prosecution case is (say) 100 times more likely than the defence case, then you can say the defendant is guilty beyond reasonable doubt.</p>\n<p>I never heard of this being done though. I recently made an analysis of the Massei report into the Amanda Knox case. It looked like this ( see&nbsp;<a href=\"http://massei-report-analysis.wikispaces.com/\">http://massei-report-analysis.wikispaces.com/</a>&nbsp;for the entire analysis and some insight into the numbers below ).</p>\n<table class=\"wiki_table\" style=\"border-collapse: collapse; margin-top: 10px; margin-right: 1px; margin-bottom: 10px; margin-left: 1px; font-size: 13px; color: #000000; font-family: arial, helvetica, sans-serif; line-height: 19px; \" border=\"0\">\n<tbody>\n<tr>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">Event<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">Prosecution Probability<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">Defence Probability<br /></td>\n</tr>\n<tr>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">Phone at cottage at 22:13<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">1%<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">99%<br /></td>\n</tr>\n<tr>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">DNA evidence correct<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">50%<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">50%<br /></td>\n</tr>\n<tr>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">Break in staged to look like Rudy did it vs unstaged break-in<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">10%<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">90%<br /></td>\n</tr>\n<tr>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">Conspiracy among 3 near strangers with no apparent motive vs burglary gone wrong<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">5%<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">95%<br /></td>\n</tr>\n<tr>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">Murder weapon was two knives rather than one<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">10%<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">90%<br /></td>\n</tr>\n<tr>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">Time of death 23:30 vs 21:10 according to empty duodenum<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">10%<br /></td>\n<td style=\"border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: solid; border-right-style: solid; border-bottom-style: solid; border-left-style: solid; border-top-color: #dddddd; border-right-color: #dddddd; border-bottom-color: #dddddd; border-left-color: #dddddd; border-image: initial; vertical-align: top; min-width: 5px; min-height: 20px; \">90%</td>\n</tr>\n</tbody>\n</table>\n<p>This is perhaps a bit vague. It's not a great example, because in the end I didn't find any credible prosecution evidence. It's not entirely clear what the \"probability\" numbers here actually are, and whether two columns are needed. But hopefully it shows that the Massei's account of the murder is quite improbable, and there is considerable doubt.</p>\n<p>I'm interested in possibly devising a more complete framework for how such an assessment should be done, the pitfalls that need to be guarded against (how uncertain are the probability estimates?), and even views as to how \"reasonable doubt\" should be quantified.</p>\n<p>Perhaps readers would like to make an assessment of other interesting cases, to explore the issues.</p>\n<p>Or how would you approach this problem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CDtF7H3cH3AKwwHK3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 8.290252211759937e-07, "legacy": true, "legacyId": "12027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-09T22:22:00.836Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup 01-11-2012", "slug": "meetup-west-la-meetup-01-11-2012", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mKyHziPKhn3iY8bLq/meetup-west-la-meetup-01-11-2012", "pageUrlRelative": "/posts/mKyHziPKhn3iY8bLq/meetup-west-la-meetup-01-11-2012", "linkUrl": "https://www.lesswrong.com/posts/mKyHziPKhn3iY8bLq/meetup-west-la-meetup-01-11-2012", "postedAtFormatted": "Monday, January 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%2001-11-2012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%2001-11-2012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmKyHziPKhn3iY8bLq%2Fmeetup-west-la-meetup-01-11-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%2001-11-2012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmKyHziPKhn3iY8bLq%2Fmeetup-west-la-meetup-01-11-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmKyHziPKhn3iY8bLq%2Fmeetup-west-la-meetup-01-11-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5y'>West LA Meetup 01-11-2012</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 January 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, January 11th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Recommended Reading:</strong></p>\n\n<ul>\n<li>Any article you like.</li>\n</ul>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome. If we have a large group, we may also play a game!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5y'>West LA Meetup 01-11-2012</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mKyHziPKhn3iY8bLq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.290314576304173e-07, "legacy": true, "legacyId": "12028", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_01_11_2012\">Discussion article for the meetup : <a href=\"/meetups/5y\">West LA Meetup 01-11-2012</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 January 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, January 11th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong id=\"Recommended_Reading_\">Recommended Reading:</strong></p>\n\n<ul>\n<li>Any article you like.</li>\n</ul>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome. If we have a large group, we may also play a game!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_01_11_20121\">Discussion article for the meetup : <a href=\"/meetups/5y\">West LA Meetup 01-11-2012</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup 01-11-2012", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_01_11_2012", "level": 1}, {"title": "Recommended Reading:", "anchor": "Recommended_Reading_", "level": 2}, {"title": "Discussion article for the meetup : West LA Meetup 01-11-2012", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_01_11_20121", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-10T04:51:17.414Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Similarity Clusters", "slug": "seq-rerun-similarity-clusters", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4jdBnjYYykWntjAaj/seq-rerun-similarity-clusters", "pageUrlRelative": "/posts/4jdBnjYYykWntjAaj/seq-rerun-similarity-clusters", "linkUrl": "https://www.lesswrong.com/posts/4jdBnjYYykWntjAaj/seq-rerun-similarity-clusters", "postedAtFormatted": "Tuesday, January 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Similarity%20Clusters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Similarity%20Clusters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4jdBnjYYykWntjAaj%2Fseq-rerun-similarity-clusters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Similarity%20Clusters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4jdBnjYYykWntjAaj%2Fseq-rerun-similarity-clusters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4jdBnjYYykWntjAaj%2Fseq-rerun-similarity-clusters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>Today's post, <a href=\"/lw/nj/similarity_clusters/\">Similarity Clusters</a> was originally published on 06 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Your verbal definition doesn't capture more than a tiny fraction of the category's shared characteristics, but you try to reason as if it does.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/99u/seq_rerun_buy_now_or_forever_hold_your_peace/\">Buy Now or Forever Hold Your Peace</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4jdBnjYYykWntjAaj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.291782372971198e-07, "legacy": true, "legacyId": "12045", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jMTbQj9XB5ah2maup", "SYCx4Y3D2LxfPXuY7", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-10T09:49:40.086Z", "modifiedAt": null, "url": null, "title": "On Leverage Research's plan for an optimal world", "slug": "on-leverage-research-s-plan-for-an-optimal-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4xum2CuYMMRq8rgsW/on-leverage-research-s-plan-for-an-optimal-world", "pageUrlRelative": "/posts/4xum2CuYMMRq8rgsW/on-leverage-research-s-plan-for-an-optimal-world", "linkUrl": "https://www.lesswrong.com/posts/4xum2CuYMMRq8rgsW/on-leverage-research-s-plan-for-an-optimal-world", "postedAtFormatted": "Tuesday, January 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Leverage%20Research's%20plan%20for%20an%20optimal%20world&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Leverage%20Research's%20plan%20for%20an%20optimal%20world%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4xum2CuYMMRq8rgsW%2Fon-leverage-research-s-plan-for-an-optimal-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Leverage%20Research's%20plan%20for%20an%20optimal%20world%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4xum2CuYMMRq8rgsW%2Fon-leverage-research-s-plan-for-an-optimal-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4xum2CuYMMRq8rgsW%2Fon-leverage-research-s-plan-for-an-optimal-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p><a href=\"http://www.leverageresearch.org/tiki-index.php?page=Our+Current+Plan\">The plan</a> currently revolves around using Connection Theory, a new psychological theory, to design \"beneficial contagious ideologies\", the spread of which will lead to the existence of \"an enormous number of actively and stably benevolent people\", who will then \"coordinate their activities\", seek power, and then use their power to eliminate scarcity, disease, harmful governments, global catastrophic threats, etc.</p>\n<p>That is not how the world works. Most positions of power are already occupied by people who have common sense, good will, and a sense of responsibility - or they have those traits, to the extent that human frailty manages to preserve them, amidst the unpredictability of life. The idea that a magic new theory of psychology will unlock human potential and create a new political majority of model citizens is a secular messianism with nothing to back it up.</p>\n<p>I suggest that the people behind Leverage Research need to decide whether they are in the business of solving problems, or in the business of solving meta-problems. The real problems of the world are hard problems, they overwhelm even highly capable people who devote their lives to making a difference. Handwaving about meta topics like psychology and methodology can't be expected to offer more than marginal assistance in any specific concrete domain.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1, "sYbszETv5rKst6gxD": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4xum2CuYMMRq8rgsW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 41, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "12051", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-10T11:59:07.288Z", "modifiedAt": null, "url": null, "title": "[Template] Questions regarding possible risks from artificial intelligence", "slug": "template-questions-regarding-possible-risks-from-artificial", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.891Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ktwHCzywBEXfS2mKG/template-questions-regarding-possible-risks-from-artificial", "pageUrlRelative": "/posts/ktwHCzywBEXfS2mKG/template-questions-regarding-possible-risks-from-artificial", "linkUrl": "https://www.lesswrong.com/posts/ktwHCzywBEXfS2mKG/template-questions-regarding-possible-risks-from-artificial", "postedAtFormatted": "Tuesday, January 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BTemplate%5D%20Questions%20regarding%20possible%20risks%20from%20artificial%20intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BTemplate%5D%20Questions%20regarding%20possible%20risks%20from%20artificial%20intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FktwHCzywBEXfS2mKG%2Ftemplate-questions-regarding-possible-risks-from-artificial%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BTemplate%5D%20Questions%20regarding%20possible%20risks%20from%20artificial%20intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FktwHCzywBEXfS2mKG%2Ftemplate-questions-regarding-possible-risks-from-artificial", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FktwHCzywBEXfS2mKG%2Ftemplate-questions-regarding-possible-risks-from-artificial", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 301, "htmlBody": "<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI. Below are some questions I am going to ask. Please help to refine the questions or suggest new and better questions.</p>\n<p>(Thanks goes to <a href=\"/r/discussion/lw/9a1/qa_with_experts_on_risks_from_ai_2/5n24\">paulfchristiano</a>, <a href=\"/r/discussion/lw/999/qa_with_experts_on_risks_from_ai_1/5mvf\">Steve Rayhawk</a> and <a href=\"/r/discussion/lw/9a1/qa_with_experts_on_risks_from_ai_2/5n1x\">Mafred</a>.)</p>\n<p><strong>Q1:</strong> <em>Assuming </em><em>beneficially political and economic development and that </em><em>no global catastrophe halts progress, by what year would you  assign a 10%/50%/90% chance of the development of artificial intelligence that is roughly</em><em> <em>as good as</em> humans at science, mathematics, engineering and programming</em><em>?</em><br /><br /><strong>Q2</strong>: <em>Once we build AI </em><em>that is roughly</em><em> <em>as good as</em> humans at science, mathematics, engineering and programming</em><em>, how much more difficult will it be for humans and/or AIs to build an AI which is  substantially better at those activities than humans?</em><br /> <strong><br /></strong><em><strong>Q3:</strong> Do you ever expect artificial intelligence to overwhelmingly outperform humans  at typical academic research, in the way that they may soon  overwhelmingly outperform humans at trivia contests, or do you expect  that humans will always play an important role in scientific progress? </em><br /> <br /> <strong>Q4: </strong><em>What probability do you assign to the possibility of </em><em>an AI with initially (professional) </em><em>human-level competence </em><em>at general reasoning</em><em> (</em><em>including science, mathematics, engineering and programming)</em><em> to self-modify its way up to vastly superhuman </em><em>capabilities within a  matter of hours/days/&lt; 5 years? </em><br /> <br /> <strong>Q5: </strong><em>How important is it to figure out how to make superhuman AI provably  friendly to us and our values (non-dangerous), before attempting to  build AI that is good enough at </em><em>general reasoning</em><em> (</em><em>including science, mathematics, engineering and programming)</em><em> to undergo radical self-modification?</em><br /><br /><strong>Q6:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of AI </em><em>capable of self-modification (that is <strong>not </strong>provably non-dangerous, if that is even possible)</em><em>?</em><em><br /></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ktwHCzywBEXfS2mKG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 8.293396076749984e-07, "legacy": true, "legacyId": "12052", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-10T17:54:47.481Z", "modifiedAt": null, "url": null, "title": "[Link] There is nothing like \"intelligence\", only an evolution is going on", "slug": "link-there-is-nothing-like-intelligence-only-an-evolution-is", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rNYsCpisTFf39Qzpt/link-there-is-nothing-like-intelligence-only-an-evolution-is", "pageUrlRelative": "/posts/rNYsCpisTFf39Qzpt/link-there-is-nothing-like-intelligence-only-an-evolution-is", "linkUrl": "https://www.lesswrong.com/posts/rNYsCpisTFf39Qzpt/link-there-is-nothing-like-intelligence-only-an-evolution-is", "postedAtFormatted": "Tuesday, January 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20There%20is%20nothing%20like%20%22intelligence%22%2C%20only%20an%20evolution%20is%20going%20on&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20There%20is%20nothing%20like%20%22intelligence%22%2C%20only%20an%20evolution%20is%20going%20on%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNYsCpisTFf39Qzpt%2Flink-there-is-nothing-like-intelligence-only-an-evolution-is%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20There%20is%20nothing%20like%20%22intelligence%22%2C%20only%20an%20evolution%20is%20going%20on%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNYsCpisTFf39Qzpt%2Flink-there-is-nothing-like-intelligence-only-an-evolution-is", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNYsCpisTFf39Qzpt%2Flink-there-is-nothing-like-intelligence-only-an-evolution-is", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 14, "htmlBody": "<p>h<a href=\"http://edge.org/conversation/infinite-stupidity-edge-conversation-with-mark-pagel\">ttp://edge.org/conversation/infinite-stupidity-edge-conversation-with-mark-pagel</a></p>\n<p>&nbsp;</p>\n<p>Random change, then a selection, says Mark Pagel. As I agree, here's the link.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rNYsCpisTFf39Qzpt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -15, "extendedScore": null, "score": 8.294738024329689e-07, "legacy": true, "legacyId": "12053", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-10T18:13:05.631Z", "modifiedAt": null, "url": null, "title": "CEV Sequence - On What is a Self - Part 1", "slug": "cev-sequence-on-what-is-a-self-part-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nfEfJiPr2nbCj6R9F/cev-sequence-on-what-is-a-self-part-1", "pageUrlRelative": "/posts/nfEfJiPr2nbCj6R9F/cev-sequence-on-what-is-a-self-part-1", "linkUrl": "https://www.lesswrong.com/posts/nfEfJiPr2nbCj6R9F/cev-sequence-on-what-is-a-self-part-1", "postedAtFormatted": "Tuesday, January 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CEV%20Sequence%20-%20On%20What%20is%20a%20Self%20-%20Part%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACEV%20Sequence%20-%20On%20What%20is%20a%20Self%20-%20Part%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfEfJiPr2nbCj6R9F%2Fcev-sequence-on-what-is-a-self-part-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CEV%20Sequence%20-%20On%20What%20is%20a%20Self%20-%20Part%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfEfJiPr2nbCj6R9F%2Fcev-sequence-on-what-is-a-self-part-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfEfJiPr2nbCj6R9F%2Fcev-sequence-on-what-is-a-self-part-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1760, "htmlBody": "<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\">Summary: The CEV sequence is an attempt to strenghten the text <a title=\"Engineering a Nice Place to Be\" href=\"http://intelligence.org/upload/CEV.html\" target=\"_blank\">Coherent Extrapolated Volition</a>. I take CEV to be the subject which offers more room for improvement in our potential route towards a desirable post-humanity. In text 1 &ldquo;<strong>On What is a Self</strong>&rdquo; I'll clarify what people are, since it is people's extrapolated volitions that ought to be implemented. My main target audiences are non-philosophers and AIs who may underestimate the complexity underlying the concepts of selves and people. To this end I'll use standard analytic philosophy methods such as conceptual, semantic and linguistic analysis. The second text &ldquo;<strong>Troubles with CEV</strong>&rdquo; will present several problems (conceptual, practical, computational) of CEV's current conception. Objections to the overall suggestion will also be considered. Some alternatives to CEV by many authors will be presented. The second text targets a general audience of CEV interested people and does not rely mainly on philosophical analysis.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\">For your convenience, I'll be posting every two days. Text 1, Part 1</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"CENTER\"><span style=\"font-size: x-large;\">On What Is a Self</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">Intro-Background</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\"><strong>Granted, Selves Are Coalescences of Symbols</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Some of what is taken for granted in this text is vividly subsumed by pg 204 and 289-290 of Hofstadters &ldquo;I Am a Strange Loop&rdquo;(2007), to those who are still in the struggle relating to monism, dualism, qualia, Mary the neuroscientist, epiphenomenons and innefable qualities, it is worth it to read through his passage to understand the background metaphysical view of the universe from which it is derived. To those on the other hand who are good willed reductionists of the non-greedy, no-skyhook, no 'design only from Above' kind may jump straight to the text:</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" align=\"JUSTIFY\"><span lang=\"en-US\">[What makes and &ldquo;I&rdquo; come seemingly out of nowhere] is ironically, an </span><span lang=\"en-US\"><em>inability - </em></span><span lang=\"en-US\">namely our [...] inability to see, feel, or sense in any way the constant frenetic, churning and roiling of micro -stuff, all the unfelt bubbling and boiling that underlies our thinking. This, our innate blindness to the world of the tiny, forces us to hallucinate a profound schism betqenn the goal-lacking material world of balls and sticks and sounds and lights, on the one hand, and a goal-pervaded abstract world of hopes and beliefs and joys and fears, on the other, in which radically different sorts of causality seem to reign. [...]</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" align=\"JUSTIFY\">&ldquo;<span lang=\"en-US\">[Your] &ldquo;I&rdquo; was not an </span><span lang=\"en-US\"><em>a priori </em></span><span lang=\"en-US\">well-defined thing that was predestined to jump, full-fledged and sharp, in to some just-created empty physical vessel at some particular instant. Nor did your &ldquo;I&rdquo; suddenly spring into existence, wholly unanticipated but in full bloom. Rather, your &ldquo;I&rdquo; was the slowly emerging outcome of a million unpredictable eventsthat befell a particular body and the brain housed in it. Your &ldquo;I&rdquo; is the self-reinforcing structure that gradually came to exist not only </span><span lang=\"en-US\"><em>in</em></span><span lang=\"en-US\"> that brain, but </span><span lang=\"en-US\"><em>thanks to</em></span><span lang=\"en-US\"> that brain. It coudn&rsquo;t have come to exist in </span><span lang=\"en-US\"><em>this </em></span><span lang=\"en-US\">brain, because </span><span lang=\"en-US\"><em>this </em></span><span lang=\"en-US\">brain went through different experiences that led to a different human being.&rdquo; </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This general view will be being taken for granted as the Meta Physically correct approach to thinking about mental entities, what will be discussed lies more in the domain of conceptual usage, word meaning, psychological conceptions, symbolic extension, explicit linguistic definition, and less on trying to find underlying substrates or metaphysical properties of selves.</p>\n<p><strong>Selves and Persons Are Similar<br /></strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; On the eight move of your weekly chess game you do what feels same as always: Reflect for a few seconds on the many layers of structure underlying the current game-state, specially regarding changes from your opponent&rsquo;s last move. It seems reasonable to eat his pawn with your bishop. After moving you look at him and see the sequence of expressions: Doubt &ldquo;Why did he do that?&rdquo;, distrust &ldquo;He must be seeing something I don&rsquo;t&rdquo;, inquiry &ldquo;Let me double check this&rdquo;, Schadenfreud &ldquo;No, he actually failed&rdquo; and finally joy &ldquo;Piece of cake, I&rsquo;ll win&rdquo;. He takes your bishop with a horse that from your perspective could only be coming from neverland. Still stunned, you resign. It is the second time in a row you lose the game due to a simple mistake. The excuse bursts naturally out of your mouth: &ldquo;I&rsquo;m not myself today&rdquo;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functional role (with plausibly evolutionary reasons) of this use of the concept of Self is easy to unscramble.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">1) Do not hold your model of me as responsible for these mistakes</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">2) Either (a) I sense something strange about the inner machinery of my mind, the algorithm feels different from the inside. Or (b) at least my now visible mistakes are realiable evidence of a difference which I detected in hindsight.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">3) If there is a person watching this game, notice how my signaling and my friend&rsquo;s not contesting it is reliable evidence I normally play chess better than this</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A few minutes later, you see your friend yelling histerically at someone in the phone, you explain to the girl who was watching: &ldquo;He is not that kind of person&rdquo;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Here we have a situation where the analogous of 1 and 3 work, but there is no way for you to tell what the algorithm feels from the inside. You still know in hindsight that your friend doesn&rsquo;t usually yell like that. Though 1, 2, and 3 still hold, 2(a) is not the case anymore.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; I suggest the property of 2(a) that blocks interchangeability of the concepts of Self and Person is &ldquo;having first person epistemic information about X&rdquo;. Selves have that, people don&rsquo;t. We use the term &lsquo;person&rsquo; when we want to talk only about the epistemically intersubjective properties of someone. Self is reserved for a person&rsquo;s perspective of herself, including, for instance, indexical facts.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Other than that, Self and Person seem to be interchangeable concepts. This generalization is useful because that means most of the problem of personhood and selfhood can be collapsed into one thing. Unfortunately, the Self/Person intersection is a concept that is itself a Mongrel Concept, so it has again to be split apart.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\"><strong>Mongrel and Cluster Concepts (and why Selves are both)<br /></strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; When a concept seems to defy easy explanability, there are two interesting possibilities of how to interact with it. The first would be to assume that the disparate uses of the term &lsquo;Self&rsquo;&rsquo; in ordinary language and science can be captured by a unique, all-encompassing notion of Self. The second is to assume that different uses of &lsquo;Self&rsquo;&rsquo; reveal a plurality of notions of Selfhood, each in need of a separate account. I will endorse this second assumption: Self is a mongrel concept in need of disambiguation. (to strenghten the analogy power of thinking about mongrels, it may help to know that Information, Consciousness and Health are thought to be mongrel concepts as well).</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Without using specific tags for the time being, let us assume that there will be 4 kinds of Self, 1,2,3, and 4. To say that Self is a concept that sometimes maps into 1, sometimes into 3 and so on is not to exaustivelly frame the concept usage. That is because 1 and 2 themselves may be cluster concepts.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The cluster concept shape is one of the most common shapes of concepts in our mental vocabulary. Concepts are associational structures. Most of the times, instead of drawing a clear line around a set in the world inside of which all X fits, and outside of which none does, concepts present a <a title=\"Yudkowsky on Cluster Concepts\" href=\"/lw/nl/the_cluster_structure_of_thingspace/\" target=\"_blank\">cluster like structure</a> with nearly all core area members belonging and nearly none in the far fetched radius belonging. Not all of their typical features are logically necessary. The recognition of features produces an activation, the strength of which depends not only on the degree to which the feature is present but a weighting factor. When the sum of the activations crosses a threshold, the concept becomes active and the stimulus is said to belong to that category.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Selves are mongrel concepts composed of different conceptual intuitions, each of which is itself a cluster concept, thus Selves are part of the most elusive, abstract, high-level entities entertained by minds. Whereas this may be aesthetically pleasant, presenting us as considerably complex entities, it is also a great ethical burden, for it leaves the domain of ethics, highly dependant on the concepts of Selfhood and Personhood, with a scattered slippery ground-level notion from which to create the building blocks of ethical theories.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" align=\"JUSTIFY\"><span lang=\"en-US\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Several analogies have been used to convey the concept of Cluster Concept, these convey images of star clusters, neural networks lighting up, and sets of properties with a majority vote. A particularly well known analogy used by Wittgenstein is the game analogy, in which language games determine/prescribe normative meanings which constrict a word&rsquo;s meaning, without determining a clear cut case. Wittgenstein defended that there was no clear set of necessary conditions that determine what a game is. Bernard Suits came up with a refutation of that claim, stating that there is such a definition (modified from &ldquo;What is a game&rdquo; 1967, </span><span lang=\"en-US\"><em>Philosophy of Science</em></span><span lang=\"en-US\"> Vol. 34, No. 2 [Jun., 1967], pp. 148-156):</span></p>\n<blockquote>\n<p class=\"western\" style=\"margin-left: 1.11cm; margin-bottom: 0cm;\" align=\"JUSTIFY\"><span lang=\"en-US\">\"To play a game is to engage in activity designed to bring about a specific state of affairs, using only means permitted by specific rules, where the means permitted by the rules are more limited in scope than they would be in the absence of such rules, and where the sole reason for accepting the rules is to make possible such activity.\"</span></p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Can we hope for a similar soon to be found understanding of Self? Let us invoke:</p>\n<p class=\"western\" style=\"margin-left: 0.79cm; margin-bottom: 0cm;\" align=\"JUSTIFY\"><span lang=\"en-US\"><em><strong>The Hidden Variable Hypothesis</strong></em></span><span lang=\"en-US\"><em>: There is a core essence which determines the class of selves from non-selves, it is just not yet within our current state-of-knowledge reach. </em></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While desirable, there are various resons to be skeptical of The Hidden Variable Hyphotesis: (1) Any plausible candidate core would have to be able to disentangle selves from Organisms in general, Superorganisms (i.e. insect societies) and institutions (2) We clearly entertain different models of what selves are for different purposes, as shown below in Section Varieties of Self-Systems Worth Having. (3) Design consideration: Being evolved structures which encompass several resources of a recently evolved mind, that came to being through a complex dual-inheritance evolution of several hundred thousand replicators belonging to two kinds (genes and memes), Selves are among the most complex structures known and thus unlikely to possess a core essence, due to causal design considerations independent of how untractable it would be to detect and describe this essence.</p>\n<p class=\"western\" style=\"text-indent: 1.27cm; margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">From now on then, I will be assuming as common ground that Selves are Mongrel concepts, comprised of some yet undiscussed number of Cluster Concepts.</p>\n<p>(To Be Continued)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nfEfJiPr2nbCj6R9F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -12, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "12054", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\">Summary: The CEV sequence is an attempt to strenghten the text <a title=\"Engineering a Nice Place to Be\" href=\"http://intelligence.org/upload/CEV.html\" target=\"_blank\">Coherent Extrapolated Volition</a>. I take CEV to be the subject which offers more room for improvement in our potential route towards a desirable post-humanity. In text 1 \u201c<strong>On What is a Self</strong>\u201d I'll clarify what people are, since it is people's extrapolated volitions that ought to be implemented. My main target audiences are non-philosophers and AIs who may underestimate the complexity underlying the concepts of selves and people. To this end I'll use standard analytic philosophy methods such as conceptual, semantic and linguistic analysis. The second text \u201c<strong>Troubles with CEV</strong>\u201d will present several problems (conceptual, practical, computational) of CEV's current conception. Objections to the overall suggestion will also be considered. Some alternatives to CEV by many authors will be presented. The second text targets a general audience of CEV interested people and does not rely mainly on philosophical analysis.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\">For your convenience, I'll be posting every two days. Text 1, Part 1</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"CENTER\"><span style=\"font-size: x-large;\">On What Is a Self</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">Intro-Background</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\"><strong id=\"Granted__Selves_Are_Coalescences_of_Symbols\">Granted, Selves Are Coalescences of Symbols</strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Some of what is taken for granted in this text is vividly subsumed by pg 204 and 289-290 of Hofstadters \u201cI Am a Strange Loop\u201d(2007), to those who are still in the struggle relating to monism, dualism, qualia, Mary the neuroscientist, epiphenomenons and innefable qualities, it is worth it to read through his passage to understand the background metaphysical view of the universe from which it is derived. To those on the other hand who are good willed reductionists of the non-greedy, no-skyhook, no 'design only from Above' kind may jump straight to the text:</p>\n<blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" align=\"JUSTIFY\"><span lang=\"en-US\">[What makes and \u201cI\u201d come seemingly out of nowhere] is ironically, an </span><span lang=\"en-US\"><em>inability - </em></span><span lang=\"en-US\">namely our [...] inability to see, feel, or sense in any way the constant frenetic, churning and roiling of micro -stuff, all the unfelt bubbling and boiling that underlies our thinking. This, our innate blindness to the world of the tiny, forces us to hallucinate a profound schism betqenn the goal-lacking material world of balls and sticks and sounds and lights, on the one hand, and a goal-pervaded abstract world of hopes and beliefs and joys and fears, on the other, in which radically different sorts of causality seem to reign. [...]</span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" align=\"JUSTIFY\">\u201c<span lang=\"en-US\">[Your] \u201cI\u201d was not an </span><span lang=\"en-US\"><em>a priori </em></span><span lang=\"en-US\">well-defined thing that was predestined to jump, full-fledged and sharp, in to some just-created empty physical vessel at some particular instant. Nor did your \u201cI\u201d suddenly spring into existence, wholly unanticipated but in full bloom. Rather, your \u201cI\u201d was the slowly emerging outcome of a million unpredictable eventsthat befell a particular body and the brain housed in it. Your \u201cI\u201d is the self-reinforcing structure that gradually came to exist not only </span><span lang=\"en-US\"><em>in</em></span><span lang=\"en-US\"> that brain, but </span><span lang=\"en-US\"><em>thanks to</em></span><span lang=\"en-US\"> that brain. It coudn\u2019t have come to exist in </span><span lang=\"en-US\"><em>this </em></span><span lang=\"en-US\">brain, because </span><span lang=\"en-US\"><em>this </em></span><span lang=\"en-US\">brain went through different experiences that led to a different human being.\u201d </span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;</p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This general view will be being taken for granted as the Meta Physically correct approach to thinking about mental entities, what will be discussed lies more in the domain of conceptual usage, word meaning, psychological conceptions, symbolic extension, explicit linguistic definition, and less on trying to find underlying substrates or metaphysical properties of selves.</p>\n<p><strong id=\"Selves_and_Persons_Are_Similar\">Selves and Persons Are Similar<br></strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; On the eight move of your weekly chess game you do what feels same as always: Reflect for a few seconds on the many layers of structure underlying the current game-state, specially regarding changes from your opponent\u2019s last move. It seems reasonable to eat his pawn with your bishop. After moving you look at him and see the sequence of expressions: Doubt \u201cWhy did he do that?\u201d, distrust \u201cHe must be seeing something I don\u2019t\u201d, inquiry \u201cLet me double check this\u201d, Schadenfreud \u201cNo, he actually failed\u201d and finally joy \u201cPiece of cake, I\u2019ll win\u201d. He takes your bishop with a horse that from your perspective could only be coming from neverland. Still stunned, you resign. It is the second time in a row you lose the game due to a simple mistake. The excuse bursts naturally out of your mouth: \u201cI\u2019m not myself today\u201d</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The functional role (with plausibly evolutionary reasons) of this use of the concept of Self is easy to unscramble.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">1) Do not hold your model of me as responsible for these mistakes</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">2) Either (a) I sense something strange about the inner machinery of my mind, the algorithm feels different from the inside. Or (b) at least my now visible mistakes are realiable evidence of a difference which I detected in hindsight.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">3) If there is a person watching this game, notice how my signaling and my friend\u2019s not contesting it is reliable evidence I normally play chess better than this</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A few minutes later, you see your friend yelling histerically at someone in the phone, you explain to the girl who was watching: \u201cHe is not that kind of person\u201d</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Here we have a situation where the analogous of 1 and 3 work, but there is no way for you to tell what the algorithm feels from the inside. You still know in hindsight that your friend doesn\u2019t usually yell like that. Though 1, 2, and 3 still hold, 2(a) is not the case anymore.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; I suggest the property of 2(a) that blocks interchangeability of the concepts of Self and Person is \u201chaving first person epistemic information about X\u201d. Selves have that, people don\u2019t. We use the term \u2018person\u2019 when we want to talk only about the epistemically intersubjective properties of someone. Self is reserved for a person\u2019s perspective of herself, including, for instance, indexical facts.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Other than that, Self and Person seem to be interchangeable concepts. This generalization is useful because that means most of the problem of personhood and selfhood can be collapsed into one thing. Unfortunately, the Self/Person intersection is a concept that is itself a Mongrel Concept, so it has again to be split apart.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\"><strong id=\"Mongrel_and_Cluster_Concepts__and_why_Selves_are_both_\">Mongrel and Cluster Concepts (and why Selves are both)<br></strong></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; When a concept seems to defy easy explanability, there are two interesting possibilities of how to interact with it. The first would be to assume that the disparate uses of the term \u2018Self\u2019\u2019 in ordinary language and science can be captured by a unique, all-encompassing notion of Self. The second is to assume that different uses of \u2018Self\u2019\u2019 reveal a plurality of notions of Selfhood, each in need of a separate account. I will endorse this second assumption: Self is a mongrel concept in need of disambiguation. (to strenghten the analogy power of thinking about mongrels, it may help to know that Information, Consciousness and Health are thought to be mongrel concepts as well).</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Without using specific tags for the time being, let us assume that there will be 4 kinds of Self, 1,2,3, and 4. To say that Self is a concept that sometimes maps into 1, sometimes into 3 and so on is not to exaustivelly frame the concept usage. That is because 1 and 2 themselves may be cluster concepts.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The cluster concept shape is one of the most common shapes of concepts in our mental vocabulary. Concepts are associational structures. Most of the times, instead of drawing a clear line around a set in the world inside of which all X fits, and outside of which none does, concepts present a <a title=\"Yudkowsky on Cluster Concepts\" href=\"/lw/nl/the_cluster_structure_of_thingspace/\" target=\"_blank\">cluster like structure</a> with nearly all core area members belonging and nearly none in the far fetched radius belonging. Not all of their typical features are logically necessary. The recognition of features produces an activation, the strength of which depends not only on the degree to which the feature is present but a weighting factor. When the sum of the activations crosses a threshold, the concept becomes active and the stimulus is said to belong to that category.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Selves are mongrel concepts composed of different conceptual intuitions, each of which is itself a cluster concept, thus Selves are part of the most elusive, abstract, high-level entities entertained by minds. Whereas this may be aesthetically pleasant, presenting us as considerably complex entities, it is also a great ethical burden, for it leaves the domain of ethics, highly dependant on the concepts of Selfhood and Personhood, with a scattered slippery ground-level notion from which to create the building blocks of ethical theories.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" align=\"JUSTIFY\"><span lang=\"en-US\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Several analogies have been used to convey the concept of Cluster Concept, these convey images of star clusters, neural networks lighting up, and sets of properties with a majority vote. A particularly well known analogy used by Wittgenstein is the game analogy, in which language games determine/prescribe normative meanings which constrict a word\u2019s meaning, without determining a clear cut case. Wittgenstein defended that there was no clear set of necessary conditions that determine what a game is. Bernard Suits came up with a refutation of that claim, stating that there is such a definition (modified from \u201cWhat is a game\u201d 1967, </span><span lang=\"en-US\"><em>Philosophy of Science</em></span><span lang=\"en-US\"> Vol. 34, No. 2 [Jun., 1967], pp. 148-156):</span></p>\n<blockquote>\n<p class=\"western\" style=\"margin-left: 1.11cm; margin-bottom: 0cm;\" align=\"JUSTIFY\"><span lang=\"en-US\">\"To play a game is to engage in activity designed to bring about a specific state of affairs, using only means permitted by specific rules, where the means permitted by the rules are more limited in scope than they would be in the absence of such rules, and where the sole reason for accepting the rules is to make possible such activity.\"</span></p>\n</blockquote>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Can we hope for a similar soon to be found understanding of Self? Let us invoke:</p>\n<p class=\"western\" style=\"margin-left: 0.79cm; margin-bottom: 0cm;\" align=\"JUSTIFY\"><span lang=\"en-US\"><em><strong>The Hidden Variable Hypothesis</strong></em></span><span lang=\"en-US\"><em>: There is a core essence which determines the class of selves from non-selves, it is just not yet within our current state-of-knowledge reach. </em></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While desirable, there are various resons to be skeptical of The Hidden Variable Hyphotesis: (1) Any plausible candidate core would have to be able to disentangle selves from Organisms in general, Superorganisms (i.e. insect societies) and institutions (2) We clearly entertain different models of what selves are for different purposes, as shown below in Section Varieties of Self-Systems Worth Having. (3) Design consideration: Being evolved structures which encompass several resources of a recently evolved mind, that came to being through a complex dual-inheritance evolution of several hundred thousand replicators belonging to two kinds (genes and memes), Selves are among the most complex structures known and thus unlikely to possess a core essence, due to causal design considerations independent of how untractable it would be to detect and describe this essence.</p>\n<p class=\"western\" style=\"text-indent: 1.27cm; margin-bottom: 0cm;\" lang=\"en-US\" align=\"JUSTIFY\">From now on then, I will be assuming as common ground that Selves are Mongrel concepts, comprised of some yet undiscussed number of Cluster Concepts.</p>\n<p>(To Be Continued)</p>", "sections": [{"title": "Granted, Selves Are Coalescences of Symbols", "anchor": "Granted__Selves_Are_Coalescences_of_Symbols", "level": 1}, {"title": "Selves and Persons Are Similar", "anchor": "Selves_and_Persons_Are_Similar", "level": 1}, {"title": "Mongrel and Cluster Concepts (and why Selves are both)", "anchor": "Mongrel_and_Cluster_Concepts__and_why_Selves_are_both_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WBw8dDkAWohFjWQSk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-10T20:05:47.767Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.502Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sDR6ioTnJzqHeHQDc/", "pageUrlRelative": "/posts/sDR6ioTnJzqHeHQDc/", "linkUrl": "https://www.lesswrong.com/posts/sDR6ioTnJzqHeHQDc/", "postedAtFormatted": "Tuesday, January 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDR6ioTnJzqHeHQDc%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDR6ioTnJzqHeHQDc%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDR6ioTnJzqHeHQDc%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sDR6ioTnJzqHeHQDc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 8.295232404017767e-07, "legacy": true, "legacyId": "12055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T01:52:24.831Z", "modifiedAt": "2020-04-24T20:39:31.125Z", "url": null, "title": "Designing Ritual", "slug": "designing-ritual", "viewCount": null, "lastCommentedAt": "2020-06-02T21:22:26.363Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Raemon", "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GEQyCqgu5Yhi5dTdb/designing-ritual", "pageUrlRelative": "/posts/GEQyCqgu5Yhi5dTdb/designing-ritual", "linkUrl": "https://www.lesswrong.com/posts/GEQyCqgu5Yhi5dTdb/designing-ritual", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Designing%20Ritual&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADesigning%20Ritual%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGEQyCqgu5Yhi5dTdb%2Fdesigning-ritual%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Designing%20Ritual%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGEQyCqgu5Yhi5dTdb%2Fdesigning-ritual", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGEQyCqgu5Yhi5dTdb%2Fdesigning-ritual", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4235, "htmlBody": "<p>This is the third post in my ritual mini-sequence. My first article was an<a href=\"https://www.lesswrong.com/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration\"> </a><u><a href=\"https://www.lesswrong.com/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration\">emotional rallying cry around of the idea of a rational-(trans)humanist culture</a></u>. My second article<a href=\"https://www.lesswrong.com/lw/93l/the_value_and_danger_of_ritual\"> <u>examined the value and potential dangers of ritual</u></a>.</p><p>So far, I remain convinced that ritual is a valuable experience for most people. I don&#x2019;t know if there can or should be a unified &#x201C;Less Wrong&#x201D; culture, but I do think individual communities should consider creating their own (&#x201C;rational-humanist&#x201D; or otherwise). Ritual can be a useful source of fun, comfort and inspiration for positive change. A decent heuristic for &quot;Should my meetup try this?&quot; is &quot;Do the people at my meetup think this sounds cool?&quot;</p><p>This article is both an explanation of certain design principles, and a case study of my attempt at one particular kind of community event.</p><p><strong>Designing Timelessness</strong></p><p>So, say your community decides to incorporate some ritual. How do you do that?</p><p>Ritual-space is large, and includes things as simple as &quot;Pass around dark chocolate at the beginning of meetups.&quot; You probably already have <em>some </em>kind of ritual going on. Over time, these small traditions can accumulate into something interesting and comforting. But if you&#x2019;re like me, you want something more powerful - you want the gravitas of an ancient cultural cornerstone, and you want it <em>now.</em></p><p>This is... a challenge.</p><p>&#x201C;Creating tradition&#x201D; almost feels like an oxymoron. To be effective, it has to have a timeless quality about it. It doesn&apos;t have to actually be <em>ancient, </em>but it needs to feel resonant, familiar and personal. Most ritual is created through a combination of artistic skill and memetic evolution. A few central traditions begin the process (say, a particular feast, prayer and/or anthem), and then the ritual begins to spread from family to family, adapting in response to memetic selection pressure, filling specific niches, often resulting in bizarre injokes and absurdities. The absurdities can actually help bind a community together, as a badge of pride.</p><p>Powerful governments and religions can attempt to steer this. The Catholic church votes on deliberate changes to their ceremonies. But the church could not have intentionally created Modern Western Christmas - a<a href=\"http://en.wikipedia.org/wiki/Christmas\"> <u>bizarre gestalt of pagan and christian mythology</u></a>,<a href=\"http://en.wikipedia.org/wiki/A_Visit_from_St._Nicholas\"> <u>random stories</u></a>, eventually infused with modern capitalism to become a monolithic culture and industry.</p><p>One of the hardest things an artist can do is create a work that <em>appears</em> to have already been weathered by natural forces. It&#x2019;s much easier to create a<a href=\"http://www.rockpapershotgun.com/images/oct07/valve/portal/port0.jpg\"> <u>clean room</u></a> than a<a href=\"http://cdn.steampowered.com/v/gfx/apps/620/ss_f3f6787d74739d3b2ec8a484b5c994b3d31ef325.1920x1080.jpg?t=1324487993\"> <u>crumbling ruin</u></a>. The weight of ancient cultural cornerstones is built out of a lot of subtle details, and if you get them wrong, you&#x2019;ll get a ridiculous, uncanny-valley effect (especially if you were taking everything really seriously).</p><p>But I think it can be done. The Solstice celebration I put together was a solid first attempt, and I feel that much more is possible. I&#x2019;m going to walk through my design process, and then discuss what areas I think needed more work. I recently finished the<a href=\"http://dl.dropbox.com/u/2000477/BookOfRituals.pdf\"> <u>Extended Edition With Commentary</u></a> of the evening&#x2019;s songbook. You can either look it over now, or after reading this article. (Ordinarily I&#x2019;d say art should stand on its own without explanation, but since the actual &#x201C;art&#x201D; was an interactive ceremony, I don&#x2019;t think it matters much. The book <em>is</em> intended to be its own kind of art, but I&#x2019;ll leave it up to you.)</p><p>&#x2026;</p><p>(Pausing here a moment so that the people who wanted to look over the book first have a second chance to do so, before being carried forward by inertia.)</p><p>&#x2026;</p><p>All right. These were among my biggest considerations, when creating the Solstice celebration:</p><ul><li>Have a Goal</li><li>Build on the Familiar</li><li>Do Research, and Cultivate Diversity of Experience</li><li>Manage Complexity</li><li>Field Testing</li><li>Remember (And Re-Evaluate) your Goal</li></ul><p><strong>Have A Goal</strong></p><p>My primary goal for the Solstice was personal, and perhaps selfish: I wanted a particular, profound, intense experience. I&#x2019;d had pieces of it in the past - communal singing, tribal belonging, reading beautiful prose that resonated with me. I&#x2019;ve been to religious events and felt a hint of their potency. I wanted to weave all these elements into a single experience.</p><p>A related goal, not quite so selfish, was wanting to channel this power into changing myself, to inspire myself to be the kind of person I wanted to be.</p><p>Closely intertwined with those goals was the desire to create a fun, and hopefully moving experience for my community. In some ways this was instrumental to the first goals, but the event wouldn&#x2019;t have worked if I hadn&#x2019;t genuinely cared about creating something that everyone could enjoy together, for their own sake.</p><p>An unrelated goal, which I had to take care not to override more important ones, is that I&#x2019;ve been wanting to have a Cthulhu-caroling party since like forever.</p><p>It took several months for these to weave into a single, unified plan for the evening. I&#x2019;ll return to that in a moment.</p><p><strong>Build on the Familiar</strong></p><p>Culture can be created, but not from nothing. You&#x2019;ll need to work off existing ideas that your tribe already shares.</p><p>One of the most important functions that tradition and ritual serve is to comfort. From what I&apos;ve read and experienced, humans are hardwired to feel on-edge when they&apos;re in an unfamiliar situation. Uncertainty is a heuristic for danger - both to your physical life, and ostracization from the group (ultimately making it harder to find a mate). My accompanying just-so story is that repeated social traditions make your brain feel safe - you&#x2019;re surrounded by people doing familiar things you understand, so you don&#x2019;t have worry about suddenly getting kicked out of the group for no reason. You&#x2019;re also surrounded by friends that could probably protect you if a tiger suddenly jumped out of the woods and attacked.</p><p>So if you want to induce that warm comfort, you&#x2019;ll need to be working with memes and activities that feel familiar. The NYC meetup has some diverse values, but we share a vision of the power of human achievement, and a future that is better specifically because of scientific progress. We also share an understanding of how dangerous and cruel the universe can be. Most of us have had these beliefs for a while, but it was Eliezer&#x2019;s Sequences that crystalized them into a coherent, specific and moving worldview, and brought us together as a community.</p><p>So the Sequences provided the content. But I turned to existing religious rituals for the structure. I knew that my family&#x2019;s Christmas Eve (where we feast, then gather and sing carols for hours, with a gradual emotion arc from silly-to-serious) was a good frame to build around. I&#x2019;ve also had experience with Catholic Mass and a Seder, which incorporated stories alongside songs. These sorts of events are ubiquitous throughout our culture. Even if you don&#x2019;t have personal experience with them, you&#x2019;ve probably grown up with ingrained stereotypes about them and vaguely identify them as &#x201C;normal.&#x201D;</p><p>I think this was the single most important point - we had content that resonated with everyone, built around a structure that was familiar.</p><p>Big ideas are important, but weaving together smaller memes was also key. I looked for songs, stories or activities that I knew were already popular among our group:</p><ul><li>Our group has a fondness for dark chocolate, which has become something of a tribal badge. I don&#x2019;t actually even like dark chocolate that much, but it&#x2019;s oddly comforting to pass it around the living room.</li><li>Songs like &#x201C;Still Alive&#x201D; and &#x201C;RE Your Brains&#x201D; are crowd favorites in our group. They aren&#x2019;t exactly on theme, but everyone knows them, and they were woven into the evening&#x2019;s narrative easily enough. Monty Python and Tom Lehrer also provided some examples, which were not only familiar but are just old enough to feel slightly &#x201C;traditional&#x201D;.</li><li>We&#x2019;re not all Lovecraft enthusiasts, but most of us are at least passingly familiar with them as a facet of nerd culture. There also is an existing large body of Lovecraft-inspired songs (on a CD entitled &#x201C;A Very Scary Solstice&#x201D;), that some of us already knew, and which parodied existing Christmas carols which would make them easier to learn.</li><li>Our group is predominantly Jewish (ethnically, at least). We actually had a Rationalist Seder earlier in the year. This gave me an initial &#x201C;reverse hanukkah&#x201D; idea (turning out lights to represent the darkness of the universe).</li></ul><p>These things made sense to consider for my community, and together they suggested a particular interpretation of the Sequences. They may <em>not</em> make sense for your community. If you want to do something like this, you&#x2019;ll have to look at your own community, identify your own proto-rituals, and use them to create something that feels like it&#x2019;s evolved around <em>your</em> group&#x2019;s selection pressures.</p><p><strong>Research, and Diversity of Experience</strong></p><p>Not only will culture look weird if you create it from scratch, but it&#x2019;s almost literally impossible to create an idea from scratch, period. Creativity is about combining different ideas together in interesting ways. It&#x2019;s a lot easier to do this if you have a variety of interesting concepts to work with.</p><p>I started this endeavor with an array of background knowledge - I&#x2019;ve had a lot of exposure to folk music and have written some amateur songs. I&#x2019;ve trained in visual art, communications and game design (what you might more generally call &#x201C;interactive media.&#x201D;). I&#x2019;ve looked at several religious communities and seen a few different ways that ceremonies have been put together. These were valuable disciplines to draw upon.</p><p>On top of that, I did research on traditional solstice celebrations and the origin of H.P. Lovecraft&#x2019;s ideas. I shouldn&#x2019;t need to sell Less Wrongers on the value of research, but a common pitfall of amateur artists is that they get one good idea and then assume that&#x2019;s good enough. They don&#x2019;t care about factual accuracy, they&#x2019;re just making &#x201C;art&#x201D;, and they&#x2019;re being &#x201C;creative&#x201D; which means inventing ideas from scratch. Which is horribly ineffective, <em>even if all you care about is art</em>. Doing research allows you to be <em>more</em> creative, since you get more ideas bouncing off each other. And if you choose to invoke poetic license, ignoring a particular fact, you can do so from a position of strength, knowing that the fact wasn&#x2019;t really essential to your point, or that it allowed you to emphasize a more important fact instead.</p><p>This is all the more important when you&#x2019;re creating something for rationalists, who are going to pick apart your story and identify<a href=\"https://www.lesswrong.com/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/5hoh\"> <u>everything you get wrong</u></a>.</p><p>The most powerful elements of the evening came from reading I did in the last few weeks. I hadn&#x2019;t even intended it as a solstice party when I first conceived it - it was just going to be a fun Lovecraft caroling party. The solstice, and Stonehenge in particular, turned out to be powerful symbols that supplied a concrete narrative. This was important, because vague symbols like &#x201C;light&#x201D; and &#x201C;darkness&#x201D; and &#x201C;life&#x201D; and &#x201C;death&#x201D; are so overused that you need a real, compelling story for them to feel meaningful. If I had just run with my initial idea, the result wouldn&#x2019;t have been worth posting on this site.</p><p><strong>Manage Complexity</strong></p><p>There&#x2019;s a proverb you may have heard:<em> &#x201C;A designer achieves perfection, not when there is nothing left to add, but when there is nothing left to take away.&#x201D;</em></p><p>As you research and develop ideas, those ideas will accumulate and grow more complex. This will happen for individual pieces, and it will happen to the work as a whole. The problem is that there&#x2019;s a limit to how much complexity people can handle. The consequences vary from artform to artform. In the case of a ceremony, songs can be too challenging to sing, and stories can get too wordy or long.</p><p>Sometimes, you may <em>want </em>twenty verses for a song to communicate a complex idea. You may want interesting harmonies or modulations that make it really beautiful. Your audience may even be able to handle it - the complexity can be worth it.</p><p>BUT. Just because your participants can handle one complex song does not mean they can handle 30 in a row. You need to make each piece as simple as it can be, so you have leftover complexity to use in more important places. This means you may need to revise some songs - even songs you really like, cutting out perfectly good lines that just weren&#x2019;t quite pulling their weight, so that you could afford to make another, more important section lengthier or more interesting.</p><p>Communal songs are not like regular song. They must be easy to sing. They should have a refrain that everyone can easily join in on even if they get lost. Ideally, the lyrics should be someone &#x201C;obvious&#x201D; so that people naturally end up singing the right words even if they aren&#x2019;t paying attention.</p><p>I ended up have 2-3 &#x201C;hard&#x201D; songs, another 2-3 &#x201C;medium&#x201D; songs, and the rest were deliberately less interesting but easier.</p><p>The Sequence readings were the most challenging part of this. I had to figure out which elements of them were most important and drastically cut them apart, while preserving the original impact of Eliezer&#x2019;s work.</p><p><strong>Field Testing</strong></p><p>No amount of planning can replace actual empirical testing. Unfortunately, pieces of rituals can&#x2019;t really be tested in isolation. By itself, a single activity can have a completely different feel compared to when you&#x2019;re in the middle of a long ceremony, light sources flickering and surrounded by friends. On top of this, it&#x2019;s impractical to hold a &#x201C;practice&#x201D; ceremony, since it&#x2019;d make the &#x201C;real&#x201D; ceremony feel repetitive.</p><p>But at least some amount of practice can be valuable. If you&#x2019;re writing new songs, or if you&#x2019;re going to be giving a speech: Record yourself performing, and play it back. An iPhone&#x2019;s voice memo app is a good way to do this. Not only will this give you vocal practice, but you&#x2019;ll know how long a piece is, and you may be surprised at how something <em>actually</em> sounds compared to how it sounded in your head. (I sang some songs for weeks, carefully tuning them, until the day I actually recorded myself and realized a bunch of obvious problems I had been ignoring)</p><p>You&#x2019;ll want at least some practice getting other people to participate. Getting people to try out a song by itself can feel a little awkward, but I managed a decent test at a meetup. I started by asking people to suggest songs they liked that were on-theme. We played them on youtube, and sang along karaoke style. This got everyone&#x2019;s energy up, which gave me more confidence to try singing unfamiliar songs I had written, without musical backup. I learned important information about what people had an easy time singing along with and what they didn&#x2019;t. People also got a lot more excited about the event. A lot of the songs people suggested made great additions to the final ceremony.</p><p>I had another important source of information from earlier in the year - the NYC had also done a Rationalist Seder in the spring. This actually had the opposite emotional arc than what I was going for - wine drinking is built into the Seder, and it becomes more jovial over time. I wanted things to start jovial, but then turn very dark before they eventually became uplifting. So I kept the dinner and singing sections separate. Those who drank during dinner were particularly jubilant at the beginning, but sober by the time we reached the parts that were intentionally grim.</p><p>One thing I did NOT test was use of light sources, which turned out to be a very complex logistical problem. More on this later.</p><p><strong>Remember (and Re-Evaluate) your Goal</strong></p><p>As you work on individual sections, it can be easy to forget your original goal. Remember that each piece isn&#x2019;t just there to be awesome - it&#x2019;s there to produce an awesome overall composition.</p><p>It&#x2019;s also okay if your goal changes - mine went from &#x201C;silly Lovecraft caroling&#x201D; to &#x201C;serious (trans)humanist ceremony&#x201D; to something in between, as I gained more data. I cut out all the Lovecraft when it seemed unnecessary, and then added chunks back in when I realized there was some genuinely interesting stuff worth including. But every now and again, I made sure to step from the work, and look at the whole. Every piece there needed to contribute to a coherent vision, even if that vision was different than the one I started with. Don&#x2019;t let your personal attachments to ideas blind you - anything that isn&#x2019;t pulling its weight should be changed or cut.</p><p>Finally, do at least one read of the entire script, to check how long it is. Don&#x2019;t rely on this for information on the emotional arc (it will be very different when you&#x2019;re reading by yourself than celebrating in a group) but try and get a general sense of the flow.</p><p><strong>My Results</strong></p><p>So, how did my event actually go?</p><p>I&#x2019;m going to recommend you take a break now, and go read the actual<a href=\"http://dl.dropbox.com/u/2000477/BookOfRituals.pdf\"> <u>extended edition</u></a> and form your own opinion, before you read my self-critique. And maybe just take a while in general. This article is long, and I don&#x2019;t think it quite warrants two separate articles to split into. A breather may be good.</p><p>&#x2026;</p><p>Okay. Back?</p><p>I actually answered this in a comment in the original post - I wanted to be upfront about the good and bad things, right off the bat. Here&#x2019;s the original comment:</p><ol><li>The party was absolutely worth doing, even if it were just for general warmth, fun and togetherness</li><li>I did not personally achieve the profound feeling I was hoping for <em>at the event in particular</em>. But I did achieve it several times over while I was planning it, and I think I burned out on profundity before I actually got to the night in question. It was also warped somewhat by performance anxiety. I didn&apos;t actually feel like a participant in the event - I felt like a performer, and to some extent a scientist observing a phenomenon. I think that was mostly unique to me, although it will probably apply to anyone putting the event together for the first time.</li><li>So far I&apos;ve spoken to a few other participants after the fact. Reactions seem to range based on how susceptible you are in general to warm fuzzies (more importantly, what I&apos;ve come to call &quot;warm shivers&quot;). Everyone seems enthusiastic about doing it again, and most people seemed to have at least one moment that touched them, but different people reacted strongly to different parts of the evening.</li><li>A fairly common reaction was &quot;this was a great idea and a good execution, but I have a strong sense that MUCH more is possible.&quot; (This was my reaction as well)</li></ol><p>Some new information I&#x2019;ve gathered since then:</p><p>5) I set up an<a href=\"http://www.admonymous.com/raemon777\"> <u>anonymous feedback box</u></a> on our mailing list, to address conformity issues. I only got two comments there, one was a person who didn&#x2019;t attend who was concerned about cult-image in general, and one was specifically concerned about the Singularity song, which I&#x2019;m still reconsidering for next year.</p><p>6) There was a little too much Lovecraft. This was my fault - it was something I personally liked, which I should have been more careful not to overemphasize. In the final, extended edition of the ritual book, I removed excess Lovecraft and replaced it with other things.</p><p>7) Some mistakes were due to time constraints. The first five songs were not very good - they were pre-existing Lovecraft songs that I got off the internet. I deliberately allowed those songs to not be very good, because I knew that when the singing started, people would still be getting the hang of it. I had limited time to prepare and focused on the important parts. I&#x2019;ve altered or replaced some of the early songs in the extended edition, but they are still deliberately less important.</p><p>8) A few people reported that I (successfully) made them almost cry during the dark sections, but that I didn&#x2019;t have enough uplifting songs to finish it off.</p><p>9) Relatedly, the lynchpin song, &#x201C;Brighter Than Today,&#x201D; which I wrote for the transition from dark to light, is rather complicated to sing. I&#x2019;m on the fence of whether I should make it simpler, or just allow the transitional anthem to be complex and expect people to get better at singing it over time. I think it would lose some power if I made it a more communal-friendly song. Different participants have given me different opinions on this.</p><p>10) Light sources turned out to be complicated. Partly because we just forgot to turn them off. I solved that by including instructions in the actual booklet. Partly because we didn&#x2019;t bring enough. I&#x2019;m going to emphasize that more and ensure we have a better variety. (I left my own Lava Lamp and<a href=\"http://www.sxc.hu/pic/m/c/ca/cashsmoker/789139_lightning_ball_.jpg\"> <u>Lightning Bal</u></a>l at my parent&#x2019;s home and forgot them). But there was a harder problem that I don&#x2019;t know how to solve:</p><p>Each light source should feel dramatic when it turns off. Which essentially means that each light should be among the more &#x201C;powerful&#x201D; remaining lights. A single candle getting snuffed out is irrelevant when all the lights are on, but powerful when it&#x2019;s the last light remaining.</p><p>But there&#x2019;s only so many lights you can turn off before it becomes hard to read. This *could* be solved by using &#x201C;True&#x201D; communal songs - songs designed so you can figure them out and sing along without any text at all. But those songs tend to be louder and more boisterous. The whole point of the enroaching darkness is to become more grim.</p><p>Having more light sources may solve this problem - giving us enough to turn off while still having illumination to read by. Yet another part of the problem was a lack of table-space: there were 20 of us, and we ended up sitting in a circle of chairs. How to resolve this problem will depend a lot on who&#x2019;s participating, how well they can read in the dark, and what kind of room/table you have to work with.</p><p>11) I went back and forth on &#x201C;The Gift We Give Tomorrow,&#x201D; and how short I wanted to make it. It reads like a conversation, and if you have two people who are both familiar with it, it can probably be okay as a longer piece. But I didn&#x2019;t find someone to read it with me early enough to practice together. So I ended up shortening it dramatically, cutting out the entire first half.</p><p>By the time I did this, I had spent three weeks wallowing in existential despair, studying a lot of grim writings about the cruelty of the universe, and I had basically lost the ability to discern emotional content. I thought I could get away with cutting away everything except the &#x201C;poetic&#x201D; sections of The Gift. It turns out you shouldn&#x2019;t do this. It wasn&#x2019;t bad, but it wasn&#x2019;t as potent as it should have been.</p><p>Next year I plan on doing the &#x201C;full&#x201D; version (i.e<a href=\"https://www.lesswrong.com/lw/8o6/the_gift_we_give_tomorrow_spoken_word_finished\"> <u>the abridged version I posted here</u></a>), and just make sure I get someone to practice with.</p><p>12) On a related note: make sure you give yourself enough time to work on something like this. You don&#x2019;t just need time to write it, you need time to take a step back, let your brain recalibrate so you can properly evaluate sadness and beauty, and then still be able to revise it for a final draft.</p><p><strong>Next Year</strong></p><p>Last year, I let these ideas gestate for about 8 months before I got serious about putting this together. This year I&apos;ll be planning ahead a lot more. I&apos;ll also be setting some other things in motion, that may interact with the Solstice celebration in ways I can&apos;t predict just yet. (Among other things, possibly getting a much cooler space to conduct it in)</p><p>I aim to have found or written more and better songs, possibly replacing some of the less on-theme ones that I included this year. I hope to collaborate more with trained musicians, do more research, and improve my own design skills. I have some specific thoughts on how to address existing problems, but those may radically change as I explore new possibilities.</p><p>I also plan to have networked with other local humanist, skeptical and rational communities. I don&#x2019;t know if the end result will be a larger Less Wrong NYC community (having found people who&#x2019;d naturally gravitate to our memes), a stronger coalition of Less Wrong communities beyond NYC, or a less specific coalition of rational/skeptical/humanist groups. Satisfying the needs of multiple tribes may<a href=\"https://www.lesswrong.com/lw/y5/the_babyeating_aliens_18\"> <u>water down your values</u></a>, but I think we can find plenty of things in common with related groups of people. Depending on the direction I go in, next year&#x2019;s Solstice may be mostly the same or radically different. (I may even hold multiple ones for different target audiences).</p><p>I won&#x2019;t be posting about this on Less Wrong - I think this website should mostly focus on quality technical posts, and I know culture-building can be a turn off to some. But I am interested in collaborating with anyone who&#x2019;s interested (and if you&#x2019;re NOT interested but are slightly scared of what I&#x2019;m trying to do, I welcome you to keep an eye on me as a Rationalist Confessor). I&#x2019;ll be starting a mailing list and possibly a design blog relating to this. Send me a PM if you&#x2019;re interested.</p><p>And if you&#x2019;re not really interested the large-scale culture building, but wanted some inspiration for your own community, Less Wrong or otherwise, I hope I helped.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 2, "vtozKm5BZ8gf6zd45": 2, "izp6eeJJEg9v5zcur": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GEQyCqgu5Yhi5dTdb", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 39, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "12056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "3bbvzoRA8n6ZgbiyK", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "clumping-solstice-singalongs-in-groups-of-2-4", "canonicalPrevPostSlug": "the-value-and-danger-of-ritual", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is the third post in my ritual mini-sequence. My first article was an<a href=\"https://www.lesswrong.com/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration\"> </a><u><a href=\"https://www.lesswrong.com/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration\">emotional rallying cry around of the idea of a rational-(trans)humanist culture</a></u>. My second article<a href=\"https://www.lesswrong.com/lw/93l/the_value_and_danger_of_ritual\"> <u>examined the value and potential dangers of ritual</u></a>.</p><p>So far, I remain convinced that ritual is a valuable experience for most people. I don\u2019t know if there can or should be a unified \u201cLess Wrong\u201d culture, but I do think individual communities should consider creating their own (\u201crational-humanist\u201d or otherwise). Ritual can be a useful source of fun, comfort and inspiration for positive change. A decent heuristic for \"Should my meetup try this?\" is \"Do the people at my meetup think this sounds cool?\"</p><p>This article is both an explanation of certain design principles, and a case study of my attempt at one particular kind of community event.</p><p><strong id=\"Designing_Timelessness\">Designing Timelessness</strong></p><p>So, say your community decides to incorporate some ritual. How do you do that?</p><p>Ritual-space is large, and includes things as simple as \"Pass around dark chocolate at the beginning of meetups.\" You probably already have <em>some </em>kind of ritual going on. Over time, these small traditions can accumulate into something interesting and comforting. But if you\u2019re like me, you want something more powerful - you want the gravitas of an ancient cultural cornerstone, and you want it <em>now.</em></p><p>This is... a challenge.</p><p>\u201cCreating tradition\u201d almost feels like an oxymoron. To be effective, it has to have a timeless quality about it. It doesn't have to actually be <em>ancient, </em>but it needs to feel resonant, familiar and personal. Most ritual is created through a combination of artistic skill and memetic evolution. A few central traditions begin the process (say, a particular feast, prayer and/or anthem), and then the ritual begins to spread from family to family, adapting in response to memetic selection pressure, filling specific niches, often resulting in bizarre injokes and absurdities. The absurdities can actually help bind a community together, as a badge of pride.</p><p>Powerful governments and religions can attempt to steer this. The Catholic church votes on deliberate changes to their ceremonies. But the church could not have intentionally created Modern Western Christmas - a<a href=\"http://en.wikipedia.org/wiki/Christmas\"> <u>bizarre gestalt of pagan and christian mythology</u></a>,<a href=\"http://en.wikipedia.org/wiki/A_Visit_from_St._Nicholas\"> <u>random stories</u></a>, eventually infused with modern capitalism to become a monolithic culture and industry.</p><p>One of the hardest things an artist can do is create a work that <em>appears</em> to have already been weathered by natural forces. It\u2019s much easier to create a<a href=\"http://www.rockpapershotgun.com/images/oct07/valve/portal/port0.jpg\"> <u>clean room</u></a> than a<a href=\"http://cdn.steampowered.com/v/gfx/apps/620/ss_f3f6787d74739d3b2ec8a484b5c994b3d31ef325.1920x1080.jpg?t=1324487993\"> <u>crumbling ruin</u></a>. The weight of ancient cultural cornerstones is built out of a lot of subtle details, and if you get them wrong, you\u2019ll get a ridiculous, uncanny-valley effect (especially if you were taking everything really seriously).</p><p>But I think it can be done. The Solstice celebration I put together was a solid first attempt, and I feel that much more is possible. I\u2019m going to walk through my design process, and then discuss what areas I think needed more work. I recently finished the<a href=\"http://dl.dropbox.com/u/2000477/BookOfRituals.pdf\"> <u>Extended Edition With Commentary</u></a> of the evening\u2019s songbook. You can either look it over now, or after reading this article. (Ordinarily I\u2019d say art should stand on its own without explanation, but since the actual \u201cart\u201d was an interactive ceremony, I don\u2019t think it matters much. The book <em>is</em> intended to be its own kind of art, but I\u2019ll leave it up to you.)</p><p>\u2026</p><p>(Pausing here a moment so that the people who wanted to look over the book first have a second chance to do so, before being carried forward by inertia.)</p><p>\u2026</p><p>All right. These were among my biggest considerations, when creating the Solstice celebration:</p><ul><li>Have a Goal</li><li>Build on the Familiar</li><li>Do Research, and Cultivate Diversity of Experience</li><li>Manage Complexity</li><li>Field Testing</li><li>Remember (And Re-Evaluate) your Goal</li></ul><p><strong id=\"Have_A_Goal\">Have A Goal</strong></p><p>My primary goal for the Solstice was personal, and perhaps selfish: I wanted a particular, profound, intense experience. I\u2019d had pieces of it in the past - communal singing, tribal belonging, reading beautiful prose that resonated with me. I\u2019ve been to religious events and felt a hint of their potency. I wanted to weave all these elements into a single experience.</p><p>A related goal, not quite so selfish, was wanting to channel this power into changing myself, to inspire myself to be the kind of person I wanted to be.</p><p>Closely intertwined with those goals was the desire to create a fun, and hopefully moving experience for my community. In some ways this was instrumental to the first goals, but the event wouldn\u2019t have worked if I hadn\u2019t genuinely cared about creating something that everyone could enjoy together, for their own sake.</p><p>An unrelated goal, which I had to take care not to override more important ones, is that I\u2019ve been wanting to have a Cthulhu-caroling party since like forever.</p><p>It took several months for these to weave into a single, unified plan for the evening. I\u2019ll return to that in a moment.</p><p><strong id=\"Build_on_the_Familiar\">Build on the Familiar</strong></p><p>Culture can be created, but not from nothing. You\u2019ll need to work off existing ideas that your tribe already shares.</p><p>One of the most important functions that tradition and ritual serve is to comfort. From what I've read and experienced, humans are hardwired to feel on-edge when they're in an unfamiliar situation. Uncertainty is a heuristic for danger - both to your physical life, and ostracization from the group (ultimately making it harder to find a mate). My accompanying just-so story is that repeated social traditions make your brain feel safe - you\u2019re surrounded by people doing familiar things you understand, so you don\u2019t have worry about suddenly getting kicked out of the group for no reason. You\u2019re also surrounded by friends that could probably protect you if a tiger suddenly jumped out of the woods and attacked.</p><p>So if you want to induce that warm comfort, you\u2019ll need to be working with memes and activities that feel familiar. The NYC meetup has some diverse values, but we share a vision of the power of human achievement, and a future that is better specifically because of scientific progress. We also share an understanding of how dangerous and cruel the universe can be. Most of us have had these beliefs for a while, but it was Eliezer\u2019s Sequences that crystalized them into a coherent, specific and moving worldview, and brought us together as a community.</p><p>So the Sequences provided the content. But I turned to existing religious rituals for the structure. I knew that my family\u2019s Christmas Eve (where we feast, then gather and sing carols for hours, with a gradual emotion arc from silly-to-serious) was a good frame to build around. I\u2019ve also had experience with Catholic Mass and a Seder, which incorporated stories alongside songs. These sorts of events are ubiquitous throughout our culture. Even if you don\u2019t have personal experience with them, you\u2019ve probably grown up with ingrained stereotypes about them and vaguely identify them as \u201cnormal.\u201d</p><p>I think this was the single most important point - we had content that resonated with everyone, built around a structure that was familiar.</p><p>Big ideas are important, but weaving together smaller memes was also key. I looked for songs, stories or activities that I knew were already popular among our group:</p><ul><li>Our group has a fondness for dark chocolate, which has become something of a tribal badge. I don\u2019t actually even like dark chocolate that much, but it\u2019s oddly comforting to pass it around the living room.</li><li>Songs like \u201cStill Alive\u201d and \u201cRE Your Brains\u201d are crowd favorites in our group. They aren\u2019t exactly on theme, but everyone knows them, and they were woven into the evening\u2019s narrative easily enough. Monty Python and Tom Lehrer also provided some examples, which were not only familiar but are just old enough to feel slightly \u201ctraditional\u201d.</li><li>We\u2019re not all Lovecraft enthusiasts, but most of us are at least passingly familiar with them as a facet of nerd culture. There also is an existing large body of Lovecraft-inspired songs (on a CD entitled \u201cA Very Scary Solstice\u201d), that some of us already knew, and which parodied existing Christmas carols which would make them easier to learn.</li><li>Our group is predominantly Jewish (ethnically, at least). We actually had a Rationalist Seder earlier in the year. This gave me an initial \u201creverse hanukkah\u201d idea (turning out lights to represent the darkness of the universe).</li></ul><p>These things made sense to consider for my community, and together they suggested a particular interpretation of the Sequences. They may <em>not</em> make sense for your community. If you want to do something like this, you\u2019ll have to look at your own community, identify your own proto-rituals, and use them to create something that feels like it\u2019s evolved around <em>your</em> group\u2019s selection pressures.</p><p><strong id=\"Research__and_Diversity_of_Experience\">Research, and Diversity of Experience</strong></p><p>Not only will culture look weird if you create it from scratch, but it\u2019s almost literally impossible to create an idea from scratch, period. Creativity is about combining different ideas together in interesting ways. It\u2019s a lot easier to do this if you have a variety of interesting concepts to work with.</p><p>I started this endeavor with an array of background knowledge - I\u2019ve had a lot of exposure to folk music and have written some amateur songs. I\u2019ve trained in visual art, communications and game design (what you might more generally call \u201cinteractive media.\u201d). I\u2019ve looked at several religious communities and seen a few different ways that ceremonies have been put together. These were valuable disciplines to draw upon.</p><p>On top of that, I did research on traditional solstice celebrations and the origin of H.P. Lovecraft\u2019s ideas. I shouldn\u2019t need to sell Less Wrongers on the value of research, but a common pitfall of amateur artists is that they get one good idea and then assume that\u2019s good enough. They don\u2019t care about factual accuracy, they\u2019re just making \u201cart\u201d, and they\u2019re being \u201ccreative\u201d which means inventing ideas from scratch. Which is horribly ineffective, <em>even if all you care about is art</em>. Doing research allows you to be <em>more</em> creative, since you get more ideas bouncing off each other. And if you choose to invoke poetic license, ignoring a particular fact, you can do so from a position of strength, knowing that the fact wasn\u2019t really essential to your point, or that it allowed you to emphasize a more important fact instead.</p><p>This is all the more important when you\u2019re creating something for rationalists, who are going to pick apart your story and identify<a href=\"https://www.lesswrong.com/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/5hoh\"> <u>everything you get wrong</u></a>.</p><p>The most powerful elements of the evening came from reading I did in the last few weeks. I hadn\u2019t even intended it as a solstice party when I first conceived it - it was just going to be a fun Lovecraft caroling party. The solstice, and Stonehenge in particular, turned out to be powerful symbols that supplied a concrete narrative. This was important, because vague symbols like \u201clight\u201d and \u201cdarkness\u201d and \u201clife\u201d and \u201cdeath\u201d are so overused that you need a real, compelling story for them to feel meaningful. If I had just run with my initial idea, the result wouldn\u2019t have been worth posting on this site.</p><p><strong id=\"Manage_Complexity\">Manage Complexity</strong></p><p>There\u2019s a proverb you may have heard:<em> \u201cA designer achieves perfection, not when there is nothing left to add, but when there is nothing left to take away.\u201d</em></p><p>As you research and develop ideas, those ideas will accumulate and grow more complex. This will happen for individual pieces, and it will happen to the work as a whole. The problem is that there\u2019s a limit to how much complexity people can handle. The consequences vary from artform to artform. In the case of a ceremony, songs can be too challenging to sing, and stories can get too wordy or long.</p><p>Sometimes, you may <em>want </em>twenty verses for a song to communicate a complex idea. You may want interesting harmonies or modulations that make it really beautiful. Your audience may even be able to handle it - the complexity can be worth it.</p><p>BUT. Just because your participants can handle one complex song does not mean they can handle 30 in a row. You need to make each piece as simple as it can be, so you have leftover complexity to use in more important places. This means you may need to revise some songs - even songs you really like, cutting out perfectly good lines that just weren\u2019t quite pulling their weight, so that you could afford to make another, more important section lengthier or more interesting.</p><p>Communal songs are not like regular song. They must be easy to sing. They should have a refrain that everyone can easily join in on even if they get lost. Ideally, the lyrics should be someone \u201cobvious\u201d so that people naturally end up singing the right words even if they aren\u2019t paying attention.</p><p>I ended up have 2-3 \u201chard\u201d songs, another 2-3 \u201cmedium\u201d songs, and the rest were deliberately less interesting but easier.</p><p>The Sequence readings were the most challenging part of this. I had to figure out which elements of them were most important and drastically cut them apart, while preserving the original impact of Eliezer\u2019s work.</p><p><strong id=\"Field_Testing\">Field Testing</strong></p><p>No amount of planning can replace actual empirical testing. Unfortunately, pieces of rituals can\u2019t really be tested in isolation. By itself, a single activity can have a completely different feel compared to when you\u2019re in the middle of a long ceremony, light sources flickering and surrounded by friends. On top of this, it\u2019s impractical to hold a \u201cpractice\u201d ceremony, since it\u2019d make the \u201creal\u201d ceremony feel repetitive.</p><p>But at least some amount of practice can be valuable. If you\u2019re writing new songs, or if you\u2019re going to be giving a speech: Record yourself performing, and play it back. An iPhone\u2019s voice memo app is a good way to do this. Not only will this give you vocal practice, but you\u2019ll know how long a piece is, and you may be surprised at how something <em>actually</em> sounds compared to how it sounded in your head. (I sang some songs for weeks, carefully tuning them, until the day I actually recorded myself and realized a bunch of obvious problems I had been ignoring)</p><p>You\u2019ll want at least some practice getting other people to participate. Getting people to try out a song by itself can feel a little awkward, but I managed a decent test at a meetup. I started by asking people to suggest songs they liked that were on-theme. We played them on youtube, and sang along karaoke style. This got everyone\u2019s energy up, which gave me more confidence to try singing unfamiliar songs I had written, without musical backup. I learned important information about what people had an easy time singing along with and what they didn\u2019t. People also got a lot more excited about the event. A lot of the songs people suggested made great additions to the final ceremony.</p><p>I had another important source of information from earlier in the year - the NYC had also done a Rationalist Seder in the spring. This actually had the opposite emotional arc than what I was going for - wine drinking is built into the Seder, and it becomes more jovial over time. I wanted things to start jovial, but then turn very dark before they eventually became uplifting. So I kept the dinner and singing sections separate. Those who drank during dinner were particularly jubilant at the beginning, but sober by the time we reached the parts that were intentionally grim.</p><p>One thing I did NOT test was use of light sources, which turned out to be a very complex logistical problem. More on this later.</p><p><strong id=\"Remember__and_Re_Evaluate__your_Goal\">Remember (and Re-Evaluate) your Goal</strong></p><p>As you work on individual sections, it can be easy to forget your original goal. Remember that each piece isn\u2019t just there to be awesome - it\u2019s there to produce an awesome overall composition.</p><p>It\u2019s also okay if your goal changes - mine went from \u201csilly Lovecraft caroling\u201d to \u201cserious (trans)humanist ceremony\u201d to something in between, as I gained more data. I cut out all the Lovecraft when it seemed unnecessary, and then added chunks back in when I realized there was some genuinely interesting stuff worth including. But every now and again, I made sure to step from the work, and look at the whole. Every piece there needed to contribute to a coherent vision, even if that vision was different than the one I started with. Don\u2019t let your personal attachments to ideas blind you - anything that isn\u2019t pulling its weight should be changed or cut.</p><p>Finally, do at least one read of the entire script, to check how long it is. Don\u2019t rely on this for information on the emotional arc (it will be very different when you\u2019re reading by yourself than celebrating in a group) but try and get a general sense of the flow.</p><p><strong id=\"My_Results\">My Results</strong></p><p>So, how did my event actually go?</p><p>I\u2019m going to recommend you take a break now, and go read the actual<a href=\"http://dl.dropbox.com/u/2000477/BookOfRituals.pdf\"> <u>extended edition</u></a> and form your own opinion, before you read my self-critique. And maybe just take a while in general. This article is long, and I don\u2019t think it quite warrants two separate articles to split into. A breather may be good.</p><p>\u2026</p><p>Okay. Back?</p><p>I actually answered this in a comment in the original post - I wanted to be upfront about the good and bad things, right off the bat. Here\u2019s the original comment:</p><ol><li>The party was absolutely worth doing, even if it were just for general warmth, fun and togetherness</li><li>I did not personally achieve the profound feeling I was hoping for <em>at the event in particular</em>. But I did achieve it several times over while I was planning it, and I think I burned out on profundity before I actually got to the night in question. It was also warped somewhat by performance anxiety. I didn't actually feel like a participant in the event - I felt like a performer, and to some extent a scientist observing a phenomenon. I think that was mostly unique to me, although it will probably apply to anyone putting the event together for the first time.</li><li>So far I've spoken to a few other participants after the fact. Reactions seem to range based on how susceptible you are in general to warm fuzzies (more importantly, what I've come to call \"warm shivers\"). Everyone seems enthusiastic about doing it again, and most people seemed to have at least one moment that touched them, but different people reacted strongly to different parts of the evening.</li><li>A fairly common reaction was \"this was a great idea and a good execution, but I have a strong sense that MUCH more is possible.\" (This was my reaction as well)</li></ol><p>Some new information I\u2019ve gathered since then:</p><p>5) I set up an<a href=\"http://www.admonymous.com/raemon777\"> <u>anonymous feedback box</u></a> on our mailing list, to address conformity issues. I only got two comments there, one was a person who didn\u2019t attend who was concerned about cult-image in general, and one was specifically concerned about the Singularity song, which I\u2019m still reconsidering for next year.</p><p>6) There was a little too much Lovecraft. This was my fault - it was something I personally liked, which I should have been more careful not to overemphasize. In the final, extended edition of the ritual book, I removed excess Lovecraft and replaced it with other things.</p><p>7) Some mistakes were due to time constraints. The first five songs were not very good - they were pre-existing Lovecraft songs that I got off the internet. I deliberately allowed those songs to not be very good, because I knew that when the singing started, people would still be getting the hang of it. I had limited time to prepare and focused on the important parts. I\u2019ve altered or replaced some of the early songs in the extended edition, but they are still deliberately less important.</p><p>8) A few people reported that I (successfully) made them almost cry during the dark sections, but that I didn\u2019t have enough uplifting songs to finish it off.</p><p>9) Relatedly, the lynchpin song, \u201cBrighter Than Today,\u201d which I wrote for the transition from dark to light, is rather complicated to sing. I\u2019m on the fence of whether I should make it simpler, or just allow the transitional anthem to be complex and expect people to get better at singing it over time. I think it would lose some power if I made it a more communal-friendly song. Different participants have given me different opinions on this.</p><p>10) Light sources turned out to be complicated. Partly because we just forgot to turn them off. I solved that by including instructions in the actual booklet. Partly because we didn\u2019t bring enough. I\u2019m going to emphasize that more and ensure we have a better variety. (I left my own Lava Lamp and<a href=\"http://www.sxc.hu/pic/m/c/ca/cashsmoker/789139_lightning_ball_.jpg\"> <u>Lightning Bal</u></a>l at my parent\u2019s home and forgot them). But there was a harder problem that I don\u2019t know how to solve:</p><p>Each light source should feel dramatic when it turns off. Which essentially means that each light should be among the more \u201cpowerful\u201d remaining lights. A single candle getting snuffed out is irrelevant when all the lights are on, but powerful when it\u2019s the last light remaining.</p><p>But there\u2019s only so many lights you can turn off before it becomes hard to read. This *could* be solved by using \u201cTrue\u201d communal songs - songs designed so you can figure them out and sing along without any text at all. But those songs tend to be louder and more boisterous. The whole point of the enroaching darkness is to become more grim.</p><p>Having more light sources may solve this problem - giving us enough to turn off while still having illumination to read by. Yet another part of the problem was a lack of table-space: there were 20 of us, and we ended up sitting in a circle of chairs. How to resolve this problem will depend a lot on who\u2019s participating, how well they can read in the dark, and what kind of room/table you have to work with.</p><p>11) I went back and forth on \u201cThe Gift We Give Tomorrow,\u201d and how short I wanted to make it. It reads like a conversation, and if you have two people who are both familiar with it, it can probably be okay as a longer piece. But I didn\u2019t find someone to read it with me early enough to practice together. So I ended up shortening it dramatically, cutting out the entire first half.</p><p>By the time I did this, I had spent three weeks wallowing in existential despair, studying a lot of grim writings about the cruelty of the universe, and I had basically lost the ability to discern emotional content. I thought I could get away with cutting away everything except the \u201cpoetic\u201d sections of The Gift. It turns out you shouldn\u2019t do this. It wasn\u2019t bad, but it wasn\u2019t as potent as it should have been.</p><p>Next year I plan on doing the \u201cfull\u201d version (i.e<a href=\"https://www.lesswrong.com/lw/8o6/the_gift_we_give_tomorrow_spoken_word_finished\"> <u>the abridged version I posted here</u></a>), and just make sure I get someone to practice with.</p><p>12) On a related note: make sure you give yourself enough time to work on something like this. You don\u2019t just need time to write it, you need time to take a step back, let your brain recalibrate so you can properly evaluate sadness and beauty, and then still be able to revise it for a final draft.</p><p><strong id=\"Next_Year\">Next Year</strong></p><p>Last year, I let these ideas gestate for about 8 months before I got serious about putting this together. This year I'll be planning ahead a lot more. I'll also be setting some other things in motion, that may interact with the Solstice celebration in ways I can't predict just yet. (Among other things, possibly getting a much cooler space to conduct it in)</p><p>I aim to have found or written more and better songs, possibly replacing some of the less on-theme ones that I included this year. I hope to collaborate more with trained musicians, do more research, and improve my own design skills. I have some specific thoughts on how to address existing problems, but those may radically change as I explore new possibilities.</p><p>I also plan to have networked with other local humanist, skeptical and rational communities. I don\u2019t know if the end result will be a larger Less Wrong NYC community (having found people who\u2019d naturally gravitate to our memes), a stronger coalition of Less Wrong communities beyond NYC, or a less specific coalition of rational/skeptical/humanist groups. Satisfying the needs of multiple tribes may<a href=\"https://www.lesswrong.com/lw/y5/the_babyeating_aliens_18\"> <u>water down your values</u></a>, but I think we can find plenty of things in common with related groups of people. Depending on the direction I go in, next year\u2019s Solstice may be mostly the same or radically different. (I may even hold multiple ones for different target audiences).</p><p>I won\u2019t be posting about this on Less Wrong - I think this website should mostly focus on quality technical posts, and I know culture-building can be a turn off to some. But I am interested in collaborating with anyone who\u2019s interested (and if you\u2019re NOT interested but are slightly scared of what I\u2019m trying to do, I welcome you to keep an eye on me as a Rationalist Confessor). I\u2019ll be starting a mailing list and possibly a design blog relating to this. Send me a PM if you\u2019re interested.</p><p>And if you\u2019re not really interested the large-scale culture building, but wanted some inspiration for your own community, Less Wrong or otherwise, I hope I helped.</p>", "sections": [{"title": "Designing Timelessness", "anchor": "Designing_Timelessness", "level": 1}, {"title": "Have A Goal", "anchor": "Have_A_Goal", "level": 1}, {"title": "Build on the Familiar", "anchor": "Build_on_the_Familiar", "level": 1}, {"title": "Research, and Diversity of Experience", "anchor": "Research__and_Diversity_of_Experience", "level": 1}, {"title": "Manage Complexity", "anchor": "Manage_Complexity", "level": 1}, {"title": "Field Testing", "anchor": "Field_Testing", "level": 1}, {"title": "Remember (and Re-Evaluate) your Goal", "anchor": "Remember__and_Re_Evaluate__your_Goal", "level": 1}, {"title": "My Results", "anchor": "My_Results", "level": 1}, {"title": "Next Year", "anchor": "Next_Year", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "76 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["jES7mcPvKpfmzMTgC", "rijoxTpkSPXcTXRbN", "bkRpALFAwJQuntHiF", "n5TqCuizyJDfAPjkr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-01-11T01:52:24.831Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T03:50:03.359Z", "modifiedAt": null, "url": null, "title": "Rationality meditation theory.", "slug": "rationality-meditation-theory", "viewCount": null, "lastCommentedAt": "2017-08-29T06:39:43.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Sqv6SPDboNF84j38K", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YaHEdo9ZanWp8MAG5/rationality-meditation-theory", "pageUrlRelative": "/posts/YaHEdo9ZanWp8MAG5/rationality-meditation-theory", "linkUrl": "https://www.lesswrong.com/posts/YaHEdo9ZanWp8MAG5/rationality-meditation-theory", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20meditation%20theory.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20meditation%20theory.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYaHEdo9ZanWp8MAG5%2Frationality-meditation-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20meditation%20theory.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYaHEdo9ZanWp8MAG5%2Frationality-meditation-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYaHEdo9ZanWp8MAG5%2Frationality-meditation-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1641, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-priority:99;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:11.0pt;\n\tfont-family:\"Calibri\",\"sans-serif\";\n\tmso-ascii-font-family:Calibri;\n\tmso-ascii-theme-font:minor-latin;\n\tmso-hansi-font-family:Calibri;\n\tmso-hansi-theme-font:minor-latin;\n\tmso-bidi-font-family:\"Times New Roman\";\n\tmso-bidi-theme-font:minor-bidi;}\n</style>\n<![endif]--></p>\n<p class=\"MsoNormal\">First allow me to show you my preliminary data.: http://www.reddit.com/r/bestof/comments/obtqe/its_they_live_these_are_reddit_robots_programmed/</p>\n<p>&nbsp;</p>\n<p><strong>Introduction</strong></p>\n<p>I think I may have found  a novel use for an old technique, which may or may not have  implications for rational decision making. I am open to constructive  criticism or even deconstructive criticism if you make a sound argument.  Ultimately, I would like the experiment to be put to the test. If you  have the supplies and know-how to carry it out, then feel free to try it  and report your findings.<br /><br /><strong>The Goal</strong>:</p>\n<ul>\n<li>&nbsp;Catalyze the brainstorming process in a way that increases both the number and quality of ideas made.</li>\n</ul>\n<p><strong>Methods</strong>:</p>\n<ul>\n<li>Find a problem that needs solving. <a href=\"lw/qt/class_project/\">Unifying general relativity and quantum mechanics</a> is a good, but ambitious, example. Some more likely problems that could  be solved are: \"How might I solve my relationship problems,\" or \"how  can I advertise my company's product to its target demographic\", or  \"what are some ideas to make quick money.\"</li>\n<li>Find 2-3  rationalists who understand the problem well. They don't need to be  expert rationalists; the most important part is that they know the  difference between <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">rationalization and rationality</a>.  In the QM example above, and in most scientific applications of the  method, all players should have access to the experimental data.</li>\n<li>Assign  two of the three rationalists to the \"brainstormers\" group (name  subject to change), whose primary concern is to make logical connections  between the data to form hypotheses.</li>\n<li>Assign the odd-rationalist-out as the Confessor, whose primary concern, like in <a href=\"lw/y4/three_worlds_collide_08/\">TWC</a>,  is to preserve sanity. It is the Confessor's job to catch the  brainstormers when they make a logical leap or use biased reasoning.  Some tactics the Confessor might use are the <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">rationalist taboo</a>, the <a href=\"http://wiki.lesswrong.com/wiki/Reversal_test\">reversal test</a>, and argument from the <a href=\"lw/2k/the_least_convenient_possible_world/\">least convenient world</a><span style=\"font-size: 8pt; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;\">.</span></li>\n<li>This is a scaled up version of what the brain seems to do. We need the brainstormers and the Confessor to act the part of <a href=\"lw/20/the_apologist_and_the_revolutionary\">the Apologist and the Revolutionary</a>, respectively.</li>\n<li>The  Confessor - brainstormer dynamic is interesting in its own right, but I  believe it can be improved. Now bear with me, because the optional step  is for the brainstormers to smoke Cannabis. Not too much, but just  enough so that connections between ideas are more quickly apparent to  them. Remember, the goal is to have the brainstormers make many  connections. They need to output quantity over quality, while the  Confessor picks out anything that is quality and gently guides the  brainstormers toward more quality ideas. Think of it like <a href=\"http://en.wikipedia.org/wiki/R/K_selection_theory#r-selection_.28unstable_environments.29\">r-selected evolution</a>.</li>\n</ul>\n<ul>\n<li>Ideally,  we would split twelve rationalists into four groups of three (the  alternative is to use the same group repeatedly). Group 1 would be told  to just brainstorm the problem. Group 2 would be told to choose one  among them to be the Confessor. Group 3 and 4 would be told the same,  but their brainstormers would be given either Cannabis or a placebo.</li>\n<li>A  placebo can be made by extracting the cannabinoids using ethanol or  glycerine. All that should be left after extraction is plant matter, and  the tincture can be used later for medicinal or recreational purposes.  The placebo Cannabis and active Cannabis will have to be rolled into  joints because extraction removes some of the plant's pigmentation. If  you have access to a lab, then you might follow&nbsp; <a href=\"http://www.rexresearch.com/hhusb/hh6thc.htm#HH62\">this procedure</a>&nbsp; for the extraction; otherwise, use&nbsp; <a href=\"http://www.greenbridgemed.com/how-to-make-cannabis-tinctures-at-home/\">this guide</a>&nbsp; for doing the extraction at home. </li>\n<li>If  you don't want to go to the trouble of making the placebo, then you may  skip the control group and only do groups 1 - 3. It would be nice to  get some preliminary data, even if skewed slightly by the placebo  effect.</li>\n<li>For data collection, the Confessor will note down any  idea made by the brainstormers, marking the ones which were discarded.  After a given amount of time, enumerate the data and compare the groups.  The hypothesized result is that the smoking group will make the  greatest quantity of ideas, followed by the non-smoking partitioned  group, followed by the normal brainstorming group. It is also  hypothesized that the smoking group will make the greatest quality  ideas, due to a combination of the highly creative nature of ideas made  while high (explained below) and the Confessor's job of immediately  discrediting any faulty reasoning.</li>\n</ul>\n<p><strong>Some evidence that the Cannabis route might be a good one to pursue</strong> (more references to be added):</p>\n<ul>\n<li>There is evidence that Cannabis engages the mind in semantic Hyper-Priming<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20122742\"><sup>1</sup></a><sup>,</sup><a href=\"http://mindhacks.com/2010/03/09/how-cannabis-makes-thoughts-tumble/\"><sup>2</sup></a>,  meaning that distantly-related concepts are primed quickly after having  been exposed to an idea. For instance, a smoker might quickly respond  to the word \"fish\" with \"submarine,\" whereas someone who is sober might  respond with \"fin.\" If I understand it correctly, then this doesn't mean  the smoker <em>cannot</em> answer \"fin,\" only that the more distantly  related concepts are given a higher priority than they normally would.  One can see why this might be advantageous for brainstorming, but I  suggest to take my - and mindhack.com's - interpretation of the paper  with a grain of salt until someone with access can read it in full.</li>\n<li>*Cannabis allows erroneous perspectives to be rapidly dismissed in the light of new evidence. While high, it is <a href=\"lw/i9/the_importance_of_saying_oops/\">easy to put one's pride aside and say \"oops\"</a>.  This is made especially easy if the user has cached understanding of  the art of rationality. In other words: they will listen to the  Confessor.* &lt;-- (I haven't found any literature to support this  claim, yet. It seems true in my experience, but it might not be true for  everyone. If the brainstormers prove to be too clingy, we could alter  the method by changing the Confessor's name to Kiritsugu and having the  brainstormers agree to always defer to the Kiritsugu's better judgement.  The Kiritsugu will have to take care to examine its own judgement and  only discard the truly irrational ideas).</li>\n</ul>\n<p><strong>Anecdotal evidence:<br /></strong></p>\n<ul>\n<li>Artists, writers, and even <a href=\"http://marijuana-uses.com/mr-x/\">scientists</a> have long used Cannabis and other psychoactive drugs as a tool to make  \"insights.\" I'm defining insight as the connection and/or creation of  ideas (erroneous or otherwise), possibly due to hyper-priming. The  Confessor, in the early pioneers' case, was usually their sober self. As  Hemingway wrote, \"write drunk; edit sober.\"</li>\n<li><a href=\"http://forum.grasscity.com/real-life-stories/480214-curious-if-anyone-does-any-brainstorming-while-toking.html\">Less gifted stoners have been doing this for ages</a>&nbsp;  but they - for the most part - are completely undisciplined, believe in  dubious pseudoscience, and/or don't have a rational observer to  moderate them.</li>\n<li>This is going a bit meta, but the outline to the  outline of this idea was made while I was high. It was the first time I  smoked since having been introduced to Less Wrong and <a href=\"http://yudkowsky.net/rational/virtues\">\"The Way\"</a>, and I was surprised to find that I still had most of my wits about me. Although I would often begin down paths that were just <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">Rationalizations</a>,  I usually caught myself. In the instances where I didn't catch myself,  and it seemed like a legitimately good insight, I wrote the idea down  for future (sober) consideration.</li>\n<li>One of the good, practical,  non-meta insights I made that night was a life plan. My plan up until  this point had been to finish my undergraduate degree and then  immediately go to grad school, relying on my schooling and a bit of luck  to maybe hopefully turn into a somewhat-successful scientist somewhere  along the road. The problem is that I suffer from quite a bit of  procrastination, in part because I don't know exactly what I want to  do. I don't have any strong passions or any real motivation. My college  career, so far, has been an uphill battle against crippling akrasia.</li>\n<li>Aided  by Cannabis, I finally saw the obvious: I need to make an effort to  find a passion. My new plan is to get a job as a computer programmer  after finishing undergrad, but to continue self-teaching in Biology and  other sciences. I've already taken the first step by having Computer  Science as my minor, and I can help my resume along <em>right now</em> by getting involved in open source projects. As for self-teaching, that's made easy by open courseware like that found on <a href=\"http://www.khanacademy.org/\">Khan Academy</a>, <a href=\"http://ocw.mit.edu/index.htm\">MIT</a>, and <a href=\"http://www.openculture.com/freeonlinecourses\">other places</a>,  and I always have the old-fashioned solution of just reading textbooks.  After following my interests for a while and learning what things I  really, really like to learn about, <em>then</em> I'll go to grad school with an actual PhD thesis in mind and money in the bank. </li>\n<li>I'm  attributing these insights (the life plan, some other ideas I'm not  mentioning, and even the hypothesis itself) to hyper-priming and later  editing, but they might have just been made because I was focused on the  problems. Hence the need for an experimentally-controlled test. </li>\n</ul>\n<p><strong>Conclusion</strong></p>\n<ul>\n<li>Cannabis  allows connections to be made between concepts which normally seem  unrelated. This is an experience commonly reported by users, and  experimentally verified. Some of these connections will inevitably be  false, but others might be true, and a third party - a Confessor - might  be able to distinguish truth from falsehood. Whether the Confessor -  brainstormer dynamic is any more efficient or productive than a normal  brainstorming session is an open question, and the only way to really  know is to test the hypothesis.</li>\n</ul>\n<p><strong>References</strong></p>\n<div class=\"csl-bib-body\" style=\"line-height: 2; padding-left: 2em; text-indent: -2em;\">\n<div class=\"csl-entry\">How cannabis makes thoughts&nbsp;tumble. (n.d.).<em>Mind Hacks</em>. Retrieved from http://mindhacks.com/2010/03/09/how-cannabis-makes-thoughts-tumble/</div>\n<div class=\"csl-entry\">Morgan,  C. J. A., Rothwell, E., Atkinson, H., Mason, O., &amp; Curran, H. V.  (2010). Hyper-priming in cannabis users: a naturalistic study of the  effects of cannabis on semantic memory function. <em>Psychiatry Research</em>, <em>176</em>(2-3), 213-218. doi:10.1016/j.psychres.2008.09.002</div>\n</div>\n<p><strong>What I'm missing. To be included later:<br /></strong></p>\n<ul>\n<li>References to the benefits and techniques of traditional brainstorming. In lieu of that, for now, here's&nbsp; <a href=\"lw/ka/hold_off_on_proposing_solutions/\">this</a>&nbsp; and&nbsp; <a href=\"lw/hu/the_third_alternative/\">this </a>.</li>\n<li>More references to Cannabis research.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YaHEdo9ZanWp8MAG5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -9, "extendedScore": null, "score": 8.296984824422343e-07, "legacy": true, "legacyId": "12070", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-priority:99;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:11.0pt;\n\tfont-family:\"Calibri\",\"sans-serif\";\n\tmso-ascii-font-family:Calibri;\n\tmso-ascii-theme-font:minor-latin;\n\tmso-hansi-font-family:Calibri;\n\tmso-hansi-theme-font:minor-latin;\n\tmso-bidi-font-family:\"Times New Roman\";\n\tmso-bidi-theme-font:minor-bidi;}\n</style>\n<![endif]--></p>\n<p class=\"MsoNormal\">First allow me to show you my preliminary data.: http://www.reddit.com/r/bestof/comments/obtqe/its_they_live_these_are_reddit_robots_programmed/</p>\n<p>&nbsp;</p>\n<p><strong id=\"Introduction\">Introduction</strong></p>\n<p>I think I may have found  a novel use for an old technique, which may or may not have  implications for rational decision making. I am open to constructive  criticism or even deconstructive criticism if you make a sound argument.  Ultimately, I would like the experiment to be put to the test. If you  have the supplies and know-how to carry it out, then feel free to try it  and report your findings.<br><br><strong>The Goal</strong>:</p>\n<ul>\n<li>&nbsp;Catalyze the brainstorming process in a way that increases both the number and quality of ideas made.</li>\n</ul>\n<p><strong>Methods</strong>:</p>\n<ul>\n<li>Find a problem that needs solving. <a href=\"lw/qt/class_project/\">Unifying general relativity and quantum mechanics</a> is a good, but ambitious, example. Some more likely problems that could  be solved are: \"How might I solve my relationship problems,\" or \"how  can I advertise my company's product to its target demographic\", or  \"what are some ideas to make quick money.\"</li>\n<li>Find 2-3  rationalists who understand the problem well. They don't need to be  expert rationalists; the most important part is that they know the  difference between <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">rationalization and rationality</a>.  In the QM example above, and in most scientific applications of the  method, all players should have access to the experimental data.</li>\n<li>Assign  two of the three rationalists to the \"brainstormers\" group (name  subject to change), whose primary concern is to make logical connections  between the data to form hypotheses.</li>\n<li>Assign the odd-rationalist-out as the Confessor, whose primary concern, like in <a href=\"lw/y4/three_worlds_collide_08/\">TWC</a>,  is to preserve sanity. It is the Confessor's job to catch the  brainstormers when they make a logical leap or use biased reasoning.  Some tactics the Confessor might use are the <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">rationalist taboo</a>, the <a href=\"http://wiki.lesswrong.com/wiki/Reversal_test\">reversal test</a>, and argument from the <a href=\"lw/2k/the_least_convenient_possible_world/\">least convenient world</a><span style=\"font-size: 8pt; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;\">.</span></li>\n<li>This is a scaled up version of what the brain seems to do. We need the brainstormers and the Confessor to act the part of <a href=\"lw/20/the_apologist_and_the_revolutionary\">the Apologist and the Revolutionary</a>, respectively.</li>\n<li>The  Confessor - brainstormer dynamic is interesting in its own right, but I  believe it can be improved. Now bear with me, because the optional step  is for the brainstormers to smoke Cannabis. Not too much, but just  enough so that connections between ideas are more quickly apparent to  them. Remember, the goal is to have the brainstormers make many  connections. They need to output quantity over quality, while the  Confessor picks out anything that is quality and gently guides the  brainstormers toward more quality ideas. Think of it like <a href=\"http://en.wikipedia.org/wiki/R/K_selection_theory#r-selection_.28unstable_environments.29\">r-selected evolution</a>.</li>\n</ul>\n<ul>\n<li>Ideally,  we would split twelve rationalists into four groups of three (the  alternative is to use the same group repeatedly). Group 1 would be told  to just brainstorm the problem. Group 2 would be told to choose one  among them to be the Confessor. Group 3 and 4 would be told the same,  but their brainstormers would be given either Cannabis or a placebo.</li>\n<li>A  placebo can be made by extracting the cannabinoids using ethanol or  glycerine. All that should be left after extraction is plant matter, and  the tincture can be used later for medicinal or recreational purposes.  The placebo Cannabis and active Cannabis will have to be rolled into  joints because extraction removes some of the plant's pigmentation. If  you have access to a lab, then you might follow&nbsp; <a href=\"http://www.rexresearch.com/hhusb/hh6thc.htm#HH62\">this procedure</a>&nbsp; for the extraction; otherwise, use&nbsp; <a href=\"http://www.greenbridgemed.com/how-to-make-cannabis-tinctures-at-home/\">this guide</a>&nbsp; for doing the extraction at home. </li>\n<li>If  you don't want to go to the trouble of making the placebo, then you may  skip the control group and only do groups 1 - 3. It would be nice to  get some preliminary data, even if skewed slightly by the placebo  effect.</li>\n<li>For data collection, the Confessor will note down any  idea made by the brainstormers, marking the ones which were discarded.  After a given amount of time, enumerate the data and compare the groups.  The hypothesized result is that the smoking group will make the  greatest quantity of ideas, followed by the non-smoking partitioned  group, followed by the normal brainstorming group. It is also  hypothesized that the smoking group will make the greatest quality  ideas, due to a combination of the highly creative nature of ideas made  while high (explained below) and the Confessor's job of immediately  discrediting any faulty reasoning.</li>\n</ul>\n<p><strong>Some evidence that the Cannabis route might be a good one to pursue</strong> (more references to be added):</p>\n<ul>\n<li>There is evidence that Cannabis engages the mind in semantic Hyper-Priming<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20122742\"><sup>1</sup></a><sup>,</sup><a href=\"http://mindhacks.com/2010/03/09/how-cannabis-makes-thoughts-tumble/\"><sup>2</sup></a>,  meaning that distantly-related concepts are primed quickly after having  been exposed to an idea. For instance, a smoker might quickly respond  to the word \"fish\" with \"submarine,\" whereas someone who is sober might  respond with \"fin.\" If I understand it correctly, then this doesn't mean  the smoker <em>cannot</em> answer \"fin,\" only that the more distantly  related concepts are given a higher priority than they normally would.  One can see why this might be advantageous for brainstorming, but I  suggest to take my - and mindhack.com's - interpretation of the paper  with a grain of salt until someone with access can read it in full.</li>\n<li>*Cannabis allows erroneous perspectives to be rapidly dismissed in the light of new evidence. While high, it is <a href=\"lw/i9/the_importance_of_saying_oops/\">easy to put one's pride aside and say \"oops\"</a>.  This is made especially easy if the user has cached understanding of  the art of rationality. In other words: they will listen to the  Confessor.* &lt;-- (I haven't found any literature to support this  claim, yet. It seems true in my experience, but it might not be true for  everyone. If the brainstormers prove to be too clingy, we could alter  the method by changing the Confessor's name to Kiritsugu and having the  brainstormers agree to always defer to the Kiritsugu's better judgement.  The Kiritsugu will have to take care to examine its own judgement and  only discard the truly irrational ideas).</li>\n</ul>\n<p><strong id=\"Anecdotal_evidence_\">Anecdotal evidence:<br></strong></p>\n<ul>\n<li>Artists, writers, and even <a href=\"http://marijuana-uses.com/mr-x/\">scientists</a> have long used Cannabis and other psychoactive drugs as a tool to make  \"insights.\" I'm defining insight as the connection and/or creation of  ideas (erroneous or otherwise), possibly due to hyper-priming. The  Confessor, in the early pioneers' case, was usually their sober self. As  Hemingway wrote, \"write drunk; edit sober.\"</li>\n<li><a href=\"http://forum.grasscity.com/real-life-stories/480214-curious-if-anyone-does-any-brainstorming-while-toking.html\">Less gifted stoners have been doing this for ages</a>&nbsp;  but they - for the most part - are completely undisciplined, believe in  dubious pseudoscience, and/or don't have a rational observer to  moderate them.</li>\n<li>This is going a bit meta, but the outline to the  outline of this idea was made while I was high. It was the first time I  smoked since having been introduced to Less Wrong and <a href=\"http://yudkowsky.net/rational/virtues\">\"The Way\"</a>, and I was surprised to find that I still had most of my wits about me. Although I would often begin down paths that were just <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">Rationalizations</a>,  I usually caught myself. In the instances where I didn't catch myself,  and it seemed like a legitimately good insight, I wrote the idea down  for future (sober) consideration.</li>\n<li>One of the good, practical,  non-meta insights I made that night was a life plan. My plan up until  this point had been to finish my undergraduate degree and then  immediately go to grad school, relying on my schooling and a bit of luck  to maybe hopefully turn into a somewhat-successful scientist somewhere  along the road. The problem is that I suffer from quite a bit of  procrastination, in part because I don't know exactly what I want to  do. I don't have any strong passions or any real motivation. My college  career, so far, has been an uphill battle against crippling akrasia.</li>\n<li>Aided  by Cannabis, I finally saw the obvious: I need to make an effort to  find a passion. My new plan is to get a job as a computer programmer  after finishing undergrad, but to continue self-teaching in Biology and  other sciences. I've already taken the first step by having Computer  Science as my minor, and I can help my resume along <em>right now</em> by getting involved in open source projects. As for self-teaching, that's made easy by open courseware like that found on <a href=\"http://www.khanacademy.org/\">Khan Academy</a>, <a href=\"http://ocw.mit.edu/index.htm\">MIT</a>, and <a href=\"http://www.openculture.com/freeonlinecourses\">other places</a>,  and I always have the old-fashioned solution of just reading textbooks.  After following my interests for a while and learning what things I  really, really like to learn about, <em>then</em> I'll go to grad school with an actual PhD thesis in mind and money in the bank. </li>\n<li>I'm  attributing these insights (the life plan, some other ideas I'm not  mentioning, and even the hypothesis itself) to hyper-priming and later  editing, but they might have just been made because I was focused on the  problems. Hence the need for an experimentally-controlled test. </li>\n</ul>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<ul>\n<li>Cannabis  allows connections to be made between concepts which normally seem  unrelated. This is an experience commonly reported by users, and  experimentally verified. Some of these connections will inevitably be  false, but others might be true, and a third party - a Confessor - might  be able to distinguish truth from falsehood. Whether the Confessor -  brainstormer dynamic is any more efficient or productive than a normal  brainstorming session is an open question, and the only way to really  know is to test the hypothesis.</li>\n</ul>\n<p><strong id=\"References\">References</strong></p>\n<div class=\"csl-bib-body\" style=\"line-height: 2; padding-left: 2em; text-indent: -2em;\">\n<div class=\"csl-entry\">How cannabis makes thoughts&nbsp;tumble. (n.d.).<em>Mind Hacks</em>. Retrieved from http://mindhacks.com/2010/03/09/how-cannabis-makes-thoughts-tumble/</div>\n<div class=\"csl-entry\">Morgan,  C. J. A., Rothwell, E., Atkinson, H., Mason, O., &amp; Curran, H. V.  (2010). Hyper-priming in cannabis users: a naturalistic study of the  effects of cannabis on semantic memory function. <em>Psychiatry Research</em>, <em>176</em>(2-3), 213-218. doi:10.1016/j.psychres.2008.09.002</div>\n</div>\n<p><strong id=\"What_I_m_missing__To_be_included_later_\">What I'm missing. To be included later:<br></strong></p>\n<ul>\n<li>References to the benefits and techniques of traditional brainstorming. In lieu of that, for now, here's&nbsp; <a href=\"lw/ka/hold_off_on_proposing_solutions/\">this</a>&nbsp; and&nbsp; <a href=\"lw/hu/the_third_alternative/\">this </a>.</li>\n<li>More references to Cannabis research.</li>\n</ul>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Anecdotal evidence:", "anchor": "Anecdotal_evidence_", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"title": "What I'm missing. To be included later:", "anchor": "What_I_m_missing__To_be_included_later_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xAXrEpF5FYjwqKMfZ", "HawFh7RvDM4RyoJ2d", "neQ7eXuaXpiYw7SBy", "ZiQqsgGX6a42Sfpii", "wCqfCLs8z5Qw4GbKS", "uHYYA32CKgKT3FagE", "erGipespbbzdG5zYb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T04:02:58.006Z", "modifiedAt": null, "url": null, "title": "The Gift I Give Tomorrow", "slug": "the-gift-i-give-tomorrow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.878Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rhj3BZLpC69m9mr4B/the-gift-i-give-tomorrow", "pageUrlRelative": "/posts/rhj3BZLpC69m9mr4B/the-gift-i-give-tomorrow", "linkUrl": "https://www.lesswrong.com/posts/rhj3BZLpC69m9mr4B/the-gift-i-give-tomorrow", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Gift%20I%20Give%20Tomorrow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Gift%20I%20Give%20Tomorrow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frhj3BZLpC69m9mr4B%2Fthe-gift-i-give-tomorrow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Gift%20I%20Give%20Tomorrow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frhj3BZLpC69m9mr4B%2Fthe-gift-i-give-tomorrow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frhj3BZLpC69m9mr4B%2Fthe-gift-i-give-tomorrow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3195, "htmlBody": "<p>&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">This is the final post in my Ritual Mini-Sequence. Previous posts include the&nbsp;<a href=\"http://www.google.com/url?q=http%3A%2F%2Flesswrong.com%2Flw%2F8x5%2Fritual_report_nyc_less_wrong_solstice_celebration&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFmgqVa-yXy3Ls21q3D8aFgoTYl6g\">Introduction</a>, a discussion on the&nbsp;<a href=\"http://www.google.com/url?q=http%3A%2F%2Flesswrong.com%2Flw%2F93l%2Fthe_value_and_danger_of_ritual&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHWYxZtmPpYnaB35KjMQ-yKubkBwQ\">Value (and Danger) of Ritual</a>, and <a href=\"/lw/9aw/designing_ritual/\">How to Design Ritual Ceremonies</a> that reflect your values.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">I wrote this as a concluding essay in the&nbsp;<a href=\"http://www.google.com/url?q=http%3A%2F%2Fdl.dropbox.com%2Fu%2F2000477%2FBookOfRituals.pdf&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEVw1WI67GB9abcuCDlgQVE2LqpSg\">Solstice ritual book</a>. It was intended to be at least&nbsp;comprehensible&nbsp;to people who weren&rsquo;t already familiar with our memes, and to communicate why I thought this was important. It builds upon themes from the ritual book, and in particular, the readings of&nbsp;<a href=\"http://www.google.com/url?q=http%3A%2F%2Flesswrong.com%2Flw%2Fuk%2Fbeyond_the_reach_of_god&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNH5xez_xaUBfXlaUaN9i7gPvUeu5g\">Beyond the Reach of God</a>&nbsp;and&nbsp;<a href=\"http://www.google.com/url?q=http%3A%2F%2Flesswrong.com%2Flw%2Fsa%2Fthe_gift_we_give_to_tomorrow%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEKg8eyaJSfE5Ec_vyjE2GlkDV_sw\">The Gift We Give to Tomorrow</a>. Working on this essay was&nbsp;<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">transformative to me - it allowed me to finally bypass my scope insensitivity and other biases, so that I could evaluate organizations like the Singularity Institute with fairness. I haven&rsquo;t yet decided what to do with my charitable dollars - it&rsquo;s a complex problem. But I&rsquo;ve overcome my emotional restistance to the idea of fighting X-Risk.</span></p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">I don&rsquo;t know if that was due to the words themselves, or to the process I had to go through to write them, but I hope others may benefit from this.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<hr />\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">I thought &lsquo;The Gift We Give to Tomorrow&rsquo; was incredibly beautiful when I first read it. I actually cried. I wanted to share it with friends and family, except that work ONLY has meaning in the context of the Sequences. Practically every line is a hyperlink to an important, earlier point, and without many hours of previous reading, it just won&rsquo;t have the impact. But to me, it felt like the perfect endcap to everything the Sequences covered, taking all of the facts and ideas and weaving them into a coherent, poetic narrative that left me feeling satisfied with my place in the world.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />Except that... I wasn&rsquo;t sure that it actually said anything.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\"><a id=\"more\"></a></p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">And when I showed it to a few other people, they reacted similarly: &ldquo;This is pretty, but what does it mean?&rdquo; I knew I wanted to include it in our Solstice celebration, if for no other reason than &ldquo;it was pretty.&rdquo; Particularly pretty in a particular way that seemed right for the occassion. But I&rsquo;m wary about things that seem beautiful and moving without really understanding why, especially when those things become part of your worldview, perhaps subtly impacting your decisions.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">In order to use The Gift as part of the Solstice, it needed to be pared down. It&rsquo;s not designed to be read out loud. This meant I needed to study it in detail, figuring out what made it beautiful so I could be sure to capture that part while pruning away the words that were difficult to pronounce or read in dim candlelight.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />Shortly afterwards, I began the same work with &ldquo;Beyond the Reach of God.&rdquo;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />Unlike The Gift, Beyond the Reach of God is important and powerful for very obvious reasons. If you have something you value more than your own happiness, if you care about your children&rsquo;s children, then you need to understand that there is no God. Or at the very least, for whatever reason, for whatever mysterious end that you don&rsquo;t understand, God doesn&rsquo;t intervene.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">If your loved ones are being tortured, or are dying of illness, or getting run over by a car, God will not save them. The actions that matter are the ones that impact the physical world, the world of interlinked causes that we can perceive. The beliefs that ultimately matter, when you care about more than your own subjective happiness, are the beliefs that allow you to make accurate predictions about the future. These beliefs allow you to establish the right social policies to protect your children from harm. They allow you to find the right medicine and treatment to keep your aging parents alive and healthy, both mentally and physically, for as long as possible. To keep the people you love part of your life. And to keep yourself part of theirs.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />Unlike some in this community, I don&rsquo;t entirely dismiss unprovable, comforting beliefs, so long as you have the right compartmentalization to keep them separate from your other decision making processes. A vague, comforting belief in an afterlife, or in a &lsquo;natural, cyclical order of things&rsquo;... returning to the earth and pushing up daisies... it can be useful to help accept the things you cannot change.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">We still live in a world where Death exists. There are things we can&rsquo;t change. Yet.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />And those things can be horrible, and I don&rsquo;t begrudge anyone a tool to work through them.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />But if someone&rsquo;s vague, comforting beliefs lead them to let a person go, not because they&rsquo;d done everything they could to save them, but because they had a notion that they&rsquo;d be together somehow in a supernatural world... if a belief leads someone to believe that they couldn&rsquo;t change something that they, in fact, could have...</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />No. I can&rsquo;t condone that.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />It can be disturbing, going down the rationality rabbit hole. I started by thinking &ldquo;I want to be succeeding at life,&rdquo; and learned about a few biases that are affecting me, and I made some better choices, and that was good. But it wasn&rsquo;t fully satisfying. I needed to form some coherent long term goals. Someone in my position might then say &ldquo;Alright, I want to be more succesful at my career.&rdquo;</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">But then maybe they realize that success at their career wasn&rsquo;t actually what was most important to them. They didn&rsquo;t need that money, what they wanted was the ability to purchase things that make them happy, and support their family, and have the security to periodically do fun projects. The career was just one way of doing that. And it may not have been the best way. And suddenly they&rsquo;re open to the entirety of possibility-space, a million different paths they could take that might or might not leave them satisfied. And they don&rsquo;t have any of the tools they need to figure out which ones to take. Some of those tools have already been invented, and they just need to find them. Others, they may need to invent for themselves.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />The problem is that most people don&rsquo;t have a good understanding of their values.. &ldquo;Be Happy&rdquo; is vague, so is &ldquo;Have a nice family,&rdquo; so is &ldquo;Make the world a better place.&rdquo; Vaguest of all is &ldquo;Some combination of the above.&rdquo;</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">If you&rsquo;re going down the rationality rabbit hole, you need to start figuring out your REAL values, instead of reciting cached thoughts that you&rsquo;ve picked up from society. You might start exploring cognitive science to give you some insight into how your mind works. And then you&rsquo;d start to learn that the mind is a machine, that follows physical rules. And that it&rsquo;s an incoherent mess, shaped by a blind idiot god that wasn&rsquo;t trying to make us happy or give us satisfying love lives or a promising future - it was just following a set of mathematical rules that caused the propagation of whatever traits increased reproductive fitness at the time.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">And it&rsquo;s not even clear that there&rsquo;s a singular you in any of this. Your brain is full of separate entities working at cross purposes; your conscious mind isn&rsquo;t necessarily responsible for your decisions; the &ldquo;you&rdquo; of today isn&rsquo;t necessarily the same as the &ldquo;you&rdquo; of yesterday or tomorrow. And like it or not, this incoherent mess is what your hopes and dreams and morals are made of.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />Maybe for a moment, you may come to believe that it all IS really meaningless. We&rsquo;re not put here with a purpose. The universe doesn&rsquo;t care about us. Love isn&rsquo;t inherently any more important than paperclips. The very concept of a continuous self isn&rsquo;t obviously true. When all is said and done, morality isn&rsquo;t &ldquo;real&rdquo; in an objective sense. There&rsquo;s just matter, and math. So why the hell worry about anything?</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">Or maybe instead you&rsquo;d flinch away from these ideas. Avoid the discomfort. You can do that. But these aren&rsquo;t just silly philosophical questions that can be ignored. Somebody has to think about them. Because as technology moves forward, we *will* be relying increasingly on automated processes. Not just to work for us, but to think for us. Computers are already better at solving certain types of problems than the average expert. Machine intelligence is almost definitely coming, and society will have to change rapidly around it, and it will become incredibly important for us to know what it is we actually care about. Partly so that we don&rsquo;t accidentally change ourselves into something we regret. But also so that if and when an AI is created which has the ability to improve itself, and rapidly becomes smart enough to convince its human creators to give it additional resources for perfectly &ldquo;good&rdquo; reasons, until it suddenly is powerful enough to grow on its own with only our initial instructions to guide it... we better hope that those initial instructions contained detailed notes about everything we hold dear.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />We better hope that the AI&rsquo;s interior world of pure math includes some kind of ghost in the machine that looks over each step and thinks &ldquo;yes, my decisions are still moving in a good direction.&rdquo; That ghost-in-the-machine will only exist if we deliberately put it there. And the only way to do that is to understand ourselves well enough to bother explaining that no, you don&rsquo;t use the atoms of people to create paperclips. You don&rsquo;t just &ldquo;save as many lives as possible&rdquo; by hooking people up to feeding tubes. You don&rsquo;t make everyone happy by pumping them full of heroin, you don&rsquo;t go changing people&rsquo;s bodies or minds without their consent.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />None of these things are remotely obvious to a ghost of perfect emptiness that wasn&rsquo;t shaped for millions of years by a blind idiot god. Many humans wouldn&rsquo;t even consider them as options. But someday people may build a decision-making machine with the capacity to surpass us, and those people will need to understand the convoluted mess of values that makes up their mind, and mine, and yours. They&rsquo;ll need to be able to reduce an understanding of love to pure math, that a computer can comprehend. Because the future is at stake.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">It would be nice to just say &ldquo;don&rsquo;t build the superintelligence.&rdquo; But in the Information Age, preventing technological development is just not a reliable safeguard.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />This may all seem far fetched, and if you weren&rsquo;t already familiar with a lot of these ideas, I wouldn&rsquo;t expect you to be convinved in these few pages (indeed, you should be demanding more than three paragraphs of assertions as evidence). But even without the risk of AI, the future is still at stake. Hell, the present is at stake. People are dying as we speak. And suffering. Losing their autonomy. Their equality. Losing the ability to control their bodies. Even those who lived good lives in modern countries, age can creep over them and cripple their ability not just to move but to think and decide, destroying everything they thought made them human until all that&rsquo;s left is a person, trapped in a&nbsp;crumbling body, who can&rsquo;t control their own life but who desperately doesn&rsquo;t want to die alone.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">This is a monstrously harsh reality. A monstrously hard problem, not at all calibrated to our current skills. The problems extend beyond the biological processes that make death a reality, and into the world of resources and politics and limited space. I<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">t&rsquo;s easy to decide that the problem is too hard, that we&rsquo;ll never be able to solve it.&nbsp;<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">And this is just the present. All of the suffering of the people currently alive pales in comparison the potential suffering of future generations, or worse, to the lives that might go unlived if humanity makes too many mistakes in an unfair universe and erases itself.</span></span></p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">What is it about the future that&rsquo;s worth protecting? What makes it worth it to drag eight thousand pound stones across 150 miles of land, for the benefit of people who won&rsquo;t be born for centuries, who you&rsquo;ll never see? I can tell you my answer: a young mind, born millenia from now, whose values are still close enough to mine that I can at least recognize them. Who has the mental framework to ask of its parents, &ldquo;Why does love exist?&rdquo; and to care about the answer to the question.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">The answer is as ludicrously simple as it is immensely complicated, and you may not have needed the Gift We Give to Tomorrow to explain it to you. Love exists, it was shaped by blind mathematical forces that don&rsquo;t care about anything. But it exists and we care about it - we care so, so very deeply. And not just about love. Creativity. Curiosity. Excitement. Autonomy. Other people. Morality. Our children&rsquo;s children. We don&rsquo;t need a reason to care about these things. We may not fully understand them. But they exist. For us, they are real.</span></p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\"><br /></span></p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">The Gift We Give to Tomorrow walked me through all this understanding. Deep, down into the heart of the abyss where nothing actually matters. Pretending no comforting lies. Cutting away the last illusions. And still, it somehow left me with a vision of the humanity, of the universe, of the future, that is beautiful and satisfying.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">It doesn&rsquo;t matter that it didn&rsquo;t really say anything new, that I hadn&rsquo;t already worked out.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">It was just beautiful. Just because.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">That beauty, that vision of the future, that is what is worth protecting. That&rsquo;s why I&rsquo;m sacrificing comfort and peace of mind. That&rsquo;s why I&rsquo;m thinking hard, rebelling against my initial instincts to make fun video games. My second instinct to give to the first charity that shows me a picture of an adorable orphan, or that I&rsquo;m already familiar with in some way. My third instinct to settle for saving maybe a few dozen lives.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />My instincts were shaped by blind mathematical forces in an ancestral environment where one orphan was the most I could be expected to worry about. And it is my prerogative, as one small conscious fragment of an incoherent sentient mind, to look at the part of my brain that thinks &ldquo;that&rsquo;s all that matters&rdquo;, and rebel. Take the cold, calculating long view. It&rsquo;s not enough to think in the moral terms that my mind is naturally good at.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">A million people feel like a statistic. They feel even more like a statistic when they live in a distant country. They feel even more like a statistic when they live in a distant future and their values have drifted somewhat from the things we care about today.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">But those people are not a statistic. A million deaths is a million tragedies. A billion deaths is a billion tragedies. The possible extinction of the human race is something fundamentally worse than a tragedy, something I still don&rsquo;t have a word for.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />I don&rsquo;t know what exactly I&rsquo;m capable of doing, to bring about the most good I can. It might be working hard at a high paying programming job and donating to effective charities. It be directly working on problems that save lives, or which educate future generations to be able to save even more. It might be investing in companies that are producing important services but in a for-profit context. It might be working on scientific research in any one of a hundred important fields. It might be creating art that moves people in important ways.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">It might be contributing to AI research. It might not. I don&rsquo;t know. This isn&rsquo;t about abandoning one familiar cause for another. When the future is at stake, I don&rsquo;t have the luxury of not thinking hard about the right choice or passing the buck to a modern Pascal&rsquo;s wager. Current organizations working on AI research might be effective enough at their jobs to be worth supporting. They might not. They might be worth supporting later, but not yet. Or vice versa. So many factors to consider.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">I have to decide what&rsquo;s good, and I have to decide alone, and I can&rsquo;t take forever to think about it.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">I can&rsquo;t expect everyone, or even me, to devote their lives to this question. I don&rsquo;t know what kind of person I am yet. Right now I&rsquo;m in a fever pitch of inspiration and I feel ready to take on the world, but when all is said and done, I do mostly care about my own happiness. And I think that&rsquo;s okay - I think most people should spend most of their time seeing to their own needs, building their own community. But my hope is that I can find a way to be happy and contribute to a greater good at the same time.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />In the meantime, I wrote this book, and planned an evening that bordered on religious service. I did this for a lot of reasons, but the biggest one was to battle parts of my mind that I am not satisfied with. The parts of me that think a rare, spectacular disease is more important that a common, easily fixed problem. The parts of me that think a smiling, hungry orphan is more important than a billion lives. The parts of me that I have to keep&nbsp;fighting&nbsp;in order to make good decisions.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />Because I am fucking sick of having to feel like a cold hearted bastard, when I try to make the choice that is good.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">I&rsquo;m willing to feel that way, if I have to. It&rsquo;s worth it. But I shouldn&rsquo;t have to, and neither should you.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">To fix this, I use art. And good art sometimes has to blur the line between fact and fiction, using certain kinds of lies so that my lizard brain can fully comprehend certain other kinds of truths. To understand why 6 billion people are more important than a single hungry orphan, it can help to tell a story.</p>\n<p style=\"height: 11pt; direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">Not about six billion people, but about one child.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">Across space and time, ages from now, ever so far away: In a universe of pure math, where there is no loving god to shelter us nor punish the Genghis Khans of the world.... there exists the possibility of a child whose values I can understand, asking their parents &ldquo;Why does love exist?&rdquo;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">That child&rsquo;s existence is not inevitable. It will be born, or not, depending on actions that humans take today. It will suffer, or not, depending on the direction that humanity steers itself. It will die in hundred, a thousand or million years, depending on how far we progress in solving the problem of death. And I don&rsquo;t know for sure whether any of this will specifically require your actions, or mine.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\"><br />That child is beautiful. The very possibility of that child is beautiful. That beauty is worth protecting. I don&rsquo;t speak for the entire Less Wrong community, but I write this to honor the birth of that child, and everything that child represents: Peace on earth, and perhaps across the galaxy. Good will, among all sentient minds. Scientific and ethical progress. All the hard work and sacrifice that these things entail.</p>\n<p style=\"direction: ltr; padding: 0px; margin: 0px;\">&nbsp;</p>\n<div style=\"font-family: Times; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469); font-size: medium;\">In the world beyond the reach of god, if we care about that child, then 'good enough' decisions just aren't good enough.</div>\n<div style=\"font-family: Times; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469); font-size: medium;\">Rationality matters.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 2, "vtozKm5BZ8gf6zd45": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rhj3BZLpC69m9mr4B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 34, "extendedScore": null, "score": 8.29703356738774e-07, "legacy": true, "legacyId": "12071", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GEQyCqgu5Yhi5dTdb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T04:13:57.274Z", "modifiedAt": null, "url": null, "title": "Long-Term Technological Forecasting", "slug": "long-term-technological-forecasting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:31.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eXdGr2LeHYEgMh9qX/long-term-technological-forecasting", "pageUrlRelative": "/posts/eXdGr2LeHYEgMh9qX/long-term-technological-forecasting", "linkUrl": "https://www.lesswrong.com/posts/eXdGr2LeHYEgMh9qX/long-term-technological-forecasting", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Long-Term%20Technological%20Forecasting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALong-Term%20Technological%20Forecasting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeXdGr2LeHYEgMh9qX%2Flong-term-technological-forecasting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Long-Term%20Technological%20Forecasting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeXdGr2LeHYEgMh9qX%2Flong-term-technological-forecasting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeXdGr2LeHYEgMh9qX%2Flong-term-technological-forecasting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 432, "htmlBody": "<p>When will <a href=\"http://en.wikipedia.org/wiki/Strong_ai\">AGI</a> be created? When will <a href=\"http://en.wikipedia.org/wiki/Mind_uploading\">WBE</a> be possible? It would be nice to have somewhat reliable methods of long-term technological forecasting. Do we? Here's my own brief overview of the subject...</p>\n<p class=\"p1\"><span class=\"s1\"><a href=\"http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf\">Nagy et al. (2010)</a></span> is, I think, the best paper in the field. At first you might think it's basically supporting Kurzweillian conclusions about exponential curves for all technologies, but there are serious qualifications to make. The first is that the prediction laws they tested are linear regression models, which fit the data well but are not theoretically appropriate for modeling the data because the assumptions of independence and so on are not satisfied. A second and bigger qualification is that Nagy &amp; company only used data from small time slices for most technologies examined in the paper. This latter problem becomes a larger source of worry when you note that we have reason to expect many technologies to follow a logistic rather than exponential growth pattern, and exponential and logistic growth patterns look the same for the first part of their curves &mdash; see <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/01/Modis-The-singularity-myth.pdf\"><span class=\"s1\">Modis (2006)</span></a>. A third qualification is that Nagy's <a href=\"http://pcdb.santafe.edu/\">performance curves database</a> is not representative of \"technology in general\" or anything like that. Fourth, Nagy's study is the <em>first</em> of its kind, not a summary of 20 years of careful work all leading to a shared set of conclusions we can be fairly confident about. The <a href=\"/lw/4yq/the_neuroscience_of_pleasure/\"><span class=\"s1\">hedonic hotspots</span></a> that fire in my brain when I engage in hyperbole want me to say that serious long-term technological forecasting is not <em>summarized</em>&nbsp;by Nagy but <em>begins</em>&nbsp;with Nagy. (But that, of course, compresses history too&nbsp;much.)</p>\n<p class=\"p1\"><span class=\"s1\"><a href=\"http://www.amazon.com/Prediction-Markets-Applications-Routledge-International/dp/041557286X\">Williams (2011)</a></span> demonstrates that prediction markets just aren't yet tested in the domain of long-term forecasting, and have several incentive-structure problems yet to be worked out. I bought the book, but it's probably not worth $125. If you go to the library and want to copy just one chapter, make it Croxson's.</p>\n<p class=\"p1\">I see basically no evidence that <em>any</em>&nbsp;expert elicitation method is reliable for long-term technological forecasting (e.g. see <a href=\"http://teaching.p-design.ch/forecasting07/texts/RoweWright2001_Delphi_Technique.pdf\"><span class=\"s1\">Rowe &amp; Wright 2001</span></a>). The first study to show positive results from expert elicitation (in this case, a particular version of the Delphi method) for long-term forecasting is a single&nbsp;paper from <em>last year</em>: <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/01/Parente-A-case-study-of-long-term-Delphi-accuracy.pdf\"><span class=\"s1\">this one</span></a>.</p>\n<p class=\"p1\">So if you want to predict the future of technology, it's best <span class=\"s1\"><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/nordmann-if-and-then-a-critique-of-speculative-nanoethics.pdf\">not to tell detailed stories</a></span>. Instead, you'll want to focus on \"disjunctive\" outcomes that, like the evolution of eyes or the emergence of markets,&nbsp;can come about through many different paths and can gather momentum once they begin. Humans actually tend to intuitively&nbsp;<em>under</em>estimate the likelihood of such convergent outcomes (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2010/12/Kahneman-Tversky-Judgment-Under-Uncertainty.pdf\"><span class=\"s1\">Tversky and Kahneman 1974</span></a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8daMDi9NEShyLqxth": 2, "3uE2pXvbcnS9nnZRE": 2, "5f5c37ee1b5cdee568cfb2b0": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eXdGr2LeHYEgMh9qX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 34, "extendedScore": null, "score": 8.297075050879124e-07, "legacy": true, "legacyId": "12048", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zThWT5Zvifo5qYaca"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T04:40:34.216Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Typicality and Asymmetrical Similarity", "slug": "seq-rerun-typicality-and-asymmetrical-similarity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wmi3hFcMAydX6g8Yf/seq-rerun-typicality-and-asymmetrical-similarity", "pageUrlRelative": "/posts/Wmi3hFcMAydX6g8Yf/seq-rerun-typicality-and-asymmetrical-similarity", "linkUrl": "https://www.lesswrong.com/posts/Wmi3hFcMAydX6g8Yf/seq-rerun-typicality-and-asymmetrical-similarity", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Typicality%20and%20Asymmetrical%20Similarity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Typicality%20and%20Asymmetrical%20Similarity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmi3hFcMAydX6g8Yf%2Fseq-rerun-typicality-and-asymmetrical-similarity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Typicality%20and%20Asymmetrical%20Similarity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmi3hFcMAydX6g8Yf%2Fseq-rerun-typicality-and-asymmetrical-similarity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmi3hFcMAydX6g8Yf%2Fseq-rerun-typicality-and-asymmetrical-similarity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Today's post, <a href=\"/lw/nk/typicality_and_asymmetrical_similarity/\">Typicality and Asymmetrical Similarity</a> was originally published on 06 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You try to treat category membership as all-or-nothing, ignoring the existence of more and less typical subclusters.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/9al/seq_rerun_similarity_clusters/\">Similarity Clusters</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wmi3hFcMAydX6g8Yf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.297175537769811e-07, "legacy": true, "legacyId": "12072", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4mEsPHqcbRWxnaE5b", "4jdBnjYYykWntjAaj", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T05:56:28.513Z", "modifiedAt": null, "url": null, "title": "Meetup : Houston Meetup - 1/15", "slug": "meetup-houston-meetup-1-15", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.800Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7RTZtH36PMJG45sGL/meetup-houston-meetup-1-15", "pageUrlRelative": "/posts/7RTZtH36PMJG45sGL/meetup-houston-meetup-1-15", "linkUrl": "https://www.lesswrong.com/posts/7RTZtH36PMJG45sGL/meetup-houston-meetup-1-15", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Houston%20Meetup%20-%201%2F15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Houston%20Meetup%20-%201%2F15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7RTZtH36PMJG45sGL%2Fmeetup-houston-meetup-1-15%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Houston%20Meetup%20-%201%2F15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7RTZtH36PMJG45sGL%2Fmeetup-houston-meetup-1-15", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7RTZtH36PMJG45sGL%2Fmeetup-houston-meetup-1-15", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5z'>Houston Meetup - 1/15</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 January 2012 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2010 Commerce Street, Houston TX 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be a meetup this Sunday at 2PM at the local hackerspace. We'll be discussing the Maps and Territories sequence, as well as plannig a reading schedule for ET Jayne's \"The Logic of Science\". All are welcome, and we can order pizza if people are interested. PM me for my cell number if you are a new member, and need help finding the place the day of.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5z'>Houston Meetup - 1/15</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7RTZtH36PMJG45sGL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.297462126395836e-07, "legacy": true, "legacyId": "12077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup___1_15\">Discussion article for the meetup : <a href=\"/meetups/5z\">Houston Meetup - 1/15</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 January 2012 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2010 Commerce Street, Houston TX 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be a meetup this Sunday at 2PM at the local hackerspace. We'll be discussing the Maps and Territories sequence, as well as plannig a reading schedule for ET Jayne's \"The Logic of Science\". All are welcome, and we can order pizza if people are interested. PM me for my cell number if you are a new member, and need help finding the place the day of.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup___1_151\">Discussion article for the meetup : <a href=\"/meetups/5z\">Houston Meetup - 1/15</a></h2>", "sections": [{"title": "Discussion article for the meetup : Houston Meetup - 1/15", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup___1_15", "level": 1}, {"title": "Discussion article for the meetup : Houston Meetup - 1/15", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup___1_151", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T12:55:45.959Z", "modifiedAt": null, "url": null, "title": "Utopian hope versus reality", "slug": "utopian-hope-versus-reality", "viewCount": null, "lastCommentedAt": "2012-01-18T22:46:01.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3EdLvRoTp3i2E7uHy/utopian-hope-versus-reality", "pageUrlRelative": "/posts/3EdLvRoTp3i2E7uHy/utopian-hope-versus-reality", "linkUrl": "https://www.lesswrong.com/posts/3EdLvRoTp3i2E7uHy/utopian-hope-versus-reality", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utopian%20hope%20versus%20reality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtopian%20hope%20versus%20reality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3EdLvRoTp3i2E7uHy%2Futopian-hope-versus-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utopian%20hope%20versus%20reality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3EdLvRoTp3i2E7uHy%2Futopian-hope-versus-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3EdLvRoTp3i2E7uHy%2Futopian-hope-versus-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2280, "htmlBody": "<p>I've seen an interesting variety of utopian hopes expressed recently. Raemon's <a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration\">\"Ritual\" sequence of posts</a> is working to affirm the viability of LW's rationalist-immortalist utopianism, not just in the midst of an indifferent universe, but in the midst of an indifferent society. <a href=\"/r/discussion/lw/9a2/introducing_leverage_research/\">Leverage Research</a> turn out to be <a href=\"/r/discussion/lw/9ar/on_leverage_researchs_plan_for_an_optimal_world/\">social-psychology utopians</a>, who plan to achieve their world of optimality by unleashing the best in human nature. And <a href=\"http://mariakonovalenko.wordpress.com/2012/01/10/transhumanist-media-content/\">Russian life-extension activist Maria Konovalenko</a> just blogged about the difficulty of getting people to adopt anti-aging research as the top priority in life, even though it's so obvious to her that it should be.</p>\n<p>This phenomenon of utopian hope - its nature, its causes, its consequences, whether it's ever realistic, whether it ever does any good - certainly deserves attention and analysis, because it affects, and even afflicts, a lot of people, on this site and far beyond. It's a vast topic, with many dimensions. All my examples above have a futurist tinge to them - an AI singularity, and a biotech society where rejuvenation is possible, are clearly futurist concepts; and even the idea of human culture being transformed for the better by new ideas about the mind, belongs within the same broad scientific-technological current of Utopia Achieved Through Progress. But if we look at all the manifestations of utopian hope in history, and not just at those which resemble our favorites, other major categories of utopia can be observed - utopia achieved by reaching <em>back</em> to the conditions of a Golden Age; utopia achieved in some other reality, like an afterlife.</p>\n<p>The most familiar form of utopia these days is the ideological social utopia, to be achieved once the world is run properly, according to the principles of some political \"-ism\". This type of utopia can cut across the categories I have mentioned so far; utopian communism, for example, has both futurist and golden-age elements to its thinking. The new society is to be created via new political forms and new philosophies, but the result is a restoration of human solidarity and community that existed before hierarchy and property... The student of utopian thought must also take note of religion, which until technology has been the main avenue through which humans have pursued their most transcendental hopes, like not having to die.</p>\n<p>But I'm not setting out to study utopian thought and utopian psychology out of a neutral scholarly interest. I have been a utopian myself and I still am, if utopianism includes belief in the possibility (though not the inevitability) of something much better. And of course, the utopias that I have taken seriously are futurist utopias, like the utopia where we do away with death, and thereby also do away with a lot of other social and psychological pathologies, which are presumed to arise from the crippling futility of the universal death sentence.</p>\n<p>However, by now, I have also lived long enough to know that my own hopes were mistaken many times over; long enough to know that sometimes the mistake was in the ideas themselves, and not just the expectation that everyone else would adopt them; and long enough to understand something of the ordinary non-utopian psychology, whose main features I would nominate as reconciliation with work and with death. Everyone experiences the frustration of having to work for a living and the quiet horror of physiological decline, but hardly anyone imagines that there might be an alternative, or rejects such a lifecycle as overall more bad than it is good.</p>\n<p>What is the relationship between ordinary psychology and utopian psychology? First, the serious utopians should recognize that they are an extreme minority. Not only has the whole of human history gone by without utopia ever managing to happen, but the majority of people who ever lived were not utopians in the existentially revolutionary sense of thinking that the intolerable yet perennial features of the human condition might be overthrown. The confrontation with the evil aspects of life must usually have proceeded more at an emotional level - for example, terror that something might be true, and horror at the realization that it <em>is</em> true; a growing sense that it is impossible to escape; resignation and defeat; and thereafter a permanently diminished vitality, often compensated by achievement in the spheres of work and family.</p>\n<p>The utopian response is typically made possible only because one imagines that there is a specific alternative to this process; and so, as ideas about alternatives are invented and circulated, it becomes easier for people to end up on the track of utopian struggle with life, rather than the track of resignation, which is why we can have enough people to form social movements and fundamentalist religions, and not just isolated weirdos. There is a continuum between full radical utopianism and very watered-down psychological phenomena which hardly deserve that name, but still have something in common - for example, a person who lives an ordinary life but draws some sustenance from the possibility of an afterlife of unspecified nature, where things might be different, and where old wrongs might be righted - but nonetheless, I would claim that the historically dominant temperament in adult human experience has been resignation to hopelessness and helplessness in ultimate matters, and an absorption in affairs where some limited achievement is possible, but which in themselves can never satisfy the utopian impulse.</p>\n<p>The new factor in our current situation is science and technology. Our modern history offers evidence that the world really can change fundamentally, and such further explosive possibilities as artificial intelligence and rejuvenation biotechnology are considered possible for good, tough-minded, empirical reasons, not just because they offer a convenient vehicle for our hopes.</p>\n<p>Technological utopians often exhibit frustration that their pet technologies and their favorite dreams of existential emancipation aren't being massively prioritized by society, and they don't understand why other people don't just immediately embrace the dream when they first hear about it. (Or they develop painful psychological theories of why the human race is ignoring the great hope.) So let's ask, what are the attitudes towards alleged technological emancipation that a person might adopt?</p>\n<p>One is the utopian attitude: the belief that here, finally, one of the perennial dreams of the human race can come true. Another is denial: which is sometimes founded on bitter experience of disappointment, which teaches that the wise thing to do is not to fool yourself when another new hope comes up to you and cheerfully asserts that this time really is different. Another is to accept the possibility but deny the utopian hope. I think this is the most important interpretation to understand.</p>\n<p>It is the one that precedent supports. History is full of new things coming to pass, but they have never yet led to utopia. So we might want to scrutinize our technological projections more closely, and see whether the utopian expectation is based on overlooking the downside. For example, let us contrast the idea of rejuvenation and the idea of immortality - not dying, ever. Just because we can take someone who is 80 and make them biologically 20, is not the same thing as making them immortal. It just means that won't die of aging, and that when they do die, it will be in a way befitting someone 20 years old. They'll die in an accident, or a suicide, or a crime. Incidentally, we should also note an element of <em>psychological</em> unrealism in the idea of never wanting to die. Forever is a long time; the whole history of the human race is about 10,000 years long. Just 10,000 years is enough to encompass all the difficulties and disappointments and permutations of outlook that have ever happened. Imagine taking the whole history of the human race into yourself; living through it personally. It's a lot to have endured.</p>\n<p>It would be unfair to say that transhumanists as a rule are dominated by utopian thinking. Perhaps just as common is a sort of futurological bipolar disorder, in which the future looks like it will bring \"utopia or oblivion\", something really good or something really bad. The conservative wisdom of historical experience says that both these expectations are wrong; bad things can happen, even catastrophes, but life keeps going for someone - that is the precedent - and the expectation of total devastating extinction is just a plunge into depression as unrealistic as the utopian hope for a personal eternity; both extremes exhibiting an inflated sense of historical or cosmic self-importance. The end of you is not the end of the world, says this historical wisdom; imagining the end of the whole world is your overdramatic response to imagining the end of you - or the end of your particular civilization.</p>\n<p>However, I think we do have some reason to suppose that this time around, the extremes are really possible. I won't go so far as to endorse the idea that (for example) intelligent life in the universe typically turns its home galaxy into one giant mass of computers; that really does look like a case of taking the concept and technology with which our current society is obsessed, and projecting it onto the cosmic unknown. But just the humbler ideas of transhumanity, posthumanity, and a genuine end to the human-dominated era on Earth, whether in extinction or in transformation. The real and verifiable developments of science and technology, and the further scientific and technological developments which they portend, are enough to justify such a radical, if somewhat nebulous, concept of the possible future. And again, while I won't simply endorse the view that of course we shall get to be as gods, and shall get to feel as good as gods might feel, it seems reasonable to suppose that there are possible futures which are genuinely and comprehensively better than anything that history has to offer - as well as futures that are just bizarrely altered, and futures which are empty and dead.</p>\n<p>So that is my limited endorsement of utopianism: In principle, there might be a utopianism which is justified. But in practice, what we have are people getting high on hope, emerging fanaticisms, personal dysfunctionality in the present, all the things that come as no surprise to a cynical student of history. The one outcome that would be most surprising to a cynic is for a genuine utopia to arrive. I'm willing to say that this is possible, but I'll also say that almost any existing reference to a better world to come, and any psychological state or social movement which draws sublime happiness from the contemplation of an expected future, has something unrealistic about it.</p>\n<p>In this regard, utopian hope is almost always an indicator of something wrong. It can just be naivete, especially in a young person. As I have mentioned, even non-utopian psychology inevitably has those terrible moments when it learns for the first time about the limits of life as we know it. If in your own life you start to enter that territory for the first time, without having been told from an early age that real life is fundamentally limited and frustrating, and perhaps with a few vague promises of hope, absorbed from diverse sources, to sustain you, then it's easy to see your hopes as, not utopian hopes, but simply a hope that life can be worth living. I think this is the experience of many young idealists in \"environmental\" and \"social justice\" movements; their culture has always implied to them that life should be a certain way, without also conveying to them that it has never once been that way in reality. The suffering of transhumanist idealists and other radical-futurist idealists, when they begin to run aground on the disjunction between their private subcultural expectations and those of the culture at large, has a lot in common with the suffering of young people whose ideals are more conventionally recognizable; and it is entirely conceivable that for some generation now coming up, rebellion against biological human limitations will be what rebellion against social limitations has been for preceding generations.</p>\n<p>I should also mention, in passing, the option of a non-utopian transhumanism, something that is far more common than my discussion so far would mention. This is the choice of people who expect, not utopia, but simply an open future. Many cryonicists would be like this. Sure, they expect the world of tomorrow to be a great place, good enough that they want to get there; but they don't think of it as an eternal paradise of wish-fulfilment that may or may not be achieved, depending on heroic actions in the present. This is simply the familiar non-utopian view that life is overall worth living, combined with the belief that life can now be lived for much longer periods; the future not as utopia, but as more history, history that hasn't happened yet, and which one might get to personally experience. If I was wanting to start a movement in favor of rejuvenation and longevity, this is the outlook I would be promoting, not the idea that abolishing death will cure all evils (and not even the idea that death as such can be abolished; rejuvenation is not immortality, it's just more good life). In the spectrum of future possibilities, it's only the issue of artificial intelligence which lends some plausibility to extreme bipolar futurism, the idea that the future can be very good (by human standards) or very bad (by human standards), depending on what sort of utility functions govern the decision-making of transhuman intelligence.</p>\n<p>That's all I have to say for now. It would be unrealistic to think we can completely avoid the pathologies associated with utopian hope, but perhaps we can moderate them, if we pay attention to the psychology involved.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3EdLvRoTp3i2E7uHy", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 31, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "12082", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jES7mcPvKpfmzMTgC", "969wcdD3weuCscvoJ", "4xum2CuYMMRq8rgsW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-01-11T12:55:45.959Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T16:33:05.210Z", "modifiedAt": null, "url": null, "title": "Meetup : Seattle, Diseased Thinking and evidence on parenting", "slug": "meetup-seattle-diseased-thinking-and-evidence-on-parenting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:06.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BbkExnmCrcCKwMQf8/meetup-seattle-diseased-thinking-and-evidence-on-parenting", "pageUrlRelative": "/posts/BbkExnmCrcCKwMQf8/meetup-seattle-diseased-thinking-and-evidence-on-parenting", "linkUrl": "https://www.lesswrong.com/posts/BbkExnmCrcCKwMQf8/meetup-seattle-diseased-thinking-and-evidence-on-parenting", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Seattle%2C%20Diseased%20Thinking%20and%20evidence%20on%20parenting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Seattle%2C%20Diseased%20Thinking%20and%20evidence%20on%20parenting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBbkExnmCrcCKwMQf8%2Fmeetup-seattle-diseased-thinking-and-evidence-on-parenting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Seattle%2C%20Diseased%20Thinking%20and%20evidence%20on%20parenting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBbkExnmCrcCKwMQf8%2Fmeetup-seattle-diseased-thinking-and-evidence-on-parenting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBbkExnmCrcCKwMQf8%2Fmeetup-seattle-diseased-thinking-and-evidence-on-parenting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/60'>Seattle, Diseased Thinking and evidence on parenting</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 January 2012 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">950 N 72nd St, Seattle 98103</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We haven't had a serious-ish meetup in a while, so I'd like to do that this Sunday. Walid has graciously volunteered to host the meetup (call me to be let in 360-602-1069). There are two cats in the apartment that can be quarantined if necessary. The plan is read <a href=\"http://lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/\">Diseased Thinking</a> about applying reductionism to notions about diseases. I'd also like to discuss the role of parenting on life outcomes of children. Bryan Caplan's book \"<a href=\"http://www.amazon.com/Selfish-Reasons-Have-More-Kids/dp/046501867X\" rel=\"nofollow\">Selfish Reasons to Have More Kids</a>\" presents strong evidence that parenting style has surprisingly little impact on long term life outcomes of children. I'll begin by summarizing the evidence in the book and some of the things Caplan uses that evidence to argue and then we'll discuss for a while. I'm a bit of a Bryan Caplan fanboy, so come with your skeptic hat on. After that we'll have dinner and hang out. I'll try to bring a case of beer.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/60'>Seattle, Diseased Thinking and evidence on parenting</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BbkExnmCrcCKwMQf8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 8.299866427003352e-07, "legacy": true, "legacyId": "12084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Seattle__Diseased_Thinking_and_evidence_on_parenting\">Discussion article for the meetup : <a href=\"/meetups/60\">Seattle, Diseased Thinking and evidence on parenting</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 January 2012 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">950 N 72nd St, Seattle 98103</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We haven't had a serious-ish meetup in a while, so I'd like to do that this Sunday. Walid has graciously volunteered to host the meetup (call me to be let in 360-602-1069). There are two cats in the apartment that can be quarantined if necessary. The plan is read <a href=\"http://lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/\">Diseased Thinking</a> about applying reductionism to notions about diseases. I'd also like to discuss the role of parenting on life outcomes of children. Bryan Caplan's book \"<a href=\"http://www.amazon.com/Selfish-Reasons-Have-More-Kids/dp/046501867X\" rel=\"nofollow\">Selfish Reasons to Have More Kids</a>\" presents strong evidence that parenting style has surprisingly little impact on long term life outcomes of children. I'll begin by summarizing the evidence in the book and some of the things Caplan uses that evidence to argue and then we'll discuss for a while. I'm a bit of a Bryan Caplan fanboy, so come with your skeptic hat on. After that we'll have dinner and hang out. I'll try to bring a case of beer.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Seattle__Diseased_Thinking_and_evidence_on_parenting1\">Discussion article for the meetup : <a href=\"/meetups/60\">Seattle, Diseased Thinking and evidence on parenting</a></h2>", "sections": [{"title": "Discussion article for the meetup : Seattle, Diseased Thinking and evidence on parenting", "anchor": "Discussion_article_for_the_meetup___Seattle__Diseased_Thinking_and_evidence_on_parenting", "level": 1}, {"title": "Discussion article for the meetup : Seattle, Diseased Thinking and evidence on parenting", "anchor": "Discussion_article_for_the_meetup___Seattle__Diseased_Thinking_and_evidence_on_parenting1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["895quRDaK6gR2rM82"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T17:07:07.250Z", "modifiedAt": null, "url": null, "title": "On accepting an argument if you have limited computational power.", "slug": "on-accepting-an-argument-if-you-have-limited-computational", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.679Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J4DbTRwtxp37b6haQ/on-accepting-an-argument-if-you-have-limited-computational", "pageUrlRelative": "/posts/J4DbTRwtxp37b6haQ/on-accepting-an-argument-if-you-have-limited-computational", "linkUrl": "https://www.lesswrong.com/posts/J4DbTRwtxp37b6haQ/on-accepting-an-argument-if-you-have-limited-computational", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20accepting%20an%20argument%20if%20you%20have%20limited%20computational%20power.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20accepting%20an%20argument%20if%20you%20have%20limited%20computational%20power.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ4DbTRwtxp37b6haQ%2Fon-accepting-an-argument-if-you-have-limited-computational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20accepting%20an%20argument%20if%20you%20have%20limited%20computational%20power.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ4DbTRwtxp37b6haQ%2Fon-accepting-an-argument-if-you-have-limited-computational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ4DbTRwtxp37b6haQ%2Fon-accepting-an-argument-if-you-have-limited-computational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 446, "htmlBody": "<p>It would seem rational to accept any argument that is not fallacious; but this leads to consideration of problems such as <a href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\">Pascal's mugging</a> and other exploits.</p>\n<p>I've had a realization of a subconscious triviality: for me to accept an argument as true, it is not enough that I find no error in it. The argument must also be so structured that I would expect to have found an error if it was invalid (or I myself must make such structured version first). That's how mathematical proofs work - they are so structured that finding an error requires little computational power (only knowledge of rules and reliability); in the extreme case an entirely unintelligent machine can check a proof.</p>\n<p>In light of this I propose that those who want to make a persuasive argument should try to structure the argument so it'd be easy to find flaws in it. This also goes for the thought experiments and hypothetical situations. Those seem rather often to be constructed with entirely opposite goal in mind - to obstruct the verification process or to try to prevent the reader from trying to find flaws.</p>\n<p>Something else tangentially related to the arguments. The faulty models are the prime cause of decision errors; yet the faulty models are the staple of thought experiment; nobody raises an eyebrow as all models are ultimately imperfect.</p>\n<p>However, to accept an argument based on imperfect model one must be capable of correctly propagating the error and estimating the error in the final conclusion, as a faulty model may be so constructed as to itself differ non substantially from the reality but in such a way that the difference diverges massively along the chain of reasoning. My example of this is the Trolley Problems. The faults of original model are nothing out of ordinary; simplified assumptions of the real world, perfect information, etc. Normally you can have those faults in model and still arrive at reasonably close outcome. The end result is throwing of fat people onto tracks, cutting up of travellers for organs, and similar behaviours which we intuitively know we could live a fair lot better without. How that happens? In real world the strongly asymmetrical relations of form 'death of 1 person saves 10 people' are very rare (as an emergent property of complexity of the real world that is lacking in the imaginary worlds of trolley problems), while the decision errors are not nearly so rare, so most of people killed to save others would end up killed in vain.</p>\n<p>I don't know how models can be structured as to facilitate propagation of model's error. But it seems to be necessary for arguments based on models to be convincing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wfW6iL96u26mbatep": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J4DbTRwtxp37b6haQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 32, "extendedScore": null, "score": 7.4e-05, "legacy": true, "legacyId": "12083", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T17:14:10.454Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins Meetup", "slug": "meetup-fort-collins-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FJXcF8Pvuwniy7DaG/meetup-fort-collins-meetup", "pageUrlRelative": "/posts/FJXcF8Pvuwniy7DaG/meetup-fort-collins-meetup", "linkUrl": "https://www.lesswrong.com/posts/FJXcF8Pvuwniy7DaG/meetup-fort-collins-meetup", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJXcF8Pvuwniy7DaG%2Fmeetup-fort-collins-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJXcF8Pvuwniy7DaG%2Fmeetup-fort-collins-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJXcF8Pvuwniy7DaG%2Fmeetup-fort-collins-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/61'>Fort Collins Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 January 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We continue to gain in numbers and in friendship. Would you be interested in meeting accomplished, interesting people to talk about Less Wrong-ish topics?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/61'>Fort Collins Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FJXcF8Pvuwniy7DaG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.300021644520495e-07, "legacy": true, "legacyId": "12085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins_Meetup\">Discussion article for the meetup : <a href=\"/meetups/61\">Fort Collins Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 January 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We continue to gain in numbers and in friendship. Would you be interested in meeting accomplished, interesting people to talk about Less Wrong-ish topics?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/61\">Fort Collins Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T17:15:16.280Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins Meetup", "slug": "meetup-fort-collins-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:35.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kkrJSE98oxWJe7bte/meetup-fort-collins-meetup-0", "pageUrlRelative": "/posts/kkrJSE98oxWJe7bte/meetup-fort-collins-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/kkrJSE98oxWJe7bte/meetup-fort-collins-meetup-0", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkkrJSE98oxWJe7bte%2Fmeetup-fort-collins-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkkrJSE98oxWJe7bte%2Fmeetup-fort-collins-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkkrJSE98oxWJe7bte%2Fmeetup-fort-collins-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/62'>Fort Collins Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 January 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The last meetup before our 3D printer field trip.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/62'>Fort Collins Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kkrJSE98oxWJe7bte", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 0, "extendedScore": null, "score": 8.300025789093276e-07, "legacy": true, "legacyId": "12086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins_Meetup\">Discussion article for the meetup : <a href=\"/meetups/62\">Fort Collins Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 January 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The last meetup before our 3D printer field trip.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/62\">Fort Collins Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T20:34:11.272Z", "modifiedAt": null, "url": null, "title": "Bill Gates asks HS students \"What are most important choices the world faces\"?", "slug": "bill-gates-asks-hs-students-what-are-most-important-choices", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.058Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r2CsDcqpSJNQRpt2P/bill-gates-asks-hs-students-what-are-most-important-choices", "pageUrlRelative": "/posts/r2CsDcqpSJNQRpt2P/bill-gates-asks-hs-students-what-are-most-important-choices", "linkUrl": "https://www.lesswrong.com/posts/r2CsDcqpSJNQRpt2P/bill-gates-asks-hs-students-what-are-most-important-choices", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bill%20Gates%20asks%20HS%20students%20%22What%20are%20most%20important%20choices%20the%20world%20faces%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABill%20Gates%20asks%20HS%20students%20%22What%20are%20most%20important%20choices%20the%20world%20faces%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr2CsDcqpSJNQRpt2P%2Fbill-gates-asks-hs-students-what-are-most-important-choices%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bill%20Gates%20asks%20HS%20students%20%22What%20are%20most%20important%20choices%20the%20world%20faces%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr2CsDcqpSJNQRpt2P%2Fbill-gates-asks-hs-students-what-are-most-important-choices", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr2CsDcqpSJNQRpt2P%2Fbill-gates-asks-hs-students-what-are-most-important-choices", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.impatientoptimists.org/Posts/2012/01/Do-You-Have-a-WorldChanging-Idea?WT.mc_id=1_11_2012_studentannualletter_fb&amp;WT.tsrc=Facebook</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r2CsDcqpSJNQRpt2P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 2, "extendedScore": null, "score": 8.300777318430535e-07, "legacy": true, "legacyId": "12087", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-11T20:48:53.950Z", "modifiedAt": null, "url": null, "title": "What jobs are safe in an automated future?", "slug": "what-jobs-are-safe-in-an-automated-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:52.408Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PuyaSharif", "createdAt": "2011-02-26T07:33:06.094Z", "isAdmin": false, "displayName": "PuyaSharif"}, "userId": "Kx2AumHK8eeJ4nHqt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XesnpzuuMCwq2gkLt/what-jobs-are-safe-in-an-automated-future", "pageUrlRelative": "/posts/XesnpzuuMCwq2gkLt/what-jobs-are-safe-in-an-automated-future", "linkUrl": "https://www.lesswrong.com/posts/XesnpzuuMCwq2gkLt/what-jobs-are-safe-in-an-automated-future", "postedAtFormatted": "Wednesday, January 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20jobs%20are%20safe%20in%20an%20automated%20future%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20jobs%20are%20safe%20in%20an%20automated%20future%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXesnpzuuMCwq2gkLt%2Fwhat-jobs-are-safe-in-an-automated-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20jobs%20are%20safe%20in%20an%20automated%20future%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXesnpzuuMCwq2gkLt%2Fwhat-jobs-are-safe-in-an-automated-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXesnpzuuMCwq2gkLt%2Fwhat-jobs-are-safe-in-an-automated-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 308, "htmlBody": "<p>The trends are clear, more and more work that was previously done by humans are being shifted to automated systems. Factories with thousands of workers has been replaced by highly efficient facilities containing industrial robots and a few human operators, bank tellers by online banking, most parts of any logistics chain by different types of automatic sorting, moving, and sending mechanisms. Offices are run by less and less people as we're handling and processing fewer and fewer physical documents. In any area less people than before are needed to do the same work as before. The world is becoming automated.</p>\n<p>These developments are not only here to stay - they are accelerating. Most of what is done by humans today could easily be done by computers in a near future. I would personally guess that most professions existing today could be replaced by affordable automated equivalents within 30 years. My question is: <strong>What jobs will be the last ones to go, and why?</strong></p>\n<p>Often education is pointed out as safe bet to ensure being needed in the future, and while that is true its not the whole story. First of all, in basically all parts of the world the fraction of the population with an academic degree is growing fast. Higher education will probably not be as good as a differentiator in the future. Second, while degrees in the fields hot in the future is hot in the future there is no guarantee that the degrees hot today will be of any use later on. Third, there is a misconception that highly theoretical tasks done by skilled experts will be among the last to go. But due to their theoretical nature such tasks are fairly easy represent virtually.</p>\n<p>Of course as we progress technologically new doors are opening and the hottest job year 2030 might not even exist today. Any suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XesnpzuuMCwq2gkLt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 10, "extendedScore": null, "score": 8.300832904063393e-07, "legacy": true, "legacyId": "12089", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T02:11:27.763Z", "modifiedAt": null, "url": null, "title": "Meetup : First Salt Lake City Meetup: 22 January 2012 03:00PM", "slug": "meetup-first-salt-lake-city-meetup-22-january-2012-03-00pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.666Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamisom", "createdAt": "2011-06-13T03:19:15.520Z", "isAdmin": false, "displayName": "adamisom"}, "userId": "eT8NPFmc2GDb5QFfc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yajQhBq66z5udkXbY/meetup-first-salt-lake-city-meetup-22-january-2012-03-00pm", "pageUrlRelative": "/posts/yajQhBq66z5udkXbY/meetup-first-salt-lake-city-meetup-22-january-2012-03-00pm", "linkUrl": "https://www.lesswrong.com/posts/yajQhBq66z5udkXbY/meetup-first-salt-lake-city-meetup-22-january-2012-03-00pm", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Salt%20Lake%20City%20Meetup%3A%2022%20January%202012%2003%3A00PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Salt%20Lake%20City%20Meetup%3A%2022%20January%202012%2003%3A00PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyajQhBq66z5udkXbY%2Fmeetup-first-salt-lake-city-meetup-22-january-2012-03-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Salt%20Lake%20City%20Meetup%3A%2022%20January%202012%2003%3A00PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyajQhBq66z5udkXbY%2Fmeetup-first-salt-lake-city-meetup-22-january-2012-03-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyajQhBq66z5udkXbY%2Fmeetup-first-salt-lake-city-meetup-22-january-2012-03-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 261, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/63'>First Salt Lake City Meetup: 22 January 2012 03:00PM</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 January 2012 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">631 West North Temple</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello and Happy Belated New Year! After the December announcement revealed that there were <em>many</em> residents of Salt Lake City who were also Less Wrongers, it is time for a Meetup!</p>\n\n<p>It will take place next Sunday the 22nd at 3:00 pm.</p>\n\n<p>The venue is Mestizo Coffee House. It's located at 631 West North Temple. I've been there once before and I liked the atmosphere. The 22nd was the first available time I could book the gallery space. The coffee house does not allow outside food or drinks.</p>\n\n<p>While browsing current Meetup announcements, I stumbled across this inspiring account (<a href=\"http://lesswrong.com/lw/4ul/less_wrong_nyc_case_study_of_a_successful/\" rel=\"nofollow\">http://lesswrong.com/lw/4ul/less_wrong_nyc_case_study_of_a_successful/</a>) of a successful meetup. It posed the question: \"Do you want a community badly enough to build one yourself?\" That reminded me that <em>yes, I think I do</em>. I have quite a large number of ideas for future meetup possibilities already.</p>\n\n<p>The agenda will first and foremost be to meet each other. We'll also discuss future meetups, both logistics and possibilities. We could also start with a short discussion near the end about some Less Wrong topic: leave a comment as to what you think that should be.</p>\n\n<p>Just to clarify: the December announcement was just to gather information and in no way means that there aren't LWers in SLC determined to have a meetup. There are plenty and we will have a meet up.</p>\n\n<p>That is all.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/63'>First Salt Lake City Meetup: 22 January 2012 03:00PM</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yajQhBq66z5udkXbY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 8.302051855119466e-07, "legacy": true, "legacyId": "12104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Salt_Lake_City_Meetup__22_January_2012_03_00PM\">Discussion article for the meetup : <a href=\"/meetups/63\">First Salt Lake City Meetup: 22 January 2012 03:00PM</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 January 2012 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">631 West North Temple</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello and Happy Belated New Year! After the December announcement revealed that there were <em>many</em> residents of Salt Lake City who were also Less Wrongers, it is time for a Meetup!</p>\n\n<p>It will take place next Sunday the 22nd at 3:00 pm.</p>\n\n<p>The venue is Mestizo Coffee House. It's located at 631 West North Temple. I've been there once before and I liked the atmosphere. The 22nd was the first available time I could book the gallery space. The coffee house does not allow outside food or drinks.</p>\n\n<p>While browsing current Meetup announcements, I stumbled across this inspiring account (<a href=\"http://lesswrong.com/lw/4ul/less_wrong_nyc_case_study_of_a_successful/\" rel=\"nofollow\">http://lesswrong.com/lw/4ul/less_wrong_nyc_case_study_of_a_successful/</a>) of a successful meetup. It posed the question: \"Do you want a community badly enough to build one yourself?\" That reminded me that <em>yes, I think I do</em>. I have quite a large number of ideas for future meetup possibilities already.</p>\n\n<p>The agenda will first and foremost be to meet each other. We'll also discuss future meetups, both logistics and possibilities. We could also start with a short discussion near the end about some Less Wrong topic: leave a comment as to what you think that should be.</p>\n\n<p>Just to clarify: the December announcement was just to gather information and in no way means that there aren't LWers in SLC determined to have a meetup. There are plenty and we will have a meet up.</p>\n\n<p>That is all.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Salt_Lake_City_Meetup__22_January_2012_03_00PM1\">Discussion article for the meetup : <a href=\"/meetups/63\">First Salt Lake City Meetup: 22 January 2012 03:00PM</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Salt Lake City Meetup: 22 January 2012 03:00PM", "anchor": "Discussion_article_for_the_meetup___First_Salt_Lake_City_Meetup__22_January_2012_03_00PM", "level": 1}, {"title": "Discussion article for the meetup : First Salt Lake City Meetup: 22 January 2012 03:00PM", "anchor": "Discussion_article_for_the_meetup___First_Salt_Lake_City_Meetup__22_January_2012_03_00PM1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CsKboswS3z5iaiutC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T05:04:03.356Z", "modifiedAt": null, "url": null, "title": "Ideas for rationalist meetup topics", "slug": "ideas-for-rationalist-meetup-topics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:29.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZPoPcQ2aB2AhwdKxf/ideas-for-rationalist-meetup-topics", "pageUrlRelative": "/posts/ZPoPcQ2aB2AhwdKxf/ideas-for-rationalist-meetup-topics", "linkUrl": "https://www.lesswrong.com/posts/ZPoPcQ2aB2AhwdKxf/ideas-for-rationalist-meetup-topics", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ideas%20for%20rationalist%20meetup%20topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdeas%20for%20rationalist%20meetup%20topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZPoPcQ2aB2AhwdKxf%2Fideas-for-rationalist-meetup-topics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ideas%20for%20rationalist%20meetup%20topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZPoPcQ2aB2AhwdKxf%2Fideas-for-rationalist-meetup-topics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZPoPcQ2aB2AhwdKxf%2Fideas-for-rationalist-meetup-topics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 350, "htmlBody": "<p>(From Zvi Mowshowitz, leader of the New York group, and based on his experience)</p>\n<p><br />Category 1: Discussions - The Base<br />A: Less Wrong topics. Usually recent posts. Often big hits.<br />B: Rationalist group planning and organization. Meta-topics. Dealing with group issues on occasion.<br />C: Spreading Rationality. Discussion of various approaches.<br />D: Contraband Topics. Discussion of things that I won't include here, but you can guess. <br />E: How To Do X, or how to do X well. Charity, meeting people, improving skills, relationships, etc.<br /><br />Category 2: Presentations - Almost Always a Winner<br />F: Sequences. Andrew Rettek did these for public meetups, went over well I think.<br />G: Personal Projects: Variance, Geoff Anders's Psychology.<br />H: General Knowledge: CBT, Starting a Business, Basic Python.<br />I: Advanced Rationality: Attempted once, successful despite poor execution. Should be explored more.<br /><br />Category 3: Game Nights<br />J: Proven Good Games: Illuminati, Poker, Citadels, 7 Wonders, Nomic. <br />K: Advanced Games: Much demand for general gaming, but not really that rationalist. Proven winners in this group: Vegas Showdown, Power Struggle, Baltimore &amp; Ohio, Tichu, Through the Ages. I can keep going, many good choices. AVOID: Settlers of Catan (personal opinion) and Fluxx (cause you can do better, honestly). <br />U: Outside Games. Ultimate Frisbee is good. <br /><br />Category 4: Other Celebrations<br />L: Karaoke<br />M: Baraccuda (bring something awesome you love)<br />N: Contraband Activities (enough said)<br />O: General Party In or Out / Pot Luck<br /><br />Category 5: Special Guests<br />P: Famous Rationalists! If you get Eliezer or Robin or Vassar, you need no other topic. Probably a few others who get there on their own as well, or any generally famous guest. <br /><br />Category 6: Self-Improvement<br />Q: Go out and do something to improve social skills.<br />R: Discuss or map out goals and plans. Set goals. Can be combined with other tasks.<br />S: Run experiments! For science!<br />T: Study Hall. Also folds into discussion of self-improvement topics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZPoPcQ2aB2AhwdKxf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 20, "extendedScore": null, "score": 8.302704206748481e-07, "legacy": true, "legacyId": "12105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T07:14:20.742Z", "modifiedAt": null, "url": null, "title": "Meetup : Monday Madison Meetup", "slug": "meetup-monday-madison-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kqzdfvDu6g2HEzDeJ/meetup-monday-madison-meetup-2", "pageUrlRelative": "/posts/kqzdfvDu6g2HEzDeJ/meetup-monday-madison-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/kqzdfvDu6g2HEzDeJ/meetup-monday-madison-meetup-2", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monday%20Madison%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monday%20Madison%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkqzdfvDu6g2HEzDeJ%2Fmeetup-monday-madison-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monday%20Madison%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkqzdfvDu6g2HEzDeJ%2Fmeetup-monday-madison-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkqzdfvDu6g2HEzDeJ%2Fmeetup-monday-madison-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/64'>Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 January 2012 06:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, wi</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet this week, again, at the Barrique's on Monroe. We have so many topics:</p>\n\n<ul>\n<li><p>With any luck, Ben will scan and post the lecture notes he swiped, on potential problems with Bayesian epistemology. I'll bring my copy of Probability Theory, so it can help defend itself. ;)</p></li>\n<li><p>Lots of spillover conversation from the reading group meeting, I suspect. In particular, exactly <em>why</em> ought one's beliefs \"pay rent in anticipated expreiences\"? We discussed this a fair amount, but we never named a direct argument, and none is given in the relevant post.</p></li>\n<li><p>Also, by popular demand from the reading group, a round of The Resistance is clearly in order.</p></li>\n<li><p>Finally, I'd like to bat around the idea of \"field trips\" some more. If anyone has any ideas, leave yourself a note somewhere so you can pull 'em out on Monday.</p></li>\n</ul>\n\n<p>If you're in the Madison area, be sure to join the Less Wrong Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/64'>Monday Madison Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kqzdfvDu6g2HEzDeJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.303196723388582e-07, "legacy": true, "legacyId": "12113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup\">Discussion article for the meetup : <a href=\"/meetups/64\">Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 January 2012 06:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, wi</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet this week, again, at the Barrique's on Monroe. We have so many topics:</p>\n\n<ul>\n<li><p>With any luck, Ben will scan and post the lecture notes he swiped, on potential problems with Bayesian epistemology. I'll bring my copy of Probability Theory, so it can help defend itself. ;)</p></li>\n<li><p>Lots of spillover conversation from the reading group meeting, I suspect. In particular, exactly <em>why</em> ought one's beliefs \"pay rent in anticipated expreiences\"? We discussed this a fair amount, but we never named a direct argument, and none is given in the relevant post.</p></li>\n<li><p>Also, by popular demand from the reading group, a round of The Resistance is clearly in order.</p></li>\n<li><p>Finally, I'd like to bat around the idea of \"field trips\" some more. If anyone has any ideas, leave yourself a note somewhere so you can pull 'em out on Monday.</p></li>\n</ul>\n\n<p>If you're in the Madison area, be sure to join the Less Wrong Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/64\">Monday Madison Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T07:23:08.593Z", "modifiedAt": null, "url": null, "title": "AI Challenge: Ants - Post Mortem", "slug": "ai-challenge-ants-post-mortem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.216Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Alex", "createdAt": "2009-07-17T08:21:38.505Z", "isAdmin": false, "displayName": "D_Alex"}, "userId": "Sriopfkdwx2qJBx4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ELCsD7MYaqnkarPBi/ai-challenge-ants-post-mortem", "pageUrlRelative": "/posts/ELCsD7MYaqnkarPBi/ai-challenge-ants-post-mortem", "linkUrl": "https://www.lesswrong.com/posts/ELCsD7MYaqnkarPBi/ai-challenge-ants-post-mortem", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Challenge%3A%20Ants%20-%20Post%20Mortem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Challenge%3A%20Ants%20-%20Post%20Mortem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELCsD7MYaqnkarPBi%2Fai-challenge-ants-post-mortem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Challenge%3A%20Ants%20-%20Post%20Mortem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELCsD7MYaqnkarPBi%2Fai-challenge-ants-post-mortem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FELCsD7MYaqnkarPBi%2Fai-challenge-ants-post-mortem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>Late last year a LessWrong team was being mooted for the Google AI challenge&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">(</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"http://aichallenge.org/\">http://aichallenge.org/</a>; &nbsp;<a style=\"color: #3d3d3e; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"http://lesswrong.com/r/discussion/lw/8ay/ai_challenge_ants/\">http://lesswrong.com/r/discussion/lw/8ay/ai_challenge_ants/</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">)</span>. Sadly, after a brief burst of activity, no \"official\" LessWrong entry appeared (AFAICT, and please let me know if I am mistaken). The best individual effort from this site's regulars (AFAICT) came from lavalamp, who finished around #300.</p>\n<p>This is a pity. This was an opportunity to achieve, or at least have a go at, a bunch of worthwhile things, including development of methods of cooperation between site members, gathering positive publicity, and yes, even advancing the understanding of AI related issues.&nbsp;</p>\n<p>So - how can things be improved for the next AI challenge (which I think is about 6 months away)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ELCsD7MYaqnkarPBi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 8.303229981275295e-07, "legacy": true, "legacyId": "12114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rrX37rZkf2o4XEfuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T07:35:58.669Z", "modifiedAt": null, "url": null, "title": "Procedural knowledge gap: public key encryption", "slug": "procedural-knowledge-gap-public-key-encryption", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:32.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5cCJ5npCLWLqkdXH8/procedural-knowledge-gap-public-key-encryption", "pageUrlRelative": "/posts/5cCJ5npCLWLqkdXH8/procedural-knowledge-gap-public-key-encryption", "linkUrl": "https://www.lesswrong.com/posts/5cCJ5npCLWLqkdXH8/procedural-knowledge-gap-public-key-encryption", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Procedural%20knowledge%20gap%3A%20public%20key%20encryption&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProcedural%20knowledge%20gap%3A%20public%20key%20encryption%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cCJ5npCLWLqkdXH8%2Fprocedural-knowledge-gap-public-key-encryption%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Procedural%20knowledge%20gap%3A%20public%20key%20encryption%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cCJ5npCLWLqkdXH8%2Fprocedural-knowledge-gap-public-key-encryption", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cCJ5npCLWLqkdXH8%2Fprocedural-knowledge-gap-public-key-encryption", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>In the spirit of uncovering procedural knowledge gaps, I'd like to know how to use public key encryption.</p>\n<p>Is there some website which generates public and private keys, and lets you encode and decode according to those keys?</p>\n<p>I'd love if there was some way I could send my encoded text via IM or email, and just decode it like we do with rot13. Is there some way of doing this?</p>\n<p>Currently, I encrypt things using TrueCrypt, but there's no way that I can communicate with people with that without securely establishing a common key beforehand.</p>\n<p>Does anyone know how to do this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5cCJ5npCLWLqkdXH8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 4, "extendedScore": null, "score": 8.303278501164905e-07, "legacy": true, "legacyId": "12115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T08:37:48.368Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Cluster Structure of Thingspace", "slug": "seq-rerun-the-cluster-structure-of-thingspace", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cKrHa9M9JfiKQxbeM/seq-rerun-the-cluster-structure-of-thingspace", "pageUrlRelative": "/posts/cKrHa9M9JfiKQxbeM/seq-rerun-the-cluster-structure-of-thingspace", "linkUrl": "https://www.lesswrong.com/posts/cKrHa9M9JfiKQxbeM/seq-rerun-the-cluster-structure-of-thingspace", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Cluster%20Structure%20of%20Thingspace&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Cluster%20Structure%20of%20Thingspace%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKrHa9M9JfiKQxbeM%2Fseq-rerun-the-cluster-structure-of-thingspace%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Cluster%20Structure%20of%20Thingspace%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKrHa9M9JfiKQxbeM%2Fseq-rerun-the-cluster-structure-of-thingspace", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcKrHa9M9JfiKQxbeM%2Fseq-rerun-the-cluster-structure-of-thingspace", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>Today's post, <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">The Cluster Structure of Thingspace</a> was originally published on 08 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A verbal definition works well enough in practice to point out the intended cluster of similar things, but you nitpick exceptions.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9bc/seq_rerun_typicality_and_asymmetrical_similarity/\">Typicality and Asymmetrical Similarity</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cKrHa9M9JfiKQxbeM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.30351224352026e-07, "legacy": true, "legacyId": "12116", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WBw8dDkAWohFjWQSk", "Wmi3hFcMAydX6g8Yf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T10:45:13.633Z", "modifiedAt": null, "url": null, "title": "Q&A with experts on risks from AI #3", "slug": "q-and-a-with-experts-on-risks-from-ai-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.975Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jv9kyH5WvqiXifsWJ/q-and-a-with-experts-on-risks-from-ai-3", "pageUrlRelative": "/posts/Jv9kyH5WvqiXifsWJ/q-and-a-with-experts-on-risks-from-ai-3", "linkUrl": "https://www.lesswrong.com/posts/Jv9kyH5WvqiXifsWJ/q-and-a-with-experts-on-risks-from-ai-3", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20experts%20on%20risks%20from%20AI%20%233&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20experts%20on%20risks%20from%20AI%20%233%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJv9kyH5WvqiXifsWJ%2Fq-and-a-with-experts-on-risks-from-ai-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20experts%20on%20risks%20from%20AI%20%233%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJv9kyH5WvqiXifsWJ%2Fq-and-a-with-experts-on-risks-from-ai-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJv9kyH5WvqiXifsWJ%2Fq-and-a-with-experts-on-risks-from-ai-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4926, "htmlBody": "<p><strong>[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI.</p>\n<p><strong>Dr. Pei Wang</strong> is trying to build general-purpose AI systems, compare them with human intelligence, analyze their theoretical assumptions, and evaluate their potential and limitation. [<a href=\"http://sites.google.com/site/narswang/PeiWangCV\">Curriculum Vitae</a>] [<a href=\"http://hplusmagazine.com/2011/01/27/pei-wang-path-artificial-general-intelligence/\">Pei Wang on the Path to Artificial General Intelligence</a>]</p>\n<p><strong>Dr. J. Storrs Hall</strong> is an independent scientist and author. His most recent book is <a href=\"http://www.amazon.com/Beyond-AI-Creating-Conscience-Machine/dp/1591025117\">Beyond AI: Creating the Conscience of the Machine</a>, published by <a href=\"http://www.prometheusbooks.com/\">Prometheus Books.</a> It is about the (possibly) imminent development of strong AI, and the desirability, if and when that happens, that such AIs be equipped with a moral sense and conscience. This is an outgrowth of his essay <a href=\"http://autogeny.org/ethics.html\">Ethics for Machines</a>. [<a href=\"http://autogeny.org/\">Homepage</a>]</p>\n<p><strong>Professor Paul Cohen</strong> is the director of the <em>School of Information: Science, Technology, and Arts</em> at the University of Arizona. His research is in artificial intelligence. He wants to model human cognitive development in silico, with robots or softbots in game environments as the \"babies\" they're trying to raise up. he is particularly interested in the sensorimotor foundations of human language. Several of his projects in the last decade have developed algorithms for sensor-to-symbol kinds of processing in service of learning the meanings of words, most recently, verbs. He also works in what they call Education Informatics, which includes intelligent tutoring systems, data mining and statistical modeling of students' mastery and engagement, assessment technologies, ontologies for representing student data and standards for content, architectures for content delivery, and so on. [<a href=\"http://www.sista.arizona.edu/~cohen/\">Homepage</a>]</p>\n<h3><strong>The Interview</strong>:</h3>\n<p><strong>Q1:</strong> <em>Assuming </em><em>beneficial political and economic development and that </em><em>no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of artificial intelligence that is roughly</em><em> </em><em>as good as</em> <em>humans at science, mathematics, engineering and programming?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>My estimations are, very roughly, 2020/2030/2050, respectively.</p>\n<p style=\"padding-left: 30px;\">Here by \"roughly as good as humans\" I mean the AI will follow roughly the same principles as human in information processing, though it does not mean that the system will have the same behavior or capability as human, due to the difference in body, experience, motivation, etc.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>2020 / 2030 / 2040</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>I wish the answer were simple. &nbsp;As early as the 1970s, AI programs were making modest scientific discoveries and discovering (or more often, rediscovering) bits of mathematics. &nbsp;Computer-based proof checkers are apparently common in math, though I don't know anything about them. &nbsp;If you are asking when machines will function as complete, autonomous scientists (or anything else) I'd say there's little reason to think that that's what we want. &nbsp;For another few decades we will be developing assistants, amplifiers, and parts of the scientific/creative process. &nbsp;There are communities who strive for complete and autonomous automated scientists, but last time I looked, a couple of years back, it was \"look ma, no hands\" demonstrations with little of interest under the hood. On the other hand, joint machine-human efforts, especially those that involve citizen scientists (e.g., Galaxy Zoo, Foldit) are apt to be increasingly productive.</p>\n<p><strong>Q2</strong>: <em>Once we build AI </em><em>that is roughly</em><em> </em><em>as good as</em> <em>humans at science, mathematics, engineering and programming, how much more difficult will it be for humans and/or AIs to build an AI which is substantially better at those activities than humans?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>After that, AI can become more powerful (in hardware), more knowledgeable, and therefore more capable in problem solving, than human beings. However, there is no evidence to believe that it can be \"substantially better\" in the principles defining intelligence.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>Difficult in what sense? &nbsp;Make 20 000 copies of your AI and organize them as Google or Apple. The difficulty is economic, not technical.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>It isn't hard to do better than humans. The earliest expert systems outperformed most humans. You can't beat a machine at chess. etc. &nbsp;Google is developing cars that I think will probably drive better than humans. The Google search engine does what no human can.&nbsp;</p>\n<p><em><strong>Q3:</strong> Do you ever expect artificial intelligence to overwhelmingly outperform humans at typical academic research, in the way that they may soon overwhelmingly outperform humans at trivia contests, or do you expect that humans will always play an important role in scientific progress?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>Even when AI follows the same principles as humans, and has more computational power and other resources than humans, they won't \"overwhelmingly outperform humans\" in all activities, due to the difference in hardware, experience, and motivations. There will always be some tasks that humans do better, and others that machines do better.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>A large part of academic research is entertainment and status fights, and it doesn't really matter whether machines are good at that or not. &nbsp;A large part of scientific research and technical development is experimentation and data gathering, and these are mostly resource-limited rather than smarts-limited. increasing AI intelligence doesn't address the bottleneck.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>One fundamental observation from sixty years of AI is that generality is hard, specialization is easy. &nbsp;This is one reason that Watson is a greater accomplishment than Deep Blue. &nbsp;Scientists specialize (although, arguably, the best scientists are not ultra-specialists but maintain a broad-enough perspective to see connections that lead to new work). &nbsp;So a narrow area of science is easier than the common sense that a home-help robot will need. &nbsp; I think it's very likely that in some areas of science, machines will do much of the creative work and also the drudge work.</p>\n<p><strong>Q4: </strong><em>What probability do you assign to the possibility of </em><em>an AI with initially (professional) </em><em>human-level competence </em><em>at general reasoning</em><em> (</em><em>including science, mathematics, engineering and programming)</em><em> to self-modify its way up to vastly superhuman </em><em>capabilities within a matter of hours/days/&lt; 5 years?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>Though there are speculations for such a \"self-modifying to superhuman\" scenario, all of them contains various wrong or unsupported assumptions. I haven't been convinced for such a possibility at all. It is possible for AI systems to become more and more capable, but I don't think they will become completely uncontrollable or incomprehensible.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>This depends entirely on when it starts, i.e. what is the current marginal cost of computation along the Moore's Law curve. A reductio ad adsurdum: in 1970, when all the computers in the world might possibly have sufficed to run one human-equivalent program, the amount of work it would have had to do to improve to superhuman would be way out of its grasp. In 2050, it will probably be trivial, since computation will be extremely cheap and the necessary algorithms and knowledge bases will likely be available as open source.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>The first step is the hardest: &nbsp;\"human level competence at general reasoning\" is our greatest challenge. &nbsp;I am quite sure that anything that could, say, read and understand what it reads would in a matter of days, weeks or months become vastly more generative than humans. &nbsp;But the first step is still well beyond our grasp.</p>\n<p><strong>Q5: </strong><em>How important is it to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to build AI that is good enough at </em><em>general reasoning</em><em> (</em><em>including science, mathematics, engineering and programming)</em><em> to undergo radical self-modification?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>I think the idea \"to make superhuman AI provably friendly\" is similar to the idea \"to make airplane provably safe\" and \"to make baby provably ethical\" --- though the motivation is respectful, the goal cannot be accurately defined, and the approximate definitions cannot be reached.</p>\n<p style=\"padding-left: 30px;\">What if the Wright brothers were asked \"to figure out how to make airplane provably safe before attempting to build it\", or all parents are asked \"to figure out how to make children provably ethical before attempting to have them\"?</p>\n<p style=\"padding-left: 30px;\">Since an AI system is adaptive (according to my opinion, as well as many others'), its behaviors won't be fully determined by its initial state or design (nature), but strongly influenced by its experience (nurture). You cannot make a friendly AI (whatever it means), but have to educate an AI to become friendly. Even in that case, it cannot be \"provably friendly\" --- only mathematical conclusions can be proved, and empirical predictions are always fallible.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>This is approximately like saying we need to require a proof, based on someone's DNA sequence, that they can never commit a sin, and that we must not allow any babies to be born until they can offer such a proof.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen:&nbsp; </strong>Same answer as above. Today we can build ultra specialist assistants (and so maintain control and make the ethical decisions ourselves) and we can't go further until we solve the problems of general intelligence -- vision, language understanding, reading, reasoning...</p>\n<p><strong>Q6:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of AI </em><em>capable of self-modification (that is <strong>not </strong>provably non-dangerous, if that is even possible)</em><em>? P(human extinction by AI | AI capable of self-modification and not provably non-dangerous is created)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>I don't think it makes much sense to talk about \"probability\" here, except to drop all of its mathematical meaning.</p>\n<p style=\"padding-left: 30px;\">Which discovery is \"provably non-dangerous\"? &nbsp;Physics, chemistry, and biology are all responsible for known ways to human extinction. Should we pause all these explorations until they are \"provably safe\"? How about the use of fire? Would the human species do better without using this \"provably dangerous\" technique?</p>\n<p style=\"padding-left: 30px;\">AI systems, like all major scientific and technical results, can lead to human extinction, but it is not the reason to stop or pause this research. Otherwise we cannot do anything, since every non-trivial action has unanticipated consequences. Though it is important to be aware of the potential danger of AI, we probably have no real alternative but to take this opportunity and challenge, and making our best decisions according to their predicted consequences.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall:&nbsp; </strong>This is unlikely but not inconceivable. &nbsp;If it happens, however, it will be because the AI was part of a doomsday device probably built by some military for \"mutual assured destruction\", and some other military tried to call their bluff. The best defense against this is for the rest of the world to be as smart as possible as fast as possible.</p>\n<p style=\"padding-left: 30px;\">To sum up, AIs can and should be vetted with standard and well-understood quality assurance and testing techniques, but defining \"friendliness to the human race\", much less proving it, is a pipe dream.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>From where I sit today, near zero. &nbsp;Besides, the danger is likely to be mostly on the human side: Irrespective of what machines can or cannot do, we will continue to be lazy, self-righteous, jingoistic, squanderers of our tiny little planet. It seems to me much more likely that we destroy will our universities and research base and devote ourselves to wars over what little remains of our water and land. &nbsp;If the current anti-intellectual rhetoric continues, if we continue to reject science for ignorance and God, then we will first destroy the research base that can produce intelligent machines and then destroy the planet. &nbsp;So I wouldn't worry too much about Dr. Evil and her Annihilating AI. &nbsp;We have more pressing matters to worry about.</p>\n<h3>William Uther</h3>\n<p>Dr. William Uther [<a href=\"http://www.cse.unsw.edu.au/~willu/\">Homepage</a>] answered two sets of old questions and also made some additional comments.</p>\n<p style=\"padding-left: 30px;\"><strong>William Uther:</strong> I can only answer for myself, not for my employer or anyone else (or even my future self).<br /> <br /> I have a few comments before I start:</p>\n<ul style=\"padding-left: 30px;\">\n<li>You ask a lot about 'human level AGI'. &nbsp;I do not think this term is well defined. &nbsp;It assumes that 'intelligence' is a one-dimensional quantity. &nbsp;It isn't. &nbsp;We already have AI systems that play chess better than the best humans, and mine data (one definition of 'learn') better than humans. &nbsp;Robots can currently drive cars roughly as well as humans can. &nbsp;We don't yet have a robot than can clean up a child's messy bedroom. &nbsp;Of course, we don't have children that can do that either. :)</li>\n<li>Intelligence is different from motivation. &nbsp;Each is different from consciousness. &nbsp;You seem to be envisioning a robot as some sort of super-human, self-motivated, conscious device. &nbsp;I don't know any AI researchers working towards that goal. &nbsp;(There may well be some, but I don't know them.) &nbsp;As such the problems we're likely to have with AI are less 'Terminator' and more 'Sorcerer's apprentice' (see <a href=\"http://en.wikipedia.org/wiki/The_Sorcerer%27s_Apprentice\" target=\"_blank\">http://en.wikipedia.org/wiki/The_Sorcerer's_Apprentice</a> ). &nbsp;These types of problems are less worrying as, in general, the AI isn't trying to actively hurt humans.</li>\n<li>As you bring up in one of you later questions, I think there are far more pressing worries at the moment than AI run amok.</li>\n</ul>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em><br /> <br /> <em>Explanatory remark to Q1:</em><br /> <br /> <em>P(human-level AI by (year) | no wars &and; no disasters &and; beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>As I said above, I don't think this question is well specified. &nbsp;It assumes that 'intelligence' is a one-dimensional quantity. &nbsp;It isn't. &nbsp;We already have AI systems that play chess better than the best humans, and mine data (one definition of learn) better than humans. &nbsp;Robots can currently drive cars roughly as well as humans can. &nbsp;We don't yet have a robot than can clean up a child's messy bedroom. &nbsp;Of course, we don't have children that can do that either. :)</p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of badly done AI?</em><br /> <br /> <em>Explanatory remark to Q2:</em><br /> <br /> <em>P(human extinction | badly done AI) = ?</em></p>\n<p><em>(Where 'badly done' = AGI capable of self-modification that is <strong>not </strong>provably non-dangerous.)</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>Again, I don't think your question is well specified. &nbsp;Most AI researchers are working on AI as a tool: given a task, the AI tries to figure out how to do it. &nbsp;They're working on artificial intelligence, not artificial self-motivation. &nbsp;I don't know that we could even measure something like 'artificial consciousness'.<br /> <br /> All tools increase the power of those that use them. &nbsp;But where does the blame lie if something goes wrong with the tool? &nbsp;In the terms of the US gun debate: Do guns kill people? &nbsp;Do people kill people? &nbsp;Do gun manufacturers kill people? &nbsp;Do kitchen knife manufacturers kill people?<br /> <br /> Personally, I don't think 'Terminator' style machines run amok is a very likely scenario. &nbsp;Hrm - I should be clearer here. &nbsp;I believe that there are already AI systems that have had malfunctions and killed people (see <a href=\"http://www.wired.com/dangerroom/2007/10/robot-cannon-ki/\" target=\"_blank\">http://www.wired.com/dangerroom/2007/10/robot-cannon-ki/</a> ). &nbsp;I also believe that when fire was first discovered there was probably some early caveman that started a forest fire and got himself roasted. &nbsp;He could even have roasted most of his village. &nbsp;I do not believe that mankind will build AI systems that will systematically seek out and deliberately destroy all humans (e.g. 'Skynet'), and I further believe that if someone started a system like this it would be destroyed by everyone else quite quickly.<br /> <br /> It isn't hard to build in an 'off' switch. &nbsp;In most cases that is a very simple solution to 'Skynet' style problems.<br /> <br /> I think there are much more worrying developments in the biological sciences. &nbsp;See <a href=\"http://www.nytimes.com/2012/01/08/opinion/sunday/an-engineered-doomsday.html\" target=\"_blank\">http://www.nytimes.com/2012/01/08/opinion/sunday/an-engineered-doomsday.html</a></p>\n<p><strong>Q3: </strong><em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </em><br /> <br /> <em>Explanatory remark to Q3:</em></p>\n<p><em>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>Again, your question is poorly specified. &nbsp;What do you mean by 'human level AGI'? &nbsp;Trying to tease this apart, do you mean a robotic system that if trained up for 20 years like a human would end up as smart as a human 20-year-old? Are you referring to that system before the 20 years learning, or after?<br /> <br /> In general, if the system has 'human level' AGI, then surely it will behave the same way as a human. &nbsp;In which case none of your scenarios are likely - I've had an internet connection for years and I'm not super-human yet.</p>\n<p><strong>Q4: </strong><em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em><br /> <br /> <em>Explanatory remark to Q4:</em></p>\n<p><em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I think this is a worthwhile goal for a small number of researchers to think about, but I don't think we need many. &nbsp;I think we are far enough away from 'super-intelligences' that it isn't urgent. &nbsp;In particular, I don't think that having 'machines smarter than humans' is some sort of magical tipping point. &nbsp;AI is HARD. &nbsp;Having machines that are smarter than humans means they'll make progress faster than humans would. &nbsp;It doesn't mean they'll make progress massively faster than humans would in the short term.<br /> <br /> I also think there are ethical issues worth considering before we have AGI. &nbsp;See <a href=\"http://m.theatlantic.com/technology/print/2011/12/drone-ethics-briefing-what-a-leading-robot-expert-told-the-cia/250060/\" target=\"_blank\">http://m.theatlantic.com/technology/print/2011/12/drone-ethics-briefing-what-a-leading-robot-expert-told-the-cia/250060/</a><br /> <br /> Note that none of those ethical issues assume some sort of super-intelligence. &nbsp;In the same that ethics in humans doesn't assume super-intelligence.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?<br /><br /></em><em>Explanatory remark to Q5:</em></p>\n<p><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I have a few worries. &nbsp;From the top of my head:</p>\n<ul style=\"padding-left: 30px;\">\n<li>i) Global warming. &nbsp;While not as urgent or sexy as AI-run-amok, I think it a far more important issue for humankind.</li>\n<li>ii) biological warfare / terrorism / insanity. &nbsp;See the article I linked to above: <a href=\"http://www.nytimes.com/2012/01/08/opinion/sunday/an-engineered-doomsday.html\" target=\"_blank\">http://www.nytimes.com/2012/01/08/opinion/sunday/an-engineered-doomsday.html</a></li>\n</ul>\n<p><strong>Q6: </strong><em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I think most people aren't worried about AI risks. &nbsp;I don't think they should be. &nbsp;I don't see a problem here.</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human-level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I still don't know what you mean by 'human level intelligence'. &nbsp;I expect artificial intelligence to be quite different to human intelligence. &nbsp;AI is already common in many businesses - if you have a bank loan then the decision about whether to lend to you was probably taken by a machine learnt system.</p>\n<p><strong>Q1a:</strong> <em>Assuming beneficially political and economic development and that no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of machine intelligence with roughly human-level efficient cross-domain optimization competence?</em></p>\n<p><strong>Q1b:</strong> <em>Once our understanding of AI and our hardware capabilities are sufficiently sophisticated to build an AI which is as good as humans at engineering or programming, how much more difficult will it be to build an AI which is substantially better than humans at mathematics or engineering?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>There is a whole field of 'automatic programming'. &nbsp;The main difficulties in that field were in specifying what you wanted programmed. &nbsp;Once you'd done that the computers were quite effective at making it. &nbsp;(I'm not sure people have tried to make computers design complex algorithms and data structures yet.)</p>\n<p><strong>Q2a:</strong> <em>Do you ever expect automated systems to overwhelmingly outperform humans at typical academic research, in the way that they may soon overwhelmingly outperform humans at trivia contests, or do you expect that humans will always play an important role in scientific progress?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I think asking about 'automated science' is a much clearer question than asking about 'Human level AGI'. &nbsp;At the moment there is already huge amounts of automation in science (from Peter Cheeseman's early work with AutoClass to the biological 'experiments on a chip' that allow a large number of parallel tests to be run). &nbsp;What is happening is similar to automation in other areas - the simpler tasks (both intellectual and physical) are being automated away and the humans are working at higher levels of abstraction. &nbsp;There will always be *a* role for humans in scientific research (in much the same way that there is currently a role for program managers in current research - they decide at a high level what research should be done after understanding as much of it as they choose).</p>\n<p><strong>Q2b: </strong><em>To what extent does human engineering and mathematical ability rely on many varied aspects of human cognition, such as social interaction and embodiment? For example, would an AI with human-level skill at mathematics and programming be able to design a new AI with sophisticated social skills, or does that require an AI which already possesses sophisticated social skills?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>Social skills require understanding humans. &nbsp;We have no abstract mathematical model of humans as yet to load into a machine, and so the only way you can learn to understand humans is by experimenting on them... er, I mean, interacting with them. :) &nbsp;That takes time, and humans who are willing to interact with you.</p>\n<p style=\"padding-left: 30px;\">Once you have the model, coming up with optimal plans for interacting with it, i.e. social skills, can happen offline. &nbsp;It is building the model of humans that is the bottleneck for an infinitely powerful machine.</p>\n<p style=\"padding-left: 30px;\">I guess you cold parallelise it by interacting with each human on the planet simultaneously. &nbsp;That would gather a large amount of data quite quickly, but be tricky to organise. &nbsp;And certain parts of learning about a system cannot be parallelised.</p>\n<p><strong>Q2c:</strong> <em>What probability do you assign to the possibility of an AI with initially (professional) human-level competence at mathematics and programming to self-modify its way up to massive superhuman efficient cross-domain optimization competence within a matter of hours/days/&lt; 5 years?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>One possible outcome is that we find out that humans are close to optimal problem solvers given the resources they allocate to the problem. &nbsp;In which case, 'massive superhuman cross-domain optimisation' may simply not be possible.</p>\n<p style=\"padding-left: 30px;\">Humans are only an existence proof for human level intelligence.</p>\n<p><strong>Q7: </strong><em>How much have you read about the formal concepts of optimal AI design which relate to searches over complete spaces of computable hypotheses or computational strategies, such as Solomonoff induction, Levin search, Hutter's algorithm M, AIXI, or G&ouml;del machines?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I know of all of those. &nbsp;I know some of the AIXI approximations quite well. &nbsp;The lesson I draw from all of those is that AI is HARD. &nbsp;In fact, the real question isn't how do you perform optimally, but how do you perform well enough given the resources you have at hand. &nbsp;Humans are a long way from optimal, but they do quite well given the resources they have.</p>\n<p style=\"padding-left: 30px;\">I'd like to make some other points as well:</p>\n<ul style=\"padding-left: 30px;\">\n<li>When trying to define 'Human level intelligence' it is often useful to consider how many humans meet your standard. &nbsp;If the answer is 'not many' then you don't have a very good measure of human level intelligence. &nbsp;Does Michael Jordan (the basketball player) have human level intelligence? &nbsp;Does Stephen Hawking? &nbsp;Does George Bush?</li>\n<li>People who are worried about the singularity often have two classes of concerns. &nbsp;First there is the class of people who worry about robots taking over and just leaving humans behind. &nbsp;I think that is highly unlikely. &nbsp;I think it much more likely that humans and machines will interact and progress together. &nbsp;Once I have my brain plugged in to an advanced computer there will be no AI that can out-think me. &nbsp;Computers already allow us to 'think' in ways that we couldn't have dreamt of 50 years ago.</li>\n</ul>\n<p style=\"padding-left: 30px;\">This brings up the second class of issues that people have. &nbsp;Once we are connected to machines, will we still be truly human. &nbsp;I have no idea what people who worry about this mean by 'truly human'. &nbsp;Is a human with a prosthetic limb truly human? &nbsp;How about a human driving a car? &nbsp;Is a human who wears classes or a hearing aid truly human? &nbsp;If these prosthetics make you non-human, then we're already past the point where they should be concerned - and they're not. &nbsp;If these prosthetics leave you human, then why would a piece of glass that allows me to see clearly be ok, and a computer that allows me to think clearly not be ok? &nbsp;Asimov investigated ideas similar to this, but from a slightly different point of view, with his story 'The Bicentennial Man'.</p>\n<p style=\"padding-left: 30px;\">The real questions are ones of ethics. &nbsp;As people become more powerful, what are the ethical ways of using that power? &nbsp;I have no great wisdom to share there, unfortunately.</p>\n<p style=\"padding-left: 30px;\">Some more thoughts...</p>\n<p style=\"padding-left: 30px;\">Does a research lab (with, say, 50 researchers) have \"above human level intelligence\"? &nbsp;If not, then it isn't clear to me that AI will ever have significantly \"above human level intelligence\" (and see below for why AI is still worthwhile). &nbsp;If so, then why haven't we had a 'research lab singularity' yet? &nbsp;Surely research labs are smarter than humans and so they can work on making still smarter research labs, until a magical point is passed and research labs have runaway intelligence. &nbsp;(That's a socratic question designed to get you to think about possible answers yourself. &nbsp;Maybe we are in the middle of a research lab singularity.)</p>\n<p style=\"padding-left: 30px;\">As for why study of AI might still be useful even if we never get above human level intelligence: there is the same Dirty, Dull, Dangerous argument that has been used many times. &nbsp;To that I'd add a point I made in a previous email: intelligence is different to motivation. &nbsp;If you get yourself another human you get both - they're intelligent, but they also have their own goals and you have to spend time convincing them to work towards your goals. &nbsp;If you get an AI, then even if it isn't more intelligent than a human at least all that intelligence is working towards your goals without argument. &nbsp;It's similar to the 'Dull' justification, but with a slightly different spin.</p>\n<h3>Alan Bundy</h3>\n<p>Professor Alan Bundy [<a href=\"http://homepages.inf.ed.ac.uk/bundy/\">homepage</a>] did not answer my questions directly.</p>\n<p style=\"padding-left: 30px;\"><strong>Alan Bundy:</strong> Whenever I see questions like this I want to start by questioning the implicit assumptions behind them.</p>\n<ul style=\"padding-left: 30px;\">\n<li>I don't think the concept of \"human-level machine intelligence\" is well formed. AI is defining a huge space of different kinds of intelligence. Most of the points in this space are unlike anything either human or animal, but are new kinds of intelligence. Most of them are very specialised to particular areas of expertise. As an extreme example, consider the best chess playing programs. They are more intelligent than any human <em>at playing chess</em>, but can do nothing else, e.g., pick up and move the pieces. There's a popular myth that intelligence is on a linear scale, like IQ, and AI is progressing along it. If so, where would you put the chess program?</li>\n<li>The biggest threat from computers comes not from intelligence but from ignorance, i.e., from computer programs that are not as smart as they need to be. I'm thinking especially of safety critical and security critical systems, such as fly-by-wire aircraft and financial trading systems. When these go wrong, aircraft crash and people are killed or the economy collapses. Worrying about intelligent machines distracts us from the real threats. </li>\n<li>As far as threats go, you can't separate the machines from the intentions of their owners. Quite stupid machines entrusted to run a war with weapons of mass destruction could cause quite enough havoc without waiting for the mythical \"human-level machine intelligence\". It will be human owners that endow their machines with goals and aims. The less intelligent the machines the more likely this is to end in tears. </li>\n<li>Given the indeterminacy of their owner's intentions, it's quite impossible to put probabilities on the questions you ask. Even if we could precisely predict the progress of the technology, which we can't, the intentions of the owners would defeat our estimates.</li>\n</ul>\n<p style=\"padding-left: 30px;\">I'm familiar with what you call the 'standard worry'. I've frequently been recruited to public debate with Kevin Warwick, who has popularised this 'worry'. I'd be happy for you to publish my answers. I'd add one more point, which I forgot to include yesterday.</p>\n<ul style=\"padding-left: 30px;\">\n<li>Consider the analogy with 'bird level flight'. Long before human flight, people aspired to fly like birds. The reality of human flight turned out to be completely different. In some respects, 'artificial' flying machines are superior to birds, e.g., they are faster. In some respects they are inferior, e.g., you have to be at the airport hours before take-off and book well in advance. The flight itself is very different, e.g., aircraft don't flap their wings. There is not much research now on flying like birds. If we really wanted to do it, we could no doubt come close, e.g., with small model birds with flapping wings, but small differences would remain and a law of diminishing returns would set in if we wanted to get closer. I think the aspiration for 'human level machine intelligence' will follow a similar trajectory --- indeed, it already has. </li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jv9kyH5WvqiXifsWJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 19, "extendedScore": null, "score": 8.303993998757151e-07, "legacy": true, "legacyId": "12118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Click_here_to_see_a_list_of_all_interviews_\">[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI.</p>\n<p><strong>Dr. Pei Wang</strong> is trying to build general-purpose AI systems, compare them with human intelligence, analyze their theoretical assumptions, and evaluate their potential and limitation. [<a href=\"http://sites.google.com/site/narswang/PeiWangCV\">Curriculum Vitae</a>] [<a href=\"http://hplusmagazine.com/2011/01/27/pei-wang-path-artificial-general-intelligence/\">Pei Wang on the Path to Artificial General Intelligence</a>]</p>\n<p><strong>Dr. J. Storrs Hall</strong> is an independent scientist and author. His most recent book is <a href=\"http://www.amazon.com/Beyond-AI-Creating-Conscience-Machine/dp/1591025117\">Beyond AI: Creating the Conscience of the Machine</a>, published by <a href=\"http://www.prometheusbooks.com/\">Prometheus Books.</a> It is about the (possibly) imminent development of strong AI, and the desirability, if and when that happens, that such AIs be equipped with a moral sense and conscience. This is an outgrowth of his essay <a href=\"http://autogeny.org/ethics.html\">Ethics for Machines</a>. [<a href=\"http://autogeny.org/\">Homepage</a>]</p>\n<p><strong>Professor Paul Cohen</strong> is the director of the <em>School of Information: Science, Technology, and Arts</em> at the University of Arizona. His research is in artificial intelligence. He wants to model human cognitive development in silico, with robots or softbots in game environments as the \"babies\" they're trying to raise up. he is particularly interested in the sensorimotor foundations of human language. Several of his projects in the last decade have developed algorithms for sensor-to-symbol kinds of processing in service of learning the meanings of words, most recently, verbs. He also works in what they call Education Informatics, which includes intelligent tutoring systems, data mining and statistical modeling of students' mastery and engagement, assessment technologies, ontologies for representing student data and standards for content, architectures for content delivery, and so on. [<a href=\"http://www.sista.arizona.edu/~cohen/\">Homepage</a>]</p>\n<h3 id=\"The_Interview_\"><strong>The Interview</strong>:</h3>\n<p><strong>Q1:</strong> <em>Assuming </em><em>beneficial political and economic development and that </em><em>no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of artificial intelligence that is roughly</em><em> </em><em>as good as</em> <em>humans at science, mathematics, engineering and programming?</em><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>My estimations are, very roughly, 2020/2030/2050, respectively.</p>\n<p style=\"padding-left: 30px;\">Here by \"roughly as good as humans\" I mean the AI will follow roughly the same principles as human in information processing, though it does not mean that the system will have the same behavior or capability as human, due to the difference in body, experience, motivation, etc.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>2020 / 2030 / 2040</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>I wish the answer were simple. &nbsp;As early as the 1970s, AI programs were making modest scientific discoveries and discovering (or more often, rediscovering) bits of mathematics. &nbsp;Computer-based proof checkers are apparently common in math, though I don't know anything about them. &nbsp;If you are asking when machines will function as complete, autonomous scientists (or anything else) I'd say there's little reason to think that that's what we want. &nbsp;For another few decades we will be developing assistants, amplifiers, and parts of the scientific/creative process. &nbsp;There are communities who strive for complete and autonomous automated scientists, but last time I looked, a couple of years back, it was \"look ma, no hands\" demonstrations with little of interest under the hood. On the other hand, joint machine-human efforts, especially those that involve citizen scientists (e.g., Galaxy Zoo, Foldit) are apt to be increasingly productive.</p>\n<p><strong>Q2</strong>: <em>Once we build AI </em><em>that is roughly</em><em> </em><em>as good as</em> <em>humans at science, mathematics, engineering and programming, how much more difficult will it be for humans and/or AIs to build an AI which is substantially better at those activities than humans?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>After that, AI can become more powerful (in hardware), more knowledgeable, and therefore more capable in problem solving, than human beings. However, there is no evidence to believe that it can be \"substantially better\" in the principles defining intelligence.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>Difficult in what sense? &nbsp;Make 20 000 copies of your AI and organize them as Google or Apple. The difficulty is economic, not technical.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>It isn't hard to do better than humans. The earliest expert systems outperformed most humans. You can't beat a machine at chess. etc. &nbsp;Google is developing cars that I think will probably drive better than humans. The Google search engine does what no human can.&nbsp;</p>\n<p><em><strong>Q3:</strong> Do you ever expect artificial intelligence to overwhelmingly outperform humans at typical academic research, in the way that they may soon overwhelmingly outperform humans at trivia contests, or do you expect that humans will always play an important role in scientific progress?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>Even when AI follows the same principles as humans, and has more computational power and other resources than humans, they won't \"overwhelmingly outperform humans\" in all activities, due to the difference in hardware, experience, and motivations. There will always be some tasks that humans do better, and others that machines do better.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>A large part of academic research is entertainment and status fights, and it doesn't really matter whether machines are good at that or not. &nbsp;A large part of scientific research and technical development is experimentation and data gathering, and these are mostly resource-limited rather than smarts-limited. increasing AI intelligence doesn't address the bottleneck.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>One fundamental observation from sixty years of AI is that generality is hard, specialization is easy. &nbsp;This is one reason that Watson is a greater accomplishment than Deep Blue. &nbsp;Scientists specialize (although, arguably, the best scientists are not ultra-specialists but maintain a broad-enough perspective to see connections that lead to new work). &nbsp;So a narrow area of science is easier than the common sense that a home-help robot will need. &nbsp; I think it's very likely that in some areas of science, machines will do much of the creative work and also the drudge work.</p>\n<p><strong>Q4: </strong><em>What probability do you assign to the possibility of </em><em>an AI with initially (professional) </em><em>human-level competence </em><em>at general reasoning</em><em> (</em><em>including science, mathematics, engineering and programming)</em><em> to self-modify its way up to vastly superhuman </em><em>capabilities within a matter of hours/days/&lt; 5 years?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>Though there are speculations for such a \"self-modifying to superhuman\" scenario, all of them contains various wrong or unsupported assumptions. I haven't been convinced for such a possibility at all. It is possible for AI systems to become more and more capable, but I don't think they will become completely uncontrollable or incomprehensible.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>This depends entirely on when it starts, i.e. what is the current marginal cost of computation along the Moore's Law curve. A reductio ad adsurdum: in 1970, when all the computers in the world might possibly have sufficed to run one human-equivalent program, the amount of work it would have had to do to improve to superhuman would be way out of its grasp. In 2050, it will probably be trivial, since computation will be extremely cheap and the necessary algorithms and knowledge bases will likely be available as open source.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>The first step is the hardest: &nbsp;\"human level competence at general reasoning\" is our greatest challenge. &nbsp;I am quite sure that anything that could, say, read and understand what it reads would in a matter of days, weeks or months become vastly more generative than humans. &nbsp;But the first step is still well beyond our grasp.</p>\n<p><strong>Q5: </strong><em>How important is it to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to build AI that is good enough at </em><em>general reasoning</em><em> (</em><em>including science, mathematics, engineering and programming)</em><em> to undergo radical self-modification?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>I think the idea \"to make superhuman AI provably friendly\" is similar to the idea \"to make airplane provably safe\" and \"to make baby provably ethical\" --- though the motivation is respectful, the goal cannot be accurately defined, and the approximate definitions cannot be reached.</p>\n<p style=\"padding-left: 30px;\">What if the Wright brothers were asked \"to figure out how to make airplane provably safe before attempting to build it\", or all parents are asked \"to figure out how to make children provably ethical before attempting to have them\"?</p>\n<p style=\"padding-left: 30px;\">Since an AI system is adaptive (according to my opinion, as well as many others'), its behaviors won't be fully determined by its initial state or design (nature), but strongly influenced by its experience (nurture). You cannot make a friendly AI (whatever it means), but have to educate an AI to become friendly. Even in that case, it cannot be \"provably friendly\" --- only mathematical conclusions can be proved, and empirical predictions are always fallible.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall: </strong>This is approximately like saying we need to require a proof, based on someone's DNA sequence, that they can never commit a sin, and that we must not allow any babies to be born until they can offer such a proof.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen:&nbsp; </strong>Same answer as above. Today we can build ultra specialist assistants (and so maintain control and make the ethical decisions ourselves) and we can't go further until we solve the problems of general intelligence -- vision, language understanding, reading, reasoning...</p>\n<p><strong>Q6:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of AI </em><em>capable of self-modification (that is <strong>not </strong>provably non-dangerous, if that is even possible)</em><em>? P(human extinction by AI | AI capable of self-modification and not provably non-dangerous is created)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Pei Wang: </strong>I don't think it makes much sense to talk about \"probability\" here, except to drop all of its mathematical meaning.</p>\n<p style=\"padding-left: 30px;\">Which discovery is \"provably non-dangerous\"? &nbsp;Physics, chemistry, and biology are all responsible for known ways to human extinction. Should we pause all these explorations until they are \"provably safe\"? How about the use of fire? Would the human species do better without using this \"provably dangerous\" technique?</p>\n<p style=\"padding-left: 30px;\">AI systems, like all major scientific and technical results, can lead to human extinction, but it is not the reason to stop or pause this research. Otherwise we cannot do anything, since every non-trivial action has unanticipated consequences. Though it is important to be aware of the potential danger of AI, we probably have no real alternative but to take this opportunity and challenge, and making our best decisions according to their predicted consequences.</p>\n<p style=\"padding-left: 30px;\"><strong>J. Storrs Hall:&nbsp; </strong>This is unlikely but not inconceivable. &nbsp;If it happens, however, it will be because the AI was part of a doomsday device probably built by some military for \"mutual assured destruction\", and some other military tried to call their bluff. The best defense against this is for the rest of the world to be as smart as possible as fast as possible.</p>\n<p style=\"padding-left: 30px;\">To sum up, AIs can and should be vetted with standard and well-understood quality assurance and testing techniques, but defining \"friendliness to the human race\", much less proving it, is a pipe dream.</p>\n<p style=\"padding-left: 30px;\"><strong>Paul Cohen: </strong>From where I sit today, near zero. &nbsp;Besides, the danger is likely to be mostly on the human side: Irrespective of what machines can or cannot do, we will continue to be lazy, self-righteous, jingoistic, squanderers of our tiny little planet. It seems to me much more likely that we destroy will our universities and research base and devote ourselves to wars over what little remains of our water and land. &nbsp;If the current anti-intellectual rhetoric continues, if we continue to reject science for ignorance and God, then we will first destroy the research base that can produce intelligent machines and then destroy the planet. &nbsp;So I wouldn't worry too much about Dr. Evil and her Annihilating AI. &nbsp;We have more pressing matters to worry about.</p>\n<h3 id=\"William_Uther\">William Uther</h3>\n<p>Dr. William Uther [<a href=\"http://www.cse.unsw.edu.au/~willu/\">Homepage</a>] answered two sets of old questions and also made some additional comments.</p>\n<p style=\"padding-left: 30px;\"><strong>William Uther:</strong> I can only answer for myself, not for my employer or anyone else (or even my future self).<br> <br> I have a few comments before I start:</p>\n<ul style=\"padding-left: 30px;\">\n<li>You ask a lot about 'human level AGI'. &nbsp;I do not think this term is well defined. &nbsp;It assumes that 'intelligence' is a one-dimensional quantity. &nbsp;It isn't. &nbsp;We already have AI systems that play chess better than the best humans, and mine data (one definition of 'learn') better than humans. &nbsp;Robots can currently drive cars roughly as well as humans can. &nbsp;We don't yet have a robot than can clean up a child's messy bedroom. &nbsp;Of course, we don't have children that can do that either. :)</li>\n<li>Intelligence is different from motivation. &nbsp;Each is different from consciousness. &nbsp;You seem to be envisioning a robot as some sort of super-human, self-motivated, conscious device. &nbsp;I don't know any AI researchers working towards that goal. &nbsp;(There may well be some, but I don't know them.) &nbsp;As such the problems we're likely to have with AI are less 'Terminator' and more 'Sorcerer's apprentice' (see <a href=\"http://en.wikipedia.org/wiki/The_Sorcerer%27s_Apprentice\" target=\"_blank\">http://en.wikipedia.org/wiki/The_Sorcerer's_Apprentice</a> ). &nbsp;These types of problems are less worrying as, in general, the AI isn't trying to actively hurt humans.</li>\n<li>As you bring up in one of you later questions, I think there are far more pressing worries at the moment than AI run amok.</li>\n</ul>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em><br> <br> <em>Explanatory remark to Q1:</em><br> <br> <em>P(human-level AI by (year) | no wars \u2227 no disasters \u2227 beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>As I said above, I don't think this question is well specified. &nbsp;It assumes that 'intelligence' is a one-dimensional quantity. &nbsp;It isn't. &nbsp;We already have AI systems that play chess better than the best humans, and mine data (one definition of learn) better than humans. &nbsp;Robots can currently drive cars roughly as well as humans can. &nbsp;We don't yet have a robot than can clean up a child's messy bedroom. &nbsp;Of course, we don't have children that can do that either. :)</p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of badly done AI?</em><br> <br> <em>Explanatory remark to Q2:</em><br> <br> <em>P(human extinction | badly done AI) = ?</em></p>\n<p><em>(Where 'badly done' = AGI capable of self-modification that is <strong>not </strong>provably non-dangerous.)</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>Again, I don't think your question is well specified. &nbsp;Most AI researchers are working on AI as a tool: given a task, the AI tries to figure out how to do it. &nbsp;They're working on artificial intelligence, not artificial self-motivation. &nbsp;I don't know that we could even measure something like 'artificial consciousness'.<br> <br> All tools increase the power of those that use them. &nbsp;But where does the blame lie if something goes wrong with the tool? &nbsp;In the terms of the US gun debate: Do guns kill people? &nbsp;Do people kill people? &nbsp;Do gun manufacturers kill people? &nbsp;Do kitchen knife manufacturers kill people?<br> <br> Personally, I don't think 'Terminator' style machines run amok is a very likely scenario. &nbsp;Hrm - I should be clearer here. &nbsp;I believe that there are already AI systems that have had malfunctions and killed people (see <a href=\"http://www.wired.com/dangerroom/2007/10/robot-cannon-ki/\" target=\"_blank\">http://www.wired.com/dangerroom/2007/10/robot-cannon-ki/</a> ). &nbsp;I also believe that when fire was first discovered there was probably some early caveman that started a forest fire and got himself roasted. &nbsp;He could even have roasted most of his village. &nbsp;I do not believe that mankind will build AI systems that will systematically seek out and deliberately destroy all humans (e.g. 'Skynet'), and I further believe that if someone started a system like this it would be destroyed by everyone else quite quickly.<br> <br> It isn't hard to build in an 'off' switch. &nbsp;In most cases that is a very simple solution to 'Skynet' style problems.<br> <br> I think there are much more worrying developments in the biological sciences. &nbsp;See <a href=\"http://www.nytimes.com/2012/01/08/opinion/sunday/an-engineered-doomsday.html\" target=\"_blank\">http://www.nytimes.com/2012/01/08/opinion/sunday/an-engineered-doomsday.html</a></p>\n<p><strong>Q3: </strong><em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </em><br> <br> <em>Explanatory remark to Q3:</em></p>\n<p><em>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>Again, your question is poorly specified. &nbsp;What do you mean by 'human level AGI'? &nbsp;Trying to tease this apart, do you mean a robotic system that if trained up for 20 years like a human would end up as smart as a human 20-year-old? Are you referring to that system before the 20 years learning, or after?<br> <br> In general, if the system has 'human level' AGI, then surely it will behave the same way as a human. &nbsp;In which case none of your scenarios are likely - I've had an internet connection for years and I'm not super-human yet.</p>\n<p><strong>Q4: </strong><em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em><br> <br> <em>Explanatory remark to Q4:</em></p>\n<p><em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I think this is a worthwhile goal for a small number of researchers to think about, but I don't think we need many. &nbsp;I think we are far enough away from 'super-intelligences' that it isn't urgent. &nbsp;In particular, I don't think that having 'machines smarter than humans' is some sort of magical tipping point. &nbsp;AI is HARD. &nbsp;Having machines that are smarter than humans means they'll make progress faster than humans would. &nbsp;It doesn't mean they'll make progress massively faster than humans would in the short term.<br> <br> I also think there are ethical issues worth considering before we have AGI. &nbsp;See <a href=\"http://m.theatlantic.com/technology/print/2011/12/drone-ethics-briefing-what-a-leading-robot-expert-told-the-cia/250060/\" target=\"_blank\">http://m.theatlantic.com/technology/print/2011/12/drone-ethics-briefing-what-a-leading-robot-expert-told-the-cia/250060/</a><br> <br> Note that none of those ethical issues assume some sort of super-intelligence. &nbsp;In the same that ethics in humans doesn't assume super-intelligence.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?<br><br></em><em>Explanatory remark to Q5:</em></p>\n<p><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I have a few worries. &nbsp;From the top of my head:</p>\n<ul style=\"padding-left: 30px;\">\n<li>i) Global warming. &nbsp;While not as urgent or sexy as AI-run-amok, I think it a far more important issue for humankind.</li>\n<li>ii) biological warfare / terrorism / insanity. &nbsp;See the article I linked to above: <a href=\"http://www.nytimes.com/2012/01/08/opinion/sunday/an-engineered-doomsday.html\" target=\"_blank\">http://www.nytimes.com/2012/01/08/opinion/sunday/an-engineered-doomsday.html</a></li>\n</ul>\n<p><strong>Q6: </strong><em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I think most people aren't worried about AI risks. &nbsp;I don't think they should be. &nbsp;I don't see a problem here.</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human-level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I still don't know what you mean by 'human level intelligence'. &nbsp;I expect artificial intelligence to be quite different to human intelligence. &nbsp;AI is already common in many businesses - if you have a bank loan then the decision about whether to lend to you was probably taken by a machine learnt system.</p>\n<p><strong>Q1a:</strong> <em>Assuming beneficially political and economic development and that no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of machine intelligence with roughly human-level efficient cross-domain optimization competence?</em></p>\n<p><strong>Q1b:</strong> <em>Once our understanding of AI and our hardware capabilities are sufficiently sophisticated to build an AI which is as good as humans at engineering or programming, how much more difficult will it be to build an AI which is substantially better than humans at mathematics or engineering?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>There is a whole field of 'automatic programming'. &nbsp;The main difficulties in that field were in specifying what you wanted programmed. &nbsp;Once you'd done that the computers were quite effective at making it. &nbsp;(I'm not sure people have tried to make computers design complex algorithms and data structures yet.)</p>\n<p><strong>Q2a:</strong> <em>Do you ever expect automated systems to overwhelmingly outperform humans at typical academic research, in the way that they may soon overwhelmingly outperform humans at trivia contests, or do you expect that humans will always play an important role in scientific progress?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I think asking about 'automated science' is a much clearer question than asking about 'Human level AGI'. &nbsp;At the moment there is already huge amounts of automation in science (from Peter Cheeseman's early work with AutoClass to the biological 'experiments on a chip' that allow a large number of parallel tests to be run). &nbsp;What is happening is similar to automation in other areas - the simpler tasks (both intellectual and physical) are being automated away and the humans are working at higher levels of abstraction. &nbsp;There will always be *a* role for humans in scientific research (in much the same way that there is currently a role for program managers in current research - they decide at a high level what research should be done after understanding as much of it as they choose).</p>\n<p><strong>Q2b: </strong><em>To what extent does human engineering and mathematical ability rely on many varied aspects of human cognition, such as social interaction and embodiment? For example, would an AI with human-level skill at mathematics and programming be able to design a new AI with sophisticated social skills, or does that require an AI which already possesses sophisticated social skills?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>Social skills require understanding humans. &nbsp;We have no abstract mathematical model of humans as yet to load into a machine, and so the only way you can learn to understand humans is by experimenting on them... er, I mean, interacting with them. :) &nbsp;That takes time, and humans who are willing to interact with you.</p>\n<p style=\"padding-left: 30px;\">Once you have the model, coming up with optimal plans for interacting with it, i.e. social skills, can happen offline. &nbsp;It is building the model of humans that is the bottleneck for an infinitely powerful machine.</p>\n<p style=\"padding-left: 30px;\">I guess you cold parallelise it by interacting with each human on the planet simultaneously. &nbsp;That would gather a large amount of data quite quickly, but be tricky to organise. &nbsp;And certain parts of learning about a system cannot be parallelised.</p>\n<p><strong>Q2c:</strong> <em>What probability do you assign to the possibility of an AI with initially (professional) human-level competence at mathematics and programming to self-modify its way up to massive superhuman efficient cross-domain optimization competence within a matter of hours/days/&lt; 5 years?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>One possible outcome is that we find out that humans are close to optimal problem solvers given the resources they allocate to the problem. &nbsp;In which case, 'massive superhuman cross-domain optimisation' may simply not be possible.</p>\n<p style=\"padding-left: 30px;\">Humans are only an existence proof for human level intelligence.</p>\n<p><strong>Q7: </strong><em>How much have you read about the formal concepts of optimal AI design which relate to searches over complete spaces of computable hypotheses or computational strategies, such as Solomonoff induction, Levin search, Hutter's algorithm M, AIXI, or G\u00f6del machines?</em></p>\n<p style=\"padding-left: 30px;\"><strong>William Uther: </strong>I know of all of those. &nbsp;I know some of the AIXI approximations quite well. &nbsp;The lesson I draw from all of those is that AI is HARD. &nbsp;In fact, the real question isn't how do you perform optimally, but how do you perform well enough given the resources you have at hand. &nbsp;Humans are a long way from optimal, but they do quite well given the resources they have.</p>\n<p style=\"padding-left: 30px;\">I'd like to make some other points as well:</p>\n<ul style=\"padding-left: 30px;\">\n<li>When trying to define 'Human level intelligence' it is often useful to consider how many humans meet your standard. &nbsp;If the answer is 'not many' then you don't have a very good measure of human level intelligence. &nbsp;Does Michael Jordan (the basketball player) have human level intelligence? &nbsp;Does Stephen Hawking? &nbsp;Does George Bush?</li>\n<li>People who are worried about the singularity often have two classes of concerns. &nbsp;First there is the class of people who worry about robots taking over and just leaving humans behind. &nbsp;I think that is highly unlikely. &nbsp;I think it much more likely that humans and machines will interact and progress together. &nbsp;Once I have my brain plugged in to an advanced computer there will be no AI that can out-think me. &nbsp;Computers already allow us to 'think' in ways that we couldn't have dreamt of 50 years ago.</li>\n</ul>\n<p style=\"padding-left: 30px;\">This brings up the second class of issues that people have. &nbsp;Once we are connected to machines, will we still be truly human. &nbsp;I have no idea what people who worry about this mean by 'truly human'. &nbsp;Is a human with a prosthetic limb truly human? &nbsp;How about a human driving a car? &nbsp;Is a human who wears classes or a hearing aid truly human? &nbsp;If these prosthetics make you non-human, then we're already past the point where they should be concerned - and they're not. &nbsp;If these prosthetics leave you human, then why would a piece of glass that allows me to see clearly be ok, and a computer that allows me to think clearly not be ok? &nbsp;Asimov investigated ideas similar to this, but from a slightly different point of view, with his story 'The Bicentennial Man'.</p>\n<p style=\"padding-left: 30px;\">The real questions are ones of ethics. &nbsp;As people become more powerful, what are the ethical ways of using that power? &nbsp;I have no great wisdom to share there, unfortunately.</p>\n<p style=\"padding-left: 30px;\">Some more thoughts...</p>\n<p style=\"padding-left: 30px;\">Does a research lab (with, say, 50 researchers) have \"above human level intelligence\"? &nbsp;If not, then it isn't clear to me that AI will ever have significantly \"above human level intelligence\" (and see below for why AI is still worthwhile). &nbsp;If so, then why haven't we had a 'research lab singularity' yet? &nbsp;Surely research labs are smarter than humans and so they can work on making still smarter research labs, until a magical point is passed and research labs have runaway intelligence. &nbsp;(That's a socratic question designed to get you to think about possible answers yourself. &nbsp;Maybe we are in the middle of a research lab singularity.)</p>\n<p style=\"padding-left: 30px;\">As for why study of AI might still be useful even if we never get above human level intelligence: there is the same Dirty, Dull, Dangerous argument that has been used many times. &nbsp;To that I'd add a point I made in a previous email: intelligence is different to motivation. &nbsp;If you get yourself another human you get both - they're intelligent, but they also have their own goals and you have to spend time convincing them to work towards your goals. &nbsp;If you get an AI, then even if it isn't more intelligent than a human at least all that intelligence is working towards your goals without argument. &nbsp;It's similar to the 'Dull' justification, but with a slightly different spin.</p>\n<h3 id=\"Alan_Bundy\">Alan Bundy</h3>\n<p>Professor Alan Bundy [<a href=\"http://homepages.inf.ed.ac.uk/bundy/\">homepage</a>] did not answer my questions directly.</p>\n<p style=\"padding-left: 30px;\"><strong>Alan Bundy:</strong> Whenever I see questions like this I want to start by questioning the implicit assumptions behind them.</p>\n<ul style=\"padding-left: 30px;\">\n<li>I don't think the concept of \"human-level machine intelligence\" is well formed. AI is defining a huge space of different kinds of intelligence. Most of the points in this space are unlike anything either human or animal, but are new kinds of intelligence. Most of them are very specialised to particular areas of expertise. As an extreme example, consider the best chess playing programs. They are more intelligent than any human <em>at playing chess</em>, but can do nothing else, e.g., pick up and move the pieces. There's a popular myth that intelligence is on a linear scale, like IQ, and AI is progressing along it. If so, where would you put the chess program?</li>\n<li>The biggest threat from computers comes not from intelligence but from ignorance, i.e., from computer programs that are not as smart as they need to be. I'm thinking especially of safety critical and security critical systems, such as fly-by-wire aircraft and financial trading systems. When these go wrong, aircraft crash and people are killed or the economy collapses. Worrying about intelligent machines distracts us from the real threats. </li>\n<li>As far as threats go, you can't separate the machines from the intentions of their owners. Quite stupid machines entrusted to run a war with weapons of mass destruction could cause quite enough havoc without waiting for the mythical \"human-level machine intelligence\". It will be human owners that endow their machines with goals and aims. The less intelligent the machines the more likely this is to end in tears. </li>\n<li>Given the indeterminacy of their owner's intentions, it's quite impossible to put probabilities on the questions you ask. Even if we could precisely predict the progress of the technology, which we can't, the intentions of the owners would defeat our estimates.</li>\n</ul>\n<p style=\"padding-left: 30px;\">I'm familiar with what you call the 'standard worry'. I've frequently been recruited to public debate with Kevin Warwick, who has popularised this 'worry'. I'd be happy for you to publish my answers. I'd add one more point, which I forgot to include yesterday.</p>\n<ul style=\"padding-left: 30px;\">\n<li>Consider the analogy with 'bird level flight'. Long before human flight, people aspired to fly like birds. The reality of human flight turned out to be completely different. In some respects, 'artificial' flying machines are superior to birds, e.g., they are faster. In some respects they are inferior, e.g., you have to be at the airport hours before take-off and book well in advance. The flight itself is very different, e.g., aircraft don't flap their wings. There is not much research now on flying like birds. If we really wanted to do it, we could no doubt come close, e.g., with small model birds with flapping wings, but small differences would remain and a law of diminishing returns would set in if we wanted to get closer. I think the aspiration for 'human level machine intelligence' will follow a similar trajectory --- indeed, it already has. </li>\n</ul>", "sections": [{"title": "[Click here to see a list of all interviews]", "anchor": "_Click_here_to_see_a_list_of_all_interviews_", "level": 2}, {"title": "The Interview:", "anchor": "The_Interview_", "level": 1}, {"title": "William Uther", "anchor": "William_Uther", "level": 1}, {"title": "Alan Bundy", "anchor": "Alan_Bundy", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T13:23:37.330Z", "modifiedAt": null, "url": null, "title": "NonGoogleables", "slug": "nongoogleables", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.943Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QcHNGd7CeCioqNRzr/nongoogleables", "pageUrlRelative": "/posts/QcHNGd7CeCioqNRzr/nongoogleables", "linkUrl": "https://www.lesswrong.com/posts/QcHNGd7CeCioqNRzr/nongoogleables", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20NonGoogleables&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANonGoogleables%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcHNGd7CeCioqNRzr%2Fnongoogleables%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=NonGoogleables%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcHNGd7CeCioqNRzr%2Fnongoogleables", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcHNGd7CeCioqNRzr%2Fnongoogleables", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<p>Recently in another topic I mentioned the \"two bishops against two knights\" chess endgame problem. I claimed it was investigated over two decades ago by a computer program and established that it is a win situation for the two bishops' side. Then I was unable to Google a solid reference for my claim.</p>\n<p>I also remember a \"Hermes Set Theory\". It was something like ZFC, regarded as a valid Set Theory axiom system for 40 years, until a paradox was found inside. Now, I can't Google it out.</p>\n<p>And then it was the so called \"Baryon&nbsp;number conservation law\", which was postulated for a short while in physics. Until it was found that a subatomic decay may in fact in/decrease the number of baryons in the process. I can't Google that one either.</p>\n<p>Is that just me, or what?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QcHNGd7CeCioqNRzr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -3, "extendedScore": null, "score": 8.304592929435288e-07, "legacy": true, "legacyId": "12119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T20:26:38.494Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh Meetup: Big Gaming Fun 2!", "slug": "meetup-pittsburgh-meetup-big-gaming-fun-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kenoubi", "createdAt": "2011-03-12T04:07:00.560Z", "isAdmin": false, "displayName": "Kenoubi"}, "userId": "DgrXt6eQMpunHRDXh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9og5NWQ7tKwxtJYaG/meetup-pittsburgh-meetup-big-gaming-fun-2", "pageUrlRelative": "/posts/9og5NWQ7tKwxtJYaG/meetup-pittsburgh-meetup-big-gaming-fun-2", "linkUrl": "https://www.lesswrong.com/posts/9og5NWQ7tKwxtJYaG/meetup-pittsburgh-meetup-big-gaming-fun-2", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%202!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%202!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9og5NWQ7tKwxtJYaG%2Fmeetup-pittsburgh-meetup-big-gaming-fun-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%202!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9og5NWQ7tKwxtJYaG%2Fmeetup-pittsburgh-meetup-big-gaming-fun-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9og5NWQ7tKwxtJYaG%2Fmeetup-pittsburgh-meetup-big-gaming-fun-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/65'>Pittsburgh Meetup: Big Gaming Fun 2!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 January 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1324 Wightman St., Pittsburgh, PA 15217</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I will be hosting board games on some Sundays. Each event will be announced separately, since the schedule will be a bit irregular; the first event will be this Sunday.  I only have 5 games (Bohnanza, Citadels, Acquire, Illuminati, and Hacker), so please bring anything you'd like to play. We can order food and go as late as 19:00. If I get paged I may have to deal with an emergency (from home, using my laptop), but if that doesn't bother you, it doesn't bother me.</p>\n\n<p>Allergy warning: I have a cat. I can put her upstairs on request, but if you have a cat allergy you might want to take some meds. If you decide not to come because you have a severe cat allergy, please tell me.</p>\n\n<p>RSVP here or by sending me a private message (but don't not show up because you didn't RSVP, I just want a rough idea of the number of attendees). Ring the bell, knock, or call or text (412) 657-1395 to get in when you get there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/65'>Pittsburgh Meetup: Big Gaming Fun 2!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9og5NWQ7tKwxtJYaG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.306192845820092e-07, "legacy": true, "legacyId": "12120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_2_\">Discussion article for the meetup : <a href=\"/meetups/65\">Pittsburgh Meetup: Big Gaming Fun 2!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 January 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1324 Wightman St., Pittsburgh, PA 15217</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I will be hosting board games on some Sundays. Each event will be announced separately, since the schedule will be a bit irregular; the first event will be this Sunday.  I only have 5 games (Bohnanza, Citadels, Acquire, Illuminati, and Hacker), so please bring anything you'd like to play. We can order food and go as late as 19:00. If I get paged I may have to deal with an emergency (from home, using my laptop), but if that doesn't bother you, it doesn't bother me.</p>\n\n<p>Allergy warning: I have a cat. I can put her upstairs on request, but if you have a cat allergy you might want to take some meds. If you decide not to come because you have a severe cat allergy, please tell me.</p>\n\n<p>RSVP here or by sending me a private message (but don't not show up because you didn't RSVP, I just want a rough idea of the number of attendees). Ring the bell, knock, or call or text (412) 657-1395 to get in when you get there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_2_1\">Discussion article for the meetup : <a href=\"/meetups/65\">Pittsburgh Meetup: Big Gaming Fun 2!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh Meetup: Big Gaming Fun 2!", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_2_", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh Meetup: Big Gaming Fun 2!", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_2_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-12T22:18:39.027Z", "modifiedAt": null, "url": null, "title": "The Noddy problem", "slug": "the-noddy-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Apprentice", "createdAt": "2009-05-20T13:06:35.976Z", "isAdmin": false, "displayName": "Apprentice"}, "userId": "X5f7X9PkRFwGyxWsb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tTBXKzx8DfmWQ5nkt/the-noddy-problem", "pageUrlRelative": "/posts/tTBXKzx8DfmWQ5nkt/the-noddy-problem", "linkUrl": "https://www.lesswrong.com/posts/tTBXKzx8DfmWQ5nkt/the-noddy-problem", "postedAtFormatted": "Thursday, January 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Noddy%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Noddy%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtTBXKzx8DfmWQ5nkt%2Fthe-noddy-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Noddy%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtTBXKzx8DfmWQ5nkt%2Fthe-noddy-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtTBXKzx8DfmWQ5nkt%2Fthe-noddy-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>An episode of the Noddy animated series has the following plot.</p>\n<p>Noddy needs to go pick up Martha Monkey at the station. But it's such a nice, sunny day that he would prefer to play around outside. He gets an idea to solve this dilemma. He casts a duplication spell on himself and his car and tells the duplicate to go fetch Martha while he goes out to play. Later, Noddy is out having fun when he suddenly spots his duplicate. It turns out that the duplicate also preferred playing outside to doing the errand so he also cast a duplication spell. Then they see another duplicate, and another...<br /><br />I think this story makes for a nice simple illustration of one of our perennial decision theoretic issues: When making decisions you should take into account that agents identical to yourself will make the same decision in the same situation. A common real-life example of the Noddy problem is when we try to <a href=\"/lw/4sh/how_i_lost_100_pounds_using_tdt/\">pawn off our dietary problems to our future selves</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dqx5k65wjFfaiJ9sQ": 1, "r7qAjcbfhj2256EHH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tTBXKzx8DfmWQ5nkt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 45, "extendedScore": null, "score": 0.000151, "legacy": true, "legacyId": "12121", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["scwoBEju75C45W5n3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-13T00:50:13.112Z", "modifiedAt": null, "url": null, "title": "Knowledge vs Technology", "slug": "knowledge-vs-technology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:26.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PuyaSharif", "createdAt": "2011-02-26T07:33:06.094Z", "isAdmin": false, "displayName": "PuyaSharif"}, "userId": "Kx2AumHK8eeJ4nHqt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S8JsW4iX9CoH6BeaT/knowledge-vs-technology", "pageUrlRelative": "/posts/S8JsW4iX9CoH6BeaT/knowledge-vs-technology", "linkUrl": "https://www.lesswrong.com/posts/S8JsW4iX9CoH6BeaT/knowledge-vs-technology", "postedAtFormatted": "Friday, January 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Knowledge%20vs%20Technology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKnowledge%20vs%20Technology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8JsW4iX9CoH6BeaT%2Fknowledge-vs-technology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Knowledge%20vs%20Technology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8JsW4iX9CoH6BeaT%2Fknowledge-vs-technology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8JsW4iX9CoH6BeaT%2Fknowledge-vs-technology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p>Here's a little thing I wrote down a couple of years back:</p>\n<p>Question:<strong> Does the following statements represent three different stages of technological development or does two of them represent the same stage?</strong> In some sense it should boil down to how we define the word 'technology', or at least its relation to 'knowledge'.</p>\n<ol>\n<li>We don't know how to build a commercially feasible fusion reactor.</li>\n<li>We know how to build a commercially feasible fusion reactor, but we haven't built one yet.</li>\n<li>He have built one commercially feasible fusion reactor.</li>\n</ol>\n<p>The key is how to handle 'know' in the second statement. One could argue that no matter how sure we are on how to construct it (down to every detail), statement three is still superior since we have actually one functioning reactor to show. But at the same time if we would collect all the uncertainties in our models/simulations/calculations etc in a variable u. What happens as u-&gt;0 ? Shouldn't the gap between 2 and 3 go to zero? That would mean that enough knowledge about a technology is equivalent to actually possessing the technology.</p>\n<p>&nbsp;</p>\n<p>EDIT: The utility of having a reactor is of course higher than knowing how to construct one, so if we measure technological level as a function of the utility gained from that technology then the resolution is trivial.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S8JsW4iX9CoH6BeaT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -5, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "12134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-13T01:28:25.336Z", "modifiedAt": null, "url": null, "title": "Can the Chain Still Hold You?", "slug": "can-the-chain-still-hold-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:56.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iETtCZcfmRyHp69w4/can-the-chain-still-hold-you", "pageUrlRelative": "/posts/iETtCZcfmRyHp69w4/can-the-chain-still-hold-you", "linkUrl": "https://www.lesswrong.com/posts/iETtCZcfmRyHp69w4/can-the-chain-still-hold-you", "postedAtFormatted": "Friday, January 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20the%20Chain%20Still%20Hold%20You%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20the%20Chain%20Still%20Hold%20You%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiETtCZcfmRyHp69w4%2Fcan-the-chain-still-hold-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20the%20Chain%20Still%20Hold%20You%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiETtCZcfmRyHp69w4%2Fcan-the-chain-still-hold-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiETtCZcfmRyHp69w4%2Fcan-the-chain-still-hold-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1089, "htmlBody": "<p><a href=\"http://www.radiolab.org/2009/oct/19/\">Robert Sapolsky</a>:</p>\n<blockquote>\n<p>Baboons...&nbsp;<em>literally</em>&nbsp;have been the textbook example of a highly aggressive, male-dominated,&nbsp;hierarchical&nbsp;society. Because these animals hunt, because they live in these aggressive troupes on the Savannah... they have a constant baseline level of aggression which inevitably spills over into their social lives.</p>\n</blockquote>\n<p>Scientists have never observed a baboon troupe that <em>wasn't</em>&nbsp;highly aggressive, and they have compelling reasons to think this is simply <em>baboon nature</em>, written into their genes. Inescapable.</p>\n<p>Or at least, that was true until the 1980s, when Kenya experienced a tourism boom.</p>\n<p>Sapolsky was a grad student, studying his first baboon troupe.&nbsp;A new tourist lodge was built at the edge of the forest where his baboons lived. The owners of the lodge dug a hole behind the lodge and dumped their trash there every morning, after which the males of several baboon troupes &mdash; including Sapolsky's &mdash; would fight over this pungent bounty.</p>\n<p>Before too long, someone noticed the baboons didn't look too good. It turned out they had eaten some infected meat and developed tuberculosis, which kills baboons in weeks. Their hands rotted away, so they hobbled around on their elbows. Half the males in Sapolsky's troupe died.</p>\n<p>This had a surprising effect. There was now almost no violence in the troupe. Males often reciprocated when females groomed them, and males even groomed other males. To a baboonologist, this was like watching Mike Tyson suddenly stop swinging in a heavyweight fight to start nuzzling Evander Holyfield. It <em>never</em>&nbsp;happened.<a id=\"more\"></a></p>\n<p>This was interesting, but Sapolsky moved to the other side of the park and began studying other baboons.&nbsp;His first troupe was \"scientifically ruined\" by such a non-natural event. But really, he was just heartbroken. He never visited.</p>\n<p>Six years later, Sapolsky wanted to show his girlfriend where he had studied his first troupe, and found that they were still there, and still surprisingly violence-free. This one troupe had apparently been so transformed by their unusual experience &mdash; and the continued availability of easy food &mdash; that they were now basically non-violent.</p>\n<p>And then it hit him.</p>\n<p>Only one of the males now in the troupe had been through the event. All the rest were new, and hadn't been raised in the tribe. The new males had come from the violent, dog-eat-dog world of normal baboon-land. But instead of coming into the new troupe and roughing everybody up as they <em>always</em>&nbsp;did, the new males had learned, \"We don't do stuff like that here.\" They had unlearned their childhood culture and adapted to the new norms of the first baboon pacifists.</p>\n<p>As it turned out, violence <em>wasn't</em>&nbsp;an unchanging part of baboon nature. In fact it changed rather quickly, when the right causal factor flipped, and &mdash; for this troupe and the new males coming in &mdash; it has <em>stayed</em>&nbsp;changed to this day.</p>\n<p>Somehow, the violence had been largely <em>circumstantial</em>. It was just that the circumstances had always been the same.</p>\n<p>Until they weren't.</p>\n<p>We still don't know&nbsp;how much baboon violence to attribute to nature vs. nurture, or exactly how this change happened. But it's worth noting that changes like this can and do happen pretty often.</p>\n<p>Slavery was ubiquitous for millennia. Until it was <a href=\"http://en.wikipedia.org/wiki/Abolition_of_slavery_timeline\">outlawed</a> in every country on Earth.</p>\n<p>Humans had never left the Earth. Until we achieved the first manned orbit and the first manned moon landing in a single decade.</p>\n<p>Smallpox occasionally decimated human populations for thousands of years. Until it was <a href=\"http://en.wikipedia.org/wiki/Smallpox#Eradication\">eradicated</a>.</p>\n<p>The human species was always too weak to render itself extinct. <a href=\"/lw/8f0/existential_risk/\">Until</a> we discovered the nuclear chain reaction and manufactured thousands of atomic bombs.</p>\n<p>Religion had a grip on 99.5% or more of humanity until 1900, and then the rate of religious adherence <a href=\"http://commonsenseatheism.com/?p=3265#footnote_0_3265\">plummeted</a> to 85% by the end of the century. Whole nations became mostly atheistic, <a href=\"http://commonsenseatheism.com/?p=3265\">largely because</a> for the first time the state provided people some basic stability and security. (Some nations became atheistic because of atheistic dictators, others because they provided security and stability to their citizens.)</p>\n<p>I would never have imagined I could have the kinds of conversations I now regularly have at the Singularity Institute, where people change their degrees of belief several times in a single conversation as new evidence and argument is presented, where everyone at the table knows and applies a broad and deep scientific understanding, where people disagree strongly and say harsh-sounding things (due to <a href=\"http://wiki.lesswrong.com/wiki/Crocker's_rules\">Crocker's rules</a>) but end up coming to agreement after 10 minutes of argument and carry on as if this is friendship and business as usual &mdash; because it is.</p>\n<p>But then, never before has humanity had the combined benefits of an <a href=\"http://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712/\">overwhelming case</a> for <a href=\"http://yudkowsky.net/rational/bayes\">one correct probability theory</a>, a <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">systematic understanding</a> of human biases and how they work, <a href=\"http://www.wikipedia.org/\">free</a> <a href=\"http://commonsenseatheism.com/?p=1404\">access</a> <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/4u84\">to</a> most scientific knowledge, and a large <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">community</a> of people dedicated to the daily practice of <a href=\"http://commonsenseatheism.com/?p=13607\">CogSci</a>-informed rationality exercises and to <a href=\"/lw/4ul/less_wrong_nyc_case_study_of_a_successful/\">helping each other improve</a>.</p>\n<p>This is part of what gives me <a href=\"/lw/2c/a_sense_that_more_is_possible/\">a sense that more is possible</a>. Compared to situational effects, we tend to overestimate the effects of lasting dispositions on people's behavior &mdash; the <a href=\"http://en.wikipedia.org/wiki/Fundamental_attribution_error\">fundamental attribution error</a>. But I, for one, was only&nbsp;<a href=\"/lw/hz/correspondence_bias/\">taught</a> to watch out for this error&nbsp;in explaining the behavior of <em>individual</em>&nbsp;humans, even though the bias also appears when explaining the behavior of humans <em>as a species</em>. I suspect this is <em>partly</em>&nbsp;due to the common misunderstanding that <a href=\"http://en.wikipedia.org/wiki/Heritability\">heritability</a> measures the degree to which a trait is due to genetic factors. Another reason may be that for obvious reasons scientists rarely try <em>very</em>&nbsp;hard to measure the effects of exposing human subjects to radically different environments like an <a href=\"http://en.wikipedia.org/wiki/Stanford_prison_experiment\">artificial prison</a> or <a href=\"http://en.wikipedia.org/wiki/Feral_child\">total human isolation</a>.</p>\n<p><img style=\"float: right; padding: 15px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/chained-elephant.jpg\" alt=\"\" width=\"250\" height=\"202\" />When taming a baby elephant, its trainer will chain one of its legs to a post. When the elephant tries to run away, the chain and the post are strong enough to keep it in place. But when the elephant grows up, it is strong enough to break the chain or uproot the post. Yet the owner can still secure the elephant with the same chain and post, because the elephant has been conditioned to believe it cannot break free. It feels the tug of the chain and gives up &mdash; a kind of <a href=\"http://en.wikipedia.org/wiki/Learned_helplessness\">learned helplessness</a>. The elephant acts as if it thinks the chain's limiting power is intrinsic to nature rather than dependent on a causal factor that held for years but holds no longer.</p>\n<p>Much has changed in the past few decades, and much will change in the coming years. Sometimes it's good to check if the chain can still hold you. Do not be tamed by the tug of history. Maybe with a few new tools and techniques you can just get up and walk away &mdash; to a place you've never seen before.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jaf5zfcGgCB2REXGw": 1, "bY5MaF2EATwDkomvu": 1, "HkiwLtMRLxpBa6zs5": 1, "sPpZRaxpNNJjw55eu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iETtCZcfmRyHp69w4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 159, "baseScore": 189, "extendedScore": null, "score": 0.000377, "legacy": true, "legacyId": "12017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 189, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 360, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FGTgeweYNxmMBx4fz", "xLm9mgJRPvmPGpo7Q", "Q5CjE8pRiACqTvhRM", "CsKboswS3z5iaiutC", "Nu3wa6npK4Ry66vFp", "DB6wbyrMugYMK5o6a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-13T03:16:58.847Z", "modifiedAt": null, "url": null, "title": "New SI publications design", "slug": "new-si-publications-design", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.774Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9nLJCbHotyzi5rLKL/new-si-publications-design", "pageUrlRelative": "/posts/9nLJCbHotyzi5rLKL/new-si-publications-design", "linkUrl": "https://www.lesswrong.com/posts/9nLJCbHotyzi5rLKL/new-si-publications-design", "postedAtFormatted": "Friday, January 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20SI%20publications%20design&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20SI%20publications%20design%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9nLJCbHotyzi5rLKL%2Fnew-si-publications-design%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20SI%20publications%20design%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9nLJCbHotyzi5rLKL%2Fnew-si-publications-design", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9nLJCbHotyzi5rLKL%2Fnew-si-publications-design", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<p>The Singularity Institute's <a href=\"http://intelligence.org/research/\">publications</a> are badly and inconsistently formatted.</p>\n<p>Our research associate&nbsp;<a href=\"http://www.danieldewey.net/\">Daniel Dewey</a>&nbsp;has made us a nice-looking LaTeX template for them (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/01/Dewey-Learning-what-to-value.pdf\">example</a>), but we need some help moving them from the original files into LaTeX. If you have a little experience with LaTeX and might be willing to help, please let me know!</p>\n<p>luke [at] intelligence.org</p>\n<p>&nbsp;</p>\n<p>Or, in general, <strong>please send an email to&nbsp;volunteers+subscribe@intelligence.org</strong> to be added to the volunteers email list.</p>\n<p>Volunteer project requests go out <strong>only a few times a month</strong>, so we won't flood your inbox, and you never know: maybe a project sent to that list will be something you are in the mood to do and have the skills to do.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9nLJCbHotyzi5rLKL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 8.30774532289983e-07, "legacy": true, "legacyId": "12135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-13T07:55:15.708Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-11", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:29.132Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bc3Dyeth7RCEq2nC7/meetup-melbourne-social-meetup-11", "pageUrlRelative": "/posts/bc3Dyeth7RCEq2nC7/meetup-melbourne-social-meetup-11", "linkUrl": "https://www.lesswrong.com/posts/bc3Dyeth7RCEq2nC7/meetup-melbourne-social-meetup-11", "postedAtFormatted": "Friday, January 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbc3Dyeth7RCEq2nC7%2Fmeetup-melbourne-social-meetup-11%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbc3Dyeth7RCEq2nC7%2Fmeetup-melbourne-social-meetup-11", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbc3Dyeth7RCEq2nC7%2Fmeetup-melbourne-social-meetup-11", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/66'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 January 2012 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>All welcome! 6:30pm for 7pm, Friday 20th</p>\n\n<p>We'll order takeaway for dinner (something paleo-ish if possible) and I'll get some snacks. BYO drinks and games.</p>\n\n<p>If you have problems getting in, you can call me on 0412 996 288.</p>\n\n<p>No need to RSVP, but if you were to post a comment below saying that you're coming, it might encourage others to attend. (We've typically been getting about 12 people.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/66'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bc3Dyeth7RCEq2nC7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.308798463736311e-07, "legacy": true, "legacyId": "12143", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/66\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 January 2012 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">see mailing list, Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>All welcome! 6:30pm for 7pm, Friday 20th</p>\n\n<p>We'll order takeaway for dinner (something paleo-ish if possible) and I'll get some snacks. BYO drinks and games.</p>\n\n<p>If you have problems getting in, you can call me on 0412 996 288.</p>\n\n<p>No need to RSVP, but if you were to post a comment below saying that you're coming, it might encourage others to attend. (We've typically been getting about 12 people.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/66\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-13T16:19:47.331Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Melbourne, Austin, Atlanta, Seattle, Philadelphia, Fort Collins", "slug": "weekly-lw-meetups-melbourne-austin-atlanta-seattle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.803Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DwkdLWCXAgjZb47CH/weekly-lw-meetups-melbourne-austin-atlanta-seattle", "pageUrlRelative": "/posts/DwkdLWCXAgjZb47CH/weekly-lw-meetups-melbourne-austin-atlanta-seattle", "linkUrl": "https://www.lesswrong.com/posts/DwkdLWCXAgjZb47CH/weekly-lw-meetups-melbourne-austin-atlanta-seattle", "postedAtFormatted": "Friday, January 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Melbourne%2C%20Austin%2C%20Atlanta%2C%20Seattle%2C%20Philadelphia%2C%20Fort%20Collins&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Melbourne%2C%20Austin%2C%20Atlanta%2C%20Seattle%2C%20Philadelphia%2C%20Fort%20Collins%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwkdLWCXAgjZb47CH%2Fweekly-lw-meetups-melbourne-austin-atlanta-seattle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Melbourne%2C%20Austin%2C%20Atlanta%2C%20Seattle%2C%20Philadelphia%2C%20Fort%20Collins%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwkdLWCXAgjZb47CH%2Fweekly-lw-meetups-melbourne-austin-atlanta-seattle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwkdLWCXAgjZb47CH%2Fweekly-lw-meetups-melbourne-austin-atlanta-seattle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 391, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/5r\">First Philadelphia Meetup of 2012:&nbsp;<span class=\"date\">08 January 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/5v\">Fort Collins, Colorado Meetup:&nbsp;<span class=\"date\">11 January 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/5p\">[TENTATIVE] Portland Meetup?:&nbsp;<span class=\"date\">14 January 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/5q\">San Diego experimental meetup:&nbsp;<span class=\"date\">15 January 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/5s\">First Sydney 2012 meetup.:&nbsp;<span class=\"date\">19 January 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/5j\">Salt Lake City, Late January 2012</a></li>\n<li><a href=\"/meetups/5n\">Columbus or Cincinnati Meetup:&nbsp;<span class=\"date\">22 January 2012 05:00PM</span></a><a href=\"/meetups/5n\"></a></li>\n<li><a href=\"/meetups/5f\">First Brussels meetup:&nbsp;<span class=\"date\">11 February 2012 11:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/5o\">Melbourne practical rationality meetup:&nbsp;<span class=\"date\">06 January 2012 07:00AM</span></a></li>\n<li><a href=\"/meetups/5t\">Austin, TX:&nbsp;<span class=\"date\">07 January 2012 01:30PM</span></a></li>\n<li><a href=\"/meetups/5u\">Atlanta Meetup:&nbsp;<span class=\"date\">07 January 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/5w\">Seattle Board Games:&nbsp;<span class=\"date\">08 January 2012 01:01PM</span></a></li>\n</ul>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong></strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DwkdLWCXAgjZb47CH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.310708425668718e-07, "legacy": true, "legacyId": "11960", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-13T17:32:11.064Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin LW Meetup", "slug": "meetup-austin-lw-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CickZdBJsNv587pov/meetup-austin-lw-meetup", "pageUrlRelative": "/posts/CickZdBJsNv587pov/meetup-austin-lw-meetup", "linkUrl": "https://www.lesswrong.com/posts/CickZdBJsNv587pov/meetup-austin-lw-meetup", "postedAtFormatted": "Friday, January 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%20LW%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%20LW%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCickZdBJsNv587pov%2Fmeetup-austin-lw-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%20LW%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCickZdBJsNv587pov%2Fmeetup-austin-lw-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCickZdBJsNv587pov%2Fmeetup-austin-lw-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/67'>Austin LW Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 January 2012 01:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Despite my absence, the Austin meetup continues! We have two people confirmed to show up, so I hope you join them. We sit on the second floor to the left, near (or often on) the stage.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/67'>Austin LW Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CickZdBJsNv587pov", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.310982553410396e-07, "legacy": true, "legacyId": "12148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin_LW_Meetup\">Discussion article for the meetup : <a href=\"/meetups/67\">Austin LW Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 January 2012 01:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Despite my absence, the Austin meetup continues! We have two people confirmed to show up, so I hope you join them. We sit on the second floor to the left, near (or often on) the stage.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin_LW_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/67\">Austin LW Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin LW Meetup", "anchor": "Discussion_article_for_the_meetup___Austin_LW_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Austin LW Meetup", "anchor": "Discussion_article_for_the_meetup___Austin_LW_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-14T19:06:53.147Z", "modifiedAt": null, "url": null, "title": "The Savage theorem and the Ellsberg paradox", "slug": "the-savage-theorem-and-the-ellsberg-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:48.511Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fool", "createdAt": "2011-06-26T01:15:43.078Z", "isAdmin": false, "displayName": "fool"}, "userId": "ZRc3MgPzbWh2TTm8X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/thHZiZBDRPtGxCM6f/the-savage-theorem-and-the-ellsberg-paradox", "pageUrlRelative": "/posts/thHZiZBDRPtGxCM6f/the-savage-theorem-and-the-ellsberg-paradox", "linkUrl": "https://www.lesswrong.com/posts/thHZiZBDRPtGxCM6f/the-savage-theorem-and-the-ellsberg-paradox", "postedAtFormatted": "Saturday, January 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Savage%20theorem%20and%20the%20Ellsberg%20paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Savage%20theorem%20and%20the%20Ellsberg%20paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FthHZiZBDRPtGxCM6f%2Fthe-savage-theorem-and-the-ellsberg-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Savage%20theorem%20and%20the%20Ellsberg%20paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FthHZiZBDRPtGxCM6f%2Fthe-savage-theorem-and-the-ellsberg-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FthHZiZBDRPtGxCM6f%2Fthe-savage-theorem-and-the-ellsberg-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3341, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability\">A summary of Savage's foundation for probability and utility.</a></p>\n<p>In 1961, <a href=\"http://en.wikipedia.org/wiki/Daniel_Ellsberg\">Daniel Ellsberg</a>, most famous for leaking the <a href=\"http://en.wikipedia.org/wiki/Pentagon_Papers\">Pentagon Papers</a>, published the decision-theoretic paradox which is now named after him <sup>1</sup>. It is a cousin to the Allais paradox. They both involve violations of an independence or separability principle. But they go off in different directions: one is a violation of expected utility, while the other is a violation of subjective probability. The Allais paradox has been <a href=\"/lw/my/the_allais_paradox/\">discussed on LW before</a>, but when I do a search it seems that the first discussion of the Ellsberg paradox on LW was my comments on the previous post <sup>2</sup>. It seems to me that from a Bayesian point of view, the Ellsberg paradox is the greater evil.</p>\n<p>But I should first explain what I mean by a violation of expected utility versus subjective probability, and for that matter, what I mean by Bayesian. I will explain a special case of Savage's representation theorem, which focuses on the subjective probability side only. Then I will describe Ellsberg's paradox. In the next episode, I will give an example of how not to be Bayesian. If I don't get voted off the island at the end of this episode.</p>\n<h2>Rationality and Bayesianism</h2>\n<p>Bayesianism is often taken to involve the maximisation of expected utility with respect to a subjective probability distribution. I would argue this label only sticks to the subjective probability side. But mainly, I wish to make a clear division between the two sides, so I can focus on one.</p>\n<p>Subjective probability and expected utility are certainly related, but they're still independent. You could be perfectly willing and able to assign belief numbers to all possible events as if they were probabilities. That is, your belief assignment obeys all the laws of probability, including Bayes' rule, which is, after all, what the -ism is named for. You could do all that, but still maximise something other than expected utility. In particular, you could combine subjective probabilities with prospect theory, which has also been <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">discussed on LW before</a>. In that case you may display Allais-paradoxical behaviour but, as we will see, not Ellsberg-paradoxical behaviour. The rationalists might excommunicate you, but it seems to me you should keep your Bayesianist card.</p>\n<p>On the other hand your behaviour could be incompatible with any subjective probability distribution. But you could still maximise utility with respect to something other than subjective probability. In particular, when faced with known probabilities, you would be maximising expected utility in the normal sense. So you can not exhibit any Allais-paradoxical behaviour, because the Allais paradox involves only objective lotteries. But you may exhibit, as we will see, Ellsberg-paradoxical behaviour. I would say you are not Bayesian.</p>\n<p>So a non-Bayesian, even the strictest frequentist, can still be an expected utility maximiser, and a perfect Bayesian need not be an expected utility maximiser. What I'm calling Bayesianist is just the idea that we should reason with our subjective beliefs the same way that we reason with objective probabilities. This also has been called having \"probabilistically sophisticated\" beliefs, if you prefer to avoid the B-word, or don't like the way I'm using it.</p>\n<p>In a lot of what follows, I will bypass utility by only considering two outcomes. Utility functions are only unique up to a constant offset and a positive scale factor. With two outcomes, they evaporate entirely. The question of maximising expected utility with respect to a subjective probability distribution reduces to the question of maximising the probability, according to that distribution, of getting the better of the two outcomes. (And if the two outcomes are equal, there is nothing to maximise.)</p>\n<p>And on the flip side, if we have a decision method for the two-outcome case, Bayesian or otherwise, then we can always tack on a utility function. The idea of utility is just that any intermediate outcome is equivalent to an objective lottery between better and worse outcomes. So if we want, we can use a utility function to reduce a decision problem with any (finite) number of outcomes to a decision problem over the best and worst outcomes in question.</p>\n<h2>Savage's representation theorem</h2>\n<p>Let me recap some of the <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability/\">previous post</a> on Savage's theorem. How might we defend Bayesianism? We could invoke <a href=\"http://en.wikipedia.org/wiki/Cox %27s_theorem\">Cox's theorem</a>. This starts by assuming possible events can be assigned real numbers corresponding to some sort of belief level on someone's part, and that there are certain functions over these numbers corresponding to logical operations. It can be proven that, if someone's belief functions obey some simple rules, then that person acts <em>as if</em> they were reasoning with subjective probability. Now, while the rules for belief functions are intuitive, the background assumptions are pretty sketchy. It is not at all clear why these mathematical constructs are requirements of rationality.</p>\n<p>One way to justify those constructs is to argue in terms of choices a rational person must make. We imagine someone is presented with choices among various bets on uncertain events. Their level of belief in these events can be gauged by which bets they choose. But if we're going to do that anyway, then, as it turns out, we can just give some simple rules directly about these choices, and bypass the belief functions entirely. This was Leonard Savage's approach <sup>3</sup>. To quote a comment on the previous post: \"This is important because agents in general don't have to use beliefs or goals, but they do all have to choose actions.\"</p>\n<p>Savage's approach actually covers both subjective probability and expected utility. The previous post discusses both, whereas I am focusing on the former. This lets me give a shorter exposition, and I think a clearer one.</p>\n<p>We start by assuming some abstract collection of possible bets. We suppose that when you are offered two bets from this collection, you will choose one over the other, or express indifference.</p>\n<p>As discussed, we will only consider two outcomes. So all bets have the same payout, the difference among them is just their winning conditions. It is not specified what it is that you win. But it is assumed that, given the choice between winning unconditionally and losing unconditionally, you would choose to win.</p>\n<p>It is assumed that the collection of bets form what is called a <a href=\"http://en.wikipedia.org/wiki/Boolean_algebra_ %28structure%29\">boolean algebra</a>. This just means we can consider combinations of bets under boolean operators like \"and\", \"or\", or \"not\". Here I will use brackets to indicate these combinations. (A or B) is a bet that wins under the conditions that make either A win, or B win, or both win. (A but not B) wins whenever A wins but B doesn't. And so on.</p>\n<p>If you are rational, your choices must, it is claimed, obey some simple rules. If so, it can be proven that you are choosing <em>as if</em> you had a assigned subjective probabilities to bets. Savage's axioms for choosing among bets are <sup>4</sup>:</p>\n<ol>\n<li>If you choose A over B, you shall not choose B over A; and, if you do not choose A over B, and do not choose B over C, you shall not choose A over C. </li>\n<li>If you choose A over B, you shall also choose (A but not B) over (B but not A); and conversely, if you choose (A but not B) over (B but not A), you shall also choose A over B. </li>\n<li>You shall not choose A over (A or B).</li>\n<li>If you choose A over B, then you shall be able to specify a finite sequence of bets C<sub>1</sub>, C<sub>2</sub>, ..., C<sub>n</sub>, such that it is guaranteed that one and only one of the C's will win, and such that, for any one of the C's, you shall still choose (A but not C) over (B or C).</li>\n</ol>\n<p>Rule 1 is a coherence requirement on rational choice. It is requires your preferences to be a <a href=\"http://en.wikipedia.org/wiki/Strict_weak_ordering#Total_preorders\">total pre-order</a>. One objection to Cox's theorem is that&nbsp;levels of belief could be incomparable. This objection does not apply to rule 1 in this context because, as we discussed above, we're talking about choices of bets, not beliefs. Faced with choices, we choose. A rational person's choices must be non-circular.</p>\n<p>Rule 2 is an independence requirement. It demands that when you compare two bets, you ignore the possibilty that they could both win. In those circumstances you would be indifferent between the two anyway. The only possibilities that are relevant to the comparison are the ones where one bet wins and the other doesn't. So, you ought to compare A to B the same way you compare (A but not B) to (B but not A). Savage called this rule the Sure-thing principle.</p>\n<p>Rule 3 is a dominance requirement on rational choice. It demands that you not choose something that cannot do better under any circumstance: whenever A would win, so would (A or B). Note that you might judge (B but not A) to be impossible a priori. So, you might legitimately express indifference between A and (A or B). We can only say it is never legitimate to choose A over (A or B).</p>\n<p>Rule 4 is the most complicated. Luckily it's not going to be relevant to the Ellsberg paradox. Call it Mostly Harmless and forget this bit if you want.</p>\n<p>What rule 4 says is that if you choose A over B, you must be willing to pay a premium for your choice. Now, we said there are only two outcomes in this context. Here, the premium is paid in terms of other bets. Rule 4 demands that you give a finite list of <a href=\"http://en.wikipedia.org/wiki/Mutually_exclusive_events\">mutually exclusive</a> and <a href=\"http://en.wikipedia.org/wiki/Collectively_exhaustive_events\">exhaustive</a> events, and still be willing to choose A over B if we take any event on your list, cut it from A, and paste it to B. You can list as many events as you need to, but it must be a finite list.</p>\n<p>For example, if you thought A was much more likely than B, you might pull out a die, and list the 6 possible outcomes of one roll. You would also be willing to choose (A but not a roll of 1) over (B or a roll of 1), (A but not a roll of 2) over (B or a roll of 2), and so on. If not, you might list the 36 possible outcomes of two consecutive rolls, and be willing to choose (A but not two rolls of 1) over (B or two rolls of 1), and so on. You could go to any finite number of rolls.</p>\n<p>In fact rule 4 is pretty liberal, it doesn't even demand that every event on your list be equiprobable, or even independent of the A and B in question. It just demands that the events be mutually exclusive and exhaustive. If you are not willing to specify <em>some</em> such list of events, then you ought to express indifference between A and B.</p>\n<p>If you obey rules 1-3, then that is sufficient for us construct a sort of qualitative subjective probability out of your choices. It might not be quantitative: for one thing, there could be <a href=\"http://en.wikipedia.org/wiki/Infinitessimal\">infinitessimally</a> likely beliefs. Another thing is that there might be more than one way to assign numbers to beliefs. Rule 4 takes care of these things. If you obey rule 4 also, then we can assign a subjective probability to every possible bet, prove that you choose among bets <em>as if</em> you were using those probabilities, and also prove that it is the only probability assignment that matches your choices. And, on the flip side, if you are choosing among bets based on a subjective probability assignment, then it is easy to prove you obey rules 1-3, as well as rule 4 if the collection of bets is suitably infinite, like if a fair die is avaialble to bet on.</p>\n<p>Savage's theorem is impressive. The background assumptions involve just the concept of choice, and no numbers at all. There are only a few simple rules. Even rule 4 isn't really all that hard to understand and accept. A subjective probability distribution appears seemingly out of nowhere. In the full version, a utility function appears out of nowhere too. This theorem has been called the crowning glory of decision theory.</p>\n<h2>The Ellsberg paradox</h2>\n<p>Let's imagine there is an urn containing 90 balls. 30 of them are red, and the other 60 are either green or blue, in unknown proportion. We will draw a ball from the urn at random. Let us bet on the colour of this ball. As above, all bets have the same payout. To be specific, let's say you get pie if you win, and a <a href=\"http://en.wikipedia.org/wiki/Boot_to_the_head\">boot to the head</a> if you lose. The first question is: do you prefer to bet that the colour will be red, or that it will be green? The second question is: do you prefer to bet that it will be (red or blue), or that it will be (green or blue)?</p>\n<p>The most common response<sup>5</sup> is to choose red over green, and (green or blue) over (red or blue). And that's all there is to it. Paradox! <sup>6</sup></p>\n<p>\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"5\">\n<tbody>\n<tr>\n<td rowspan=\"2\">&nbsp;</td>\n<td align=\"center\" bgcolor=\"red\"><span style=\"font-size: xx-small;\">30</span></td>\n<td colspan=\"2\" align=\"center\" bgcolor=\"cyan\"><span style=\"font-size: xx-small;\">60</span></td>\n</tr>\n<tr>\n<td align=\"center\" bgcolor=\"red\"><span style=\"font-size: xx-small;\"><strong>Red</strong></span></td>\n<td align=\"center\" bgcolor=\"green\"><span style=\"font-size: xx-small;\"><strong>Green</strong></span></td>\n<td align=\"center\" bgcolor=\"blue\"><span style=\"font-size: xx-small;\"><strong>Blue</strong></span></td>\n</tr>\n<tr>\n<td colspan=\"5\">\n<hr />\n</td>\n</tr>\n<tr>\n<th>A</th>\n<td align=\"center\" bgcolor=\"red\">pie</td>\n<td align=\"center\" bgcolor=\"green\">BOOT</td>\n<td align=\"center\" bgcolor=\"blue\">BOOT</td>\n<td rowspan=\"2\">&nbsp;</td>\n<td rowspan=\"2\"><span><em>A is preferred to B</em></span></td>\n</tr>\n<tr>\n<th>B</th>\n<td align=\"center\" bgcolor=\"red\">BOOT</td>\n<td align=\"center\" bgcolor=\"green\">pie</td>\n<td align=\"center\" bgcolor=\"blue\">BOOT</td>\n</tr>\n<tr>\n<td colspan=\"5\">\n<hr />\n</td>\n</tr>\n<tr>\n<th>C</th>\n<td align=\"center\" bgcolor=\"red\">pie</td>\n<td align=\"center\" bgcolor=\"green\">BOOT</td>\n<td align=\"center\" bgcolor=\"blue\">pie</td>\n<td rowspan=\"2\">&nbsp;</td>\n<td rowspan=\"2\"><span><em>D is preferred to C</em></span></td>\n</tr>\n<tr>\n<th>D</th>\n<td align=\"center\" bgcolor=\"red\">BOOT</td>\n<td align=\"center\" bgcolor=\"green\">pie</td>\n<td align=\"center\" bgcolor=\"blue\">pie</td>\n</tr>\n<tr>\n<td colspan=\"5\">&nbsp;</td>\n<td align=\"center\"><span><em>Paradox!</em></span></td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>&nbsp;</p>\n<p>If choices were based solely on an assignment of subjective probability, then because the three colours are mutually exclusive, P(red or blue) = P(red) + P(blue), and P(green or blue) = P(green) + P(blue). So, since P(red) &gt; P(green) then P (red or blue) &gt; P(green or blue), but instead we have P(red or blue) &lt; P(green or blue).</p>\n<p>Knowing Savage's representation theorem, we expect to get a formal contradiction from the 4 rules above plus the 2 expressed choices. Something has to give, so we'd like to know which rules are really involved. You can see that we are talking only about rule 2, the Sure-thing principle. It says we shall compare (red or blue) to (green or blue) the same way as we compare red to green.</p>\n<p>This behaviour has been called <a href=\"http://en.wikipedia.org/wiki/Ambiguity_aversion\">ambiguity aversion</a>. Now, perhaps this is just a cognitive bias. It wouldn't be the first time that people behave a certain way, but the analysis of their decisions shows a clear error. And indeed, when explained, some people do repent of their sins against Bayes. They change their choices to obey rule 2. But others don't. To quote Ellsberg:</p>\n<blockquote>\n<p>...after rethinking all their 'offending' decisions in light of [Savage's] axioms, a number of people who are not only sophisticated but reasonable decide that they wish to persist in their choices. This includes people who previously felt a 'first order commitment' to the axioms, many of them surprised and some dismayed to find that they wished, in these situations, to violate the Sure-thing Principle. Since this group included L.J. Savage, when last tested by me (I have been reluctant to try him again), it seems to deserve respectful consideration.</p>\n</blockquote>\n<p>I include myself in the group that thinks rule 2 is what should be dropped. But I don't have any dramatic (de-)conversion story to tell. I was somewhat surprised, but not at all dismayed, and I can't say I felt much if any prior commitment to the rules. And as to whether I'm sophisticated or reasonable, well never mind! Even if there are a number of other people who are all of the above, and even if Savage himself may have been one of them for a while, I do realise that smart people can be Just Plain Wrong. So I'd better have something more to say for myself.</p>\n<p>Well, red obviously has a probability of 1/3. Our best guess is to apply the <a href=\"http://en.wikipedia.org/wiki/Principle_of_indifference\">principle of indifference</a> to also assign probability 1/3 to green or blue. But our best guess is not necessarily a good guess. The probabilities we assign to red, and to (green or blue), are objective. We're guessing the probability of green, and of (red or blue). It seems wise to take this difference into account when choosing what to bet on, doesn't it? And surely it will be all the more wise when dealing with real-life, non- symetrical situations where we can't even appeal to the principle of indifference.</p>\n<p>Or maybe I'm just some fool talking <a href=\"http://www.urbandictionary.com/define.php?term=jibba%20jabba\">jibba jabba</a>. Against this sort of talk, the <a href=\"/lw/my/the_allais_paradox/\">LW post on the Allais paradox</a> presents a version of Howard Raiffa's dynamic inconsistency argument. This makes no references to internal thought processes, it is a purely <em>external</em> argument about the decisions themselves. As stated in that post, \"There is always a price to pay for leaving the Bayesian Way.\" <sup>7</sup> This is expanded upon in <a href=\"/lw/mt/beautiful_probability/\">an earlier post</a>:</p>\n<blockquote>\n<p>Sometimes you must seek an approximation; often, indeed. This doesn't mean that probability theory has ceased to apply, any more than your inability to calculate the aerodynamics of a 747 on an atom-by-atom basis implies that the 747 is not made out of atoms. Whatever approximation you use, it works to the extent that it approximates the ideal Bayesian calculation - and fails to the extent that it departs.</p>\n<p>Bayesianism's coherence and uniqueness proofs cut both ways ... anything that is not Bayesian must fail one of the coherency tests. This, in turn, opens you to punishments like Dutch-booking (accepting combinations of bets that are sure losses, or rejecting combinations of bets that are sure gains).</p>\n</blockquote>\n<p>Now even if you believe this about the Allais paradox, I've argued that this doesn't really have much to do with Bayesianism one way or the other. The Ellsberg paradox is what actually strays from the Path. So, does God also punish ambiguity aversion?</p>\n<p>Tune in next time<sup>8</sup>, when I present a two-outcome decision method that obeys rules 1, 3, and 4, and even a weaker form of rule 2. But it exhibits ambiguity aversion, in gross violation of the original rule 2, so that it's not even approximately Bayesian. I will try to present it in a way that advocates for its internal cognitive merit. But the main thing <sup>9</sup> is that, externally, it is dynamically consistent. We do not get booked, by the Dutch or any other nationality.</p>\n<h2>Notes</h2>\n<p>&nbsp;</p>\n<ol>\n<li> Ellsberg's original paper is: <em>Risk, ambiguity, and the Savage axioms</em>, Quarterly Journal of Economics 75 (1961) pp 643-669</li>\n<li> Some discussion followed, in which I did rather poorly. Actually I had to admit defeat. Twice. But, as they say: fool me once, shame on me; fool me twice, won't get fooled again! </li>\n<li>Savage presents his theorem in his book: <em>The Foundations of Statistics</em>, Wiley, New York, 1954. </li>\n<li> To compare to Savage's setup: for the two outcome case, we deal directly with \"actions\" or equivalently \"events\", here called \"bets\". We can dispense with \"states\"; in particular we don't have to demand that the collection of bets be <a href=\"http://en.wikipedia.org/wiki/Sigma_algebra\">countably complete</a>, or even a power-set algebra of states, just that it be some boolean algebra. Savage's axioms of course have a descriptive interpretation, but it is their normativity that is at issue here, so I state them as \"you shall\". Rules 1-3 are his P1-P3, and 4 is P6. P4 and P7 are irrelevant in the two- outcome case. P5 is included in the background assumption that you would choose to win. I do not call this normative, because the payoff wasn't specified.</li>\n<li> Ellsberg originally proposed this just as a thought experiment, and canvassed various victims for their thoughts under what he called \"absolutely non-expiremental conditions\". He used $100 and $0 instead of pie and a boot to the head. Which is dull of course, but it shouldn't make a difference<sup>10</sup>. The experiment has since been repeated under more experimental conditions. The expirementers also invariably opt for the more boring cash payouts.</li>\n<li>Some people will say this isn't \"really\" a paradox. Meh.</li>\n<li>Actually, I inserted \"to pay\". It wasn't in the original post. But it should have been.</li>\n<li><a href=\"https://docs.google.com/open?id=0B4xY6UslgZW2YWI1ZjZjODUtMjM2NC00MDI3LWJlMTktYmI3ZDQ2ZWZkODgz\">Sneak preview</a></li>\n<li>As <a href=\"http://en.wikipedia.org/wiki/Forrest_gump\">a great decision theorist</a> once said, \"Stupid is as stupid does.\" </li>\n<li>...or should it? Savage's rule P4 demands that it shall not. And the method I have in mind obeys this rule. But it turns out this is another rule that God won't enforce. And that's yet another post, if I get to it at all. </li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "thHZiZBDRPtGxCM6f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 19, "extendedScore": null, "score": 8.316795781084174e-07, "legacy": true, "legacyId": "12172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to</strong>: <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability\">A summary of Savage's foundation for probability and utility.</a></p>\n<p>In 1961, <a href=\"http://en.wikipedia.org/wiki/Daniel_Ellsberg\">Daniel Ellsberg</a>, most famous for leaking the <a href=\"http://en.wikipedia.org/wiki/Pentagon_Papers\">Pentagon Papers</a>, published the decision-theoretic paradox which is now named after him <sup>1</sup>. It is a cousin to the Allais paradox. They both involve violations of an independence or separability principle. But they go off in different directions: one is a violation of expected utility, while the other is a violation of subjective probability. The Allais paradox has been <a href=\"/lw/my/the_allais_paradox/\">discussed on LW before</a>, but when I do a search it seems that the first discussion of the Ellsberg paradox on LW was my comments on the previous post <sup>2</sup>. It seems to me that from a Bayesian point of view, the Ellsberg paradox is the greater evil.</p>\n<p>But I should first explain what I mean by a violation of expected utility versus subjective probability, and for that matter, what I mean by Bayesian. I will explain a special case of Savage's representation theorem, which focuses on the subjective probability side only. Then I will describe Ellsberg's paradox. In the next episode, I will give an example of how not to be Bayesian. If I don't get voted off the island at the end of this episode.</p>\n<h2 id=\"Rationality_and_Bayesianism\">Rationality and Bayesianism</h2>\n<p>Bayesianism is often taken to involve the maximisation of expected utility with respect to a subjective probability distribution. I would argue this label only sticks to the subjective probability side. But mainly, I wish to make a clear division between the two sides, so I can focus on one.</p>\n<p>Subjective probability and expected utility are certainly related, but they're still independent. You could be perfectly willing and able to assign belief numbers to all possible events as if they were probabilities. That is, your belief assignment obeys all the laws of probability, including Bayes' rule, which is, after all, what the -ism is named for. You could do all that, but still maximise something other than expected utility. In particular, you could combine subjective probabilities with prospect theory, which has also been <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">discussed on LW before</a>. In that case you may display Allais-paradoxical behaviour but, as we will see, not Ellsberg-paradoxical behaviour. The rationalists might excommunicate you, but it seems to me you should keep your Bayesianist card.</p>\n<p>On the other hand your behaviour could be incompatible with any subjective probability distribution. But you could still maximise utility with respect to something other than subjective probability. In particular, when faced with known probabilities, you would be maximising expected utility in the normal sense. So you can not exhibit any Allais-paradoxical behaviour, because the Allais paradox involves only objective lotteries. But you may exhibit, as we will see, Ellsberg-paradoxical behaviour. I would say you are not Bayesian.</p>\n<p>So a non-Bayesian, even the strictest frequentist, can still be an expected utility maximiser, and a perfect Bayesian need not be an expected utility maximiser. What I'm calling Bayesianist is just the idea that we should reason with our subjective beliefs the same way that we reason with objective probabilities. This also has been called having \"probabilistically sophisticated\" beliefs, if you prefer to avoid the B-word, or don't like the way I'm using it.</p>\n<p>In a lot of what follows, I will bypass utility by only considering two outcomes. Utility functions are only unique up to a constant offset and a positive scale factor. With two outcomes, they evaporate entirely. The question of maximising expected utility with respect to a subjective probability distribution reduces to the question of maximising the probability, according to that distribution, of getting the better of the two outcomes. (And if the two outcomes are equal, there is nothing to maximise.)</p>\n<p>And on the flip side, if we have a decision method for the two-outcome case, Bayesian or otherwise, then we can always tack on a utility function. The idea of utility is just that any intermediate outcome is equivalent to an objective lottery between better and worse outcomes. So if we want, we can use a utility function to reduce a decision problem with any (finite) number of outcomes to a decision problem over the best and worst outcomes in question.</p>\n<h2 id=\"Savage_s_representation_theorem\">Savage's representation theorem</h2>\n<p>Let me recap some of the <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability/\">previous post</a> on Savage's theorem. How might we defend Bayesianism? We could invoke <a href=\"http://en.wikipedia.org/wiki/Cox %27s_theorem\">Cox's theorem</a>. This starts by assuming possible events can be assigned real numbers corresponding to some sort of belief level on someone's part, and that there are certain functions over these numbers corresponding to logical operations. It can be proven that, if someone's belief functions obey some simple rules, then that person acts <em>as if</em> they were reasoning with subjective probability. Now, while the rules for belief functions are intuitive, the background assumptions are pretty sketchy. It is not at all clear why these mathematical constructs are requirements of rationality.</p>\n<p>One way to justify those constructs is to argue in terms of choices a rational person must make. We imagine someone is presented with choices among various bets on uncertain events. Their level of belief in these events can be gauged by which bets they choose. But if we're going to do that anyway, then, as it turns out, we can just give some simple rules directly about these choices, and bypass the belief functions entirely. This was Leonard Savage's approach <sup>3</sup>. To quote a comment on the previous post: \"This is important because agents in general don't have to use beliefs or goals, but they do all have to choose actions.\"</p>\n<p>Savage's approach actually covers both subjective probability and expected utility. The previous post discusses both, whereas I am focusing on the former. This lets me give a shorter exposition, and I think a clearer one.</p>\n<p>We start by assuming some abstract collection of possible bets. We suppose that when you are offered two bets from this collection, you will choose one over the other, or express indifference.</p>\n<p>As discussed, we will only consider two outcomes. So all bets have the same payout, the difference among them is just their winning conditions. It is not specified what it is that you win. But it is assumed that, given the choice between winning unconditionally and losing unconditionally, you would choose to win.</p>\n<p>It is assumed that the collection of bets form what is called a <a href=\"http://en.wikipedia.org/wiki/Boolean_algebra_ %28structure%29\">boolean algebra</a>. This just means we can consider combinations of bets under boolean operators like \"and\", \"or\", or \"not\". Here I will use brackets to indicate these combinations. (A or B) is a bet that wins under the conditions that make either A win, or B win, or both win. (A but not B) wins whenever A wins but B doesn't. And so on.</p>\n<p>If you are rational, your choices must, it is claimed, obey some simple rules. If so, it can be proven that you are choosing <em>as if</em> you had a assigned subjective probabilities to bets. Savage's axioms for choosing among bets are <sup>4</sup>:</p>\n<ol>\n<li>If you choose A over B, you shall not choose B over A; and, if you do not choose A over B, and do not choose B over C, you shall not choose A over C. </li>\n<li>If you choose A over B, you shall also choose (A but not B) over (B but not A); and conversely, if you choose (A but not B) over (B but not A), you shall also choose A over B. </li>\n<li>You shall not choose A over (A or B).</li>\n<li>If you choose A over B, then you shall be able to specify a finite sequence of bets C<sub>1</sub>, C<sub>2</sub>, ..., C<sub>n</sub>, such that it is guaranteed that one and only one of the C's will win, and such that, for any one of the C's, you shall still choose (A but not C) over (B or C).</li>\n</ol>\n<p>Rule 1 is a coherence requirement on rational choice. It is requires your preferences to be a <a href=\"http://en.wikipedia.org/wiki/Strict_weak_ordering#Total_preorders\">total pre-order</a>. One objection to Cox's theorem is that&nbsp;levels of belief could be incomparable. This objection does not apply to rule 1 in this context because, as we discussed above, we're talking about choices of bets, not beliefs. Faced with choices, we choose. A rational person's choices must be non-circular.</p>\n<p>Rule 2 is an independence requirement. It demands that when you compare two bets, you ignore the possibilty that they could both win. In those circumstances you would be indifferent between the two anyway. The only possibilities that are relevant to the comparison are the ones where one bet wins and the other doesn't. So, you ought to compare A to B the same way you compare (A but not B) to (B but not A). Savage called this rule the Sure-thing principle.</p>\n<p>Rule 3 is a dominance requirement on rational choice. It demands that you not choose something that cannot do better under any circumstance: whenever A would win, so would (A or B). Note that you might judge (B but not A) to be impossible a priori. So, you might legitimately express indifference between A and (A or B). We can only say it is never legitimate to choose A over (A or B).</p>\n<p>Rule 4 is the most complicated. Luckily it's not going to be relevant to the Ellsberg paradox. Call it Mostly Harmless and forget this bit if you want.</p>\n<p>What rule 4 says is that if you choose A over B, you must be willing to pay a premium for your choice. Now, we said there are only two outcomes in this context. Here, the premium is paid in terms of other bets. Rule 4 demands that you give a finite list of <a href=\"http://en.wikipedia.org/wiki/Mutually_exclusive_events\">mutually exclusive</a> and <a href=\"http://en.wikipedia.org/wiki/Collectively_exhaustive_events\">exhaustive</a> events, and still be willing to choose A over B if we take any event on your list, cut it from A, and paste it to B. You can list as many events as you need to, but it must be a finite list.</p>\n<p>For example, if you thought A was much more likely than B, you might pull out a die, and list the 6 possible outcomes of one roll. You would also be willing to choose (A but not a roll of 1) over (B or a roll of 1), (A but not a roll of 2) over (B or a roll of 2), and so on. If not, you might list the 36 possible outcomes of two consecutive rolls, and be willing to choose (A but not two rolls of 1) over (B or two rolls of 1), and so on. You could go to any finite number of rolls.</p>\n<p>In fact rule 4 is pretty liberal, it doesn't even demand that every event on your list be equiprobable, or even independent of the A and B in question. It just demands that the events be mutually exclusive and exhaustive. If you are not willing to specify <em>some</em> such list of events, then you ought to express indifference between A and B.</p>\n<p>If you obey rules 1-3, then that is sufficient for us construct a sort of qualitative subjective probability out of your choices. It might not be quantitative: for one thing, there could be <a href=\"http://en.wikipedia.org/wiki/Infinitessimal\">infinitessimally</a> likely beliefs. Another thing is that there might be more than one way to assign numbers to beliefs. Rule 4 takes care of these things. If you obey rule 4 also, then we can assign a subjective probability to every possible bet, prove that you choose among bets <em>as if</em> you were using those probabilities, and also prove that it is the only probability assignment that matches your choices. And, on the flip side, if you are choosing among bets based on a subjective probability assignment, then it is easy to prove you obey rules 1-3, as well as rule 4 if the collection of bets is suitably infinite, like if a fair die is avaialble to bet on.</p>\n<p>Savage's theorem is impressive. The background assumptions involve just the concept of choice, and no numbers at all. There are only a few simple rules. Even rule 4 isn't really all that hard to understand and accept. A subjective probability distribution appears seemingly out of nowhere. In the full version, a utility function appears out of nowhere too. This theorem has been called the crowning glory of decision theory.</p>\n<h2 id=\"The_Ellsberg_paradox\">The Ellsberg paradox</h2>\n<p>Let's imagine there is an urn containing 90 balls. 30 of them are red, and the other 60 are either green or blue, in unknown proportion. We will draw a ball from the urn at random. Let us bet on the colour of this ball. As above, all bets have the same payout. To be specific, let's say you get pie if you win, and a <a href=\"http://en.wikipedia.org/wiki/Boot_to_the_head\">boot to the head</a> if you lose. The first question is: do you prefer to bet that the colour will be red, or that it will be green? The second question is: do you prefer to bet that it will be (red or blue), or that it will be (green or blue)?</p>\n<p>The most common response<sup>5</sup> is to choose red over green, and (green or blue) over (red or blue). And that's all there is to it. Paradox! <sup>6</sup></p>\n<p>\n</p><table border=\"0\" cellspacing=\"0\" cellpadding=\"5\">\n<tbody>\n<tr>\n<td rowspan=\"2\">&nbsp;</td>\n<td align=\"center\" bgcolor=\"red\"><span style=\"font-size: xx-small;\">30</span></td>\n<td colspan=\"2\" align=\"center\" bgcolor=\"cyan\"><span style=\"font-size: xx-small;\">60</span></td>\n</tr>\n<tr>\n<td align=\"center\" bgcolor=\"red\"><span style=\"font-size: xx-small;\"><strong>Red</strong></span></td>\n<td align=\"center\" bgcolor=\"green\"><span style=\"font-size: xx-small;\"><strong>Green</strong></span></td>\n<td align=\"center\" bgcolor=\"blue\"><span style=\"font-size: xx-small;\"><strong>Blue</strong></span></td>\n</tr>\n<tr>\n<td colspan=\"5\">\n<hr>\n</td>\n</tr>\n<tr>\n<th>A</th>\n<td align=\"center\" bgcolor=\"red\">pie</td>\n<td align=\"center\" bgcolor=\"green\">BOOT</td>\n<td align=\"center\" bgcolor=\"blue\">BOOT</td>\n<td rowspan=\"2\">&nbsp;</td>\n<td rowspan=\"2\"><span><em>A is preferred to B</em></span></td>\n</tr>\n<tr>\n<th>B</th>\n<td align=\"center\" bgcolor=\"red\">BOOT</td>\n<td align=\"center\" bgcolor=\"green\">pie</td>\n<td align=\"center\" bgcolor=\"blue\">BOOT</td>\n</tr>\n<tr>\n<td colspan=\"5\">\n<hr>\n</td>\n</tr>\n<tr>\n<th>C</th>\n<td align=\"center\" bgcolor=\"red\">pie</td>\n<td align=\"center\" bgcolor=\"green\">BOOT</td>\n<td align=\"center\" bgcolor=\"blue\">pie</td>\n<td rowspan=\"2\">&nbsp;</td>\n<td rowspan=\"2\"><span><em>D is preferred to C</em></span></td>\n</tr>\n<tr>\n<th>D</th>\n<td align=\"center\" bgcolor=\"red\">BOOT</td>\n<td align=\"center\" bgcolor=\"green\">pie</td>\n<td align=\"center\" bgcolor=\"blue\">pie</td>\n</tr>\n<tr>\n<td colspan=\"5\">&nbsp;</td>\n<td align=\"center\"><span><em>Paradox!</em></span></td>\n</tr>\n</tbody>\n</table>\n<p></p>\n<p>&nbsp;</p>\n<p>If choices were based solely on an assignment of subjective probability, then because the three colours are mutually exclusive, P(red or blue) = P(red) + P(blue), and P(green or blue) = P(green) + P(blue). So, since P(red) &gt; P(green) then P (red or blue) &gt; P(green or blue), but instead we have P(red or blue) &lt; P(green or blue).</p>\n<p>Knowing Savage's representation theorem, we expect to get a formal contradiction from the 4 rules above plus the 2 expressed choices. Something has to give, so we'd like to know which rules are really involved. You can see that we are talking only about rule 2, the Sure-thing principle. It says we shall compare (red or blue) to (green or blue) the same way as we compare red to green.</p>\n<p>This behaviour has been called <a href=\"http://en.wikipedia.org/wiki/Ambiguity_aversion\">ambiguity aversion</a>. Now, perhaps this is just a cognitive bias. It wouldn't be the first time that people behave a certain way, but the analysis of their decisions shows a clear error. And indeed, when explained, some people do repent of their sins against Bayes. They change their choices to obey rule 2. But others don't. To quote Ellsberg:</p>\n<blockquote>\n<p>...after rethinking all their 'offending' decisions in light of [Savage's] axioms, a number of people who are not only sophisticated but reasonable decide that they wish to persist in their choices. This includes people who previously felt a 'first order commitment' to the axioms, many of them surprised and some dismayed to find that they wished, in these situations, to violate the Sure-thing Principle. Since this group included L.J. Savage, when last tested by me (I have been reluctant to try him again), it seems to deserve respectful consideration.</p>\n</blockquote>\n<p>I include myself in the group that thinks rule 2 is what should be dropped. But I don't have any dramatic (de-)conversion story to tell. I was somewhat surprised, but not at all dismayed, and I can't say I felt much if any prior commitment to the rules. And as to whether I'm sophisticated or reasonable, well never mind! Even if there are a number of other people who are all of the above, and even if Savage himself may have been one of them for a while, I do realise that smart people can be Just Plain Wrong. So I'd better have something more to say for myself.</p>\n<p>Well, red obviously has a probability of 1/3. Our best guess is to apply the <a href=\"http://en.wikipedia.org/wiki/Principle_of_indifference\">principle of indifference</a> to also assign probability 1/3 to green or blue. But our best guess is not necessarily a good guess. The probabilities we assign to red, and to (green or blue), are objective. We're guessing the probability of green, and of (red or blue). It seems wise to take this difference into account when choosing what to bet on, doesn't it? And surely it will be all the more wise when dealing with real-life, non- symetrical situations where we can't even appeal to the principle of indifference.</p>\n<p>Or maybe I'm just some fool talking <a href=\"http://www.urbandictionary.com/define.php?term=jibba%20jabba\">jibba jabba</a>. Against this sort of talk, the <a href=\"/lw/my/the_allais_paradox/\">LW post on the Allais paradox</a> presents a version of Howard Raiffa's dynamic inconsistency argument. This makes no references to internal thought processes, it is a purely <em>external</em> argument about the decisions themselves. As stated in that post, \"There is always a price to pay for leaving the Bayesian Way.\" <sup>7</sup> This is expanded upon in <a href=\"/lw/mt/beautiful_probability/\">an earlier post</a>:</p>\n<blockquote>\n<p>Sometimes you must seek an approximation; often, indeed. This doesn't mean that probability theory has ceased to apply, any more than your inability to calculate the aerodynamics of a 747 on an atom-by-atom basis implies that the 747 is not made out of atoms. Whatever approximation you use, it works to the extent that it approximates the ideal Bayesian calculation - and fails to the extent that it departs.</p>\n<p>Bayesianism's coherence and uniqueness proofs cut both ways ... anything that is not Bayesian must fail one of the coherency tests. This, in turn, opens you to punishments like Dutch-booking (accepting combinations of bets that are sure losses, or rejecting combinations of bets that are sure gains).</p>\n</blockquote>\n<p>Now even if you believe this about the Allais paradox, I've argued that this doesn't really have much to do with Bayesianism one way or the other. The Ellsberg paradox is what actually strays from the Path. So, does God also punish ambiguity aversion?</p>\n<p>Tune in next time<sup>8</sup>, when I present a two-outcome decision method that obeys rules 1, 3, and 4, and even a weaker form of rule 2. But it exhibits ambiguity aversion, in gross violation of the original rule 2, so that it's not even approximately Bayesian. I will try to present it in a way that advocates for its internal cognitive merit. But the main thing <sup>9</sup> is that, externally, it is dynamically consistent. We do not get booked, by the Dutch or any other nationality.</p>\n<h2 id=\"Notes\">Notes</h2>\n<p>&nbsp;</p>\n<ol>\n<li> Ellsberg's original paper is: <em>Risk, ambiguity, and the Savage axioms</em>, Quarterly Journal of Economics 75 (1961) pp 643-669</li>\n<li> Some discussion followed, in which I did rather poorly. Actually I had to admit defeat. Twice. But, as they say: fool me once, shame on me; fool me twice, won't get fooled again! </li>\n<li>Savage presents his theorem in his book: <em>The Foundations of Statistics</em>, Wiley, New York, 1954. </li>\n<li> To compare to Savage's setup: for the two outcome case, we deal directly with \"actions\" or equivalently \"events\", here called \"bets\". We can dispense with \"states\"; in particular we don't have to demand that the collection of bets be <a href=\"http://en.wikipedia.org/wiki/Sigma_algebra\">countably complete</a>, or even a power-set algebra of states, just that it be some boolean algebra. Savage's axioms of course have a descriptive interpretation, but it is their normativity that is at issue here, so I state them as \"you shall\". Rules 1-3 are his P1-P3, and 4 is P6. P4 and P7 are irrelevant in the two- outcome case. P5 is included in the background assumption that you would choose to win. I do not call this normative, because the payoff wasn't specified.</li>\n<li> Ellsberg originally proposed this just as a thought experiment, and canvassed various victims for their thoughts under what he called \"absolutely non-expiremental conditions\". He used $100 and $0 instead of pie and a boot to the head. Which is dull of course, but it shouldn't make a difference<sup>10</sup>. The experiment has since been repeated under more experimental conditions. The expirementers also invariably opt for the more boring cash payouts.</li>\n<li>Some people will say this isn't \"really\" a paradox. Meh.</li>\n<li>Actually, I inserted \"to pay\". It wasn't in the original post. But it should have been.</li>\n<li><a href=\"https://docs.google.com/open?id=0B4xY6UslgZW2YWI1ZjZjODUtMjM2NC00MDI3LWJlMTktYmI3ZDQ2ZWZkODgz\">Sneak preview</a></li>\n<li>As <a href=\"http://en.wikipedia.org/wiki/Forrest_gump\">a great decision theorist</a> once said, \"Stupid is as stupid does.\" </li>\n<li>...or should it? Savage's rule P4 demands that it shall not. And the method I have in mind obeys this rule. But it turns out this is another rule that God won't enforce. And that's yet another post, if I get to it at all. </li>\n</ol>\n<p>&nbsp;</p>", "sections": [{"title": "Rationality and Bayesianism", "anchor": "Rationality_and_Bayesianism", "level": 1}, {"title": "Savage's representation theorem", "anchor": "Savage_s_representation_theorem", "level": 1}, {"title": "The Ellsberg paradox", "anchor": "The_Ellsberg_paradox", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "54 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5J34FAKyEmqKaT7jt", "zJZvoiwydJ5zvzTHK", "LQp9cZPzJncFKh5c8", "bkSkRwo9SRYxJMiSY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-15T08:05:37.476Z", "modifiedAt": null, "url": null, "title": "Idea: Add books to an SRS without splitting them into facts", "slug": "idea-add-books-to-an-srs-without-splitting-them-into-facts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:32.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Malik", "createdAt": "2011-01-05T12:45:17.182Z", "isAdmin": false, "displayName": "D_Malik"}, "userId": "9dhw3PngyAWKqTymS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/reqRF2mgQcweTMgyA/idea-add-books-to-an-srs-without-splitting-them-into-facts", "pageUrlRelative": "/posts/reqRF2mgQcweTMgyA/idea-add-books-to-an-srs-without-splitting-them-into-facts", "linkUrl": "https://www.lesswrong.com/posts/reqRF2mgQcweTMgyA/idea-add-books-to-an-srs-without-splitting-them-into-facts", "postedAtFormatted": "Sunday, January 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Idea%3A%20Add%20books%20to%20an%20SRS%20without%20splitting%20them%20into%20facts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdea%3A%20Add%20books%20to%20an%20SRS%20without%20splitting%20them%20into%20facts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FreqRF2mgQcweTMgyA%2Fidea-add-books-to-an-srs-without-splitting-them-into-facts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Idea%3A%20Add%20books%20to%20an%20SRS%20without%20splitting%20them%20into%20facts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FreqRF2mgQcweTMgyA%2Fidea-add-books-to-an-srs-without-splitting-them-into-facts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FreqRF2mgQcweTMgyA%2Fidea-add-books-to-an-srs-without-splitting-them-into-facts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 644, "htmlBody": "<p><a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition\">Spaced repetition systems</a> use math to determine the optimal way to study things. This post is about an idea I've been trying for a few months for improving SRS for some subjects.</p>\n<p>&nbsp;</p>\n<p>SRSs usually use a pretty rigid system of asking questions and demanding answers. I think that for many subjects it's not very important to know specific answers, either because such answers can be looked up easily or because the gist of a subject is more important. So here's an idea: add an ebook to a spaced repetition system and read/skim each chapter or page when it's due for review. This can be used for ebooks, physical books, or articles from the internet or elsewhere.</p>\n<p>&nbsp;</p>\n<p>For books or ebooks, there are two ways to do this: either add each page as an individual card (with an image of the page right on the card) or create a card for each section or chapter. The latter technique can be used for non-electronic books. If each page is its own card, you can review things more quickly because you don't have to open an ebook or book each time you review, but you'll need to convert the ebook to images first. You can also add annotations, either by editing page images, typing notes onto pages' cards, or adding annotations with your ebook-reading software.</p>\n<p>&nbsp;</p>\n<p>One way to convert ebooks to images is to use imagemagick. On Linux,</p>\n<blockquote>\n<p>convert -density 180x180 BOOK.pdf folder/imgname.png</p>\n</blockquote>\n<p>Change the density if images are too small or too large. You'll have to convert ebooks to pdf format first. This command creates all the pages as imgname-1.png, imgname-2.png, etc. Move the images into a .media folder where your other anki decks are. Use a script to make a card for each page. For example, using python:</p>\n<blockquote>\n<p>f = open(\"bookcards\", \"w\")</p>\n<p>for i in range(0,NUMPAGES): f.write(str(i+1) + \";&lt;img src=\\\"imgname-\" + str(i) + \".png\\\" /&gt;\\n\")</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>You probably want to review cards in the order they were created (so that you'll review due cards by page number). This option doesn't exist in anki, so you'll need to make each book a separate deck and use the <em>patch </em>command to apply this diff to /usr/share/anki/anki/deck.py, or wherever that file is on your computer:</p>\n<blockquote>\n<p>64a65<br />&gt; REV_CARDS_CREATED_FIRST = 4<br />389c390,391<br />&lt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"priority desc, factId, ordinal\")[self.revCardOrder]<br />---<br />&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"priority desc, factId, ordinal\",<br />&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"created asc\")[self.revCardOrder]<br />3557a3560,3561<br />&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'createdDesc':<br />&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; '(created desc)',<br />3566a3571,3572<br />&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if self.revCardOrder == REV_CARDS_CREATED_FIRST:<br />&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; required.append(\"createdDesc\")<br />4507a4514<br />&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4: _(\"Review in ORDER CREATED\"),</p>\n</blockquote>\n<p>Also, for each deck, go Settings-&gt;Advanced-&gt;Initial button intervals and set them so there's no randomness.</p>\n<p>&nbsp;</p>\n<p>Pros of this technique:</p>\n<ul>\n<li>You can add lots of content quickly.</li>\n<li>You can use an SRS to learn things that you wouldn't be able to otherwise. How would you add Godel, Escher, Bach to an SRS in question-answer format?</li>\n<li>Reading books without memorizing them is silly unless your aims in reading do not require long-term retention of the books' contents.</li>\n<li>You can keep the context of facts, and you automatically preserve the original phrasing.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Cons of this technique</p>\n<ul>\n<li>The spacing is probably not optimal.</li>\n<li>Review is passive, not active. Active recall has been shown to improve memory. This technique trades away memory-detail for time (and other things).</li>\n<li>You may make irrelevant associations between things just because they're next to each other in the book.</li>\n<li>Reading through things takes long. If you skim through your due cards, then you might miss things. You'll also have to skim elementary explanations again (perhaps that's a good thing) or suspend them.</li>\n<li>It takes time to convert books to images, or to open a book or ebook each time you need to review it.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "reqRF2mgQcweTMgyA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "12176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-15T16:53:58.165Z", "modifiedAt": null, "url": null, "title": "Who's in the AI business?", "slug": "who-s-in-the-ai-business", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.552Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LEB72acv7oa2WQTxZ/who-s-in-the-ai-business", "pageUrlRelative": "/posts/LEB72acv7oa2WQTxZ/who-s-in-the-ai-business", "linkUrl": "https://www.lesswrong.com/posts/LEB72acv7oa2WQTxZ/who-s-in-the-ai-business", "postedAtFormatted": "Sunday, January 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Who's%20in%20the%20AI%20business%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWho's%20in%20the%20AI%20business%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLEB72acv7oa2WQTxZ%2Fwho-s-in-the-ai-business%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Who's%20in%20the%20AI%20business%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLEB72acv7oa2WQTxZ%2Fwho-s-in-the-ai-business", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLEB72acv7oa2WQTxZ%2Fwho-s-in-the-ai-business", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p>Is any member of this community able and willing to tell the whole world, what AI related work he is doing?</p>\n<p>A concrete examples, like \"I am with the Google translator team\" or \"I, alone, have done this and this\" - are very welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LEB72acv7oa2WQTxZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 16, "extendedScore": null, "score": 8.32175570595011e-07, "legacy": true, "legacyId": "12177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T00:56:04.835Z", "modifiedAt": null, "url": null, "title": "Open Thread, January 15-31, 2012", "slug": "open-thread-january-15-31-2012", "viewCount": null, "lastCommentedAt": "2018-11-22T03:43:02.565Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cit3HYXehBsr4d36Q/open-thread-january-15-31-2012", "pageUrlRelative": "/posts/cit3HYXehBsr4d36Q/open-thread-january-15-31-2012", "linkUrl": "https://www.lesswrong.com/posts/cit3HYXehBsr4d36Q/open-thread-january-15-31-2012", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20January%2015-31%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20January%2015-31%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcit3HYXehBsr4d36Q%2Fopen-thread-january-15-31-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20January%2015-31%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcit3HYXehBsr4d36Q%2Fopen-thread-january-15-31-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcit3HYXehBsr4d36Q%2Fopen-thread-january-15-31-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">(I plan to make these threads from now on. Downvote if you disapprove. If I miss one, feel free to do it yourself.)</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cit3HYXehBsr4d36Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "12190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 248, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T04:47:17.549Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Disguised Queries", "slug": "seq-rerun-disguised-queries", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2LuFTGeQzx3sAnBmp/seq-rerun-disguised-queries", "pageUrlRelative": "/posts/2LuFTGeQzx3sAnBmp/seq-rerun-disguised-queries", "linkUrl": "https://www.lesswrong.com/posts/2LuFTGeQzx3sAnBmp/seq-rerun-disguised-queries", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Disguised%20Queries&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Disguised%20Queries%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LuFTGeQzx3sAnBmp%2Fseq-rerun-disguised-queries%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Disguised%20Queries%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LuFTGeQzx3sAnBmp%2Fseq-rerun-disguised-queries", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2LuFTGeQzx3sAnBmp%2Fseq-rerun-disguised-queries", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>Today's post, <a href=\"/lw/nm/disguised_queries/\">Disguised Queries</a> was originally published on 09 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You ask whether something \"is\" or \"is not\" a category member but can't name the question you really want answered. What is a \"man\"? Is Barney the Baby Boy a \"man\"? The \"correct\" answer may depend considerably on whether the query you really want answered is \"Would hemlock be a good thing to feed Barney?\" or \"Will Barney make a good husband?\"</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/9ck/seq_rerun_the_cluster_structure_of_thingspace/\">The Cluster Structure of Thingspace</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2LuFTGeQzx3sAnBmp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.324463778711048e-07, "legacy": true, "legacyId": "12195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4FcxgdvdQP45D6Skg", "cKrHa9M9JfiKQxbeM", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T06:51:05.412Z", "modifiedAt": null, "url": null, "title": "[LINK]The Edge's yearly question", "slug": "link-the-edge-s-yearly-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.560Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a27M8LX5xDohwnhsX/link-the-edge-s-yearly-question", "pageUrlRelative": "/posts/a27M8LX5xDohwnhsX/link-the-edge-s-yearly-question", "linkUrl": "https://www.lesswrong.com/posts/a27M8LX5xDohwnhsX/link-the-edge-s-yearly-question", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5DThe%20Edge's%20yearly%20question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5DThe%20Edge's%20yearly%20question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa27M8LX5xDohwnhsX%2Flink-the-edge-s-yearly-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5DThe%20Edge's%20yearly%20question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa27M8LX5xDohwnhsX%2Flink-the-edge-s-yearly-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa27M8LX5xDohwnhsX%2Flink-the-edge-s-yearly-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 22, "htmlBody": "<p>What is your favorite explanation?</p>\n<p><a href=\"http://www.edge.org/responses/what-is-your-favorite-deep-elegant-or-beautiful-explanation\">http://www.edge.org/responses/what-is-your-favorite-deep-elegant-or-beautiful-explanation</a></p>\n<p>And what's yours?</p>\n<p>&nbsp;</p>\n<p>EDIT: I mean yours of those at edge.org - and/or yours which you will explain here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a27M8LX5xDohwnhsX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 8.324933924896647e-07, "legacy": true, "legacyId": "12202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T16:16:18.136Z", "modifiedAt": null, "url": null, "title": "The lessons of a world without Hitler", "slug": "the-lessons-of-a-world-without-hitler", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.964Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5jkuBeiLpLKjKowCs/the-lessons-of-a-world-without-hitler", "pageUrlRelative": "/posts/5jkuBeiLpLKjKowCs/the-lessons-of-a-world-without-hitler", "linkUrl": "https://www.lesswrong.com/posts/5jkuBeiLpLKjKowCs/the-lessons-of-a-world-without-hitler", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20lessons%20of%20a%20world%20without%20Hitler&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20lessons%20of%20a%20world%20without%20Hitler%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5jkuBeiLpLKjKowCs%2Fthe-lessons-of-a-world-without-hitler%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20lessons%20of%20a%20world%20without%20Hitler%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5jkuBeiLpLKjKowCs%2Fthe-lessons-of-a-world-without-hitler", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5jkuBeiLpLKjKowCs%2Fthe-lessons-of-a-world-without-hitler", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 566, "htmlBody": "<p>What would the world look like without Hitler? Fiction is <a href=\"http://en.wikipedia.org/wiki/The_Years_of_Rice_and_Salt\">generally</a> <a href=\"http://en.wikipedia.org/wiki/Making_History_(novel)\">unequivocal</a> about this: the removal of Hitler makes no difference, the world will still lurch towards a world war through some other path. WWII and the Holocaust are such major, defining events of the twentieth century, that we twist&nbsp;counterfactual&nbsp;events to ensure they still happen.</p>\n<p>Against this, <a href=\"http://www.amazon.co.uk/Better-Angels-Our-Nature-Violence/dp/1846140935\">some</a> <a href=\"http://www.commentarymagazine.com/article/no-hitler-no-holocaust/\">have</a> <a href=\"http://www.infobarrel.com/No_Hitler_No_Holocaust\">made</a> the argument that Hitler was essentially sole responsible for WWII and especially for the Holocaust - no Hitler, no war, no extermination camps. The no Holocaust argument is quite solid: the extermination system was expensive, militarily counter-productive, and could only have happened given a leader lacking checks and balance and with an&nbsp;id&eacute;e fixe that&nbsp;overrode&nbsp;everything else (general&nbsp;European&nbsp;antisemitism&nbsp;<em>allowed</em> the Holocaust, but didn't <em>cause</em> it). The no WWII argument points out that Hitler was both irrational and lucky: he often took great risks, on flimsy evidence, and got away with them. Certainly his decisions in the later, post-<a href=\"http://en.wikipedia.org/wiki/Operation_Barbarossa\">Barbarossa</a>&nbsp;period of his reign belie political, military or organisational genius. And it was the height of stupidity to have gone to war, for a half of Poland, with&nbsp;simultaneously&nbsp;the world's greatest empire and what appeared to be the overwhelmingly strong French army. Yes Gamelin, the French commander in chief, did behave like a concussed duckling, and the German army outfought the French - but no-one could have predicted this, and no-one sensible would have counted on it, and hence they wouldn't have risked the war. Hitler wan't sensible, and lucked out.<a id=\"more\"></a></p>\n<p>Lay aside whether that argument is true, and let's explore its consequences. The counterfactual history is fascinating enough on its own - no rise of the USA and USSR as military superpowers, no Manhattan project, most likely no war between Japan and any western powers in the Pacific, the continuing occupation of China, and probably a much slower and Japan-influenced&nbsp;decolonisation&nbsp;process. <a href=\"http://en.wikipedia.org/wiki/Command_%26_Conquer:_Red_Alert_series\">Speculation</a> and more sensible <a href=\"http://www.jstor.org/stable/194248\">models</a> do point towards another war: most of Europe allied against the USSR, a war that the USSR would most likely have lost.</p>\n<p>That is all entertaining; but much more important is the fact that if WWII was an unlikely event, then the lessons we've learnt from it have been over-learnt. WWII proved that a&nbsp;developed, modern nation, <em>could</em> instigate genocide against segments of its population - but this doesn't mean that it's particularly likely. Similarly, appeasement backed up with&nbsp;implicit&nbsp;and then explicit threats failed to contain the irrational Hitler - but that doesn't mean they wouldn't have worked with slightly more rational leaders. Through&nbsp;aggressiveness&nbsp;and focus Hitler did conquer much territory - but the Nazi state was a crumbling morass of conflicting groups that would not have survived their leader. And in a counterfactual world without Hitler, with a contained or defeated USSR and an occupied China, there would not have been the rise of the great ideologies that shaped conflict in the 20th century. There would have been no European Union or UN. And how nuclear weapons would have been developed and spread is a great unknown.</p>\n<p>So, if instead of seeing WWII and the Holocaust as inevitable, we see Hitler as singularly responsible, where would&nbsp;that leave us? More confident in the niceness of developed nations. More confident in the use of negotiations. Less likely to see dictatorships as&nbsp;effective&nbsp;governments. Less likely to see secular ideology as an intrinsically powerful force. Less likely to believe that supra-national organisations are easy to put together. And less likely to see nuclear weapons as intrinsically stabilising influences.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5jkuBeiLpLKjKowCs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": -5, "extendedScore": null, "score": 8.327081029166341e-07, "legacy": true, "legacyId": "12207", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T17:38:04.771Z", "modifiedAt": null, "url": null, "title": "Completeness, incompleteness, and what it all means: first versus second order logic", "slug": "completeness-incompleteness-and-what-it-all-means-first", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:41.444Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MLqhJ8eDy5smbtGrf/completeness-incompleteness-and-what-it-all-means-first", "pageUrlRelative": "/posts/MLqhJ8eDy5smbtGrf/completeness-incompleteness-and-what-it-all-means-first", "linkUrl": "https://www.lesswrong.com/posts/MLqhJ8eDy5smbtGrf/completeness-incompleteness-and-what-it-all-means-first", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Completeness%2C%20incompleteness%2C%20and%20what%20it%20all%20means%3A%20first%20versus%20second%20order%20logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACompleteness%2C%20incompleteness%2C%20and%20what%20it%20all%20means%3A%20first%20versus%20second%20order%20logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMLqhJ8eDy5smbtGrf%2Fcompleteness-incompleteness-and-what-it-all-means-first%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Completeness%2C%20incompleteness%2C%20and%20what%20it%20all%20means%3A%20first%20versus%20second%20order%20logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMLqhJ8eDy5smbtGrf%2Fcompleteness-incompleteness-and-what-it-all-means-first", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMLqhJ8eDy5smbtGrf%2Fcompleteness-incompleteness-and-what-it-all-means-first", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3413, "htmlBody": "<p>First order arithmetic is incomplete. Except that it's also complete. Second order arithmetic is more expressive - except when it's not - and is also incomplete and also complete, except when it means something different.&nbsp;Oh, and full second order-logic might not really be a logic at all. But then, first order logic has no idea what the reals and natural numbers are, especially when it tries to talk about them.</p>\n<p>That was about the state of my confusion, and I set out to try and clear it up.&nbsp;Here I'll try and share an understanding of what is really going on with first and second order logic and why they differ so radically. It will be deliberately informal, so I won't be distinguishing between functions, predicates and subsets, and will be using little notation.&nbsp;It'll be exactly what I wish someone had told me before I started looking into the whole field.&nbsp;</p>\n<h2>Meaningful Models</h2>\n<p>An old man starts talking to you about addition, subtraction and multiplication, and how they <a href=\"http://en.wikipedia.org/wiki/Ring_(mathematics)#Formal_definition\">interact</a>. You assume he was talking about the integers; <a href=\"http://en.wikipedia.org/wiki/Leopold_Kronecker\">turns out</a> he means the rational numbers. The integers and the rationals are <a href=\"http://en.wikipedia.org/wiki/Model_theory\">both</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Structure_(mathematical_logic)\">models</a> of addition, subtraction and multiplication, in that they obey all the properties that the old man set out. But notice though he had the rationals in mind, he didn't mention them at all, he just listed the properties, and the rational numbers turned out, very non-coincidentally, to obey them.</p>\n<p>These models are generally taken to give meaning to the abstract symbols in the axioms - to give semantics to the syntax. In this view, \"for all x,y xy=yx\" is a series of elegant squiggles, but once we have the model of the integers (or the rationals) in mind, we realise that this means that multiplication is commutative.</p>\n<p><a id=\"more\"></a></p>\n<p>Similarly, models can define the \"truth\" of sentences. Consider the following sentences:</p>\n<p style=\"padding-left: 30px;\">(1) 2 has a multiplicative inverse.<br />(2) There exists a number that squares to -1.<br />(3) 2 is not equal to zero.<br />(4) If a+b=0, then a<sup>2</sup>=b<sup>2</sup>.<br />(5) No number is equal to zero.</p>\n<p>Are these true? You and the old man would disagree on (1), with him saying yes and you saying no - your models have enabled you to attach truth-values to the statement. You would both claim (2) is false, but there are other models - such as the complex numbers - where it is true. You would both claim (3) is true, but there are other models - such as the field with two elements - where it is false. So truth is model dependent.</p>\n<p>The last two are interesting, because it turns out that (4) is true in every model, and (5) is false. Statements like (4) and \"not (5)\" that are true in every model are called valid. Since they are independent of the choice of models, these statements are, in a certain sense, true from pure syntax. Both these statement can also be deduced purely from the axioms. It would be nice if all valid statements could also be deduced. But only first order logic allows this.</p>\n<p>What would also be nice if you could agree on the model you're using. Maybe the old man could add \"2 (and&nbsp;every non-zero number) has a multiplicative inverse\", giving the <a href=\"http://en.wikipedia.org/wiki/Field_axioms#Definition_and_illustration\">field axioms</a>, and ruling your integers right out. But there are still many fields - the rationals, the reals, the complex numbers, and many in between. But maybe with a few more axioms, you could really narrow thing downs, and treat the axioms and the model interchangeably. But only second order logic allows <em>this</em>.</p>\n<h1>First order fun</h1>\n<p>First order theories are those where you can quantify over the basic objects in your theory, and phrase statements like \"all Greeks enjoy dancing\" and \"there exists a blind millionaire\". This distinguishes them from second order theories where you can quantify over higher order objects (predicates and functions), phrasing sentiments like \"all nationalities are equal\" or \"there exists a dominant social class\" - in first order logic with humans as basic objects, nationality and social class are predicates, and you can't quantify over them. You don't appreciate how limiting first order logic can be until you've worked with it a while; nevertheless, it's a good logic to start with and possesses certain key properties not shared by higher order logics. Let's start with the most famous result, the incompleteness theorem.</p>\n<h2>G&ouml;del's incompleteness theorem</h2>\n<p>G&ouml;del's (first) <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems#First_incompleteness_theorem\">incompleteness theorem</a>, is a theorem about an arithmetic but also (implicitly) about a model. The implicit model is the natural numbers: any arithmetic that can model them suffers from the incompleteness theorem. But it is not really about any model, beyond that requirement: it's an intrinsic limitation of the system.</p>\n<p>Let's assume we have the usual (first order) <a href=\"http://en.wikipedia.org/wiki/Peano_axioms#The_axioms\">Peano axioms</a> for ordering, addition and multiplication. We also need an <a href=\"http://en.wikipedia.org/wiki/Axiom_schema\">infinity of axioms</a> to define induction, but that isn't as bad as it seem: given a specific sentence, it's easy to check whether or not it's an axiom, in a fast and efficient way. To nobody's surprise, the natural number are a model of first order Peano arithmetic.</p>\n<p>And inside this model, we can construct the&nbsp;G&ouml;del sentence G, which is equivalent with \"there is no proof of G\". By 'proof', we mean a natural number n that is the&nbsp;<a href=\"http://en.wikipedia.org/wiki/G%C3%B6del_numbering\">G&ouml;del number</a>&nbsp;that encodes a proof of G. Obviously, Peano arithmetic cannot prove G without being inconsistent; but this is precisely what G is saying, so we can actually see that G is true. And hence that \"not G\" cannot be provable if our arithmetic is consistent. This is the incompleteness theorem: neither G nor \"not G\" are provable, so the proof system is \"incomplete\".</p>\n<h2>G&ouml;del's completeness theorem</h2>\n<p>Enough with <em>in</em>completeness; what about&nbsp;G&ouml;del <a href=\"http://en.wikipedia.org/wiki/Completeness_theorem\">completeness theorem</a>? Unlike the previous theorem, this is a statement about about the axiomatic system and <em>all</em>&nbsp;of its models. It simply says that if a sentence is valid (true in every model) for a first order theory, then it can be proved from the axioms. This provides a bridge between the semantic concept of \"true\" (true in every model) and the syntactic concept of provable (can be proved by these formal manipulations). It also implies that we can enumerate all the sentences that are valid in a first order system, simply by enumerating all the proofs.</p>\n<p>Where does this leave the G&ouml;del sentence G? We've seen we can't prove it from the axioms, hence it cannot be true in all models. Therefore there must exist a model N' of first order Peano arithmetic in which G is false. What does that mean? G claims that \"there does not exist a number n with (certain properties)\", so if G is false, such an n does exist. Now we know (because we've constructed it that way) that if that n were a natural number, then those (certain properties) means that it must encode a proof of G. Since there is no proof of G, n cannot be a natural number, but must be an extra, a non-standard number, from beyond our usual universe. This also means that those&nbsp;(certain properties) do not capture what we thought they did: they only mean \"encodes a proof of G\" for the standard natural numbers.</p>\n<p>This seems somewhat troubling, that Peano arithmetic would admit two distinct models and fail to say what we thought it said; but it gets worse.</p>\n<h2>The L&ouml;wenheim&ndash;Skolem theorem</h2>\n<p>The <a href=\"http://en.wikipedia.org/wiki/L%C3%B6wenheim%E2%80%93Skolem_theorem\">L&ouml;wenheim&ndash;Skolem</a> theorem says that if any countable first order theory (such as Peano arithmetic) has an infinite model, then it has a model for every size (\"<a href=\"http://en.wikipedia.org/wiki/Cardinality\">cardinality</a>\") of infinity. Therefore first order Peano arithmetic has to have many, many models; at a minimum, it has to have a model of same size as the reals.</p>\n<p>This also means that no matter how much first order information we add to Peano arithmetic, we cannot restrict it to only being about the natural numbers; the models and the axioms can never be interchangeable. But it gets still worse.</p>\n<p>Uncountable models of Peano arithmetic are bad enough, but it turns out that Peano arithmetic also has <a href=\"http://en.wikipedia.org/wiki/Non-standard_model_of_arithmetic#Structure_of_countable_non-standard_models\">many other countable&nbsp;models</a> - models of same&nbsp;<a href=\"http://en.wikipedia.org/wiki/Countable\">size</a>&nbsp;as the natural numbers, but weirdly different. Weirdly is an apt summation of these models where <a href=\"http://en.wikipedia.org/wiki/Tennenbaum%27s_theorem\">neither multiplication nor addition can be computed</a>.</p>\n<p>So first order Peano arithmetic is not really about the natural numbers at all - but about the natural numbers and all these strange countable and uncountable models that we can't really describe. Maybe second order theories will do better?</p>\n<h1>Second order scariness</h1>\n<p>In second order theories, we can finally do what we've been itching to do: apply the existential and universal quantifiers to predicates, functions, sets of numbers and objects of that ilk. We can triumphantly toss away the infinitely many axioms needed to define induction in first order Peano arithmetic, and replace them with a simple:</p>\n<ul>\n<li>\"Every (non-empty) set of numbers has a least element.\"</li>\n</ul>\n<p>This, as every schoolboy should know, is enough to uniquely define the natural numbers. Or is it?</p>\n<h2>The importance of semantics</h2>\n<p>That sentence remains a series of squiggles until we've decided what those&nbsp;squiggles actually <em>mean</em>. This takes on an extra importance in second order logic that it didn't have in first order. When we say \"every set of numbers\", what do we mean? In terms of meaning and models, what models are we going to be considering?</p>\n<p>The first idea is the obvious one: \"every set of numbers\" means, duh, \"<em>every</em> set of numbers\". When we specify a model, we'll give the 'universe of discourse', the basic objects (maybe the natural numbers or the reals), and the quantifier 'every' will range over all possible subsets of this universe (every subset of the natural or real numbers). This is called the full or standard semantics for second order arithmetic, and the models are called full models.</p>\n<p>But note what we have done here: we have brought in extra information to clarify what we are talking about.&nbsp;In order to defined full semantics, we've had to bring set theory into the mix, to define all these subset. This caused Quine to <a href=\"http://books.google.ch/books/about/Philosophy_of_logic.html?id=Jg1LGX1zW1UC&amp;redir_esc=y\">accuse</a> second order logic of not being a logic at all, but a branch of set theory.</p>\n<p>Also with all these <a href=\"http://en.wikipedia.org/wiki/Russel_paradox\">Russell Paradoxes</a>&nbsp;flying around, we might be a bit wary of jumping immediately into the 'every set' semantics. Maybe instead of defining a model by just giving the 'universe of discourse', we would want to also define the subsets we are interested in, listing explicitly all the sets we are going to allow the quantifiers to range over.</p>\n<p>But this could get ridiculous - do we really want a model which includes the natural numbers, but only allows us to quantify over the sets {1, 7, 13908}, {0} and the empty set? We can define, for instance, what an even number is (a number that is equal to two times something), and so why can't we get the set of even numbers into our model?</p>\n<h2>Henkin semantics</h2>\n<p>We really want our 'universe of quantifiable sets' to include any set we can define. It turns out this something we can get from inside second-order logic, by using the <a href=\"http://en.wikipedia.org/wiki/Comprehension_axiom\">comprehension axioms</a>. They roughly say that \"any set/predicate we can define, is in the universe of sets/predicates we can quantify over\". There are infinitely many such comprehension axioms, covering each definition.</p>\n<p>Then <a href=\"http://en.wikipedia.org/wiki/Second-order_logic#Semantics\">Henkin semantics</a> is second-order logic, with all the comprehension axioms, and no further restrictions on the possible models. These '<a href=\"http://plato.stanford.edu/entries/logic-higher-order/\">Henkin models</a>' will have&nbsp;both a defined universe of discourse (the list of basic objects in the model we can quantify over) and a defined 'universe of sets/predicates' (a list of sets of basic objects that we can quantify over), with the comprehension axioms making sure they are compatible. Though they are called 'Henkin semantics', they could really be called 'Henkin syntaxes', since we aren't giving any extra restrictions on the models apart from the internal axioms.</p>\n<p>It should be noted that a full model (where the 'universe of sets' include <a href=\"http://en.wikipedia.org/wiki/Powerset\">all possible subsets</a>) automatically obeys the comprehension axioms, since it can quantify over every set. So every full model in a Henkin model, and it might seem that Henkin semantics are a simple extension of full semantics, and that they have a greater 'expressive power'. Few things could be further from the truth.</p>\n<h2>First or second order?</h2>\n<p>If the old man of previously had claimed \"every number is even\", and, when challenged with \"3\" had responded \"3 is not a number\", you might be justified in questioning his grasp of the meaning of 'every' and 'number'. Similarly, if he had said \"every (non-empty) set of <em>integers</em> has a least element,\" and when challenged with \"the negative integers\" had responded \"that collection is not a set\", you would also question his use of 'every'.</p>\n<p>Similarly, since Henkin semantics allows us to restrict the meaning of \"every set\", depending on the model under consideration, statements such as \"every set is blah\" are much weaker in Henkin semantics than in full semantics. For instance, take the axioms for an ordered field, and add:</p>\n<ul>\n<li>\"Every (non-empty) bounded set has a supremum\"</li>\n</ul>\n<p>As every schoolgirl should know, this is enough to model the real numbers... in full semantics. But in Henkin semantics, 'every bounded set' can mean 'every definable bounded set' and we can take the '<a href=\"http://en.wikipedia.org/wiki/Definable_real_number\">definable reals</a>' as a model: the supremum of a definable set is definable. And this does not include all the real numbers; for a start, the definable reals are countable, so there are far fewer of them than there are reals.</p>\n<p>This may seem a feature, rather than a bug. What are these strange, 'non-definable reals' that clutter up the place; why would we want them anyway? But the definable reals are just one Henkin model of these axioms, there are others perfectly valid models, including the reals themselves. So these axioms have not allowed us, in Henkin semantics, to pick out one particular model.</p>\n<p>This seems familiar: the first order Peano axioms also failed to specify the natural numbers. The familiarity is not an illusion: Henkin semantics is actually a first order theory (a 'many sorted' one, where some classes of objects have different properties). Hence the completeness theorem still applies: any result true in every Henkin model, can be proved from the basic axioms. But this is not much help if we have many models, and unfortunately the&nbsp;L&ouml;wenheim&ndash;Skolem&nbsp;theorem also still applies: if we have one infinite model, we have many, many others. So not only do we have the countable 'definable reals' and the reals themselves as models but larger '<a href=\"http://en.wikipedia.org/wiki/Hyperreals\">hyperreals</a>' and '<a href=\"http://en.wikipedia.org/wiki/Superreal_number\">superreals</a>' with many more elements to them.</p>\n<h2>Skolem's paradox</h2>\n<p>In fact, Henkin semantics can behave much worse than standard first order logic, as it can express more. Express more - but ultimately, not mean more. For instance, in second order language, we can express the sentence \"there exists an uncountable set\". We could start by defining an infinite set as one with a one-to-one correspondence with a strict subset of itself, <em>&aacute;&nbsp;la</em><a href=\"http://en.wikipedia.org/wiki/Georg_Cantor\"> Cantor</a>. We could define an uncountable infinite set as one that has a subset that is also infinite, but that doesn't have a one-to-one correspondence with it (the subset is of lower cardinality). There are other, probably better, ways of phrasing the same concept, but that will do for here.</p>\n<p>Then basic second order logic with Henkin semantics and the additional axiom \"there exists an uncountable set\" certainly has a model: the reals, for instance. Then by the&nbsp;L&ouml;wenheim&ndash;Skolem theorem, it must have a countable model.</p>\n<p>Wait a moment there. A logic that asserts the existence of an uncountable set... has a countable model? This was <a href=\"http://en.wikipedia.org/wiki/Skolem's_paradox\">Skolem's paradox</a>, and one of his arguments against first order logic. The explanation for the paradox involves those one-to-one correspondences mentioned above. An uncountable set is an infinite set without any one-to-one functions to any countable set. But in a Henkin model 'any one-to-one function' means 'any one to one function on the list of allowable functions in this model'. So the 'uncountable set' in the countable model is, in fact, countable: it has one-to-one functions to other countable set. But all these functions are banned from the Henkin model, so the model cannot see, internally, that that set is actually countable.</p>\n<p>So we can <em>express</em> a lot of statements in Henkin semantics - \"every bounded set has a supremum\", \"there exists an uncountable set\" - but these don't actually <em>mean</em> what we thought they did.</p>\n<h2>Full second order semantics</h2>\n<p>Having accepted the accusations of sneaking in set theory, and the disturbing fact that we had to bring in meaning and semantics (by excluding a lot of potential models), rather than relying on the syntax... what can we do with full second order semantics?</p>\n<p>Well, for start, finally nail down the natural numbers and the reals. With second order Peano arithmetic, including the second order induction axiom \"every (non-empty) set of numbers has a least element\", we know that we have only one (full) model: the natural numbers. Similarly, if we have the axioms for an ordered field and toss in \"every bounded set has a supremum\", then the reals are the only full model that stands up.</p>\n<p>This immediately implies that full second order semantics are not complete, unlike Hekin semantics and first order theories. We can see this from the incompleteness result (though don't confuse incompleteness with non-completeness). Take second order Peano arithmetic. This has a&nbsp;G&ouml;del statement G which is true but unprovable. But there is only one model of second order Peano arithmetic! So G is both unprovable and true in every model for the theory.</p>\n<p>It may seem surprising that completeness fails for full semantics: after all, it is true in Henkin semantics, and every full model is also a Henkin model, so how can this happen? It derives from the restriction of possible models: completeness means that every sentence that is true in every Henkin model, must be provable. That does <a href=\"http://www.math.helsinki.fi/logic/people/jouko.vaananen/VaaSec.pdf\">not mean</a> that every sentence that is true in every full model, must also be provable. The G sentence is indeed false in some models - but only in Henkin models that are not full models.</p>\n<p>The lack of completeness means that the truths of second order logic cannot be enumerated - it has no complete <a href=\"http://en.wikipedia.org/wiki/Proof_procedure\">proof procedure</a>. This causes some to reject full second order logic on these grounds. <a href=\"http://www.era.lib.ed.ac.uk/bitstream/1842/1345/1/KetlandSecondOrderLogic.pdf\">Others</a> argued that completeness is not the important factor, but rather decidability: listing all the provable statements might be light entertainment, but what we really want is an algorithm to be able to prove (or disprove) any given sentence. But the <a href=\"http://en.wikipedia.org/wiki/Church%27s_theorem\">Church-Turing theorem</a> demonstrates that this cannot be done, in either first or second order logic: hence neither system can claim to be superior in this respect.</p>\n<h2>Higher-order logic within full second order logic</h2>\n<p>Higher order logic is the next step up - quantifying over predicates of predicates, functions of functions. This would seem to make everything more complicated. However there is a <a href=\"http://plato.stanford.edu/entries/logic-higher-order/#4\">result</a> due to Hintikka that any sentence in full higher order logic can be shown to be equivalent (in an effective manner) with a sentence in full second order logic, using many-sorting.&nbsp;So there is, in a certain sense, no need to go beyond, and the important debate is between first order and full second order logic.</p>\n<h2>Conclusion</h2>\n<p>So, which logic is superior? It depends to some extent on what we need it for. Anything provable in first order logic can be proved in second order logic, so if we have a choice of proofs, picking the first order one is the better option.&nbsp;First order logic has more pleasing internal properties, such as the completeness theorem, and one can preserve this in second order via Henkin semantics without losing the ability to formally express certain properties. Finally, one needs to make use of set theory and semantics to define full second order logic, while first order logic (and Henkin semantics) get away with pure syntax.</p>\n<p>On the other hand, first order logic is completely incapable of controlling its infinite models, as they multiply, uncountable and generally incomprehensible. If rather that looking at the logic internally, we have a particular model in mind, we have to use second order logic for that. If we'd prefer not to use infinitely many axioms to express a simple idea, second-order logic is for us.&nbsp;And if we really want to properly express ideas like \"every (non-empty) set has a least element\", \"every analytic function is uniquely defined by its power series\" - and not just express them, but have them mean what we want them to mean - then full second order logic is essential.</p>\n<p><em><strong>EDIT</strong>: an <a href=\"/r/discussion/lw/ab2/second_order_logic_in_first_order_settheory_what/\">addendum</a> addresses the problem of using set theory (a first order theory) to define second order logic.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MLqhJ8eDy5smbtGrf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 78, "extendedScore": null, "score": 0.000176, "legacy": true, "legacyId": "11798", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>First order arithmetic is incomplete. Except that it's also complete. Second order arithmetic is more expressive - except when it's not - and is also incomplete and also complete, except when it means something different.&nbsp;Oh, and full second order-logic might not really be a logic at all. But then, first order logic has no idea what the reals and natural numbers are, especially when it tries to talk about them.</p>\n<p>That was about the state of my confusion, and I set out to try and clear it up.&nbsp;Here I'll try and share an understanding of what is really going on with first and second order logic and why they differ so radically. It will be deliberately informal, so I won't be distinguishing between functions, predicates and subsets, and will be using little notation.&nbsp;It'll be exactly what I wish someone had told me before I started looking into the whole field.&nbsp;</p>\n<h2 id=\"Meaningful_Models\">Meaningful Models</h2>\n<p>An old man starts talking to you about addition, subtraction and multiplication, and how they <a href=\"http://en.wikipedia.org/wiki/Ring_(mathematics)#Formal_definition\">interact</a>. You assume he was talking about the integers; <a href=\"http://en.wikipedia.org/wiki/Leopold_Kronecker\">turns out</a> he means the rational numbers. The integers and the rationals are <a href=\"http://en.wikipedia.org/wiki/Model_theory\">both</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Structure_(mathematical_logic)\">models</a> of addition, subtraction and multiplication, in that they obey all the properties that the old man set out. But notice though he had the rationals in mind, he didn't mention them at all, he just listed the properties, and the rational numbers turned out, very non-coincidentally, to obey them.</p>\n<p>These models are generally taken to give meaning to the abstract symbols in the axioms - to give semantics to the syntax. In this view, \"for all x,y xy=yx\" is a series of elegant squiggles, but once we have the model of the integers (or the rationals) in mind, we realise that this means that multiplication is commutative.</p>\n<p><a id=\"more\"></a></p>\n<p>Similarly, models can define the \"truth\" of sentences. Consider the following sentences:</p>\n<p style=\"padding-left: 30px;\">(1) 2 has a multiplicative inverse.<br>(2) There exists a number that squares to -1.<br>(3) 2 is not equal to zero.<br>(4) If a+b=0, then a<sup>2</sup>=b<sup>2</sup>.<br>(5) No number is equal to zero.</p>\n<p>Are these true? You and the old man would disagree on (1), with him saying yes and you saying no - your models have enabled you to attach truth-values to the statement. You would both claim (2) is false, but there are other models - such as the complex numbers - where it is true. You would both claim (3) is true, but there are other models - such as the field with two elements - where it is false. So truth is model dependent.</p>\n<p>The last two are interesting, because it turns out that (4) is true in every model, and (5) is false. Statements like (4) and \"not (5)\" that are true in every model are called valid. Since they are independent of the choice of models, these statements are, in a certain sense, true from pure syntax. Both these statement can also be deduced purely from the axioms. It would be nice if all valid statements could also be deduced. But only first order logic allows this.</p>\n<p>What would also be nice if you could agree on the model you're using. Maybe the old man could add \"2 (and&nbsp;every non-zero number) has a multiplicative inverse\", giving the <a href=\"http://en.wikipedia.org/wiki/Field_axioms#Definition_and_illustration\">field axioms</a>, and ruling your integers right out. But there are still many fields - the rationals, the reals, the complex numbers, and many in between. But maybe with a few more axioms, you could really narrow thing downs, and treat the axioms and the model interchangeably. But only second order logic allows <em>this</em>.</p>\n<h1 id=\"First_order_fun\">First order fun</h1>\n<p>First order theories are those where you can quantify over the basic objects in your theory, and phrase statements like \"all Greeks enjoy dancing\" and \"there exists a blind millionaire\". This distinguishes them from second order theories where you can quantify over higher order objects (predicates and functions), phrasing sentiments like \"all nationalities are equal\" or \"there exists a dominant social class\" - in first order logic with humans as basic objects, nationality and social class are predicates, and you can't quantify over them. You don't appreciate how limiting first order logic can be until you've worked with it a while; nevertheless, it's a good logic to start with and possesses certain key properties not shared by higher order logics. Let's start with the most famous result, the incompleteness theorem.</p>\n<h2 id=\"G_del_s_incompleteness_theorem\">G\u00f6del's incompleteness theorem</h2>\n<p>G\u00f6del's (first) <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems#First_incompleteness_theorem\">incompleteness theorem</a>, is a theorem about an arithmetic but also (implicitly) about a model. The implicit model is the natural numbers: any arithmetic that can model them suffers from the incompleteness theorem. But it is not really about any model, beyond that requirement: it's an intrinsic limitation of the system.</p>\n<p>Let's assume we have the usual (first order) <a href=\"http://en.wikipedia.org/wiki/Peano_axioms#The_axioms\">Peano axioms</a> for ordering, addition and multiplication. We also need an <a href=\"http://en.wikipedia.org/wiki/Axiom_schema\">infinity of axioms</a> to define induction, but that isn't as bad as it seem: given a specific sentence, it's easy to check whether or not it's an axiom, in a fast and efficient way. To nobody's surprise, the natural number are a model of first order Peano arithmetic.</p>\n<p>And inside this model, we can construct the&nbsp;G\u00f6del sentence G, which is equivalent with \"there is no proof of G\". By 'proof', we mean a natural number n that is the&nbsp;<a href=\"http://en.wikipedia.org/wiki/G%C3%B6del_numbering\">G\u00f6del number</a>&nbsp;that encodes a proof of G. Obviously, Peano arithmetic cannot prove G without being inconsistent; but this is precisely what G is saying, so we can actually see that G is true. And hence that \"not G\" cannot be provable if our arithmetic is consistent. This is the incompleteness theorem: neither G nor \"not G\" are provable, so the proof system is \"incomplete\".</p>\n<h2 id=\"G_del_s_completeness_theorem\">G\u00f6del's completeness theorem</h2>\n<p>Enough with <em>in</em>completeness; what about&nbsp;G\u00f6del <a href=\"http://en.wikipedia.org/wiki/Completeness_theorem\">completeness theorem</a>? Unlike the previous theorem, this is a statement about about the axiomatic system and <em>all</em>&nbsp;of its models. It simply says that if a sentence is valid (true in every model) for a first order theory, then it can be proved from the axioms. This provides a bridge between the semantic concept of \"true\" (true in every model) and the syntactic concept of provable (can be proved by these formal manipulations). It also implies that we can enumerate all the sentences that are valid in a first order system, simply by enumerating all the proofs.</p>\n<p>Where does this leave the G\u00f6del sentence G? We've seen we can't prove it from the axioms, hence it cannot be true in all models. Therefore there must exist a model N' of first order Peano arithmetic in which G is false. What does that mean? G claims that \"there does not exist a number n with (certain properties)\", so if G is false, such an n does exist. Now we know (because we've constructed it that way) that if that n were a natural number, then those (certain properties) means that it must encode a proof of G. Since there is no proof of G, n cannot be a natural number, but must be an extra, a non-standard number, from beyond our usual universe. This also means that those&nbsp;(certain properties) do not capture what we thought they did: they only mean \"encodes a proof of G\" for the standard natural numbers.</p>\n<p>This seems somewhat troubling, that Peano arithmetic would admit two distinct models and fail to say what we thought it said; but it gets worse.</p>\n<h2 id=\"The_L_wenheim_Skolem_theorem\">The L\u00f6wenheim\u2013Skolem theorem</h2>\n<p>The <a href=\"http://en.wikipedia.org/wiki/L%C3%B6wenheim%E2%80%93Skolem_theorem\">L\u00f6wenheim\u2013Skolem</a> theorem says that if any countable first order theory (such as Peano arithmetic) has an infinite model, then it has a model for every size (\"<a href=\"http://en.wikipedia.org/wiki/Cardinality\">cardinality</a>\") of infinity. Therefore first order Peano arithmetic has to have many, many models; at a minimum, it has to have a model of same size as the reals.</p>\n<p>This also means that no matter how much first order information we add to Peano arithmetic, we cannot restrict it to only being about the natural numbers; the models and the axioms can never be interchangeable. But it gets still worse.</p>\n<p>Uncountable models of Peano arithmetic are bad enough, but it turns out that Peano arithmetic also has <a href=\"http://en.wikipedia.org/wiki/Non-standard_model_of_arithmetic#Structure_of_countable_non-standard_models\">many other countable&nbsp;models</a> - models of same&nbsp;<a href=\"http://en.wikipedia.org/wiki/Countable\">size</a>&nbsp;as the natural numbers, but weirdly different. Weirdly is an apt summation of these models where <a href=\"http://en.wikipedia.org/wiki/Tennenbaum%27s_theorem\">neither multiplication nor addition can be computed</a>.</p>\n<p>So first order Peano arithmetic is not really about the natural numbers at all - but about the natural numbers and all these strange countable and uncountable models that we can't really describe. Maybe second order theories will do better?</p>\n<h1 id=\"Second_order_scariness\">Second order scariness</h1>\n<p>In second order theories, we can finally do what we've been itching to do: apply the existential and universal quantifiers to predicates, functions, sets of numbers and objects of that ilk. We can triumphantly toss away the infinitely many axioms needed to define induction in first order Peano arithmetic, and replace them with a simple:</p>\n<ul>\n<li>\"Every (non-empty) set of numbers has a least element.\"</li>\n</ul>\n<p>This, as every schoolboy should know, is enough to uniquely define the natural numbers. Or is it?</p>\n<h2 id=\"The_importance_of_semantics\">The importance of semantics</h2>\n<p>That sentence remains a series of squiggles until we've decided what those&nbsp;squiggles actually <em>mean</em>. This takes on an extra importance in second order logic that it didn't have in first order. When we say \"every set of numbers\", what do we mean? In terms of meaning and models, what models are we going to be considering?</p>\n<p>The first idea is the obvious one: \"every set of numbers\" means, duh, \"<em>every</em> set of numbers\". When we specify a model, we'll give the 'universe of discourse', the basic objects (maybe the natural numbers or the reals), and the quantifier 'every' will range over all possible subsets of this universe (every subset of the natural or real numbers). This is called the full or standard semantics for second order arithmetic, and the models are called full models.</p>\n<p>But note what we have done here: we have brought in extra information to clarify what we are talking about.&nbsp;In order to defined full semantics, we've had to bring set theory into the mix, to define all these subset. This caused Quine to <a href=\"http://books.google.ch/books/about/Philosophy_of_logic.html?id=Jg1LGX1zW1UC&amp;redir_esc=y\">accuse</a> second order logic of not being a logic at all, but a branch of set theory.</p>\n<p>Also with all these <a href=\"http://en.wikipedia.org/wiki/Russel_paradox\">Russell Paradoxes</a>&nbsp;flying around, we might be a bit wary of jumping immediately into the 'every set' semantics. Maybe instead of defining a model by just giving the 'universe of discourse', we would want to also define the subsets we are interested in, listing explicitly all the sets we are going to allow the quantifiers to range over.</p>\n<p>But this could get ridiculous - do we really want a model which includes the natural numbers, but only allows us to quantify over the sets {1, 7, 13908}, {0} and the empty set? We can define, for instance, what an even number is (a number that is equal to two times something), and so why can't we get the set of even numbers into our model?</p>\n<h2 id=\"Henkin_semantics\">Henkin semantics</h2>\n<p>We really want our 'universe of quantifiable sets' to include any set we can define. It turns out this something we can get from inside second-order logic, by using the <a href=\"http://en.wikipedia.org/wiki/Comprehension_axiom\">comprehension axioms</a>. They roughly say that \"any set/predicate we can define, is in the universe of sets/predicates we can quantify over\". There are infinitely many such comprehension axioms, covering each definition.</p>\n<p>Then <a href=\"http://en.wikipedia.org/wiki/Second-order_logic#Semantics\">Henkin semantics</a> is second-order logic, with all the comprehension axioms, and no further restrictions on the possible models. These '<a href=\"http://plato.stanford.edu/entries/logic-higher-order/\">Henkin models</a>' will have&nbsp;both a defined universe of discourse (the list of basic objects in the model we can quantify over) and a defined 'universe of sets/predicates' (a list of sets of basic objects that we can quantify over), with the comprehension axioms making sure they are compatible. Though they are called 'Henkin semantics', they could really be called 'Henkin syntaxes', since we aren't giving any extra restrictions on the models apart from the internal axioms.</p>\n<p>It should be noted that a full model (where the 'universe of sets' include <a href=\"http://en.wikipedia.org/wiki/Powerset\">all possible subsets</a>) automatically obeys the comprehension axioms, since it can quantify over every set. So every full model in a Henkin model, and it might seem that Henkin semantics are a simple extension of full semantics, and that they have a greater 'expressive power'. Few things could be further from the truth.</p>\n<h2 id=\"First_or_second_order_\">First or second order?</h2>\n<p>If the old man of previously had claimed \"every number is even\", and, when challenged with \"3\" had responded \"3 is not a number\", you might be justified in questioning his grasp of the meaning of 'every' and 'number'. Similarly, if he had said \"every (non-empty) set of <em>integers</em> has a least element,\" and when challenged with \"the negative integers\" had responded \"that collection is not a set\", you would also question his use of 'every'.</p>\n<p>Similarly, since Henkin semantics allows us to restrict the meaning of \"every set\", depending on the model under consideration, statements such as \"every set is blah\" are much weaker in Henkin semantics than in full semantics. For instance, take the axioms for an ordered field, and add:</p>\n<ul>\n<li>\"Every (non-empty) bounded set has a supremum\"</li>\n</ul>\n<p>As every schoolgirl should know, this is enough to model the real numbers... in full semantics. But in Henkin semantics, 'every bounded set' can mean 'every definable bounded set' and we can take the '<a href=\"http://en.wikipedia.org/wiki/Definable_real_number\">definable reals</a>' as a model: the supremum of a definable set is definable. And this does not include all the real numbers; for a start, the definable reals are countable, so there are far fewer of them than there are reals.</p>\n<p>This may seem a feature, rather than a bug. What are these strange, 'non-definable reals' that clutter up the place; why would we want them anyway? But the definable reals are just one Henkin model of these axioms, there are others perfectly valid models, including the reals themselves. So these axioms have not allowed us, in Henkin semantics, to pick out one particular model.</p>\n<p>This seems familiar: the first order Peano axioms also failed to specify the natural numbers. The familiarity is not an illusion: Henkin semantics is actually a first order theory (a 'many sorted' one, where some classes of objects have different properties). Hence the completeness theorem still applies: any result true in every Henkin model, can be proved from the basic axioms. But this is not much help if we have many models, and unfortunately the&nbsp;L\u00f6wenheim\u2013Skolem&nbsp;theorem also still applies: if we have one infinite model, we have many, many others. So not only do we have the countable 'definable reals' and the reals themselves as models but larger '<a href=\"http://en.wikipedia.org/wiki/Hyperreals\">hyperreals</a>' and '<a href=\"http://en.wikipedia.org/wiki/Superreal_number\">superreals</a>' with many more elements to them.</p>\n<h2 id=\"Skolem_s_paradox\">Skolem's paradox</h2>\n<p>In fact, Henkin semantics can behave much worse than standard first order logic, as it can express more. Express more - but ultimately, not mean more. For instance, in second order language, we can express the sentence \"there exists an uncountable set\". We could start by defining an infinite set as one with a one-to-one correspondence with a strict subset of itself, <em>\u00e1&nbsp;la</em><a href=\"http://en.wikipedia.org/wiki/Georg_Cantor\"> Cantor</a>. We could define an uncountable infinite set as one that has a subset that is also infinite, but that doesn't have a one-to-one correspondence with it (the subset is of lower cardinality). There are other, probably better, ways of phrasing the same concept, but that will do for here.</p>\n<p>Then basic second order logic with Henkin semantics and the additional axiom \"there exists an uncountable set\" certainly has a model: the reals, for instance. Then by the&nbsp;L\u00f6wenheim\u2013Skolem theorem, it must have a countable model.</p>\n<p>Wait a moment there. A logic that asserts the existence of an uncountable set... has a countable model? This was <a href=\"http://en.wikipedia.org/wiki/Skolem's_paradox\">Skolem's paradox</a>, and one of his arguments against first order logic. The explanation for the paradox involves those one-to-one correspondences mentioned above. An uncountable set is an infinite set without any one-to-one functions to any countable set. But in a Henkin model 'any one-to-one function' means 'any one to one function on the list of allowable functions in this model'. So the 'uncountable set' in the countable model is, in fact, countable: it has one-to-one functions to other countable set. But all these functions are banned from the Henkin model, so the model cannot see, internally, that that set is actually countable.</p>\n<p>So we can <em>express</em> a lot of statements in Henkin semantics - \"every bounded set has a supremum\", \"there exists an uncountable set\" - but these don't actually <em>mean</em> what we thought they did.</p>\n<h2 id=\"Full_second_order_semantics\">Full second order semantics</h2>\n<p>Having accepted the accusations of sneaking in set theory, and the disturbing fact that we had to bring in meaning and semantics (by excluding a lot of potential models), rather than relying on the syntax... what can we do with full second order semantics?</p>\n<p>Well, for start, finally nail down the natural numbers and the reals. With second order Peano arithmetic, including the second order induction axiom \"every (non-empty) set of numbers has a least element\", we know that we have only one (full) model: the natural numbers. Similarly, if we have the axioms for an ordered field and toss in \"every bounded set has a supremum\", then the reals are the only full model that stands up.</p>\n<p>This immediately implies that full second order semantics are not complete, unlike Hekin semantics and first order theories. We can see this from the incompleteness result (though don't confuse incompleteness with non-completeness). Take second order Peano arithmetic. This has a&nbsp;G\u00f6del statement G which is true but unprovable. But there is only one model of second order Peano arithmetic! So G is both unprovable and true in every model for the theory.</p>\n<p>It may seem surprising that completeness fails for full semantics: after all, it is true in Henkin semantics, and every full model is also a Henkin model, so how can this happen? It derives from the restriction of possible models: completeness means that every sentence that is true in every Henkin model, must be provable. That does <a href=\"http://www.math.helsinki.fi/logic/people/jouko.vaananen/VaaSec.pdf\">not mean</a> that every sentence that is true in every full model, must also be provable. The G sentence is indeed false in some models - but only in Henkin models that are not full models.</p>\n<p>The lack of completeness means that the truths of second order logic cannot be enumerated - it has no complete <a href=\"http://en.wikipedia.org/wiki/Proof_procedure\">proof procedure</a>. This causes some to reject full second order logic on these grounds. <a href=\"http://www.era.lib.ed.ac.uk/bitstream/1842/1345/1/KetlandSecondOrderLogic.pdf\">Others</a> argued that completeness is not the important factor, but rather decidability: listing all the provable statements might be light entertainment, but what we really want is an algorithm to be able to prove (or disprove) any given sentence. But the <a href=\"http://en.wikipedia.org/wiki/Church%27s_theorem\">Church-Turing theorem</a> demonstrates that this cannot be done, in either first or second order logic: hence neither system can claim to be superior in this respect.</p>\n<h2 id=\"Higher_order_logic_within_full_second_order_logic\">Higher-order logic within full second order logic</h2>\n<p>Higher order logic is the next step up - quantifying over predicates of predicates, functions of functions. This would seem to make everything more complicated. However there is a <a href=\"http://plato.stanford.edu/entries/logic-higher-order/#4\">result</a> due to Hintikka that any sentence in full higher order logic can be shown to be equivalent (in an effective manner) with a sentence in full second order logic, using many-sorting.&nbsp;So there is, in a certain sense, no need to go beyond, and the important debate is between first order and full second order logic.</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>So, which logic is superior? It depends to some extent on what we need it for. Anything provable in first order logic can be proved in second order logic, so if we have a choice of proofs, picking the first order one is the better option.&nbsp;First order logic has more pleasing internal properties, such as the completeness theorem, and one can preserve this in second order via Henkin semantics without losing the ability to formally express certain properties. Finally, one needs to make use of set theory and semantics to define full second order logic, while first order logic (and Henkin semantics) get away with pure syntax.</p>\n<p>On the other hand, first order logic is completely incapable of controlling its infinite models, as they multiply, uncountable and generally incomprehensible. If rather that looking at the logic internally, we have a particular model in mind, we have to use second order logic for that. If we'd prefer not to use infinitely many axioms to express a simple idea, second-order logic is for us.&nbsp;And if we really want to properly express ideas like \"every (non-empty) set has a least element\", \"every analytic function is uniquely defined by its power series\" - and not just express them, but have them mean what we want them to mean - then full second order logic is essential.</p>\n<p><em><strong>EDIT</strong>: an <a href=\"/r/discussion/lw/ab2/second_order_logic_in_first_order_settheory_what/\">addendum</a> addresses the problem of using set theory (a first order theory) to define second order logic.</em></p>", "sections": [{"title": "Meaningful Models", "anchor": "Meaningful_Models", "level": 2}, {"title": "First order fun", "anchor": "First_order_fun", "level": 1}, {"title": "G\u00f6del's incompleteness theorem", "anchor": "G_del_s_incompleteness_theorem", "level": 2}, {"title": "G\u00f6del's completeness theorem", "anchor": "G_del_s_completeness_theorem", "level": 2}, {"title": "The L\u00f6wenheim\u2013Skolem theorem", "anchor": "The_L_wenheim_Skolem_theorem", "level": 2}, {"title": "Second order scariness", "anchor": "Second_order_scariness", "level": 1}, {"title": "The importance of semantics", "anchor": "The_importance_of_semantics", "level": 2}, {"title": "Henkin semantics", "anchor": "Henkin_semantics", "level": 2}, {"title": "First or second order?", "anchor": "First_or_second_order_", "level": 2}, {"title": "Skolem's paradox", "anchor": "Skolem_s_paradox", "level": 2}, {"title": "Full second order semantics", "anchor": "Full_second_order_semantics", "level": 2}, {"title": "Higher-order logic within full second order logic", "anchor": "Higher_order_logic_within_full_second_order_logic", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "39 comments"}], "headingsCount": 15}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DavM6jKpPMk9Hr3PK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T17:59:17.741Z", "modifiedAt": null, "url": null, "title": "My interview with Nikola Danaylov for 'Singularity 1 on 1' [link]", "slug": "my-interview-with-nikola-danaylov-for-singularity-1-on-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.763Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pFwKdjvTDwzSHW4ys/my-interview-with-nikola-danaylov-for-singularity-1-on-1", "pageUrlRelative": "/posts/pFwKdjvTDwzSHW4ys/my-interview-with-nikola-danaylov-for-singularity-1-on-1", "linkUrl": "https://www.lesswrong.com/posts/pFwKdjvTDwzSHW4ys/my-interview-with-nikola-danaylov-for-singularity-1-on-1", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20interview%20with%20Nikola%20Danaylov%20for%20'Singularity%201%20on%201'%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20interview%20with%20Nikola%20Danaylov%20for%20'Singularity%201%20on%201'%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFwKdjvTDwzSHW4ys%2Fmy-interview-with-nikola-danaylov-for-singularity-1-on-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20interview%20with%20Nikola%20Danaylov%20for%20'Singularity%201%20on%201'%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFwKdjvTDwzSHW4ys%2Fmy-interview-with-nikola-danaylov-for-singularity-1-on-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFwKdjvTDwzSHW4ys%2Fmy-interview-with-nikola-danaylov-for-singularity-1-on-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p><a href=\"http://www.singularityweblog.com/luke-muehlhauser-on-singularity-1-on-1-superhuman-ai-is-coming-this-century/\">Here</a>. Audio and video available.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pFwKdjvTDwzSHW4ys", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 1, "extendedScore": null, "score": 8.327472379144481e-07, "legacy": true, "legacyId": "12210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T18:00:50.618Z", "modifiedAt": null, "url": null, "title": "link to a Feynman video (and a 2nd (distantly related) video)", "slug": "link-to-a-feynman-video-and-a-2nd-distantly-related-video", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.454Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Craig_Heldreth", "createdAt": "2010-06-14T23:30:28.110Z", "isAdmin": false, "displayName": "Craig_Heldreth"}, "userId": "hhKowsjZBQSyBE6c5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LHjfX8dhEKuEPyKJ2/link-to-a-feynman-video-and-a-2nd-distantly-related-video", "pageUrlRelative": "/posts/LHjfX8dhEKuEPyKJ2/link-to-a-feynman-video-and-a-2nd-distantly-related-video", "linkUrl": "https://www.lesswrong.com/posts/LHjfX8dhEKuEPyKJ2/link-to-a-feynman-video-and-a-2nd-distantly-related-video", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20link%20to%20a%20Feynman%20video%20(and%20a%202nd%20(distantly%20related)%20video)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Alink%20to%20a%20Feynman%20video%20(and%20a%202nd%20(distantly%20related)%20video)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLHjfX8dhEKuEPyKJ2%2Flink-to-a-feynman-video-and-a-2nd-distantly-related-video%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=link%20to%20a%20Feynman%20video%20(and%20a%202nd%20(distantly%20related)%20video)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLHjfX8dhEKuEPyKJ2%2Flink-to-a-feynman-video-and-a-2nd-distantly-related-video", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLHjfX8dhEKuEPyKJ2%2Flink-to-a-feynman-video-and-a-2nd-distantly-related-video", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p><a href=\"http://www.youtube.com/watch?v=kd0xTfdt6qw&amp;feature=related\">Video presentation from 1963 at Cornell. Character of Physical Law #2 The Relation of Mathematics to Physics.</a></p>\n<p>This may be the best one of all the Feynman videos. He explains how physicists do mathematics and distinguishes how physicists do math from how mathematicians do math. He describes the two flavors as Greek and Babylonian.</p>\n<p>The money quote if you don't have time to watch it: \"Physicists do Babylonian mathematics and pay little attention to precise reasoning from fixed axioms.\"</p>\n<p>Comment number 1; the quality of the video ain't all that great--in particular there is an annoying very small synch error which is just large enough to be definitely noticeable. Nevertheless it is absorbing because of the power of his presentation. The occasional audience shots show an audience <em>transfixed.</em> They also show the majority of the people wearing eyeglasses.</p>\n<p>Comment number 2; in his earlier days his working class New York accent seemed far more obvious. I am reminded of Art Carney from <a href=\"http://www.youtube.com/watch?v=QNauilZRzHk\">The Honeymooners.</a> (That is a four minute clip.) Feynman was born in Queens in 1918. Carney was born in Mount Vernon in 1918.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LHjfX8dhEKuEPyKJ2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 8.327478261078756e-07, "legacy": true, "legacyId": "12212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T18:18:33.149Z", "modifiedAt": null, "url": null, "title": "[LINK] Being No One (~50 min talk on the self-model in your brain)", "slug": "link-being-no-one-50-min-talk-on-the-self-model-in-your", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "machrider", "createdAt": "2010-10-12T23:37:29.342Z", "isAdmin": false, "displayName": "machrider"}, "userId": "jFSugHAcHBNKua4k4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z5RN5Yhj69ky8oa3D/link-being-no-one-50-min-talk-on-the-self-model-in-your", "pageUrlRelative": "/posts/Z5RN5Yhj69ky8oa3D/link-being-no-one-50-min-talk-on-the-self-model-in-your", "linkUrl": "https://www.lesswrong.com/posts/Z5RN5Yhj69ky8oa3D/link-being-no-one-50-min-talk-on-the-self-model-in-your", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Being%20No%20One%20(~50%20min%20talk%20on%20the%20self-model%20in%20your%20brain)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Being%20No%20One%20(~50%20min%20talk%20on%20the%20self-model%20in%20your%20brain)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ5RN5Yhj69ky8oa3D%2Flink-being-no-one-50-min-talk-on-the-self-model-in-your%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Being%20No%20One%20(~50%20min%20talk%20on%20the%20self-model%20in%20your%20brain)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ5RN5Yhj69ky8oa3D%2Flink-being-no-one-50-min-talk-on-the-self-model-in-your", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ5RN5Yhj69ky8oa3D%2Flink-being-no-one-50-min-talk-on-the-self-model-in-your", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p><strong>Summary: </strong>This is a ~50 minute talk (plus some introductory ado) by Thomas Metzinger on the problem of the experiencing, subjective self (why it exists, what it even means, how it arises). Not to be too clich&eacute;, but he attacks the problem by <a href=\"/lw/of/dissolving_the_question/\">dissolving the question</a>, and the solution he arrives at sounds a lot like <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">how an algorithm feels from inside.</a></p>\n<p>Using several examples from neuroscience (particularly the many illuminating failure modes of the brain), he explains how the brain models the self and its place in the center of experiential space. He discusses the limitations of our access to our own cognitive systems, and how those limitations force us to be naive realists.</p>\n<p>I hesitate to summarize further, because there is a lot of value in hearing the entire argument. (I will say that he gets a little cute at the end, but that doesn't detract from the excellent content.)</p>\n<p><strong>Link:</strong> <a href=\"http://www.youtube.com/watch?v=mthDxnFXs9k\">Being No One</a> on Youtube.</p>\n<p><em>(Normally I think LWers dislike the talk format because it's inherently time-consuming, but I'd say this one is information dense and well worth your time.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z5RN5Yhj69ky8oa3D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 8.327545554124435e-07, "legacy": true, "legacyId": "12208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX", "yA4gF5KrboK2m2Xu7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-16T23:38:18.978Z", "modifiedAt": null, "url": null, "title": "What would you do with a financial safety net?", "slug": "what-would-you-do-with-a-financial-safety-net", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:31.755Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "faul_sname", "createdAt": "2011-07-10T06:35:12.911Z", "isAdmin": false, "displayName": "faul_sname"}, "userId": "uAdCfrYxKKCJT5nK7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8d7KtMCNMCHr7cgzE/what-would-you-do-with-a-financial-safety-net", "pageUrlRelative": "/posts/8d7KtMCNMCHr7cgzE/what-would-you-do-with-a-financial-safety-net", "linkUrl": "https://www.lesswrong.com/posts/8d7KtMCNMCHr7cgzE/what-would-you-do-with-a-financial-safety-net", "postedAtFormatted": "Monday, January 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20would%20you%20do%20with%20a%20financial%20safety%20net%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20would%20you%20do%20with%20a%20financial%20safety%20net%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8d7KtMCNMCHr7cgzE%2Fwhat-would-you-do-with-a-financial-safety-net%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20would%20you%20do%20with%20a%20financial%20safety%20net%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8d7KtMCNMCHr7cgzE%2Fwhat-would-you-do-with-a-financial-safety-net", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8d7KtMCNMCHr7cgzE%2Fwhat-would-you-do-with-a-financial-safety-net", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p>In the open thread, moridinamael <a href=\"/r/discussion/lw/9em/open_thread_january_1531_2012/\">hypothesized</a>&nbsp;that LWers would be willing to take more risks in order to become rich if they had a financial safety net. This seems like an idea worth exploring further.</p>\n<p>&nbsp;</p>\n<p>What would you do if you had a financial safety net (maybe a year's worth of living expenses) to fall back on if your venture failed?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8d7KtMCNMCHr7cgzE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 8.328760813848497e-07, "legacy": true, "legacyId": "12214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cit3HYXehBsr4d36Q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T01:06:28.375Z", "modifiedAt": null, "url": null, "title": "Random thought: What is the optimal PD strategy under imperfect information? ", "slug": "random-thought-what-is-the-optimal-pd-strategy-under", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.842Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z4fmrs598fSeYP5i8/random-thought-what-is-the-optimal-pd-strategy-under", "pageUrlRelative": "/posts/Z4fmrs598fSeYP5i8/random-thought-what-is-the-optimal-pd-strategy-under", "linkUrl": "https://www.lesswrong.com/posts/Z4fmrs598fSeYP5i8/random-thought-what-is-the-optimal-pd-strategy-under", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Random%20thought%3A%20What%20is%20the%20optimal%20PD%20strategy%20under%20imperfect%20information%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARandom%20thought%3A%20What%20is%20the%20optimal%20PD%20strategy%20under%20imperfect%20information%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ4fmrs598fSeYP5i8%2Frandom-thought-what-is-the-optimal-pd-strategy-under%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Random%20thought%3A%20What%20is%20the%20optimal%20PD%20strategy%20under%20imperfect%20information%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ4fmrs598fSeYP5i8%2Frandom-thought-what-is-the-optimal-pd-strategy-under", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ4fmrs598fSeYP5i8%2Frandom-thought-what-is-the-optimal-pd-strategy-under", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>We know that Tit-for-Tat and variants do very well in iterated-Prisoner's-Dilemma tournaments. However, such tournaments are a bit unrealistic in that they give the agents instant and complete information about each other's actions. What if this signal is obscured? Suppose, for example, that if I press \"Cooperate\", there is a small chance that my action is reported to you as \"Defect\", presumably causing you to retaliate; and conversely, if I press \"Defect\" there is a chance that you see \"Cooperate\", thus letting me get away with cheating. Does this affect the optimal strategy? Does the probability of getting wrong information matter? What if it is asymmetric, ie P(observe C | actual D) != P(Observe D | actual C)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z4fmrs598fSeYP5i8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.329095907150717e-07, "legacy": true, "legacyId": "12219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T02:34:47.334Z", "modifiedAt": null, "url": null, "title": "ICONN 2012 nanotechnology conference in Perth", "slug": "iconn-2012-nanotechnology-conference-in-perth", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:37.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6H4aFR7eHKZvuj7LP/iconn-2012-nanotechnology-conference-in-perth", "pageUrlRelative": "/posts/6H4aFR7eHKZvuj7LP/iconn-2012-nanotechnology-conference-in-perth", "linkUrl": "https://www.lesswrong.com/posts/6H4aFR7eHKZvuj7LP/iconn-2012-nanotechnology-conference-in-perth", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20ICONN%202012%20nanotechnology%20conference%20in%20Perth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AICONN%202012%20nanotechnology%20conference%20in%20Perth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6H4aFR7eHKZvuj7LP%2Ficonn-2012-nanotechnology-conference-in-perth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=ICONN%202012%20nanotechnology%20conference%20in%20Perth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6H4aFR7eHKZvuj7LP%2Ficonn-2012-nanotechnology-conference-in-perth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6H4aFR7eHKZvuj7LP%2Ficonn-2012-nanotechnology-conference-in-perth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p>I won a prize and get to travel to Perth, Australia to attend the 2012 ICONN nanotechnology conference.</p>\n<p>Is anyone else from LessWrong going to be there?</p>\n<p>I don't actually know much about nanotechnology. Does anyone have any particular&nbsp;recommendations&nbsp;for some a good introduction?</p>\n<p>Thanks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6H4aFR7eHKZvuj7LP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 8.329431629508869e-07, "legacy": true, "legacyId": "12230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T05:19:13.777Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Neural Categories", "slug": "seq-rerun-neural-categories", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XDQzobvDbHZZCigXy/seq-rerun-neural-categories", "pageUrlRelative": "/posts/XDQzobvDbHZZCigXy/seq-rerun-neural-categories", "linkUrl": "https://www.lesswrong.com/posts/XDQzobvDbHZZCigXy/seq-rerun-neural-categories", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Neural%20Categories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Neural%20Categories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDQzobvDbHZZCigXy%2Fseq-rerun-neural-categories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Neural%20Categories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDQzobvDbHZZCigXy%2Fseq-rerun-neural-categories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDQzobvDbHZZCigXy%2Fseq-rerun-neural-categories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<p>Today's post, <a href=\"/lw/nn/neural_categories/\">Neural Categories</a> was originally published on 10 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You treat intuitively perceived hierarchical categories like the only correct way to parse the world, without realizing that other forms of statistical inference are possible even though your brain doesn't use them. It's much easier for a human to notice whether an object is a \"blegg\" or \"rube\"; than for a human to notice that red objects never glow in the dark, but red furred objects have all the other characteristics of bleggs. Other statistical algorithms work differently.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/9er/seq_rerun_disguised_queries/\">Disguised Queries</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XDQzobvDbHZZCigXy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.330056796050904e-07, "legacy": true, "legacyId": "12233", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yFDKvfN6D87Tf5J9f", "2LuFTGeQzx3sAnBmp", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T09:43:10.805Z", "modifiedAt": null, "url": null, "title": "Q&A with Abram Demski on risks from AI", "slug": "q-and-a-with-abram-demski-on-risks-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:34.974Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kToFGkGj5u5eYLJPF/q-and-a-with-abram-demski-on-risks-from-ai", "pageUrlRelative": "/posts/kToFGkGj5u5eYLJPF/q-and-a-with-abram-demski-on-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/kToFGkGj5u5eYLJPF/q-and-a-with-abram-demski-on-risks-from-ai", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20Abram%20Demski%20on%20risks%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20Abram%20Demski%20on%20risks%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkToFGkGj5u5eYLJPF%2Fq-and-a-with-abram-demski-on-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20Abram%20Demski%20on%20risks%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkToFGkGj5u5eYLJPF%2Fq-and-a-with-abram-demski-on-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkToFGkGj5u5eYLJPF%2Fq-and-a-with-abram-demski-on-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2603, "htmlBody": "<p><strong>[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p><a href=\"http://www.facebook.com/abramdemski\">Abram Demski</a> is a computer science <span class=\"st\">Ph.D&nbsp;</span>student<span class=\"st\"> at </span>the University of Southern California<span class=\"st\"> </span>who has previously studied cognitive science at Central Michigan University. He is an artificial intelligence enthusiast looking for the logic of thought. He is interested in AGI in general and universal theories of intelligence in particular, but also probabilistic reasoning, logic, and the combination of the two (\"relational methods\"). Also, utility-theoretic reasoning.</p>\n<p>I interviewed Abram Demski due to feedback from lesswrong. <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">cousin_it</a>, top contributer and <a href=\"http://intelligence.org/aboutus/researchassociates\">research associate</a> of the Singularity institute, <a href=\"/lw/999/qa_with_experts_on_risks_from_ai_1/5mrx\">wrote the following</a>:</p>\n<blockquote>\n<p>I'm afraid of Abram Demski who wrote brilliant comments on LW and still got paid to help design a self-improving AGI (Genifer).</p>\n</blockquote>\n<p>Enough already, here goes....</p>\n<h3><strong>The Interview</strong>:</h3>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em></p>\n<p><em>Explanatory remark to Q1:<br /><br />P(human-level AI by (year) | no wars &and; no disasters &and; beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong></p>\n<p style=\"padding-left: 30px;\">10%: 5 years (2017).<br />50%: 15 years (2027).<br />90%: 50 years (2062).</p>\n<p style=\"padding-left: 30px;\">Of course, just numbers is not very informative. The year numbers I gave are unstable under reflection, at a factor of about 2 (meaning I have doubled and halved these estimates in the past minutes while considering it). More relevant is the variance; I think the year of development is fundamentally hard to predict, so that it's rational to give a significant probability mass to within 10 years, but also to it taking another 50 years or more. However, the largest bulk of my probability mass would be roughly between 2020 and 2030, since (1) the computing hardware to simulate the human brain would become widely available and (2) I believe less than that will be sufficient, but the software may lag behind the hardware potential by 5 to 10 years. (I would estimate more lag, except that it looks like we are making good progress right now.)</p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction as a result of badly done AI?<br /><br />Explanatory remark to Q2:<br /><br />P(human extinction | badly done AI) = ?<br /><br />(Where 'badly done' = AGI capable of self-modification that is not provably non-dangerous.)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>This is somewhat difficult. We could say that AIs matching that description have already been created (with few negative consequences). I presume that \"roughly human-level\" is also intended, though.</p>\n<p style=\"padding-left: 30px;\">If the human-level AGI</p>\n<p style=\"padding-left: 30px;\">0) is autonomous (has, or forms, long-term goals)<br />1) is not socialized<br />2) figures out how to access spare computing power on the internet<br />3) has a goal which is very bad for humans (ie, implies extinction)<br />4) is alone (has no similarly-capable peers)</p>\n<p style=\"padding-left: 30px;\">then the probability of human extinction is quite high, though not 1. The probability of #0 is somewhat low; #1 is somewhat low; #2 is fairly high; #3 is difficult to estimate; #4 is somewhat low.</p>\n<p style=\"padding-left: 30px;\">#1 is important because a self-modifying system will tend to respond to negative reinforcement concerning sociopathic behaviors resulting from #3-- though, it must be admitted, this will depend on how deeply the ability to self-modify runs. Not all architectures will be capable of effectively modifying their goals in response to social pressures. (In fact, rigid goal-structure under self-modification will usually be seen as an important design-point.)</p>\n<p style=\"padding-left: 30px;\">#3 depends a great deal on just how smart the agent is. Given an agent of merely human capability, human extinction would be very improbable even with an agent that was given the explicit goal of destroying humans. Given an agent of somewhat greater intelligence, the risk would be there, but it's not so clear what range of goals would be bad for humans (many goals could be accomplished through cooperation). For a vastly more intelligent agent, predicting behavior is naturally a bit more difficult, but cooperation with humans would not be as necessary for survival. So, that is why #2 becomes very important: an agent that is human-level when run on the computing power of a single machine (or small network) could be much more intelligent with access to even a small fraction of the world's computing power.</p>\n<p style=\"padding-left: 30px;\">#4 is a common presumption in singularity stories, because there has to be a first super-human AI at some point. However, the nature of software is such that once the fundamental innovation is made, creating and deploying many is easy. Furthermore, a human-like system may have a human-like training time (to become adult-level that is), in which case it may have many peers (which gets back to #1). In case #4 is *not* true, then condition #3 must be rewritten to \"most such systems have goals which are bad for humans\".</p>\n<p style=\"padding-left: 30px;\">It's very difficult to give an actual probability estimate for this question because of the way \"badly done AI\" pushes around the probability. (By definition, there should be some negative consequences, or it wasn't done badly enough...) However, I'll naively multiply the factors I've given, with some very rough numbers:</p>\n<p style=\"padding-left: 30px;\">P(#0)P(#1)P(#2)P(#3)P(#4)<br />= .1 * .1 * .9 * .5 * .1<br />= .00045<br /><br />I described a fairly narrow scenario, so we might expect significant probability mass to come from other possibilities. However, I think it's the most plausible. So, keeping in mind that it's very rough, let's say .001.</p>\n<p style=\"padding-left: 30px;\">I note that this is significantly lower than estimates I've made before, despite trying harder at that time to refute the hypothesis.</p>\n<p><strong>Q3:</strong> <em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years?<br /><br />Explanatory remark to Q3:<br /><br />P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>Very near zero, very near zero, and very near zero. My feeling is that intelligence is a combination of processing power and knowledge. In this case, knowledge will keep pouring in, but processing power will become a limiting factor. Self-modification does not help this. So, such a system might become superhuman within 5 years, but not massively.</p>\n<p style=\"padding-left: 30px;\">If the system does copy itself or otherwise gain more processing power, then I assign much higher probability; 1% within hours, 5% within days, 90% within 5 years.</p>\n<p style=\"padding-left: 30px;\">Note that there is a very important ambiguity in the term \"human-level\", though. It could mean child-level or adult-level. (IE, a human-level system may take 20 years to train to the adult level.) The above assumes you mean \"adult level\". If not, add 20 years.</p>\n<p><strong>Q4:</strong> <em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence? </em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>\"Provably non-dangerous\" may not be the best way of thinking about the problem. Overall, the goal is to reduce risk. Proof may not be possible or may not be the most effective route.</p>\n<p style=\"padding-left: 30px;\">So: is it important to solve the problem of safety before trying to solve the problem of intelligence?</p>\n<p style=\"padding-left: 30px;\">I don't think this is possible. Designs for safe systems have to be designs for systems, so they must be informed by solutions to the intelligence problem.</p>\n<p style=\"padding-left: 30px;\">It would also be undesirable to stall progress while considering the consequences. Serious risks are associated with many areas of research, but it typically seems better to mitigate those risks while moving forward rather than beforehand.</p>\n<p style=\"padding-left: 30px;\">That said, it seems like a good idea to put some thought into safety &amp; friendliness while we are solving the general intelligence problem.</p>\n<p><strong>Q4-sub:</strong> <em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>I am a bias authority for this question, but in general, increased funding to AI would be good news to me. My opinion is that the world has a lot of problems which we are slowly solving, but which could be addressed much more effectively if we had more intelligence with which to attack the problem. AI research is beginning to bear fruits in this way (becoming a profitable industry).</p>\n<p style=\"padding-left: 30px;\">So, if I had my say, I would re-assign perhaps half the budget currently assigned to the military to AI research. (The other half would go to NASA and particle physicists...)</p>\n<p style=\"padding-left: 30px;\">What amount of the AI budget should be concentrated on safety? Moderately more than at present. Almost no one is working on safety right now.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>I would rank nanotech as fairly low on my list of concerns, because cells are fairly close to optimal replicators in the present environment. (IE, I don't buy the grey-goo stories: the worst that I find plausible is a nanobot plague, and normal biological weapons would be easier to make.)</p>\n<p style=\"padding-left: 30px;\">Anyway, AI would be lower on my list than global warming.</p>\n<p><strong>Q5-sub: </strong><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk? </em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>Another world war with the present technology &amp; weapon stockpiles (or greater).</p>\n<p><strong>Q6:</strong> <em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>The general population seems to be highly aware of the risks of AI, with very little awareness of the benefits.</p>\n<p style=\"padding-left: 30px;\">Within the research community, the situation was completely opposite until recently. I would say present awareness levels in the research community is roughly optimal...</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human-level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>No. Predictions about AI research have historically been mostly wrong, so it would be incorrect to make such predictions.</p>\n<p><strong>Q8:</strong> <em>Are you familiar with formal concepts of optimal AI design which relate to searches over complete spaces of computable hypotheses or computational strategies, such as Solomonoff induction, Levin search, Hutter's algorithm M, AIXI, or G&ouml;del machines?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>Yes.</p>\n<h3>Addendum</h3>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>I have been, and am, generally conflicted about these issues. I first encountered the writings of Eliezer three or four years ago. I was already a Bayesian at the time, and I was working with great conviction on the problem of finding the \"one true logic\" which could serve as a foundation of reasoning. Central to this was finding the correct formal theory of truth (a problem which is hard thanks to the Liar paradox). Reading Eliezer's material, it was clear that he would be interested in the same things, but wasn't writing a great deal about them in public.</p>\n<p style=\"padding-left: 30px;\">I sent him an email about it. (I had the good habit of emailing random researchers, a habit I recommend to anyone.) His response was that he needed to speak to me on the phone before collaborating with me, since much more about a person is conveyed by audio. So, we set up a phone call.</p>\n<p style=\"padding-left: 30px;\">I tried to discuss logic on the phone, and was successful for a few minutes, but Eliezer's purpose was to deliver the argument for existential risk from AI as a set-up for the central question which would determine whether he would be willing to work with me: If I found the correct logic, would I publish? I answered yes, which meant that we could not work together. The risk for him was too high.</p>\n<p style=\"padding-left: 30px;\">Was I rational in my response? What reason should I have to publish, that would outweigh the risk of someone taking the research and misusing it? (Eliezer made the comment that with the correct logic in hand, it might take a year to implement a seed AI capable of bootstrapping itself to superhuman intelligence.) My perception of my own thought process is much closer to \"clinging to normality\" than one of \"carefully evaluating the correct position\". Shortly after that, trying to resolve my inner conflict (prove to myself that I was being rational) I wrote this:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://dragonlogic-ai.blogspot.com/2009/04/some-numbers-continues-risk-estimate.html\">http://dragonlogic-ai.blogspot.com/2009/04/some-numbers-continues-risk-estimate.html</a></p>\n<p style=\"padding-left: 30px;\">The numbers there just barely indicate that I should keep working on AI (and I give a cut-off date, beyond which it would be too dangerous). Despite trying hard to prove that AI was not so risky, I was experiencing an anchoring bias. AI being terribly risky was the start position from which estimates should be improved.</p>\n<p style=\"padding-left: 30px;\">Still, although I attempted to be fair in my new probability estimates for this interview, it must be admitted that my argument took the form of listing factors which might plausibly reduce the risk and multiplying them together so that the risk gets smaller and smaller. Does this pattern reflect the real physics of the situation, or does it still reflect anchor-and-reduce type reasoning? Have I encountered more factors to reduce the probability because that's been what I've looked for?</p>\n<p style=\"padding-left: 30px;\">My central claims are:</p>\n<ul>\n<li>In order for destruction of humanity to be a good idea, the AI would have to be so powerful that it could simply brush humanity aside, or have a very malevolent goal, or both.</li>\n<li>In order to have massively superhuman intelligence, massively more processing power is needed (than is needed to achieve human-level intelligence).</li>\n<li>It seems unlikely that a malevolent system would emerge in an environment empty of potential rivals which may be more pro-human, which further increases the processing power requirement (because the malevolent system should be much more powerful than rivals in order for cooperation to be an unimportant option), while making massive acquisition of such processing power (taking over the internet single-handedly) less plausible.</li>\n<li>In any case, these negative singularity scenarios tend to assume an autonomous AI system (one with persistent goals). It is more likely that the industry will focus on creating non-autonomous systems in the near-term, and it seems like these would have to become autonomous to be dangerous in that way. (Autonomous systems will be created more for entertainment &amp; eventually companionship, in which case much more attention will be paid to friendly goal systems.)</li>\n</ul>\n<p style=\"padding-left: 30px;\">Some danger comes from originally non-autonomous systems which become autonomous (ie, whose capabilities are expanded to general planning to maximize some industrially useful utility function such as production quality, cash flow, ad clicks, etc). These are more likely to have strange values not good for us. The hope here lies in the possibility that these would be so numerous that cooperation with society &amp; other AIs would be the only option. A scenario where the first AI capable of true recursive self improvement became an industrial AI and rose to power before its makers could re-sell it to many different jobs seems unlikely (because the loop between research and industry is not usually like that). More likely, by the time the first human-level system is five years old (old enough to start thinking about world domination), different versions of it with different goals and experiences will be in many places having many experiences, cooperating with humankind more than each other.</p>\n<p style=\"padding-left: 30px;\">But, anyway, all of this is an argument that the probability is low, not that it would be impossible or that the consequences aren't hugely undesirable. That's why I said the level of awareness in the AI community is good and that research into safe AI could use a bit more funding.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "ZFrgTgzwEfStg26JL": 1, "DigEmY3RrF3XL5cwe": 1, "9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kToFGkGj5u5eYLJPF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 33, "extendedScore": null, "score": 8.331058716361989e-07, "legacy": true, "legacyId": "12239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Click_here_to_see_a_list_of_all_interviews_\">[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p><a href=\"http://www.facebook.com/abramdemski\">Abram Demski</a> is a computer science <span class=\"st\">Ph.D&nbsp;</span>student<span class=\"st\"> at </span>the University of Southern California<span class=\"st\"> </span>who has previously studied cognitive science at Central Michigan University. He is an artificial intelligence enthusiast looking for the logic of thought. He is interested in AGI in general and universal theories of intelligence in particular, but also probabilistic reasoning, logic, and the combination of the two (\"relational methods\"). Also, utility-theoretic reasoning.</p>\n<p>I interviewed Abram Demski due to feedback from lesswrong. <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">cousin_it</a>, top contributer and <a href=\"http://intelligence.org/aboutus/researchassociates\">research associate</a> of the Singularity institute, <a href=\"/lw/999/qa_with_experts_on_risks_from_ai_1/5mrx\">wrote the following</a>:</p>\n<blockquote>\n<p>I'm afraid of Abram Demski who wrote brilliant comments on LW and still got paid to help design a self-improving AGI (Genifer).</p>\n</blockquote>\n<p>Enough already, here goes....</p>\n<h3 id=\"The_Interview_\"><strong>The Interview</strong>:</h3>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em></p>\n<p><em>Explanatory remark to Q1:<br><br>P(human-level AI by (year) | no wars \u2227 no disasters \u2227 beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong id=\"Abram_Demski__\">Abram Demski: </strong></p>\n<p style=\"padding-left: 30px;\">10%: 5 years (2017).<br>50%: 15 years (2027).<br>90%: 50 years (2062).</p>\n<p style=\"padding-left: 30px;\">Of course, just numbers is not very informative. The year numbers I gave are unstable under reflection, at a factor of about 2 (meaning I have doubled and halved these estimates in the past minutes while considering it). More relevant is the variance; I think the year of development is fundamentally hard to predict, so that it's rational to give a significant probability mass to within 10 years, but also to it taking another 50 years or more. However, the largest bulk of my probability mass would be roughly between 2020 and 2030, since (1) the computing hardware to simulate the human brain would become widely available and (2) I believe less than that will be sufficient, but the software may lag behind the hardware potential by 5 to 10 years. (I would estimate more lag, except that it looks like we are making good progress right now.)</p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction as a result of badly done AI?<br><br>Explanatory remark to Q2:<br><br>P(human extinction | badly done AI) = ?<br><br>(Where 'badly done' = AGI capable of self-modification that is not provably non-dangerous.)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>This is somewhat difficult. We could say that AIs matching that description have already been created (with few negative consequences). I presume that \"roughly human-level\" is also intended, though.</p>\n<p style=\"padding-left: 30px;\">If the human-level AGI</p>\n<p style=\"padding-left: 30px;\">0) is autonomous (has, or forms, long-term goals)<br>1) is not socialized<br>2) figures out how to access spare computing power on the internet<br>3) has a goal which is very bad for humans (ie, implies extinction)<br>4) is alone (has no similarly-capable peers)</p>\n<p style=\"padding-left: 30px;\">then the probability of human extinction is quite high, though not 1. The probability of #0 is somewhat low; #1 is somewhat low; #2 is fairly high; #3 is difficult to estimate; #4 is somewhat low.</p>\n<p style=\"padding-left: 30px;\">#1 is important because a self-modifying system will tend to respond to negative reinforcement concerning sociopathic behaviors resulting from #3-- though, it must be admitted, this will depend on how deeply the ability to self-modify runs. Not all architectures will be capable of effectively modifying their goals in response to social pressures. (In fact, rigid goal-structure under self-modification will usually be seen as an important design-point.)</p>\n<p style=\"padding-left: 30px;\">#3 depends a great deal on just how smart the agent is. Given an agent of merely human capability, human extinction would be very improbable even with an agent that was given the explicit goal of destroying humans. Given an agent of somewhat greater intelligence, the risk would be there, but it's not so clear what range of goals would be bad for humans (many goals could be accomplished through cooperation). For a vastly more intelligent agent, predicting behavior is naturally a bit more difficult, but cooperation with humans would not be as necessary for survival. So, that is why #2 becomes very important: an agent that is human-level when run on the computing power of a single machine (or small network) could be much more intelligent with access to even a small fraction of the world's computing power.</p>\n<p style=\"padding-left: 30px;\">#4 is a common presumption in singularity stories, because there has to be a first super-human AI at some point. However, the nature of software is such that once the fundamental innovation is made, creating and deploying many is easy. Furthermore, a human-like system may have a human-like training time (to become adult-level that is), in which case it may have many peers (which gets back to #1). In case #4 is *not* true, then condition #3 must be rewritten to \"most such systems have goals which are bad for humans\".</p>\n<p style=\"padding-left: 30px;\">It's very difficult to give an actual probability estimate for this question because of the way \"badly done AI\" pushes around the probability. (By definition, there should be some negative consequences, or it wasn't done badly enough...) However, I'll naively multiply the factors I've given, with some very rough numbers:</p>\n<p style=\"padding-left: 30px;\">P(#0)P(#1)P(#2)P(#3)P(#4)<br>= .1 * .1 * .9 * .5 * .1<br>= .00045<br><br>I described a fairly narrow scenario, so we might expect significant probability mass to come from other possibilities. However, I think it's the most plausible. So, keeping in mind that it's very rough, let's say .001.</p>\n<p style=\"padding-left: 30px;\">I note that this is significantly lower than estimates I've made before, despite trying harder at that time to refute the hypothesis.</p>\n<p><strong>Q3:</strong> <em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years?<br><br>Explanatory remark to Q3:<br><br>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>Very near zero, very near zero, and very near zero. My feeling is that intelligence is a combination of processing power and knowledge. In this case, knowledge will keep pouring in, but processing power will become a limiting factor. Self-modification does not help this. So, such a system might become superhuman within 5 years, but not massively.</p>\n<p style=\"padding-left: 30px;\">If the system does copy itself or otherwise gain more processing power, then I assign much higher probability; 1% within hours, 5% within days, 90% within 5 years.</p>\n<p style=\"padding-left: 30px;\">Note that there is a very important ambiguity in the term \"human-level\", though. It could mean child-level or adult-level. (IE, a human-level system may take 20 years to train to the adult level.) The above assumes you mean \"adult level\". If not, add 20 years.</p>\n<p><strong>Q4:</strong> <em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence? </em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>\"Provably non-dangerous\" may not be the best way of thinking about the problem. Overall, the goal is to reduce risk. Proof may not be possible or may not be the most effective route.</p>\n<p style=\"padding-left: 30px;\">So: is it important to solve the problem of safety before trying to solve the problem of intelligence?</p>\n<p style=\"padding-left: 30px;\">I don't think this is possible. Designs for safe systems have to be designs for systems, so they must be informed by solutions to the intelligence problem.</p>\n<p style=\"padding-left: 30px;\">It would also be undesirable to stall progress while considering the consequences. Serious risks are associated with many areas of research, but it typically seems better to mitigate those risks while moving forward rather than beforehand.</p>\n<p style=\"padding-left: 30px;\">That said, it seems like a good idea to put some thought into safety &amp; friendliness while we are solving the general intelligence problem.</p>\n<p><strong>Q4-sub:</strong> <em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>I am a bias authority for this question, but in general, increased funding to AI would be good news to me. My opinion is that the world has a lot of problems which we are slowly solving, but which could be addressed much more effectively if we had more intelligence with which to attack the problem. AI research is beginning to bear fruits in this way (becoming a profitable industry).</p>\n<p style=\"padding-left: 30px;\">So, if I had my say, I would re-assign perhaps half the budget currently assigned to the military to AI research. (The other half would go to NASA and particle physicists...)</p>\n<p style=\"padding-left: 30px;\">What amount of the AI budget should be concentrated on safety? Moderately more than at present. Almost no one is working on safety right now.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>I would rank nanotech as fairly low on my list of concerns, because cells are fairly close to optimal replicators in the present environment. (IE, I don't buy the grey-goo stories: the worst that I find plausible is a nanobot plague, and normal biological weapons would be easier to make.)</p>\n<p style=\"padding-left: 30px;\">Anyway, AI would be lower on my list than global warming.</p>\n<p><strong>Q5-sub: </strong><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk? </em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>Another world war with the present technology &amp; weapon stockpiles (or greater).</p>\n<p><strong>Q6:</strong> <em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>The general population seems to be highly aware of the risks of AI, with very little awareness of the benefits.</p>\n<p style=\"padding-left: 30px;\">Within the research community, the situation was completely opposite until recently. I would say present awareness levels in the research community is roughly optimal...</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human-level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>No. Predictions about AI research have historically been mostly wrong, so it would be incorrect to make such predictions.</p>\n<p><strong>Q8:</strong> <em>Are you familiar with formal concepts of optimal AI design which relate to searches over complete spaces of computable hypotheses or computational strategies, such as Solomonoff induction, Levin search, Hutter's algorithm M, AIXI, or G\u00f6del machines?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>Yes.</p>\n<h3 id=\"Addendum\">Addendum</h3>\n<p style=\"padding-left: 30px;\"><strong>Abram Demski: </strong>I have been, and am, generally conflicted about these issues. I first encountered the writings of Eliezer three or four years ago. I was already a Bayesian at the time, and I was working with great conviction on the problem of finding the \"one true logic\" which could serve as a foundation of reasoning. Central to this was finding the correct formal theory of truth (a problem which is hard thanks to the Liar paradox). Reading Eliezer's material, it was clear that he would be interested in the same things, but wasn't writing a great deal about them in public.</p>\n<p style=\"padding-left: 30px;\">I sent him an email about it. (I had the good habit of emailing random researchers, a habit I recommend to anyone.) His response was that he needed to speak to me on the phone before collaborating with me, since much more about a person is conveyed by audio. So, we set up a phone call.</p>\n<p style=\"padding-left: 30px;\">I tried to discuss logic on the phone, and was successful for a few minutes, but Eliezer's purpose was to deliver the argument for existential risk from AI as a set-up for the central question which would determine whether he would be willing to work with me: If I found the correct logic, would I publish? I answered yes, which meant that we could not work together. The risk for him was too high.</p>\n<p style=\"padding-left: 30px;\">Was I rational in my response? What reason should I have to publish, that would outweigh the risk of someone taking the research and misusing it? (Eliezer made the comment that with the correct logic in hand, it might take a year to implement a seed AI capable of bootstrapping itself to superhuman intelligence.) My perception of my own thought process is much closer to \"clinging to normality\" than one of \"carefully evaluating the correct position\". Shortly after that, trying to resolve my inner conflict (prove to myself that I was being rational) I wrote this:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://dragonlogic-ai.blogspot.com/2009/04/some-numbers-continues-risk-estimate.html\">http://dragonlogic-ai.blogspot.com/2009/04/some-numbers-continues-risk-estimate.html</a></p>\n<p style=\"padding-left: 30px;\">The numbers there just barely indicate that I should keep working on AI (and I give a cut-off date, beyond which it would be too dangerous). Despite trying hard to prove that AI was not so risky, I was experiencing an anchoring bias. AI being terribly risky was the start position from which estimates should be improved.</p>\n<p style=\"padding-left: 30px;\">Still, although I attempted to be fair in my new probability estimates for this interview, it must be admitted that my argument took the form of listing factors which might plausibly reduce the risk and multiplying them together so that the risk gets smaller and smaller. Does this pattern reflect the real physics of the situation, or does it still reflect anchor-and-reduce type reasoning? Have I encountered more factors to reduce the probability because that's been what I've looked for?</p>\n<p style=\"padding-left: 30px;\">My central claims are:</p>\n<ul>\n<li>In order for destruction of humanity to be a good idea, the AI would have to be so powerful that it could simply brush humanity aside, or have a very malevolent goal, or both.</li>\n<li>In order to have massively superhuman intelligence, massively more processing power is needed (than is needed to achieve human-level intelligence).</li>\n<li>It seems unlikely that a malevolent system would emerge in an environment empty of potential rivals which may be more pro-human, which further increases the processing power requirement (because the malevolent system should be much more powerful than rivals in order for cooperation to be an unimportant option), while making massive acquisition of such processing power (taking over the internet single-handedly) less plausible.</li>\n<li>In any case, these negative singularity scenarios tend to assume an autonomous AI system (one with persistent goals). It is more likely that the industry will focus on creating non-autonomous systems in the near-term, and it seems like these would have to become autonomous to be dangerous in that way. (Autonomous systems will be created more for entertainment &amp; eventually companionship, in which case much more attention will be paid to friendly goal systems.)</li>\n</ul>\n<p style=\"padding-left: 30px;\">Some danger comes from originally non-autonomous systems which become autonomous (ie, whose capabilities are expanded to general planning to maximize some industrially useful utility function such as production quality, cash flow, ad clicks, etc). These are more likely to have strange values not good for us. The hope here lies in the possibility that these would be so numerous that cooperation with society &amp; other AIs would be the only option. A scenario where the first AI capable of true recursive self improvement became an industrial AI and rose to power before its makers could re-sell it to many different jobs seems unlikely (because the loop between research and industry is not usually like that). More likely, by the time the first human-level system is five years old (old enough to start thinking about world domination), different versions of it with different goals and experiences will be in many places having many experiences, cooperating with humankind more than each other.</p>\n<p style=\"padding-left: 30px;\">But, anyway, all of this is an argument that the probability is low, not that it would be impossible or that the consequences aren't hugely undesirable. That's why I said the level of awareness in the AI community is good and that research into safe AI could use a bit more funding.</p>", "sections": [{"title": "[Click here to see a list of all interviews]", "anchor": "_Click_here_to_see_a_list_of_all_interviews_", "level": 2}, {"title": "The Interview:", "anchor": "The_Interview_", "level": 1}, {"title": "Abram Demski: ", "anchor": "Abram_Demski__", "level": 2}, {"title": "Addendum", "anchor": "Addendum", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "71 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Bj244uWzDBXvE2N2S"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T11:02:55.284Z", "modifiedAt": null, "url": null, "title": "Leveling Up in Rationality: A Personal Journey", "slug": "leveling-up-in-rationality-a-personal-journey", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:30.422Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HFYWpLNfdwnDwQ3wy/leveling-up-in-rationality-a-personal-journey", "pageUrlRelative": "/posts/HFYWpLNfdwnDwQ3wy/leveling-up-in-rationality-a-personal-journey", "linkUrl": "https://www.lesswrong.com/posts/HFYWpLNfdwnDwQ3wy/leveling-up-in-rationality-a-personal-journey", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Leveling%20Up%20in%20Rationality%3A%20A%20Personal%20Journey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALeveling%20Up%20in%20Rationality%3A%20A%20Personal%20Journey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFYWpLNfdwnDwQ3wy%2Fleveling-up-in-rationality-a-personal-journey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Leveling%20Up%20in%20Rationality%3A%20A%20Personal%20Journey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFYWpLNfdwnDwQ3wy%2Fleveling-up-in-rationality-a-personal-journey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFYWpLNfdwnDwQ3wy%2Fleveling-up-in-rationality-a-personal-journey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1379, "htmlBody": "<p><small>See also: <a href=\"/lw/50p/reflections_on_rationality_a_year_out/\">Reflections on rationality a year out</a></small></p>\n<p>My favorite part of&nbsp;<em>Lord of the Rings</em>&nbsp;was&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Scouring_of_the_Shire#Adaptations\">skipped</a> in both film adaptations. It occurs when our four hobbit heroes (Sam, Frodo, Merry and Pippin) return to the Shire and learn it has been taken over by a gang of ruffians. Merry assumes Gandalf will help them free their home, but Gandalf declines:</p>\n<blockquote>\n<p>I am not coming to the Shire. You must settle its affairs yourselves; that is what you have been trained for... My dear friends, you will need no help. You are grown up now. Grown indeed very high...</p>\n</blockquote>\n<p>As it turns out, the hobbits <em>have</em> acquired many powers along their journey &mdash; powers they use to lead a resistance and free the Shire.</p>\n<p>That is how I felt when I flew home for the holidays this December. Minnesota wasn't ruled by ruffians, but the familiar faces and places reminded me of the person I had been <a href=\"/lw/7dy/a_rationalists_tale/\">before</a> I moved away, just a few years ago.</p>\n<p>And I'm just <em>so</em>&nbsp;much more powerful than I used to be.</p>\n<p>And in <em>my</em>&nbsp;case, at least, many of my newfound powers seem to come from having seriously leveled up in <em>rationality</em>.</p>\n<p><a id=\"more\"></a></p>\n<h4><br /></h4>\n<h4>Power 0: Curiosity</h4>\n<p>I was always \"curious,\" by which I mean I <em>felt</em>&nbsp;like I wanted to know things. I read lots of books and asked lots of questions. But I didn't <em>really</em>&nbsp;want to know the truth, <a href=\"/lw/96j/what_curiosity_looks_like/\">because</a> I didn't care enough about the truth to study, say, <a href=\"http://yudkowsky.net/rational/bayes\">probability theory</a> and the cognitive science of <a href=\"/r/lesswrong/lw/7e5/the_cognitive_science_of_rationality/\">how we deceive ourselves</a>. I just studied different Christian theologies &mdash; and, when I was <em>really</em>&nbsp;daring, different supernatural religions &mdash; and told myself <em>that</em> was what honest truth-seeking <a href=\"/lw/96j/what_curiosity_looks_like/\">looked like</a>.</p>\n<p>It took <em>20 years</em> for reality to pierce my comfortable, carefully cultivated bubble of Christian indoctrination. But when it finally popped, I realized I had (mostly) wasted my life thus far, and I was <em>angry</em>. Now I studied things not just for the pleasure of discovery and the gratifying&nbsp;<em>feeling</em>&nbsp;of caring about truth, but because I <em>really</em>&nbsp;wanted an accurate model of the world so I wouldn't do stupid things like waste two decades of life.</p>\n<p>And it was this curiosity, more than anything else, that led to everything else. So long as I burned for reality, I was bound to level up.</p>\n<p>&nbsp;</p>\n<h4>Power 1: Belief Propagation</h4>\n<p>One factor that helped religion cling to me for so long was my ability to compartmentalize, to shield certain parts of my beliefs from attack, to apply different standards to different beliefs like <a href=\"/lw/gv/outside_the_laboratory/\">the scientist outside the laboratory</a>. When genuine curiosity tore down those walls, it didn't take long for the implications of my atheism to propagate. I noticed that <a href=\"http://www.naturalism.org/freewill.htm\">contra-causal free will</a> made no sense for the same reasons God made no sense. I noticed that whatever value existed in the universe was made of atoms. I assumed <a href=\"http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/\">the basics of transhumanism</a> without knowing there was a thing called \"transhumanism.\" I noticed that minds didn't need to be made of meat, and that machines could be made more moral than humans. (I called them \"<a href=\"http://commonsenseatheism.com/?p=1924\">artificial superbrains</a>\" at the time.) I <a href=\"http://commonsenseatheism.com/?p=7124\">noticed</a> that scientific progress could actually be <em>bad</em>, because it's easier to destroy the world than to protect it. I also noticed we should therefore \"encourage scientific research that saves and protects lives, and discourage scientific research that may destroy us\" &mdash; and this was <em>before</em> I had read about existential risk and \"<a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>.\"</p>\n<p>Somehow, I didn't notice that naturalism + scientific progress also implied <a href=\"http://intelligenceexplosion.com/\">intelligence explosion</a>. I had to <em>read</em>&nbsp;that one. But when I did, it set off <a href=\"/lw/8ib/connecting_your_beliefs_a_call_for_help/\">another round</a> of rapid belief updates. I noticed that the entire world could be lost, that moral theory was <a href=\"http://commonsenseatheism.com/?p=14013\">an urgent engineering problem</a>, that technological utopia is actually possible (however unlikely), and more.</p>\n<p>The power of belief propagation gives me clarity of thought and coherence of action. My actions are now less likely to be informed by multiple incompatible beliefs, though this still occurs <em>sometimes</em> due to <a href=\"/lw/k5/cached_thoughts/\">cached thoughts</a>.</p>\n<p>&nbsp;</p>\n<h4>Power 2: Scholarship</h4>\n<p>I was always one to look things up, but before my deconversion my scholarship heuristic seems to have been \"Find something that shares most of my assumptions and tells me roughly what I want to hear, filled with lots of evidence to reassure me of my opinion.\" That's not what I <em>thought</em>&nbsp;I was doing at the time, but looking back at my reading choices, that's what it <em>looks</em>&nbsp;like I was doing.</p>\n<p>After being taken by genuine curiosity, my heuristic became something more like \"Check what the mainstream scientific consensus is on the subject, along with the major alternative views and most common criticisms.\" Later, I added qualifications like \"But watch out for <a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">signs</a> that an entire field of inquiry is fundamentally unsound.\"</p>\n<p>The power of <em>looking shit up</em>&nbsp;proved to have enormous practical value. How could I make <em>Common Sense Atheism</em>&nbsp;popular, quickly? I studied how to build blog traffic, applied the major lessons, and within 6 months I had one of the most popular atheism blogs on the internet. How could I improve my success with women? I skim-read dozens of books on the subject, filtered out the best advice, applied it (after much trepidation), and eventually had enough success that I didn't need to worry about it anymore. What are values, and how do they work? My search lead me from&nbsp;<a href=\"http://www.amazon.com/Three-Faces-Desire-Philosophy-Mind/dp/019517237X/\">philosophy</a> to <a href=\"http://www.amazon.com/Pleasures-Affective-Science-Morten-Kringelbach/dp/0195331028/\">affective neuroscience</a> and finally to <a href=\"http://en.wikipedia.org/wiki/Neuroeconomics\">neuroeconomics</a>, where I hit the jackpot and wrote <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">A Crash Course in the Neuroscience of Human Motivation</a>. How could I be happier? I studied <a href=\"/lw/4su/how_to_be_happy/\">the science of happiness</a>, applied its lessons, and went from occasionally suicidal to stably happy. How could I make the Singularity Institute more effective? &nbsp;I studied non-profit management and fundraising, and am currently (with lots of help) doing <a href=\"http://intelligence.org/blog/2012/01/16/singularity-institute-progress-report-december-2011/\">quite a lot</a> to make the organization more efficient and credible.</p>\n<p>My most useful scholarship win had to do with beating akrasia. Eliezer wrote <a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">a post</a> about procrastination that drew from personal anecdote but not a single experiment. This prompted me to write <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">my first post</a>, which suggested he ought to have done a bit of research on procrastination, so he could stand on the shoulders of giants. A simple Google scholar search on \"<a href=\"http://scholar.google.com/scholar?as_q=procrastination&amp;num=10&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_sdt=1&amp;as_sdtf=&amp;as_sdts=5&amp;btnG=Search+Scholar&amp;hl=en\">procrastination</a>\" turned up a recent \"meta-analytic and theoretical review\" of the field as the 8th result, which pointed me to the resources I used to write <a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a>. Mastering that post's algorithm for beating akrasia might be the most useful thing I've ever done, since it empowers everything else I try to do.</p>\n<p>&nbsp;</p>\n<h4>Power 3: Acting on Ideas</h4>\n<p>Another lesson from my religious deconversion was that <em>abstract ideas have consequences</em>. Because of my belief in the supernatural, I had spent 20 years (1) studying theology instead of math and science, (2) avoiding sexual relationships, and (3) training myself in fantasy-world \"skills\" like prayer and \"sensing the Holy Spirit.\" If I wanted to benefit from having a more <em>accurate</em> model of the world as much as I had been harmed by having a false model, I'd need to actually <em>act</em>&nbsp;in response to the most probable models of the world I could construct.</p>\n<p>Thus, when I realized I didn't like the Minnesota cold and could be happy without seeing my friends and family that often, I threw all my belongings in my car and moved to California. When I came to take intelligence explosion seriously, I quit my job in L.A., moved to Berkeley, interned with the Singularity Institute, worked hard, got hired as a researcher, and was later appointed Executive Director.</p>\n<p>&nbsp;</p>\n<h4>Winning with Rationality</h4>\n<p>These are just a few of my rationality-powers. Yes, I could have gotten these powers another way, but in my case they seemed to flow largely from that first <a href=\"http://yudkowsky.net/rational/virtues\">virtue of rationality</a>: <a href=\"/lw/96j/what_curiosity_looks_like/\">genuine curiosity</a>. Yes, I've compressed my story and made it sound less messy than it really was, but I do believe I've been gaining in rationalist power &mdash; <a href=\"/lw/5i8/the_power_of_agency/\">the power of agency</a>, <a href=\"/lw/7i/rationality_is_systematized_winning/\">systematized winning</a> &mdash; and that my life is much better as a result. And yes, <em>most</em> people won't get these results, due to&nbsp;<a href=\"/lw/9p/rationality_its_not_that_great/\">things like akrasia</a>, but maybe if we <a href=\"/lw/5x8/teachable_rationality_skills/\">figure out</a> how to <a href=\"/lw/l/teaching_the_unteachable/\">teach the unteachable</a>, <a href=\"/lw/99t/can_the_chain_still_hold_you/\">those chains won't hold us anymore</a>.</p>\n<p>What does a Level 60 rationalist look like? Maybe <a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a>&nbsp;+&nbsp;<a href=\"http://www.fourhourworkweek.com/blog/\">Tim Ferris</a>? That sounds like a worthy goal! A few dozen people<em>&nbsp;that</em> powerful might be able to, like, <a href=\"http://lukeprog.com/SaveTheWorld.html\">save the world</a> or something.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"irYLXtT9hkPXoZqhH": 1, "WqLn4pAWi5hn6McHQ": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HFYWpLNfdwnDwQ3wy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 49, "extendedScore": null, "score": 0.000111, "legacy": true, "legacyId": "12238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>See also: <a href=\"/lw/50p/reflections_on_rationality_a_year_out/\">Reflections on rationality a year out</a></small></p>\n<p>My favorite part of&nbsp;<em>Lord of the Rings</em>&nbsp;was&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Scouring_of_the_Shire#Adaptations\">skipped</a> in both film adaptations. It occurs when our four hobbit heroes (Sam, Frodo, Merry and Pippin) return to the Shire and learn it has been taken over by a gang of ruffians. Merry assumes Gandalf will help them free their home, but Gandalf declines:</p>\n<blockquote>\n<p>I am not coming to the Shire. You must settle its affairs yourselves; that is what you have been trained for... My dear friends, you will need no help. You are grown up now. Grown indeed very high...</p>\n</blockquote>\n<p>As it turns out, the hobbits <em>have</em> acquired many powers along their journey \u2014 powers they use to lead a resistance and free the Shire.</p>\n<p>That is how I felt when I flew home for the holidays this December. Minnesota wasn't ruled by ruffians, but the familiar faces and places reminded me of the person I had been <a href=\"/lw/7dy/a_rationalists_tale/\">before</a> I moved away, just a few years ago.</p>\n<p>And I'm just <em>so</em>&nbsp;much more powerful than I used to be.</p>\n<p>And in <em>my</em>&nbsp;case, at least, many of my newfound powers seem to come from having seriously leveled up in <em>rationality</em>.</p>\n<p><a id=\"more\"></a></p>\n<h4><br></h4>\n<h4 id=\"Power_0__Curiosity\">Power 0: Curiosity</h4>\n<p>I was always \"curious,\" by which I mean I <em>felt</em>&nbsp;like I wanted to know things. I read lots of books and asked lots of questions. But I didn't <em>really</em>&nbsp;want to know the truth, <a href=\"/lw/96j/what_curiosity_looks_like/\">because</a> I didn't care enough about the truth to study, say, <a href=\"http://yudkowsky.net/rational/bayes\">probability theory</a> and the cognitive science of <a href=\"/r/lesswrong/lw/7e5/the_cognitive_science_of_rationality/\">how we deceive ourselves</a>. I just studied different Christian theologies \u2014 and, when I was <em>really</em>&nbsp;daring, different supernatural religions \u2014 and told myself <em>that</em> was what honest truth-seeking <a href=\"/lw/96j/what_curiosity_looks_like/\">looked like</a>.</p>\n<p>It took <em>20 years</em> for reality to pierce my comfortable, carefully cultivated bubble of Christian indoctrination. But when it finally popped, I realized I had (mostly) wasted my life thus far, and I was <em>angry</em>. Now I studied things not just for the pleasure of discovery and the gratifying&nbsp;<em>feeling</em>&nbsp;of caring about truth, but because I <em>really</em>&nbsp;wanted an accurate model of the world so I wouldn't do stupid things like waste two decades of life.</p>\n<p>And it was this curiosity, more than anything else, that led to everything else. So long as I burned for reality, I was bound to level up.</p>\n<p>&nbsp;</p>\n<h4 id=\"Power_1__Belief_Propagation\">Power 1: Belief Propagation</h4>\n<p>One factor that helped religion cling to me for so long was my ability to compartmentalize, to shield certain parts of my beliefs from attack, to apply different standards to different beliefs like <a href=\"/lw/gv/outside_the_laboratory/\">the scientist outside the laboratory</a>. When genuine curiosity tore down those walls, it didn't take long for the implications of my atheism to propagate. I noticed that <a href=\"http://www.naturalism.org/freewill.htm\">contra-causal free will</a> made no sense for the same reasons God made no sense. I noticed that whatever value existed in the universe was made of atoms. I assumed <a href=\"http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/\">the basics of transhumanism</a> without knowing there was a thing called \"transhumanism.\" I noticed that minds didn't need to be made of meat, and that machines could be made more moral than humans. (I called them \"<a href=\"http://commonsenseatheism.com/?p=1924\">artificial superbrains</a>\" at the time.) I <a href=\"http://commonsenseatheism.com/?p=7124\">noticed</a> that scientific progress could actually be <em>bad</em>, because it's easier to destroy the world than to protect it. I also noticed we should therefore \"encourage scientific research that saves and protects lives, and discourage scientific research that may destroy us\" \u2014 and this was <em>before</em> I had read about existential risk and \"<a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>.\"</p>\n<p>Somehow, I didn't notice that naturalism + scientific progress also implied <a href=\"http://intelligenceexplosion.com/\">intelligence explosion</a>. I had to <em>read</em>&nbsp;that one. But when I did, it set off <a href=\"/lw/8ib/connecting_your_beliefs_a_call_for_help/\">another round</a> of rapid belief updates. I noticed that the entire world could be lost, that moral theory was <a href=\"http://commonsenseatheism.com/?p=14013\">an urgent engineering problem</a>, that technological utopia is actually possible (however unlikely), and more.</p>\n<p>The power of belief propagation gives me clarity of thought and coherence of action. My actions are now less likely to be informed by multiple incompatible beliefs, though this still occurs <em>sometimes</em> due to <a href=\"/lw/k5/cached_thoughts/\">cached thoughts</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"Power_2__Scholarship\">Power 2: Scholarship</h4>\n<p>I was always one to look things up, but before my deconversion my scholarship heuristic seems to have been \"Find something that shares most of my assumptions and tells me roughly what I want to hear, filled with lots of evidence to reassure me of my opinion.\" That's not what I <em>thought</em>&nbsp;I was doing at the time, but looking back at my reading choices, that's what it <em>looks</em>&nbsp;like I was doing.</p>\n<p>After being taken by genuine curiosity, my heuristic became something more like \"Check what the mainstream scientific consensus is on the subject, along with the major alternative views and most common criticisms.\" Later, I added qualifications like \"But watch out for <a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">signs</a> that an entire field of inquiry is fundamentally unsound.\"</p>\n<p>The power of <em>looking shit up</em>&nbsp;proved to have enormous practical value. How could I make <em>Common Sense Atheism</em>&nbsp;popular, quickly? I studied how to build blog traffic, applied the major lessons, and within 6 months I had one of the most popular atheism blogs on the internet. How could I improve my success with women? I skim-read dozens of books on the subject, filtered out the best advice, applied it (after much trepidation), and eventually had enough success that I didn't need to worry about it anymore. What are values, and how do they work? My search lead me from&nbsp;<a href=\"http://www.amazon.com/Three-Faces-Desire-Philosophy-Mind/dp/019517237X/\">philosophy</a> to <a href=\"http://www.amazon.com/Pleasures-Affective-Science-Morten-Kringelbach/dp/0195331028/\">affective neuroscience</a> and finally to <a href=\"http://en.wikipedia.org/wiki/Neuroeconomics\">neuroeconomics</a>, where I hit the jackpot and wrote <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">A Crash Course in the Neuroscience of Human Motivation</a>. How could I be happier? I studied <a href=\"/lw/4su/how_to_be_happy/\">the science of happiness</a>, applied its lessons, and went from occasionally suicidal to stably happy. How could I make the Singularity Institute more effective? &nbsp;I studied non-profit management and fundraising, and am currently (with lots of help) doing <a href=\"http://intelligence.org/blog/2012/01/16/singularity-institute-progress-report-december-2011/\">quite a lot</a> to make the organization more efficient and credible.</p>\n<p>My most useful scholarship win had to do with beating akrasia. Eliezer wrote <a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">a post</a> about procrastination that drew from personal anecdote but not a single experiment. This prompted me to write <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">my first post</a>, which suggested he ought to have done a bit of research on procrastination, so he could stand on the shoulders of giants. A simple Google scholar search on \"<a href=\"http://scholar.google.com/scholar?as_q=procrastination&amp;num=10&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_sdt=1&amp;as_sdtf=&amp;as_sdts=5&amp;btnG=Search+Scholar&amp;hl=en\">procrastination</a>\" turned up a recent \"meta-analytic and theoretical review\" of the field as the 8th result, which pointed me to the resources I used to write <a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a>. Mastering that post's algorithm for beating akrasia might be the most useful thing I've ever done, since it empowers everything else I try to do.</p>\n<p>&nbsp;</p>\n<h4 id=\"Power_3__Acting_on_Ideas\">Power 3: Acting on Ideas</h4>\n<p>Another lesson from my religious deconversion was that <em>abstract ideas have consequences</em>. Because of my belief in the supernatural, I had spent 20 years (1) studying theology instead of math and science, (2) avoiding sexual relationships, and (3) training myself in fantasy-world \"skills\" like prayer and \"sensing the Holy Spirit.\" If I wanted to benefit from having a more <em>accurate</em> model of the world as much as I had been harmed by having a false model, I'd need to actually <em>act</em>&nbsp;in response to the most probable models of the world I could construct.</p>\n<p>Thus, when I realized I didn't like the Minnesota cold and could be happy without seeing my friends and family that often, I threw all my belongings in my car and moved to California. When I came to take intelligence explosion seriously, I quit my job in L.A., moved to Berkeley, interned with the Singularity Institute, worked hard, got hired as a researcher, and was later appointed Executive Director.</p>\n<p>&nbsp;</p>\n<h4 id=\"Winning_with_Rationality\">Winning with Rationality</h4>\n<p>These are just a few of my rationality-powers. Yes, I could have gotten these powers another way, but in my case they seemed to flow largely from that first <a href=\"http://yudkowsky.net/rational/virtues\">virtue of rationality</a>: <a href=\"/lw/96j/what_curiosity_looks_like/\">genuine curiosity</a>. Yes, I've compressed my story and made it sound less messy than it really was, but I do believe I've been gaining in rationalist power \u2014 <a href=\"/lw/5i8/the_power_of_agency/\">the power of agency</a>, <a href=\"/lw/7i/rationality_is_systematized_winning/\">systematized winning</a> \u2014 and that my life is much better as a result. And yes, <em>most</em> people won't get these results, due to&nbsp;<a href=\"/lw/9p/rationality_its_not_that_great/\">things like akrasia</a>, but maybe if we <a href=\"/lw/5x8/teachable_rationality_skills/\">figure out</a> how to <a href=\"/lw/l/teaching_the_unteachable/\">teach the unteachable</a>, <a href=\"/lw/99t/can_the_chain_still_hold_you/\">those chains won't hold us anymore</a>.</p>\n<p>What does a Level 60 rationalist look like? Maybe <a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a>&nbsp;+&nbsp;<a href=\"http://www.fourhourworkweek.com/blog/\">Tim Ferris</a>? That sounds like a worthy goal! A few dozen people<em>&nbsp;that</em> powerful might be able to, like, <a href=\"http://lukeprog.com/SaveTheWorld.html\">save the world</a> or something.</p>", "sections": [{"title": "Power 0: Curiosity", "anchor": "Power_0__Curiosity", "level": 1}, {"title": "Power 1: Belief Propagation", "anchor": "Power_1__Belief_Propagation", "level": 1}, {"title": "Power 2: Scholarship", "anchor": "Power_2__Scholarship", "level": 1}, {"title": "Power 3: Acting on Ideas", "anchor": "Power_3__Acting_on_Ideas", "level": 1}, {"title": "Winning with Rationality", "anchor": "Winning_with_Rationality", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "59 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SFG9Cm7mf5eP4juKs", "9WX59u7g2sdKqnjDm", "3oYaLja5h8qL5adDn", "xLm9mgJRPvmPGpo7Q", "N2pENnTPB75sfc9kb", "kHL6qX9eArmvNWY99", "2MD3NMLBPCqPfnfre", "fyZBtNB3Ki3fM4a6Y", "hN2aRnu798yas5b2k", "ZbgCx2ntD5eu8Cno9", "9o3QBg2xJXcRCxGjS", "64FdKLwmea8MCLWkE", "RWo4LwFzpHNQCTcYt", "vbcjYg6h3XzuqaaN8", "4ARtkT3EYox3THYjF", "LgavAYtzFQZKg95WC", "f4CZNEHirweN3XEjs", "9SaAyq7F7MAuzAWNN", "iETtCZcfmRyHp69w4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T13:44:24.539Z", "modifiedAt": null, "url": null, "title": "[Placeholder] Against dystopia, rally before Kant", "slug": "placeholder-against-dystopia-rally-before-kant", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:33.884Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Multiheaded", "createdAt": "2011-07-02T10:10:20.692Z", "isAdmin": false, "displayName": "Multiheaded"}, "userId": "moGiw35FowgiAnzfg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XT8Srd4trgMKuEtM5/placeholder-against-dystopia-rally-before-kant", "pageUrlRelative": "/posts/XT8Srd4trgMKuEtM5/placeholder-against-dystopia-rally-before-kant", "linkUrl": "https://www.lesswrong.com/posts/XT8Srd4trgMKuEtM5/placeholder-against-dystopia-rally-before-kant", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPlaceholder%5D%20Against%20dystopia%2C%20rally%20before%20Kant&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPlaceholder%5D%20Against%20dystopia%2C%20rally%20before%20Kant%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXT8Srd4trgMKuEtM5%2Fplaceholder-against-dystopia-rally-before-kant%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPlaceholder%5D%20Against%20dystopia%2C%20rally%20before%20Kant%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXT8Srd4trgMKuEtM5%2Fplaceholder-against-dystopia-rally-before-kant", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXT8Srd4trgMKuEtM5%2Fplaceholder-against-dystopia-rally-before-kant", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 494, "htmlBody": "<p>I was just sitting there studying for an exam today (passed fine btw), when my mind, eh, made up its mind on a subject. I've been pondering Robin Hanson's happy resignation to a distant efficiency-obsessed <a href=\"http://www.overcomingbias.com/2009/09/this-is-the-dream-time.html\">dystopia</a>. Then it struck me that, one meta level up, it's no different from the milder, blander current incarnation of the (neoliberal or whatever) Church of Efficiency - which LWians who work as small cogs in big corporations must be all to familliar with. And that creed, although unattractive in itself, is part of a proud tradition: utopian or generally \"far-mode\" thinking which clearly spelt disaster even in its inception to any thinking contemporary who held <em>the complexity, richness and beauty of the human condition</em> as their absolute and overriding value.</p>\n<p>Such was the case of Dostoevsky, who rose far above his attacks on everyday anti-humanist thought when he wrote his <em>Legend of The Grand Inquisitor</em>. This short story, probably the most significant one in world literature, makes a thorough, convincing and compelling case for the opposition, the likes of which LW holds to be the best standard of argument. Moreover, he doesn't just hack apart the rather average faux-Nietzchean, utopian socialist, right-wing Catholic and other brands of thought, then makes a stronger enemy out of them in his Inquisitor. He, identifying with Christ, refuses to use any postulate of a benevolent God to shut down the opposition. And does his corpus of work present a convincing response to the Inquisitor's icy wall of reason? Not in any single point, as far as I've found. Yet a great idea - nearly flawless in itself, once you get rid of any debatable or weakening connotations - was already <a href=\"http://en.wikipedia.org/wiki/Categorical_imperative#The_Second_Formulation\">found</a> by Kant (and reused by Dostoevsky and others from different standpoints). Importantly, the meta-meta-meta-rule of \"treating humanity as an end in itself and not just the means\" does not, by itself, require any connection with deontologism. My view is that any utilitarian who likes the existence of beings like themselves and is concerned about them adopt the Second Formulation without caveats, as a recipe against false second-order values like \"hedons\" or \"efficiency\" that would undermine the quality not merely of human life, but of human condition in itself if left to reign unchecked and taken to their absurd/logical conclusion. Would Calvinism, the Reign of Terror, Stalin's policy or Pol Pot's genocide have happened if the generally intelligent, unselfish people who acted as the catalysts behind all those dystopian blunders took a good, honest look at the wisdom of Kant's humanist suggestion?</p>\n<p>&nbsp;</p>\n<p>Sorry, I'm tired as hell. I realize that I have nowhere near enough evidence or detail, and maybe there's a flaw in this reasoning somewhere. Will return to it later, and try to think harder.</p>\n<p>Damn, this style does read too much like WillNewsome. No offense, Will.</p>\n<p>&nbsp;</p>\n<p>Edit: OK, might be not very readable, but why don't you start arguing this so I can make the post better as the fog of not-exactly-war clears?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XT8Srd4trgMKuEtM5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -9, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "12240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T20:32:31.003Z", "modifiedAt": null, "url": null, "title": "Gambler's Reward: Optimal Betting Size", "slug": "gambler-s-reward-optimal-betting-size", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:30.078Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "b1shop", "createdAt": "2010-07-21T22:58:23.412Z", "isAdmin": false, "displayName": "b1shop"}, "userId": "YYM9ouBdPbNFxvF2G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iFCMa4s47NeJHbRxn/gambler-s-reward-optimal-betting-size", "pageUrlRelative": "/posts/iFCMa4s47NeJHbRxn/gambler-s-reward-optimal-betting-size", "linkUrl": "https://www.lesswrong.com/posts/iFCMa4s47NeJHbRxn/gambler-s-reward-optimal-betting-size", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gambler's%20Reward%3A%20Optimal%20Betting%20Size&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGambler's%20Reward%3A%20Optimal%20Betting%20Size%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFCMa4s47NeJHbRxn%2Fgambler-s-reward-optimal-betting-size%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gambler's%20Reward%3A%20Optimal%20Betting%20Size%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFCMa4s47NeJHbRxn%2Fgambler-s-reward-optimal-betting-size", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFCMa4s47NeJHbRxn%2Fgambler-s-reward-optimal-betting-size", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 277, "htmlBody": "<p>I've been trying my hand at card counting lately, and I've been doing some thinking about how a perfect gambler would act at the table. I'm not sure how to derive the optimal bet size.</p>\n<p>Overall, the expected value of blackjack is small and negative. However, there is high variance in the expected value. By varying his bet size and sitting out rounds, the player can wager more money when expected value is higher and less money when expected value is lower. Overall, this can result in an edge.</p>\n<p>However, I'm not sure what the optimal bet size is. Going all-in with a 60 percent chance of winning is EV+, but the 40 percent chance of loss would not only destroy your bankroll, it would also prevent you from participating in future EV+ situations. Ideally, one would want to not only increase EV, but also decrease variance.</p>\n<p><strong>Objective:</strong> Given a distribution of expected values, develop a function that transforms the current expected value into the percentage of the bankroll that should be placed at risk.</p>\n<p>I'm not sure how to begin. Even if I had worked out the distribution of expected values. Are other inputs required (i.e. utility of marginal dollar won, desired risk of ruin)? Should the approach perhaps be to maximize expected value after one playing session? Why not a month of playing sessions, or a billion? Is there any chance the optimal betting size would produce behavior similar to the behavior predicted by <a href=\"/lw/6kf/prospect_theory_a_framework_for_understanding/\">prospect theory</a>?</p>\n<p>I eagerly await an informative discussion. If you have something against gambling, just pretend we're talking about how much of your wealth you plan on investing in an oil well with positive expected value.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iFCMa4s47NeJHbRxn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 8.333530420351129e-07, "legacy": true, "legacyId": "12242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LQp9cZPzJncFKh5c8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T21:08:38.646Z", "modifiedAt": null, "url": null, "title": "[Link] Five ways to classify belief systems ", "slug": "link-five-ways-to-classify-belief-systems", "viewCount": null, "lastCommentedAt": "2012-05-01T08:21:18.311Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J2ErxgYutRgH96hnE/link-five-ways-to-classify-belief-systems", "pageUrlRelative": "/posts/J2ErxgYutRgH96hnE/link-five-ways-to-classify-belief-systems", "linkUrl": "https://www.lesswrong.com/posts/J2ErxgYutRgH96hnE/link-five-ways-to-classify-belief-systems", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Five%20ways%20to%20classify%20belief%20systems%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Five%20ways%20to%20classify%20belief%20systems%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ2ErxgYutRgH96hnE%2Flink-five-ways-to-classify-belief-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Five%20ways%20to%20classify%20belief%20systems%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ2ErxgYutRgH96hnE%2Flink-five-ways-to-classify-belief-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ2ErxgYutRgH96hnE%2Flink-five-ways-to-classify-belief-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1585, "htmlBody": "<p><em>On recommendation from several LessWrongers I've been over the past year or so occasionally digging into the many long posts to be found in the archives of <a href=\"http://unqualified-reservations.blogspot.com/\">Unqualified Reservations</a> (archive links best accessed from <a href=\"http://moldbuggery.blogspot.com/\">here</a>). It is written by Mencius Moldbug, who is probably familiar to many of us here as well as to readers of <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a>. He is an erudite, controversial and most of all contrarian social critic and writer.</em></p>\n<p><em>He sometimes repeats and often refines his key ideas. He uses his writing style as a barrier to entry (it is debatable if this does more harm than good for his quality of thought and communication, but it is an interesting way to aim for the <a href=\"/lw/1kh/the_correct_contrarian_cluster/\">correct contrarian cluster</a>), thus he is an acquired taste, posters have recommended the <a href=\"http://unqualified-reservations.blogspot.com/2009/01/gentle-introduction-to-unqualified.html\">gentle introduction</a> series as good place to start reading him. <a href=\"http://unqualified-reservations.blogspot.com/2008/04/open-letter-to-open-minded-progressives.html\">This series</a> is similar, while <a href=\"http://unqualified-reservations.blogspot.com/2010/03/divine-right-monarchy-for-modern.html\">this one</a> and <a href=\"http://unqualified-reservations.blogspot.com/2007/04/formalist-manifesto-originally-posted.html\">The Formalist Manifesto</a> focus more on summarizing his political thought, which may also be useful in itself. </em></p>\n<p><em>Link to topical entry is </em><em><a href=\"http://unqualified-reservations.blogspot.com/2007/05/six-ways-to-classify-belief-systems.html\">here</a></em><em>. Link to discussion on previous entry </em><em>I read is <a href=\"/lw/8w8/link_belief_in_religion_considered_harmful/\">here</a>.</em></p>\n<blockquote>\n<h2>Five ways to classify belief systems<br /></h2>\n<p><strong>I use the word <a href=\"http://en.wikipedia.org/wiki/Kernel_%28computing%29\"><span style=\"font-style: italic;\">kernel</span></a> to mean \"belief system.\"</strong> Kernels, like Gaul, are divided into three parts: assertions about the real world (Hume's \"is\"), moral judgments about the real world (Hume's \"ought\"), and paranormal or other metaphysical propositions (such as David Stove's wonderful ruminations on the <a href=\"http://web.maths.unsw.edu.au/%7Ejim/wrongthoughts.html\">number 3</a>).<br /><br />Everyone, no matter how smart or stupid, has exactly one kernel. However, kernels are not assigned randomly, as if in some weird Buddhist boot process. <strong>For example, your kernel is likely to show similarities to that of your parents, friends, teachers, karate masters, favorite anchormen, etc, etc.</strong><br /><br /><strong>Let's call a kernel pattern which many people share a <span style=\"font-style: italic;\">prototype</span>.</strong> Methodism, environmentalism, firearms practice, snake handling and Burning Man attendance are all prototypes. While there are few Methodist environmentalists who are also snake-handling marksmen and never miss a burn, various subcombinations are not uncommon.<br /><br /><strong>In general we are most interested in <span style=\"font-style: italic;\">complete</span> prototypes, that is, kernel patterns that are broad enough to serve as identities.</strong> It is common to describe someone as \"a Methodist,\" or (not quite in the same way) as \"an environmentalist.\" People who match the other prototypes above may use nouns for themselves, but they're must less likely to be described or introduced as such. An incomplete prototype simply says less about you. For example, many snake handlers are also committed peace activists who drive Range Rovers and shop at Pottery Barn.</p>\n<p>Two common examples of a complete prototype are <span style=\"font-style: italic;\">religions</span>, which involve convictions about one or more anthropomorphic paranormal entities, and <span style=\"font-style: italic;\">idealisms</span>, which involve convictions about one or more undefined universals, or <span style=\"font-style: italic;\">ideals</span>.</p>\n<p><strong>Many people consider the distinction between religion and idealism important and/or interesting, but here at UR we don't much care for it</strong>, since only metaphysical propositions can distinguish the two. You can go from religion to idealism and back simply by adding and subtracting gods, angels, demons, saints, ghosts, etc. I personally have slain many ghosts and quite a few demons, and I once kidnapped an angel and forced her at swordpoint to lead me to the altar of Thoth, where I sacrificed her for 20,000 experience points, permanent immunity to fire, and an alignment change to chaotic evil. However, this was not in real life. And even in D&amp;D, I've never had the misfortune to encounter a god.</p>\n</blockquote>\n<p>On LessWrong already discussed a more extensive form of this argument in <a href=\"/lw/8w8/link_belief_in_religion_considered_harmful/\">\"Belief in religion considered harmful?\"</a>. &nbsp;</p>\n<blockquote>\n<p>Therefore, we'll just use the word <span style=\"font-style: italic;\">prototype</span> to mean either religion or idealism. Of course one can study either forever. In fact, most scholars in history have spent most of their time investigating the twisty little passages, all alike, of one single prototype. However, since here at UR we are generalists, not Irish monks, Talmudic scribes or Koranic talibs, we will try and work a little more broadly.<br /><br />Before you can really think about prototypes, you have to be able to name and classify them. One obvious analogy is the study of languages, which are transmitted from person to person in a vaguely similar way. Prototype transmission really has nothing in common with language transmission, but the metaproblems are the same: what does it mean to say, \"X descends from Y?\" Is a classification tree a tree, or a directed acyclic graph? Is variation continuous, or discrete? Etc, etc, etc.<br /><br /><strong>Probably readers can add a few, but I can think of five ways to classify prototypes: <span style=\"font-style: italic;\">nominalist</span>, <span style=\"font-style: italic;\">typological</span>, <span style=\"font-style: italic;\">morphological</span>, <span style=\"font-style: italic;\">cladistic</span>, and <span style=\"font-style: italic;\">adaptive</span>.</strong><br /><br />As our example for each, let's use the movement generally known as the <a href=\"http://en.wikipedia.org/wiki/Age_of_Enlightenment\">Enlightenment</a>. There is no noun for people whose kernels match the Enlightenment prototype, but there should be, because this noun arguably applies to almost everyone on earth. Let's call these suspicious characters <span style=\"font-style: italic;\">Luminists</span>. Their sinister views can be described as <span style=\"font-style: italic;\">Luminism</span>.<br /><br />A <strong><span style=\"font-style: italic;\">nominalist</span></strong> classification simply accepts the prototype's classification of itself. Luminists, for example, believe there is no such thing as Luminism. (This is very common.) Rather, they are simply people who have seen the light of reason. It just so happened that they all saw more or less the same light at more or less the same time. <strong> But since by definition there's only one such thing as reason, this explanation is not inherently implausible.<br /></strong><br />A <strong><span style=\"font-style: italic;\">typological</span> </strong>classification distinguishes prototypes according to specific features. For example, when you distinguish between religions and idealisms - as between Christianity and Luminism - you are performing an act of typology. The flaws in this approach can be seen by the fact that a typological classification of languages tells us Old Saxon is a dialect of Early Apache, since they both have arbitrary word order and long, incomprehensible sentences. Meanwhile, a vampire bat is a grinning, hairy owl, IHOP and Domino's both serve round food, Congress is considering a new O visa for ostriches, Burmese tribeswomen and other long-necked bipeds, and Luminism is a kind of Confucian Sufi-Buddhism.<br /><br />A <strong><span style=\"font-style: italic;\">morphological</span></strong> classification is like a typological classification with a clue. It attempts to construct a historical descent tree by looking at multiple points of similarity. Morphological classification tells us that Luminism is actually a sect of Christianity, because Luminists share a wide range of kernel features with many Christians, and there are even intermediate forms which can reasonably be described as Christian Luminists or Luminist Christians.<br /><br />A <strong><span style=\"font-style: italic;\">cladistic</span></strong> classification also produces a historical descent tree, but it uses a completely different method. Cladistic classification ignores actual beliefs and looks only at patterns of conversion. It asks: if you are a Luminist and your parents were not Luminists, what were they? Since the answer is usually (if not always) \"Christian,\" in this case cladistics produces the same result as morphology For obvious reasons, this is often so.<br /><br />Besides the usual trees, both morphological and cladistic methods can also produce graph structure, that is, patterns of combination or <span style=\"font-style: italic;\">syncretism</span>. For example, both methods identify Hellenistic and Jewish roots for Christianity, with the cladistic method adding various Roman cults such as those of Augustus, Sol Invictus, and Mithra.<br /><br />An <strong><span style=\"font-style: italic;\">adaptive</span></strong> classification is not interested at all in descent. Rather, it focuses on how and why the prototype <span style=\"font-style: italic;\">succeeds</span>. For example, Luminism, Christianity, Sol Invictus and Islam are all prototypes that succeeded (at one time or another) by virtue of being an <span style=\"font-style: italic;\">official prototype</span>, that is, by explaining the legitimacy of a government - helping to organize its supporters, strike fear into the hearts of its enemies, brainwash its dutiful taxpaying serfs, etc, etc, etc. But with the exception of the third, all the above have also done just fine in an unofficial capacity, so this <span style=\"font-style: italic;\">official selection</span> is not a complete explanation of their success.<br /><br />Of course, I personally find the last three classification methods the most compelling, with my favorites being the morphological and adaptive methods. But words are just words, and anyone can look at these phenomena any way they like. And if you can suggest any additions to the list, the comments section is, as usual, open.</p>\n</blockquote>\n<p>Upon introspection I generally seem to implicitly use adaptive frames for \"kernels\" in ancient societies I don't know very well (or which don't have a well preserved written history - say like an explanation for widespread human sacrifice in Mesoamerica) and when I just read something by <a href=\"http://en.wikipedia.org/wiki/Meme\">Dawkins</a>. Morphological when thinking about religion in ancient literate societies I know quite a bit about like say the Roman Empire and nominalist when deciding how I classify modern religions like Mormonism.</p>\n<p>There are other examples, but overall Moldbug's division seems to me to capture most of my differing approaches to thinking about \"kernels\" and indeed they do seem to be running on different algorithms. <em>The obvious question which I hope to discuss in the comment section is which of these approaches is most useful under different sets of circumstances and goals.</em></p>\n<p>Doing some thought on it his take on the concept and his division of the categories. In itself it seems a <strong><em>somewhat useful</em></strong> framework for thinking about intellectual fashion and ideological or religious transformation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J2ErxgYutRgH96hnE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 18, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "12243", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9KvefburLia7ptEE3", "fMHq4djhTRBFedQyq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-01-17T21:08:38.646Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-17T22:35:42.774Z", "modifiedAt": null, "url": null, "title": "Histocracy: Open, Effective Group Decision-Making With Weighted Voting", "slug": "histocracy-open-effective-group-decision-making-with", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:05.371Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MDQnDGEKQuCAggRLe/histocracy-open-effective-group-decision-making-with", "pageUrlRelative": "/posts/MDQnDGEKQuCAggRLe/histocracy-open-effective-group-decision-making-with", "linkUrl": "https://www.lesswrong.com/posts/MDQnDGEKQuCAggRLe/histocracy-open-effective-group-decision-making-with", "postedAtFormatted": "Tuesday, January 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Histocracy%3A%20Open%2C%20Effective%20Group%20Decision-Making%20With%20Weighted%20Voting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHistocracy%3A%20Open%2C%20Effective%20Group%20Decision-Making%20With%20Weighted%20Voting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDQnDGEKQuCAggRLe%2Fhistocracy-open-effective-group-decision-making-with%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Histocracy%3A%20Open%2C%20Effective%20Group%20Decision-Making%20With%20Weighted%20Voting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDQnDGEKQuCAggRLe%2Fhistocracy-open-effective-group-decision-making-with", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDQnDGEKQuCAggRLe%2Fhistocracy-open-effective-group-decision-making-with", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1538, "htmlBody": "<p>The following is slightly edited from a pitch I wrote for a general audience. I've added blog-specific content afterwards.</p>\n<hr />\n<p style=\"font-weight: normal; line-height: 100%;\"><a id=\"more\"></a></p>\n<p><span>Information technology allows for unprecedented levels of collaboration and debate. When an issue arises people communicate freely, are held accountable for misrepresentations, and are celebrated for cogent analysis. We share information and opinion better than ever before. And then we leave the actual decision up to one person, or a tiny committee, or a poll of a population that for the most part wasn't paying attention, or at best an unpredictably irrational market. The one thing we still don't aggregate in a sophisticated way is human judgment.</span></p>\n<p><span> Organizations evolve complex decision-making structures because variance in human judgement is complicated. We try to put the most competent person in charge&mdash;but there is wisdom in crowds, and so a wise leader gets buy-in from a broad pool of competent subordinates. We must constantly try to evaluate who has the best record, to see who's been right in the past...and we get it wrong all the time. We overestimate our own competence. In hindsight, we misremember the right decision as being obvious. We trust the man with the better hair. Any organization with group buy-in on decisions amasses a solid amount of data on the competence of its members, but it does not curate or use this data effectively.</span></p>\n<p><span> We can do better, using current technology, some simple software, and some relatively simple math. The solution is called histocracy. It is most easily explained with a use case.</span></p>\n<p><span> The H Foundation is a hypothetical philanthropic organization, with a board of twelve people overseeing a large fund. Each year, they receive and review several hundred grant applications, and choose a few applicants to give money to. Sometimes these applicants use the money effectively, and sometimes they fail. Often an applicant they turn down will get funding elsewhere and experience notable success or failure. In short, it is often obvious to the board in hindsight whether they made the right decision. For each application, the yay or nay of each board member is recorded. If and when, later, the board reaches a consensus on whether that application <em>should</em><em> </em><span style=\"font-style: normal;\">have</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">been</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">approved,</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">this</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">consensus</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">is</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">recorded</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">as</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">well.</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">The</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">result</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">is</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">that</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">each</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">board</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">member</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">accumulates</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">a</span><span style=\"font-style: normal;\"> </span><em>score</em><span style=\"font-style: normal;\">.</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">Alice's</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">votes</span><em> </em><span style=\"font-style: normal;\">have</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">been</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">right</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">331</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">times</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">and</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">wrong</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">59</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">times,</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">while</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">Bob's</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">votes</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">have</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">been</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">right</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">213</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">times</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">and</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">wrong</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">110</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">times</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">(they</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">weren't</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">always</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">present</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">for</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">the</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">same</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">votes).</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">Already</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">from</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">this</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">raw</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">data</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">we</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">can</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">see</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">that</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">Alice's</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">opinion</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">should</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">count</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">for</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">more</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">than</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">Bob's.</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">With</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">a</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">computer's</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">ease</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">with</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">arithmetic,</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">we</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">can</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">quantify</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">this.</span><span style=\"font-style: normal;\"> </span><a href=\"http://www.makefoil.com/histocracy/Histocracy_Math_with_Independence.pdf\"><span style=\"font-style: normal;\">Some</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">math</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">is</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">given</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">in</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">an</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">appendix</span></a><span style=\"font-style: normal;\">;</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">here</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">it</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">suffices</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">to</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">say</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">that</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">it</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">would</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">be</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">reasonable</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">to</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">give</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">Alice's</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">vote</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">a</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">little</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">over</span><span style=\"font-style: normal;\"> 7</span><span style=\"font-style: normal;\">/4ths</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">the</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">weight</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">of</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">Bob's:</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">if</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">the</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">board</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">is</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">to</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">maximize</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">its</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">chance</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">of</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">making</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">the</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">correct</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">choice,</span><span style=\"font-style: normal;\"> 4 </span><span style=\"font-style: normal;\">Alices</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">should</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">be</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">able</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">to</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">outvote</span><span style=\"font-style: normal;\"> 7 </span><span style=\"font-style: normal;\">Bobs.</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">The</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">board</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">members</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">each</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">connect</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">to</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">a</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">shared</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">server</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">and</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">vote</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">on</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">each</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">application;</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">the</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">software</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">performs</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">the</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">relevant</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">calculations</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">and</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">determines</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">the</span><span style=\"font-style: normal;\"> </span><span style=\"font-style: normal;\">victor.</span><span style=\"font-style: normal;\"> </span></span></p>\n<p><span> In this system, the board members perform the massively complex task of evaluating the applicants, a job requiring expert judgment and intuition, while the computer dispassionately and precisely evaluates the board. The result is a system wiser than any individual board member.</span></p>\n<p><span> When scaling this solution up to a large business with thousands of employees, the math stays the same while the interface changes. Decisions need to be shared and discussed on a corporate intranet, and tagged by type so that employees can find and vote on only those decisions they feel competent to vote on. Employees who try to make decisions on matters beyond their competence will fail to accumulate enough voting weight to skew the decision; this means that decisions in all areas can be opened to the entire field. Managers should be encouraged to reframe decisions they are pondering as corporation-wide referenda. Evaluating a decision in hindsight should in this case be reserved to the owners and shareholders, or to a system or charter they have approved.</span></p>\n<p><span> Expanding the scale even further, the same approach could be applied to advice, solicited or unsolicited. Consider a site to which clients could pay to submit polls on decisions that concerned them. The polls would be conducted and reported histocratically. The client would later be asked to report whether the advice given by the community turned out to be correct. Prizes and recognition could be given to those solvers who accumulate the highest voting weights, thereby incentivizing participation and excellence. For unsolicited advice, a similar approach could be used with petitions.</span></p>\n<p><span> In summary, we note that human judgment is essentially a set of predictions, and thus can be judged empirically and aggregated mathematically. Group decision-making is such an omnipresent and consequential task that optimizing it may be the single most important thing we can do. Let's do it rigorously, and let's do it now.</span></p>\n<hr />\n<p>&nbsp;</p>\n<p><span><span style=\"font-weight: normal;\"> On Sunday, I posted <a href=\"/lw/9eo/a_call_for_solutions_and_a_tease_of_mine/\">a call for solutions</a></span></span><span> in advance of this post. Which is a weird thing to do, but I have a terror of Irrevocable Actions, and I can't untell you something. (Coincidentally, at the same time as people were chiding me for this, a discussion started about my also mildly eccentric decision to put my play behind a <a href=\"/lw/86m/fiction_hamlet_and_the_philosophers_stone/\">semipermeable paymembrane</a>, which has a similar explanation; it's easier to make something free that was once non-free than the reverse, and in many circles charging for something is actually higher-status.)</span></p>\n<p><span> I didn't mention <a href=\"http://hanson.gmu.edu/futarchy.pdf\">prediction</a> <a href=\"/lw/y9/three_worlds_decide_58/\">markets</a> because I didn't want people to anchor on it&mdash;it's just a hop, skip, and a jump from futarchy to histocracy, so that would obviate the point. <a href=\"http://predictionbook.com/predictions/5381\">As expected</a>, <a href=\"/lw/9eo/a_call_for_solutions_and_a_tease_of_mine/5oc1\">people went there immediately</a> anyway, and from there to <a href=\"/lw/9eo/a_call_for_solutions_and_a_tease_of_mine/5odd\">something very close to my idea.</a> Much of the discussion centered around the difficulty of creating a well-defined charter. While I certainly agree that a quantifiable group utility function is usually difficult, if you go up a level of meta you'll see that well-defined charters are everywhere: a decision is correct if and only if the people in power judge it to have been correct. To be a democracy, we don't need to explicitly vote on values&mdash;we just need to let people vote on consequences in accordance with their values. The king's order may be ambiguously worded, but your true duty is clear: please the king.</span></p>\n<p><span> There are some clear advantages to histocracy over futarchy: most relevantly, I believe histocracy will work well on a small scale, while prediction markets require a large crowd. Given enough time and participation, histocracy will inevitably beat a market. There's less moral hazard, and less vulnerability to manipulation.</span></p>\n<p><span> Futarchy beats histocracy in that there's a built-in incentive to participate and excel: but people vote in elections and serve on non-profit boards for free, so I don't see a huge need to inject cash. Futarchy allows for individual actors to express degrees of confidence in a way that my model of histocracy doesn't, but this could be remedied where feasible. And Hanson's ideas for how to judge consequences in hindsight might be appropriate for some histocracies.</span></p>\n<p><span> The potential pitfalls of histocracy depend on the specific implementation. I see politics, in the blue vs. green mind-killing sense, and difficulty of evaluating consequences even in hindsight as the two major Achilles heels; but as far as I can see these are universal. There is a danger of a subgroup amassing a large voting weight, then abusing it in the window before they are removed from power, which can perhaps best be guarded against with some sort of constitutional system, perhaps even one formally incorporated into the system as a high Bayesian prior against certain classes of actions being correct.</span></p>\n<p><span> I should also concede up front that my &ldquo;mathematical&rdquo; appendix glosses over the serious AI challenge of doing this right: hopefully, the computing power available to a histocracy will grow much faster than the number of voters. Log(LaPlace(Record)) will double-count terribly in large groups, but it does have the advantage of being simple and transparent&mdash;entrusting your government to a black box is scary.</span></p>\n<p><span> Groups giving histocracy a try should start by making it nonbinding. Only when it's working better than your current system should it be adopted. Unless, of course, your current system is a majority vote, in which case you might as well start using it right away.</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RGPpwYoCHrPNB86TW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MDQnDGEKQuCAggRLe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 19, "extendedScore": null, "score": 8.333999184564051e-07, "legacy": true, "legacyId": "12244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zjNWfNpDmorQSFukR", "mWb2cCqjvjng7Pzcp", "Z263n4TXJimKn6A8Z"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-18T01:45:13.583Z", "modifiedAt": null, "url": null, "title": "Group Funding", "slug": "group-funding", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.487Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PPuvrRnBoPoRwbGzs/group-funding", "pageUrlRelative": "/posts/PPuvrRnBoPoRwbGzs/group-funding", "linkUrl": "https://www.lesswrong.com/posts/PPuvrRnBoPoRwbGzs/group-funding", "postedAtFormatted": "Wednesday, January 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Funding&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Funding%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPuvrRnBoPoRwbGzs%2Fgroup-funding%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Funding%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPuvrRnBoPoRwbGzs%2Fgroup-funding", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPPuvrRnBoPoRwbGzs%2Fgroup-funding", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p>There are more people who would like to do research than people that would like to fund them.&nbsp; Many of these people could be earning 10x their living expenses.&nbsp; Why don't they get together in groups of 10 with similar goals and draw lots to see which one of them gets a job, and that person funds the others?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PPuvrRnBoPoRwbGzs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 4, "extendedScore": null, "score": 8.334720377051665e-07, "legacy": true, "legacyId": "12257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-18T01:55:43.372Z", "modifiedAt": null, "url": null, "title": "Ethics of piracy", "slug": "ethics-of-piracy", "viewCount": null, "lastCommentedAt": "2012-01-22T19:50:04.077Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PuyaSharif", "createdAt": "2011-02-26T07:33:06.094Z", "isAdmin": false, "displayName": "PuyaSharif"}, "userId": "Kx2AumHK8eeJ4nHqt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ryuRpfXhPCw3tEQ6/ethics-of-piracy", "pageUrlRelative": "/posts/6ryuRpfXhPCw3tEQ6/ethics-of-piracy", "linkUrl": "https://www.lesswrong.com/posts/6ryuRpfXhPCw3tEQ6/ethics-of-piracy", "postedAtFormatted": "Wednesday, January 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethics%20of%20piracy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthics%20of%20piracy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ryuRpfXhPCw3tEQ6%2Fethics-of-piracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethics%20of%20piracy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ryuRpfXhPCw3tEQ6%2Fethics-of-piracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ryuRpfXhPCw3tEQ6%2Fethics-of-piracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 359, "htmlBody": "<p>&nbsp;</p>\n<p>I could discuss the large scale effects of piracy (copyright infringement) for days! From a game-theoretical/utilitarian -, ethical - or any other perspective. I have a set of views and suggestions for topics that could be interesting to break down and address, but instead of writing a long post addressing many different topics, Ill start with the first one in my mind.</p>\n<p>\n<hr />\n</p>\n<p>Just a thought:</p>\n<p>For a subset of activities you could map the question of the ethical status of illegal downloading of a software <em>p</em> (preferred choice) to the existence of a certain kind of element <em>a</em> in a set <em>S,</em> which I'll call the <em>set of alternatives</em> (assuming the risk of getting caught is very small).</p>\n<p>Lets say that you for some reason need a graphics editor and your preferred choice is Photoshop CS5. You could either:</p>\n<ol>\n<li>Buy it (650$ on Amazon).</li>\n<li>Download it (free)</li>\n</ol>\n<p>In the case you have chosen to illegally download a copy of the software, some people would compare that to stealing (certainly the folks at Adobe). Would that really be fair to say? At least in my opinion that depends on whether or not you would have bought a copy in the absence of the 'download' alternative. Your preferred choice is indeed Photoshop CS5, but that is one among many choices, the rest being in the <em>set of alternatives</em> <em>S</em>. Most users with illegal copies wouldn't pay the 650$ when there are free alternatives. Those alternatives may be much less attractive with less features&nbsp; but many of them would still do the job.</p>\n<p>So if there exist an <em>a</em> in <em>S</em>, such that you would prefer <em>a</em> over <em>p </em>in the absence of alternative 2, then in a game between you and Adobe, the choice <em>a </em>would not be Pareto optimal. Your utility is maximized by choosing <em>p</em> (downloading Photoshop), Adobes utility left unchanged. --&gt; Maximizing total utility (ignoring potential side-effects, such as effects overall attitude towards piracy and so on)</p>\n<p>Today there exists an <em>S</em> for almost anything.</p>\n<p>Whats your opinion on this in regards to utility maximization (utility of society). Can we really break it down like this looking at the individual case?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ryuRpfXhPCw3tEQ6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -18, "extendedScore": null, "score": -8e-06, "legacy": true, "legacyId": "12258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-18T05:53:32.129Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] How An Algorithm Feels From Inside", "slug": "seq-rerun-how-an-algorithm-feels-from-inside", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:30.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RciNGxAp8JkTsJEAo/seq-rerun-how-an-algorithm-feels-from-inside", "pageUrlRelative": "/posts/RciNGxAp8JkTsJEAo/seq-rerun-how-an-algorithm-feels-from-inside", "linkUrl": "https://www.lesswrong.com/posts/RciNGxAp8JkTsJEAo/seq-rerun-how-an-algorithm-feels-from-inside", "postedAtFormatted": "Wednesday, January 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20How%20An%20Algorithm%20Feels%20From%20Inside&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20How%20An%20Algorithm%20Feels%20From%20Inside%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRciNGxAp8JkTsJEAo%2Fseq-rerun-how-an-algorithm-feels-from-inside%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20How%20An%20Algorithm%20Feels%20From%20Inside%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRciNGxAp8JkTsJEAo%2Fseq-rerun-how-an-algorithm-feels-from-inside", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRciNGxAp8JkTsJEAo%2Fseq-rerun-how-an-algorithm-feels-from-inside", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>Today's post, <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">How An Algorithm Feels From Inside</a> was originally published on 11 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You argue about a category membership even after screening off all questions that could possibly depend on a category-based inference. After you observe that an object is blue, egg-shaped, furred, flexible, opaque, luminescent, and palladium-containing, what's left to ask by arguing, \"Is it a blegg?\" But if your brain's categorizing neural network contains a (metaphorical) central unit corresponding to the inference of blegg-ness, it may still feel like there's a leftover question.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9ft/seq_rerun_neural_categories/\">Neural Categories</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RciNGxAp8JkTsJEAo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 8.33566548497769e-07, "legacy": true, "legacyId": "12263", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yA4gF5KrboK2m2Xu7", "XDQzobvDbHZZCigXy", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-18T18:35:18.867Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-9", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z9nG45Jb4z7S8LsKd/meetup-austin-tx-9", "pageUrlRelative": "/posts/Z9nG45Jb4z7S8LsKd/meetup-austin-tx-9", "linkUrl": "https://www.lesswrong.com/posts/Z9nG45Jb4z7S8LsKd/meetup-austin-tx-9", "postedAtFormatted": "Wednesday, January 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ9nG45Jb4z7S8LsKd%2Fmeetup-austin-tx-9%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ9nG45Jb4z7S8LsKd%2Fmeetup-austin-tx-9", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ9nG45Jb4z7S8LsKd%2Fmeetup-austin-tx-9", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/68'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 January 2012 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Welcome back! UT is back in full swing and so are we. We sit on the second floor to the left, near (or often on) the stage. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/68'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z9nG45Jb4z7S8LsKd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.338566133092982e-07, "legacy": true, "legacyId": "12272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/68\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 January 2012 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Welcome back! UT is back in full swing and so are we. We sit on the second floor to the left, near (or often on) the stage. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/68\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-18T19:48:58.375Z", "modifiedAt": null, "url": null, "title": "[Meta] No LessWrong Blackout?", "slug": "meta-no-lesswrong-blackout", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CaveJohnson", "createdAt": "2011-04-29T11:11:15.254Z", "isAdmin": false, "displayName": "CaveJohnson"}, "userId": "EK4shk2WdXy7LxDmB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YFNmYCn3waR2bT8xv/meta-no-lesswrong-blackout", "pageUrlRelative": "/posts/YFNmYCn3waR2bT8xv/meta-no-lesswrong-blackout", "linkUrl": "https://www.lesswrong.com/posts/YFNmYCn3waR2bT8xv/meta-no-lesswrong-blackout", "postedAtFormatted": "Wednesday, January 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMeta%5D%20No%20LessWrong%20Blackout%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMeta%5D%20No%20LessWrong%20Blackout%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFNmYCn3waR2bT8xv%2Fmeta-no-lesswrong-blackout%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMeta%5D%20No%20LessWrong%20Blackout%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFNmYCn3waR2bT8xv%2Fmeta-no-lesswrong-blackout", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFNmYCn3waR2bT8xv%2Fmeta-no-lesswrong-blackout", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 172, "htmlBody": "<p>Our sister site apparently is:</p>\n<blockquote>\n<h3>Overcoming Bias will resume normal service on Jan 18 8:00pm (Eastern Time).</h3>\n<p>Today Overcoming Bias joins <a href=\"http://wikimediafoundation.org/wiki/Press_releases/English_Wikipedia_to_go_dark\">Wikipedia</a> and <a href=\"http://www.sopastrike.com/\">many other sites</a> in protesting proposed legislation &mdash; the Stop Online Piracy Act (SOPA) in the U.S. House of Representatives, and the PROTECTIP Act (PIPA) in the U.S. Senate.</p>\n<p>If passed, SOPA and PIPA will give the US Justice Department and courts tremendous power to shut down entire sites. These bills endanger free speech both in the United States and abroad, potentially setting a frightening precedent of Internet censorship for the world.</p>\n<p>You can find out more about the impact of SOPA and PIPA from the <a href=\"https://www.eff.org/deeplinks/2012/01/how-pipa-and-sopa-violate-white-house-principles-supporting-free-speech\">Electronic Frontier Foundation</a>.</p>\n</blockquote>\n<p>Interesting, my model of Robin Hanson had him say something about the blackout and how it shows people are hypocrites. Though obviously he has strong opinions on intellectual property. I think it would have been a good idea to Blackout LessWrong today. It would have given us a status boost in most of the communities we frequent.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YFNmYCn3waR2bT8xv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 23, "extendedScore": null, "score": 8.338846699341836e-07, "legacy": true, "legacyId": "12273", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-18T22:30:56.058Z", "modifiedAt": null, "url": null, "title": "The Singularity Institute's Arrogance Problem", "slug": "the-singularity-institute-s-arrogance-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:36.221Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7bdLucRPBq334kFkm/the-singularity-institute-s-arrogance-problem", "pageUrlRelative": "/posts/7bdLucRPBq334kFkm/the-singularity-institute-s-arrogance-problem", "linkUrl": "https://www.lesswrong.com/posts/7bdLucRPBq334kFkm/the-singularity-institute-s-arrogance-problem", "postedAtFormatted": "Wednesday, January 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Singularity%20Institute's%20Arrogance%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Singularity%20Institute's%20Arrogance%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7bdLucRPBq334kFkm%2Fthe-singularity-institute-s-arrogance-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Singularity%20Institute's%20Arrogance%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7bdLucRPBq334kFkm%2Fthe-singularity-institute-s-arrogance-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7bdLucRPBq334kFkm%2Fthe-singularity-institute-s-arrogance-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 328, "htmlBody": "<p>I intended <a href=\"/lw/9fy/leveling_up_in_rationality_a_personal_journey/\">Leveling Up in Rationality</a> to communicate this:</p>\n<blockquote>\n<p>Despite worries that <a href=\"/lw/9p/rationality_its_not_that_great/\">extreme rationality isn't that great</a>, I think there's reason to hope that it <em>can</em>&nbsp;be great if some other causal factors are flipped the right way (e.g. mastery over akrasia). Here are some detailed examples I can share because they're from my own life...</p>\n</blockquote>\n<p>But some people seem to have read it and heard this instead:</p>\n<blockquote>\n<p>I'm super-awesome. Don't you wish you were more like me? Yay rationality!</p>\n</blockquote>\n<p>This failure (on my part) fits into a larger pattern of the Singularity Institute <em>seeming</em>&nbsp;too arrogant and (perhaps)&nbsp;<em>being</em>&nbsp;too arrogant. As one friend recently told me:</p>\n<blockquote>\n<p>At least among&nbsp;Caltech undergrads and academic mathematicians, it's taboo to toot your own&nbsp;horn. In these worlds, one's achievements speak for themselves, so whether one&nbsp;is a Fields Medalist or a failure, one gains status purely passively, and must&nbsp;appear not to care about being smart or accomplished. I think because you and&nbsp;Eliezer don't have formal technical training, you don't instinctively grasp&nbsp;this taboo. Thus Eliezer's claim of world-class mathematical ability, in&nbsp;combination with his lack of technical publications, make it hard for a&nbsp;mathematician to take him seriously, because his social stance doesn't&nbsp;pattern-match to anything good. Eliezer's arrogance as evidence of technical&nbsp;cluelessness, was one of the reasons I didn't donate until I met [someone at SI in person]. So for&nbsp;instance, your <a href=\"/lw/9fy/leveling_up_in_rationality_a_personal_journey/\">boast</a> that at SI discussions \"everyone at the table knows and&nbsp;applies an insane amount of all the major sciences\" would make any Caltech&nbsp;undergrad roll their eyes; your standard of an \"insane amount\" seems to be&nbsp;relative to the general population, not relative to actual scientists. And&nbsp;posting a list of powers you've acquired doesn't make anyone any more impressed&nbsp;than they already were, and isn't a high-status move.</p>\n</blockquote>\n<p>So, I have a few questions:</p>\n<p>&nbsp;</p>\n<ol>\n<li>What are the most egregious examples of SI's arrogance?</li>\n<li>On which subjects and in which ways is SI too arrogant? Are there subjects and ways in which SI isn't arrogant enough?</li>\n<li>What should SI do about this?</li>\n</ol>\n<div><br /></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "2EFq8dJbxKNzforjM": 1, "Q6P8jLn8hH7kbuXRr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7bdLucRPBq334kFkm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 84, "extendedScore": null, "score": 0.000191, "legacy": true, "legacyId": "12274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 84, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 308, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFYWpLNfdwnDwQ3wy", "LgavAYtzFQZKg95WC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T00:56:07.321Z", "modifiedAt": null, "url": null, "title": "The problem with too many rational memes", "slug": "the-problem-with-too-many-rational-memes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.874Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/stb3Jjumzhv49zCEb/the-problem-with-too-many-rational-memes", "pageUrlRelative": "/posts/stb3Jjumzhv49zCEb/the-problem-with-too-many-rational-memes", "linkUrl": "https://www.lesswrong.com/posts/stb3Jjumzhv49zCEb/the-problem-with-too-many-rational-memes", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20problem%20with%20too%20many%20rational%20memes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20problem%20with%20too%20many%20rational%20memes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fstb3Jjumzhv49zCEb%2Fthe-problem-with-too-many-rational-memes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20problem%20with%20too%20many%20rational%20memes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fstb3Jjumzhv49zCEb%2Fthe-problem-with-too-many-rational-memes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fstb3Jjumzhv49zCEb%2Fthe-problem-with-too-many-rational-memes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 969, "htmlBody": "<p>\n<p>Like so many of my posts, this one starts with a personal anecdote.&nbsp;</p>\n<p>A few weeks ago, my boyfriend was invited to a community event through <a href=\"http://www.meetup.com/\">Meetup.com</a>. The purpose of the meetup was to watch the movie <a href=\"http://www.youtube.com/watch?v=ULlR_pkHjUQ\">The Elegant Universe</a> and follow up with a discussion. As it turns out, this particular meetup was run by a man who I&rsquo;ll call &lsquo;Charlie&rsquo;, the leader of some local Ottawa group designed to help new immigrants to Canada find a social support net. Which, in my mind, is an excellent goal.&nbsp;</p>\n<p>Charlie turned out to be a pretty neat guy, too: charismatic, funny, friendly, encouraging everyone to share his or her opinion. Criticizing or shutting out other people&rsquo;s views was explicitly forbidden. It was a diverse group, as he obviously wanted it to be, and by the end everyone seemed to feel pretty comfortable.&nbsp;</p>\n<p>My boyfriend, an extremely social being whose main goal in life is networking, was raving by the end about what a neat idea it was to start this kind of group, and how Charlie was a really cool guy. I was the one who should have had fun, since I&rsquo;m about 100 times more interested in physics than he is, but I was fuming silently.&nbsp;</p>\n<p>Why? Because, at various points in the evening, Charlie talked about his own interest in the paranormal and the spiritual, and the books he&rsquo;d written about it. When we were discussing string theory and its extra dimensions, he made a comment, the gist of which was &lsquo;if people&rsquo;s souls go to other dimensions when they die, Grandma could be communicating with you right now from another dimension by tapping spoons.&rsquo;&nbsp;</p>\n<p>Final straw. I bit my tongue and didn&rsquo;t say anything and tried not to show how irritated I was. Which is strange, because I&rsquo;ve always been fairly tolerant, fairly <a href=\"/lw/6dm/reasons_for_being_rational/\">agreeable</a>, and very eager to please others. Which is why, when my brain responded &lsquo;because he&rsquo;s WRONG and I can&rsquo;t call him out on it because of the no criticism rule!&rsquo; to the query of &lsquo;why are you pissed off?&rsquo;, I was a bit suspicious of that answer.&nbsp;</p>\n<p>I do think that Charlie is wrong. I would have thought he was wrong a long time ago. But it wouldn&rsquo;t have bothered me; I know that because I managed to attend various churches for years, even though I thought a lot of their beliefs were wrong, because it didn&rsquo;t matter. They had certain goals in common with me, like wanting to make the world a better place, and there were certain things I could get out of being a community member, like incredibly peaceful experiences of bliss that would reset my always-high stress levels to zero and allow me to survive the rest of the week. Some of the sub-goals they had planned to make the world a better place, like converting people in Third World countries to Christianity, were ones that I thought were sub-optimal or even damaging. But overall, there were more goals we had in common than goals we didn&rsquo;t have in common, and I could, I judged, accomplish those goals we had in common more effectively with them than on my own. And anyway, the church would still be there whether or not I went; if I did go, at least I could talk about stuff like physics with awe and joy (no faking required, thinking about physics does make me feel awe and joy), and increase some of the congregation&rsquo;s scientific literacy a little bit.&nbsp;</p>\n<p>Then I stopped going to church, and I started spending more time on Less Wrong, and if I were to try to go back, I&rsquo;m worried it would be exactly the same as the community meetup. I would sit there fuming because they were wrong and it was socially unacceptable for me to tell them that.&nbsp;</p>\n<p>I&rsquo;m worried because I don&rsquo;t think those feelings are the result of a clearheaded, logical value calculation. Yeah, churches and people who believe in the paranormal <a href=\"/lw/2ku/welcome_to_less_wrong_20102011/5oi2?context=3\">waste a lot of money and energy</a>, which could be spent on really useful things otherwise. Yes, that could be a valid reason to reject them, to refuse to be their allies even if some of your goals are the same. But it&rsquo;s not my <a href=\"/lw/wj/is_that_your_true_rejection/\">true rejection</a>. My true rejection is that them being wrong is too annoying for me to want to cooperate. Why? I haven&rsquo;t changed my mind, really, about how much damage versus good I think churches do for the world.&nbsp;</p>\n<p>I&rsquo;m worried that the same process which normalized religion for me is now operating in the opposite direction. I&rsquo;m worried that a lot of Less Wrong memes, ideas that show membership to the &lsquo;rationalist&rsquo; or &lsquo;skeptic&rsquo; cultures, such as atheism itself, or the idea that <a href=\"/lw/11m/atheism_untheism_antitheism/\">religion is bad for humanity</a>...I&rsquo;m worried that they&rsquo;re sneaking into my head and becoming virulent, that I'm becoming an <a href=\"/lw/1ww/undiscriminating_skepticism/\">undiscriminating skeptic</a>. Not because I&rsquo;ve been presented with way more evidence for them, and updated on my beliefs (although I have updated on some beliefs based on things I read here), but because that agreeable, eager-to-please subset of my brains sees the Less Wrong community and wants to fit in. There&rsquo;s a part of me that evaluates what I read, or hear people say, or find myself thinking, and imagines Eliezer&rsquo;s response to it. And if that response is negative...ooh, mine had better be negative too.&nbsp;</p>\n<p>And that&rsquo;s not strategic, optimal, or rational. In fact, it&rsquo;s preventing me from doing something that might otherwise be a goal for me: joining and volunteering and becoming active in a group that does good things for the Ottawa community. And this transformation has managed to happen without me even noticing, which is a bit scary. I&rsquo;ve always thought of myself as someone who was aware of my own thoughts, but apparently not.&nbsp;</p>\n<p>Anyone else have the same experience?&nbsp;</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 1, "wzgcQCrwKfETcBpR9": 1, "9YFoDPFwMoWthzgkY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "stb3Jjumzhv49zCEb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 119, "baseScore": 118, "extendedScore": null, "score": 0.000235, "legacy": true, "legacyId": "12241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 118, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 342, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JogmRwWLWwTMCecy9", "TGux5Fhcd7GmTfNGC", "PYtus925Gcg7cqTEq", "Jko7pt7MwwTBrfG3A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T04:05:45.033Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Disputing Definitions", "slug": "seq-rerun-disputing-definitions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:57.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gTWD2Tx8MsymmavdW/seq-rerun-disputing-definitions", "pageUrlRelative": "/posts/gTWD2Tx8MsymmavdW/seq-rerun-disputing-definitions", "linkUrl": "https://www.lesswrong.com/posts/gTWD2Tx8MsymmavdW/seq-rerun-disputing-definitions", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Disputing%20Definitions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Disputing%20Definitions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTWD2Tx8MsymmavdW%2Fseq-rerun-disputing-definitions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Disputing%20Definitions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTWD2Tx8MsymmavdW%2Fseq-rerun-disputing-definitions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTWD2Tx8MsymmavdW%2Fseq-rerun-disputing-definitions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>Today's post, <a href=\"/lw/np/disputing_definitions/\">Disputing Definitions</a> was originally published on 12 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You allow an argument to slide into being about definitions, even though it isn't what you originally wanted to argue about. If, before a dispute started about whether a tree falling in a deserted forest makes a \"sound\", you asked the two soon-to-be arguers whether they thought a \"sound\" should be defined as \"acoustic vibrations\" or \"auditory experiences\", they'd probably tell you to flip a coin. Only after the argument starts does the definition of a word become politically charged.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9gn/seq_rerun_how_an_algorithm_feels_from_inside/\">How An Algorithm Feels From Inside</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gTWD2Tx8MsymmavdW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.340739367431546e-07, "legacy": true, "legacyId": "12291", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7X2j8HAkWdmMoS8PE", "RciNGxAp8JkTsJEAo", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T06:36:25.414Z", "modifiedAt": null, "url": null, "title": "How to un-kill your mind - maybe.", "slug": "how-to-un-kill-your-mind-maybe", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "APMason", "createdAt": "2011-08-30T22:24:06.796Z", "isAdmin": false, "displayName": "APMason"}, "userId": "2QGaDhC6QbaR5sLh8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wh5rHgBfeuhaM5Xeo/how-to-un-kill-your-mind-maybe", "pageUrlRelative": "/posts/Wh5rHgBfeuhaM5Xeo/how-to-un-kill-your-mind-maybe", "linkUrl": "https://www.lesswrong.com/posts/Wh5rHgBfeuhaM5Xeo/how-to-un-kill-your-mind-maybe", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20un-kill%20your%20mind%20-%20maybe.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20un-kill%20your%20mind%20-%20maybe.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWh5rHgBfeuhaM5Xeo%2Fhow-to-un-kill-your-mind-maybe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20un-kill%20your%20mind%20-%20maybe.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWh5rHgBfeuhaM5Xeo%2Fhow-to-un-kill-your-mind-maybe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWh5rHgBfeuhaM5Xeo%2Fhow-to-un-kill-your-mind-maybe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 837, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">It has been the case since I had opinions on these things that I have struggled to identify my &ldquo;favourite writer of all time&rdquo;. I've thought perhaps it was Shakespeare, as everyone does &ndash; who composed over thirty plays in his lifetime, from any of which a single line would be so far beyond my ability as to make me laughable. Other times I've thought it may be Saul Bellow, who seems to understand human nature in an intuitive way I can't quite reach, but which always touches me when I read his books. And more often than not I've thought it was Raymond Chandler, who in each of his seven novels broke my heart and refused to apologise &ndash; because he knew what kind of universe we live in. But since perhaps the year 2007, I have, or should I say had, not been in the slightest doubt as to who my favourite living writer was &ndash; Christopher Eric Hitchens.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">This post is not about how much I admired him. It's not about how surprisingly upset I was about his death (I have since said that I didn't know him except through his writing &ndash; a proposition something like &ldquo;I didn't have sex with her except through her vagina&rdquo;) - although I must say that even now thinking about this subject is having rather more of an effect on me than I would like. This post is about a rather strange change that has come over me since his death on the 15<sup>th</sup> of December. Before that time I was a staunch defender of the proposition that the removal of Saddam Hussein from power in Iraq was an obvious boon to the human race, and that the war in Iraq was therefore a wise and moral undertaking. Since then, however, I have found my opinion softening on the subject &ndash; I have found myself far more open to cost/ benefit analyses that have come down on the side of non-intervention, and much less indignant when others disagreed. It still seems to me that there are obvious benefits that have arisen from the war in Iraq &ndash; by no means am I willing to admit that it was an utter catastrophe, as so many seem convinced it was &ndash; but I have found my opinion shifting toward the non-committal middle ground of &ldquo;I dunno&rdquo;.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Well, Mrs. Mason didn't raise all that many fools. It could be that what's happening here is I'm identifying closely with the Ron Paul campaign, and that since I agree with Paul on many things but not on American foreign policy (and, as it happens, I'm British &ndash; but consider myself internationalist enough that American arguments significantly influence my views), and so am shifting towards his point of view. But I think it's rather more likely &ndash; embarrassing as this is to admit &ndash; that the sheer fact that the Hitch could no longer possibly be my friend &ndash; could no longer congratulate me on my enlightened point of view, or go into coalition with me against the forces of irrationality &ndash; has freed up my opinions on the Iraq war, and I have dropped into the centre-ground of &ldquo;Not enough information&rdquo;. This, as I said, is embarrassing &ndash; whether or not the best writer in the world approves of your opinion is no basis for sticking to it. But this is the position I find myself in: weak; fragile; irrational &ndash; at least as far as politics go.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">So here is my half-way solution: extreme and not perfect, by any means, but I think, given the unearthing of this appalling weakness, necessary: from this point onwards, until January 1<sup>st</sup> 2013 (yes, an arbitrary point in the future), I am not allowed to settle on a political or moral opinion (ethics &ndash; the question of what constitutes the good life - I consider comparatively easy, and so exempt). Even when presented with apparently knock-down arguments, I am forbidden from professing allegiance from any moral or political position for the rest of the year. Yes, it is going to be hard to prevent myself from deciding on moral questions, or on political questions &ndash; but I am hoping that if I can at least prevent myself from defending any position for the rest of the year, I will, at the end of it, no longer be emotionally attached to any particular ideology, and be able to assess the difference at least semi-rationally. I don't want to believe anything just because Hitchens believed it. I don't want to be motivated by perceived-but-illusory friendship. I want the right answer. And I'm hoping that depriving my brain of the reinforcement that becoming part of a team &ndash; no matter how small &ndash; gives, I will be able to consider the matter rationally.</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">Until 2013, then, this is it for me. No longer are Marxism, fascism, anarcho-syndicalism etc. incorrect. They're interesting ideas, and I'd like to hear more about them. This is my slightly-less-than-a-year off from ideology. Let's hope that it works.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wh5rHgBfeuhaM5Xeo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 7, "extendedScore": null, "score": 8.341313565081974e-07, "legacy": true, "legacyId": "12298", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T06:50:04.438Z", "modifiedAt": null, "url": null, "title": "POSITION: Design and Write Rationality Curriculum", "slug": "position-design-and-write-rationality-curriculum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.836Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ifL8f4Xzy2D9Bb6zs/position-design-and-write-rationality-curriculum", "pageUrlRelative": "/posts/ifL8f4Xzy2D9Bb6zs/position-design-and-write-rationality-curriculum", "linkUrl": "https://www.lesswrong.com/posts/ifL8f4Xzy2D9Bb6zs/position-design-and-write-rationality-curriculum", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20POSITION%3A%20Design%20and%20Write%20Rationality%20Curriculum&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APOSITION%3A%20Design%20and%20Write%20Rationality%20Curriculum%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifL8f4Xzy2D9Bb6zs%2Fposition-design-and-write-rationality-curriculum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=POSITION%3A%20Design%20and%20Write%20Rationality%20Curriculum%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifL8f4Xzy2D9Bb6zs%2Fposition-design-and-write-rationality-curriculum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifL8f4Xzy2D9Bb6zs%2Fposition-design-and-write-rationality-curriculum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1402, "htmlBody": "<p><strong>Update March 2012: We are still accepting and processing applications for this work on an ongoing basis.</strong></p>\n<p>Imagine trying to learn baseball by reading essays about baseball techniques. [1]</p>\n<p>We're trying to make the jump to teaching people rationality by, metaphorically speaking, having them throw, catch, and hit baseballs in the company of friends. And as we develop curriculum to do that, we're noticing that we often improve quite a lot ourselves in the course of coming up with 20 examples of the sunk cost fallacy. This suggests that the best of us have a lot to gain from practicing basic skills more systematically. Quoth Anna Salamon:</p>\n<blockquote>\n<p>There are huge numbers of basic, obviously useful rationality habits that I do about 10% as often as it would be useful to do them. Like \"run cheap experiments/tests often&rdquo;, and &ldquo;notice mental flinches, and track down the thought you&rsquo;re avoiding&rdquo;.</p>\n</blockquote>\n<p>Eliezer Yudkowsky, Anna Salamon, several others paid on an hourly basis, and a few volunteers, have been designing exercises and exercise-sets for a rationality curriculum. Our current working point is on the exercises for \"Motivated Cognition\". Currently the only completed session is \"Sunk Costs\", which is still being tested - yes, we're actually testing these things repeatedly as we build them. The main purpose of the sessions is to be performed in person, <em>not </em>read online, but nonetheless the current version of the Sunk Costs material - presentation and exercise booklets - is available as a sample: [<a href=\"http://dl.dropbox.com/u/163098/Sunk%20costs%20kata/sunkcostskata.pptx\">0</a>] [<a href=\"http://dl.dropbox.com/u/163098/Sunk%20costs%20kata/SunkCostBooklets-skill1.pdf\">1</a>] [<a href=\"http://dl.dropbox.com/u/163098/Sunk%20costs%20kata/SunkCostBooklets-skill2.pdf\">2</a>] [<a href=\"http://dl.dropbox.com/u/163098/Sunk%20costs%20kata/SunkCostBooklets-skill3.pdf\">3</a>] [<a href=\"http://dl.dropbox.com/u/163098/Sunk%20costs%20kata/SunkCostBooklets-skill4.pdf\">4</a>] [<a href=\"http://dl.dropbox.com/u/163098/Sunk%20costs%20kata/SunkCostBooklets-skill5-a.pdf\">5</a>]. This is a presentation on sunk costs in which background explanations are interspersed with \"do as many of these exercises as you can in 3 minutes\", followed by \"now pair up with others to do the 'transfer step' parts where you look for instances in your past life and probable future life.\"</p>\n<p>We're looking for 1-2 fulltime employees who can help us build more things like that (unless the next round of tests shows that the current format doesn't work), and possibly a number of hourly contractors (who may be local or distant). We will definitely want to try your work on an hourly or monthly basis before making any full-time hires.</p>\n<p><a id=\"more\"></a>The <em>complete </em>labor for building a rationality kata - <strong>we are <em>not</em> looking for someone who can do all of this work at once, we are looking for anyone who can do one or more steps</strong> - looks something like this:</p>\n<p>Select an important rationality skill and clearly perceive the sort of thinking that goes into executing it. Invent several new exercises which make people's brains execute that type of thinking. Compose many instances of those exercises. Compose any background explanations required for the skills. Figure out three things to tell people to watch out for, or do, over the next week. Turn all of that into a complete 90-minute user experience which includes random cute illustrations for the exercise booklets, designing graphics for any low-level technical points made, building a presentation, testing it in front of a live naive audience, making large changes, and testing it again.</p>\n<p>We are <em>not</em> looking only for people who can do all of this labor simultaneously. If you think you can help on <em><strong>one or more</strong> </em>of those steps, consider <a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dElRYS1hY3N4a3JIRV90R0lmdE9MN0E6MQ\">applying</a> &mdash; for a full-time job, a part-time hourly gig (perhaps from a distance), or as a volunteer position. We will want anyone hired to try hourly work or a trial month before making any full-time hires. Salary will be SIAI-standard, i.e. $3K/month, but if you do strong work <em>and </em>Rationality-Inst takes off your salary will <em>eventually </em>go much higher. Very strong candidates who can do large amounts of work independently may request higher salaries. You will be working mostly with Anna Salamon and will report to her (although in the short term you may also be working directly with Eliezer on the \"isolate a useful skill and invent new exercises to develop it\" phase).</p>\n<p>If you think you have the idea for a complete rationality kata and want to develop the entire thing on your own, <a href=\"mailto:anna@intelligence.org\">send us a short email</a> about your idea - we're open to setting a lump-sum price.</p>\n<hr />\n<p><em>Skills needed:</em></p>\n<p>We need folks with at least <em>one </em>of the following skills (do <em>not </em><strong>feel you need them all; </strong>you'll be <strong>part of a </strong><strong><em>team</em>;</strong> and <em>repeated</em> experience shows that <strong>the people we end up actually hiring, report that they almost didn't contact us because they thought they weren't worthy</strong>):</p>\n<ul>\n<li>Catchy professional writing. We need folks who can take rough-draft exercises and explanations, and make them fun to read &mdash; at the level of published books.</li>\n</ul>\n<ul>\n<li>Curriculum design. We need folks who can zoom in on the component skills for rationality (the analogs of throwing, catching, keeping your eye on the ball), and who can invent new exercises that systematically practice those components. E.g., the thought process that goes from \"sunk cost fallacy\" to \"<span style=\"text-decoration: underline;\">transform a sunk cost to a purchased option</span>\".</li>\n</ul>\n<ul>\n<li>Example generation. Given an exercise, we need someone who can think of lots of specific examples from real life or important real-world domains, which illustrate the exact intended point and not something almost-like the intended point. E.g., turn \"Sunk cost fallacy\" into 20 story snippets like \"Lara is playing poker and has bet $200 in previous rounds...\" (Our experience shows that this is a key bottleneck in writing a kata, and a surprisingly separate capacity from coming up with the <em>first</em> exercise.)</li>\n</ul>\n<ul>\n<li>Teaching or tutoring experience in whichever subjects (e.g., math / programming / science, martial arts / sports / dance, cognitive behavioral therapy, corporate trainings, social skills, meditation);</li>\n</ul>\n<ul>\n<li>Technical diagram design. We need someone who can be asked for \"A diagram that somehow represents the human tendency to overweight near pains relative to distant pains\", understand the concept that is being conveyed, and invent a new diagram that conveys it.</li>\n</ul>\n<ul>\n<li>Presentation design. The current intended form of a rationality kata involves a visual presentation with accompanying spoken words.</li>\n</ul>\n<ul>\n<li>Powerpoint and Photoshop polishing. See above.</li>\n</ul>\n<ul>\n<li>Illustration / cartooning. It would be nice if the exercises were accompanied by small, whimsical drawings. These drawings should prime the reader to both: (a) feel warmly toward the characters in the story-snippets (who will generally be struggling with rationality errors); (b) notice how ridiculous those characters, and the rest of us, are.</li>\n</ul>\n<ul>\n<li>Social initiative enough to gather guinea pigs and run many practice trials of draft curriculum, while collecting data.</li>\n</ul>\n<p>Bonuses:</p>\n<ul>\n<li>Skill at running scientific literature searches; knowledge of the heuristics and biases literature, the literature on how to teach critical thinking or <a href=\"http://web.mac.com/kstanovich/Site/Research_on_Reasoning.html\">rationality</a>, neuroscience literature, or other literatures that should inform our curriculum design;</li>\n</ul>\n<ul>\n<li>Background in game design, curriculum design, or in other disciplines that help with designing exercises that are fun and conducive to learning;</li>\n</ul>\n<ul>\n<li>Having read and understood the <a href=\"http://wiki.lesswrong.com/wiki/Sequences#Core_Sequences\">core Sequences</a>; having a serious interest in learning and teaching rationality.</li>\n</ul>\n<p>If this project appeals to you and you think you may have something to add, <a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dElRYS1hY3N4a3JIRV90R0lmdE9MN0E6MQ\">apply using this short form</a> or just <a href=\"mailto:anna@intelligence.org\">shoot us an email</a>. <em>Please</em> err on the side of applying; so many freaking amazing people have told us that they waited months before applying because they &ldquo;didn&rsquo;t want to waste our time&rdquo;, or didn&rsquo;t think they were good enough. This project needs many sorts of talents, and volunteers also welcome &mdash; so if you&rsquo;d like to help launch an awesome curriculum, send us an email. Your email doesn&rsquo;t have to be super-detailed or polished &mdash; just tell us how you might be able to contribute, and any experience we should know about.</p>\n<hr />\n<p>[1] If the baseball analogy seems far-fetched, consider algebra. To learn algebra, one typically drills one subskill at a time &mdash; one spends a day on exponent rules, for example, understanding why x^a * x^b = x^(a+b) and then practicing it bunches of times, in bunches of algebra problems, until it is a part of your problem-solving habits and reflexes, a step you can do fluently while attending to larger puzzles. If there were a world in which algebra had been learned only through reading essays, without subskill-by-subskill practice, it would not be surprising if the world&rsquo;s best algebra practitioners could be outperformed by an ordinary student who worked diligently through the exercises in a standard textbook. We&rsquo;d like you to help us build that first textbook.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ifL8f4Xzy2D9Bb6zs", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 75, "extendedScore": null, "score": 8.341365588605176e-07, "legacy": true, "legacyId": "12287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": "2019-06-14T20:18:04.351Z", "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": true, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 179, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T07:34:41.661Z", "modifiedAt": null, "url": null, "title": "The MIT Mystery Hunt and the Illusion of Transparency", "slug": "the-mit-mystery-hunt-and-the-illusion-of-transparency", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:31.753Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "novalis", "createdAt": "2010-02-22T00:37:37.766Z", "isAdmin": false, "displayName": "novalis"}, "userId": "wBabXhGtRaTr94FcJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9ik8K2q8YfAAd36vQ/the-mit-mystery-hunt-and-the-illusion-of-transparency", "pageUrlRelative": "/posts/9ik8K2q8YfAAd36vQ/the-mit-mystery-hunt-and-the-illusion-of-transparency", "linkUrl": "https://www.lesswrong.com/posts/9ik8K2q8YfAAd36vQ/the-mit-mystery-hunt-and-the-illusion-of-transparency", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20MIT%20Mystery%20Hunt%20and%20the%20Illusion%20of%20Transparency&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20MIT%20Mystery%20Hunt%20and%20the%20Illusion%20of%20Transparency%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ik8K2q8YfAAd36vQ%2Fthe-mit-mystery-hunt-and-the-illusion-of-transparency%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20MIT%20Mystery%20Hunt%20and%20the%20Illusion%20of%20Transparency%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ik8K2q8YfAAd36vQ%2Fthe-mit-mystery-hunt-and-the-illusion-of-transparency", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ik8K2q8YfAAd36vQ%2Fthe-mit-mystery-hunt-and-the-illusion-of-transparency", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 575, "htmlBody": "<p><a href=\"http://www.mit.edu/~puzzle\">The MIT Mystery Hunt</a> is a collection of puzzles, solved in teams over a long weekend every year. The prize for winning is that your team gets to write next year's hunt. Mystery Hunt puzzles are generally designed to take a few hours for a few people. A hunt typically has around 100 such puzzles, organized into a dozen or so metapuzzles; the metapuzzles can typically be solved with only a subset of the answers from the puzzles for that round, so not every puzzle needs to be solved to win.</p>\n<p>My team, Codex, won in 2011 and thus wrote the 2012 hunt, which has just concluded. I wanted to share some thoughts about the hunt, and also share one of the puzzles that didn't make it in, but that I think Less Wrong will appreciate.</p>\n<p><a href=\"http://blog.ezyang.com/2012/01/mystery-hunt-and-the-scientific-endeavour/\">Edward Z. Yang compared</a> the process of solving puzzles to science. It's not always that way -- in particular, <a href=\"http://www.mit.edu/~puzzle/00/set5/6/Puzzle.html\">Duck Konundrum</a> is the prototype of a class of puzzle which merely requires following a very complicated set of instructions, while <a href=\"http://web.mit.edu/puzzle/www/05/setec/square_mess/solution.html\">Square Mess</a> is a simple matter of programming (well, and univat n ovt rabhtu qvpgvbanel). But it's a pretty good way of looking at things.</p>\n<p>This year, I was a puzzle editor as well as an author. One of the things I learned about puzzles is that authors always think their puzzles are solvable, whether or not they are. This is the Illusion of Transparency in action -- it's obvious to <em>the author</em> how the puzzle ought to be solved. One job of editors is to ensure that every aha is properly clued, and that there is internal confirmation that solvers are on the right track. Internal confirmation means that when there are two steps to solving a puzzle, the intermediate result contains something intelligible even with omissions or errors. For example, if an intermediate result is a set of trigrams, those trigrams should be plausibly English-like. In nature, internal confirmation comes naturally, since all of nature follows a single set of rules. But in a puzzle, the rules are entirely arbitrary, so internal confirmation must be added.</p>\n<p>In past hunts, a number of puzzles went completely unsolved, because there wasn't a rigorous testsolving process. Some puzzles were released with serious undetected errors, and some puzzles were simply too hard. In 2012, every puzzle was solved forwards (that is, without inferring the answer from the constraints in the metapuzzle) at least once.</p>\n<p>The only way to tell if a puzzle really works is to have some solvers test it. Of course, these solvers can't just be people picked off the street -- they should be familiar with the conventions of the form (for instance, when converting between numbers and letters, A=1, and A+A=B, generally). Sometimes specialized knowledge is needed; some of the puzzles I wrote could not have been solved by non-programmers, and one of Codex's puzzles which failed testing required a solver with perfect pitch. But generally, it should be clear from looking at a puzzle what kind of knowledge is needed (at least for the first step). Codex avoided the problems of the past by testing every puzzle. Every puzzle that wasn't solved cleanly (and some that were) got revised and tested until it either passed, or was cut.</p>\n<p>One of the puzzles that failed testing was one that I wrote with Danielle Sucher and Emily Morgan: <a href=\"http://novalis.org/puzzles/writemore/writemore.html\">Write More</a>. We think Less Wrong readers might appreciate it anyway, so I'm posting it here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1ec": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9ik8K2q8YfAAd36vQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 20, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "12301", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T14:40:39.158Z", "modifiedAt": null, "url": null, "title": "A call for solutions and a tease of mine", "slug": "a-call-for-solutions-and-a-tease-of-mine", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:31.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zjNWfNpDmorQSFukR/a-call-for-solutions-and-a-tease-of-mine", "pageUrlRelative": "/posts/zjNWfNpDmorQSFukR/a-call-for-solutions-and-a-tease-of-mine", "linkUrl": "https://www.lesswrong.com/posts/zjNWfNpDmorQSFukR/a-call-for-solutions-and-a-tease-of-mine", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20call%20for%20solutions%20and%20a%20tease%20of%20mine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20call%20for%20solutions%20and%20a%20tease%20of%20mine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzjNWfNpDmorQSFukR%2Fa-call-for-solutions-and-a-tease-of-mine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20call%20for%20solutions%20and%20a%20tease%20of%20mine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzjNWfNpDmorQSFukR%2Fa-call-for-solutions-and-a-tease-of-mine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzjNWfNpDmorQSFukR%2Fa-call-for-solutions-and-a-tease-of-mine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 343, "htmlBody": "<p>So here's the problem: Given a well-defined group charter, how should groups make decisions? You have an issue, you've talked it over, and now it's time for the group to take action. Different members have different opinions, because they're not perfect reasoners and because their interests don't reliably align with those of the group. What do you do? Historical solutions include direct democracy, representative democracy, various hierarchies, dictatorships, oligarchies, consensus...But what's the <a href=\"http://xkcd.com/977/\">shoes-with-toes</a> solution? How do they do it in Weirdtopia? What is the universally correct method that could be implemented by organizations, corporations, and governments alike?</p>\n<p style=\"margin-bottom: 0in;\"><a id=\"more\"></a></p>\n<p>This Tuesday, <a href=\"/lw/9g4/histocracy_open_effective_group_decisionmaking/\">I posted an idea</a>. I came up with it about ten, maybe fifteen years ago, decided it was awesome and revolutionary, and spent a few years doing some extremely ineffectual advocacy for it. I've pushed for it in various contexts on and off since then, but I'd basically shelved it until I had a better platform to talk about it. And recently I realized, belatedly, that Less Wrong is probably the perfect audience. You're going to be open to it, and you'll be able to competently critique it. And maybe some of you will use it.</p>\n<p>I posted this tease first because the idea is a solution to a problem you may not have thought for five minutes about. I want to give you the opportunity to come up with other solutions, or discuss how to rigorously evaluate ideas. Also, I'm really curious to see whether anybody independently reinvents mine. Over the years, I've had the experience several times of reading someone else's work and thinking they're about to start pitching my idea. But they never do. It all gives me complicated feelings.</p>\n<p style=\"margin-bottom: 0in;\">Please discuss in the comments. Remember, we're aggregating the <em>opinions</em>, <span style=\"font-style: normal;\">the </span><em>judgment, </em><span style=\"font-style: normal;\">of the group members, not their personal preferences. So answers like &ldquo;iterated runoff voting,&rdquo; &ldquo;bargaining theory X,&rdquo; or &ldquo;coherent extrapolated volition&rdquo; are not in themselves what I'm looking for. If you happen to know what my idea is, or manage to find it by google-stalking me, please don't give it away yet.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zjNWfNpDmorQSFukR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": -6, "extendedScore": null, "score": 8.343159382579123e-07, "legacy": true, "legacyId": "12192", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MDQnDGEKQuCAggRLe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T16:11:58.959Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:31.177Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gYTyK3tAeYk5L3n3t/", "pageUrlRelative": "/posts/gYTyK3tAeYk5L3n3t/", "linkUrl": "https://www.lesswrong.com/posts/gYTyK3tAeYk5L3n3t/", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYTyK3tAeYk5L3n3t%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYTyK3tAeYk5L3n3t%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYTyK3tAeYk5L3n3t%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gYTyK3tAeYk5L3n3t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 8.343507601584223e-07, "legacy": true, "legacyId": "12057", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T16:29:53.996Z", "modifiedAt": null, "url": null, "title": "Q&A with experts on risks from AI #4", "slug": "q-and-a-with-experts-on-risks-from-ai-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:38.155Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7nPtpmBwoiQWDKvKz/q-and-a-with-experts-on-risks-from-ai-4", "pageUrlRelative": "/posts/7nPtpmBwoiQWDKvKz/q-and-a-with-experts-on-risks-from-ai-4", "linkUrl": "https://www.lesswrong.com/posts/7nPtpmBwoiQWDKvKz/q-and-a-with-experts-on-risks-from-ai-4", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20experts%20on%20risks%20from%20AI%20%234&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20experts%20on%20risks%20from%20AI%20%234%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nPtpmBwoiQWDKvKz%2Fq-and-a-with-experts-on-risks-from-ai-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20experts%20on%20risks%20from%20AI%20%234%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nPtpmBwoiQWDKvKz%2Fq-and-a-with-experts-on-risks-from-ai-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nPtpmBwoiQWDKvKz%2Fq-and-a-with-experts-on-risks-from-ai-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8590, "htmlBody": "<p><strong>[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>Professor <strong>Michael G. Dyer</strong> is an author of over 100 publications, including <em>In-Depth Understanding</em>, MIT Press, 1983. He serves on the editorial board of the journals: <em>Applied Intelligence, Connection Science, Knowledge-Based Systems, International Journal of Expert Systems,</em> and <em>Cognitive Systems Research.</em> His research interests are centered around semantic processing of natural language, through symbolic, connectionist, and evolutionary techniques. [<a href=\"http://www.cs.ucla.edu/~dyer/\">Homepage</a>]</p>\n<p>Dr. <strong>John Tromp</strong> is interested in Board Games and Artificial Intelligence, Algorithms, Complexity, Algorithmic Information Theory, Distributed Computing, <a href=\"http://www.bioasp.nl/\">Computational biology</a>. His recent research has focused on the Combinatorics of Go, specifically <a href=\"http://homepages.cwi.nl/%7Etromp/go/legal.html\">counting the number of legal positions</a>. <a href=\"http://homepages.cwi.nl/~tromp/\">[Homepage</a>]</p>\n<p>Dr. <strong>Kevin Korb</strong> both developed and taught the following subjects at Monash University: Machine Learning, Bayesian Reasoning, Causal Reasoning, The Computer Industry: historical, social and professional issues, Research Methods, Bayesian Models, Causal Discovery, Epistemology of Computer Simulation, The Art of Causal. [<a href=\"http://www.csse.monash.edu.au/~korb/cv05.html\">Curriculum vitae</a>] [<a href=\"http://www.csse.monash.edu.au/bai/\">Bayesian Artificial Intelligence</a>]</p>\n<p>Dr. <strong>Leo Pape</strong> is a postdoc in <a href=\"/r/discussion/lw/682/j%C3%BCrgen_schmidhuber_on_risks_from_ai\">J&uuml;rgen Schmidhuber</a>'s group at IDSIA (Dalle Molle Institute for Artificial Intelligence). He is interested in artificial curiosity, chaos, metalearning, music, nonlinearity, order, philosophy of science, predictability, recurrent neural networks, reinforcement learning, robotics, science of metaphysics, sequence learning, transcendental idealism, unifying principles. [<a href=\"http://www.idsia.ch/~pape/\">Homepage</a>] [<a href=\"http://www.idsia.ch/~pape/publications.html\">Publications</a>]</p>\n<p>Professor <strong>Peter Gacs</strong> is interested in Fault-tolerant cellular automata, algorithmic information theory, computational complexity theory, quantum information theory. [<a href=\"http://www.cs.bu.edu/~gacs/\">Homepage</a>]</p>\n<p>Professor <strong>Donald Loveland</strong> does focus his research on automated theorem proving, logic programming, knowledge evaluation, expert systems, test-and-treatment problem. [<a href=\"http://www.cs.duke.edu/~dwl/CV/\">Curriculum vitae</a>]</p>\n<p><strong>Eray Ozkural</strong> is a computer scientist <span class=\"commentBody\">whose</span> research interests are mainly in parallel computing, data mining, artificial intelligence, information theory, and computer architecture. He has an Msc. and is trying to complete a long overdue PhD in his field. He also has a keen interest in philosophical foundations of artificial intelligence. With regards to AI, his current goal is to complete an AI system based on the Alpha architecture of Solomonoff. His most recent work (<a rel=\"nofollow nofollow\" href=\"http://arxiv.org/abs/1107.2788\" target=\"_blank\">http://arxiv.org/abs/1107.2788</a>) discusses axiomatization of AI.</p>\n<p>Dr. <strong>Laurent Orseau</strong> is mainly interested in Artificial General Intelligence, which overall goal is the grand goal of AI: building an intelligent, autonomous machine. [<a href=\"http://www.agroparistech.fr/mia/doku.php?id=equipes:membres:page:laurent\">Homepage</a>] [<a href=\"http://www.agroparistech.fr/mia/doku.php?id=productions:publications\">Publications</a>] [<a href=\"http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/orseau-ring-AGI-2011-mortal.pdf\">Self-Modification and Mortality in Artificial Agents</a>]</p>\n<p><strong>Richard Loosemore</strong> is currently a lecturer in the Department of Mathematical and Physical Sciences at Wells College, Aurora NY, USA. Loosemore's principle expertise is in the field known as Artificial General Intelligence, which seeks a return to the original roots of AI (the construction of complete, human-level thinking systems). Unlike many AGI researchers, his approach is as much about psychology as traditional AI, because he believes that the complex-system nature of thinking systems make it almost impossible to build a safe and functioning AGI unless its design is as close as possible to the design of the human cognitive system. [<a href=\"http://www.richardloosemore.com/\">Homepage</a>]</p>\n<p><strong>Monica Anderson</strong> has been interested in the quest for computer based cognition since college, and ever since then has sought out positions with startup companies that have used cutting-edge technologies that have been labeled as \"AI\". However, those that worked well, such as expert systems, have clearly been of the \"Weak AI\" variety. In 2001 she moved from using AI techiques as a programmer to trying to advance the field of \"Strong AI\" as a researcher. She is the founder of <a href=\"http://syntience.com/\">Syntience Inc.</a>, which was established to manage funding for her exploration of this field. She has a Master's degree in Computer Science from Link&ouml;ping University in Sweden. She created three expert systems for Cisco Systems for product configuration verification; She has co-designed systems to automatically classify documents by content; She has (co-)designed and/or (co-)written LISP interpreters, debuggers, chat systems, OCR output parsers, visualization tools, operating system kernels, MIDI control real-time systems for music, virtual worlds, and peer-to-peer distributed database systems. She was Manager of Systems Support for Schlumberger Palo Alto Research. She has worked with robotics, industrial control, marine, and other kinds of embedded systems. She has worked on improving the quality of web searches for Google. She wrote a Genetic Algorithm which successfully generated solutions for the Set Coverage Problem (which has been shown to be NP-hard) around 1994. She has used more than a dozen programming languages professionally and designed or co-designed at least four programming languages, large or small. English is her third human language out of four or five. [<a href=\"http://artificial-intuition.com/anderson.html\">More</a>]</p>\n<h3>The Interview (New Questions)<br /></h3>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>I will try to answer your questions, but probably not all, and with some disclaimers.</p>\n<p style=\"padding-left: 30px;\"><strong></strong>As another disclaimer: the questions, and the website lesswrong.com that I glanced at, seem to be influenced by Raymond Kurzweil's books.&nbsp; I have not read those books, though of course, I heard about them in conversations, and have seen some reviews.&nbsp; I do not promise never to read them, but waiting for this would delay my answers indefinitely.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau: </strong>Keep in mind that the thoughts expressed here reflect my state of mind and my knowledge at the time of the writing, and may significantly differ after further discussions, readings and thoughts. I have no definite idea about any of the given questions.<strong><br /></strong></p>\n<p><strong>Q1:</strong> <em>Assuming beneficial political and economic development and that no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of artificial intelligence that is roughly as good as humans at science, mathematics, engineering and programming?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>2050/2200/2500</p>\n<p style=\"padding-left: 30px;\">The assumptions, by the way, are unrealistic. There will be disruptions.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>I believe that, in my lifetime, computers will only be proficient at well-defined and specialized tasks. Success in the above disciplines requires too much real-world understanding and social interaction. I will not even attempt projections beyond my lifetime (let's say beyond 40 years).<strong><br /></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>See Ray Kurzweil's book:&nbsp; The Singularity Is Near.</p>\n<p style=\"padding-left: 30px;\">As I recall, he thinks it will occur before mid-century.</p>\n<p style=\"padding-left: 30px;\">I think he is off by at least an additional 50 years (but I think we'll have as manypersonal robots as cars by 2100.)</p>\n<p style=\"padding-left: 30px;\">One must also distinguish between the first breakthrough of a technology vs. that breakthrough becoming cheap enough to be commonplace, so I won't give you any percentages.&nbsp; (Several decades passed between the first cell phone and billions of people having cell phones.)</p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs:&nbsp;</strong>I cannot calibrate my answer as exactly as the percentages require, so I will<br />just concentrate on the 90%.&nbsp; The question is a common one, but in my opinion<br />history will not answer it in this form.&nbsp; Machines do not develop in direct<br />competition of human capabilities, but rather in attempts to enhance and<br />complement them.&nbsp; If they still become better at certain tasks, this is a side<br />effect.&nbsp; But as a side effect, it will indeed happen that more and more tasks<br />that we proudly claim to be creative in a human way, will be taken over by<br />computer systems.&nbsp; Given that the promise of artificial intelligence is by now<br />50 years old, I am very cautious with numbers, and will say that at least 80<br />more years are needed before jokes about the stupidity of machines will become<br />outdated.</p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>2025/2030/2045.<br /><br />Assuming that we have the right program in 2035 by 100% probability, it could still take about 10 years to train it adequately, even though we might find that our programs by then learn much faster than humans. I anticipate that the most expensive part of developing an AI will be training, although we tend to assume that after we bring it up to primary school level, i.e. it can read and write, it would be able to learn much on its own. I optimistically estimated that it would take $10 million dollars and 10 years to train an AI in basic science. Extending that to cover all four of science, mathematics, engineering and programming could take even longer. It takes a human, arguably 15-20 years of training to be a good programmer, and very few humans can program well after that much educational effort and expense.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau:</strong></p>\n<p style=\"padding-left: 30px;\">10%: 2017<br />50%: 2032<br />90%: 2100<br /><br />With a quite high uncertainty though.<br />My current estimate is that (I hope) we will know we have built a core AGI by 2025, but a lot of both research and engineering work and time (and learning for the AGI) will be required for the AGI to reach human level in most domains, up to 20 years in the worst case I speculate and 5 years at least, considering that a lot of people will probably be working on it at that time. That is, if we really want to make it human-like. <strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>2015 - 2020 - 2025<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong></p>\n<p style=\"padding-left: 30px;\">10%&nbsp; 2020<br />50%&nbsp; 2026<br />90%&nbsp; 2034<br /><br />These are all Reductionist sciences. I assume the question is whether we'll have machines capable of performing Reduction in these fields. If working on pre-reduced problems, where we already have determined which Models (formulas, equations, etc) to use and know the values of all input variables, then we already have Mathematica. But here the Reduction was done by a human so Mathematica is not AI.<br /><br />AIs would be useful for more everyday things, such as (truly) Understanding human languages years before they Understand enough to learn the Sciences and can perform full-blown Reduction. This is a much easier task, but is still AI-Complete. I think the chance we'll see a program truly Understand a human language at the level of a 14-year old teenager is<br /><br />10% 2014<br />50% 2018<br />90% 2022<br /><br />Such an AI would be worth hundreds of billions of dollars and makes a worthy near-term research goal. It could help us radically speed up research in all areas by allowing for vastly better text-based information filtering and gathering capabilities, perfect voice based input, perfect translation, etc.<strong><br /></strong></p>\n<p><strong>Q2:</strong> <em>Once we build AI that is roughly as good as humans at science, mathematics, engineering and programming, how much more difficult will it be for humans and/or AIs to build an AI which is substantially better at those activities than humans?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>It depends upon how AGI is achieved. If it's through design breakthroughs in AI architecture, then the Singularity will follow. If it's through mimicking nanorecordings, then no Singularity is implied and may not occur at all.</p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>Not much. I would guess something on the order of a decade or two.<strong><br /></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>Machines and many specific algorithms are <strong>already </strong>substantially better at their tasks than humans.<br />(What human can compete with a relational database?, or with a Bayesian reasoner? or with scheduler?, or with an intersection-search mechanism like WATSON? etc.)</p>\n<p style=\"padding-left: 30px;\">For dominance over humans, machines have to first acquire the ability to understand human language and to have thoughts in the way humans have thoughts.&nbsp;&nbsp; Even though the WATSON program is impressive, it does NOT know what a word actually means (in the sense of being able to answer the question:&nbsp; \"How does the meaning of the word \"walk\"&nbsp; differ from the meaning of the word \"dance\", physically, emotionally, cognitively, socially?\"</p>\n<p style=\"padding-left: 30px;\">It's much easier to get computers to beat humans at technical tasks (such as sci, math, eng. prog.) but humans are vastly superior at understanding language, which makes humans the master of the planet.&nbsp; So the real question is:&nbsp; At what point will computers understand natural language as well as humans?<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>This is also hard to quantify, since in some areas machines will still be behind, while in others they will already be substantially better: in my opinion, this is already the case.&nbsp; If I still need to give a number, I say 30 years.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>I expect that by the time such a knowledgeable AI is developed, it will already be thinking and learning faster than an average human. Therefore, I think, simply by virtue of continuing miniaturization of computer architecture, or other technological developments that increase our computational resources (e.g., cheaper energy technologies such as fusion), a general-purpose AI could vastly transcend human-level intelligence.</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau: </strong>Wild guess: It will follow Moore's law (see below).</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>Very little difficulty. I expect it to happen immediately after the first achievement, because at the very least we could simply increase the clock speed in relevant areas. It does depend exactly how you measure \"better\", though.</p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson:</strong> What does \"better\" mean? If we believe, as many do, that Intelligence is for Prediction, and that the best measure of our intelligence is whether we can predict the future in complex domains, then we can interpret the given question as \"when can an AI significantly outpredict a human in their mundane everyday environment\".<br /><br />For any reasonable definition of \"significant\", the answer is \"never\". The world is too complex to be predictable. All intelligences are \"best-effort\" systems where we do as best we can and learn from our mistakes when we fail, for fail we must. Human intelligences have evolved to the level they have because it is a reasonable level for superior survival chances in the environments in which we've evolved. More processing power, faster machines, etc.&nbsp; do not necessarily translate into an improved ability to predict the environment, especially if we add AIs to this environment. A larger number of competent agents like AIs will make the domain even MORE complex, leading to LOWER predictability. For more about this, see <a href=\"http://hplusmagazine.com/2010/12/15/problem-solved-unfriendly-ai\">http://hplusmagazine.com/2010/12/15/problem-solved-unfriendly-ai</a>.<br /><br />Improved ability to handle Models (creating a \"super Mathematica\") is of limited utility for the purpose of making longer-term predictions. Chains of Reductionist Models attempting to predict the future tend to look like Rube Goldberg machines and are very likely to fail, and to fail spectacularly (which is what Brittleness is all about).<br /><br />Computers will not get better at Reduction (the main skill required for Science, Mathematics, Engineering, and Programming) until they gather a lot of experience of the real world. For instance, a programming task is 1% about Understanding programming and 99% about Understanding the complex reality expressed in the spec of the program. This can only be improved by Understanding reality better, which is a slow process with the limitations described above. For an introduction to this topic, see my article \"Reduction Considered Harmful\" at <a href=\"http://hplusmagazine.com/2011/03/31/reduction-considered-harmful\">http://hplusmagazine.com/2011/03/31/reduction-considered-harmful</a>.<br /><br />The \"Problem with Reduction\" is actually \"The Frame Problem\" as described by John McCarthy and Pat Hayes, viewed from a different angle. It is not a problem that AI research can continue to ignore, which is what we've done for decades. It will not go away. The only approach that works is to sidestep the issue of continuous Model update by not using Models. AIs must use nothing but Model Free Methods since these work without performing Reduction (to Models) and hence can be used to IMPLEMENT automatic Reduction.<strong></strong></p>\n<p><strong>Q3:</strong> <em>Do you ever expect artificial intelligence to overwhelmingly outperform humans at typical academic research, in the way that they may soon overwhelmingly outperform humans at trivia contests, or do you expect that humans will always play an important role in scientific progress?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb:</strong> They will overwhelmingly outperform if and only if we achieve artificial general intelligence through human understanding of intelligence or through artificial understanding of intelligence (vs nanomeasurements).</p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>Like I said, not in my lifetime, and projections beyond that are somewhat meaningless I think.</p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>Regarding trivia contests, WATSON's performance fools people into thinking that the language problem has been solved, but the WATSON program does not understand language.&nbsp;&nbsp; It does an intersection search across text so, for example, if it knows that the answer category is human and that a clue is \"Kitty Hawk\" then it can do an intersection search and come up with Wright Brothers.&nbsp; The question can sound complicated but WATSON can avoid comprehending the question and just return the best intersection that fits the answer category.&nbsp; It can treat each sentence as a bag of words/phrases.&nbsp; WATSON cannot read a child's story and answer questions about what the characters wanted and why they did what they did.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>Absolutely, but the contest will never be direct, and therefore the victory will never have to be acknowledged.&nbsp; Whatever tasks the machines are taking over, will always be considered as tasks that are just not worthy of humans.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>Yes. In Ray Solomonoff's paper titled \"The time scale of artificial intelligence: Reflections on social effects\", he predicted that it will be possible to build an AI that is as intelligent as many times the entire computer science community (AI Milestone F). He predicted that it would take a short time to go from a human-level AI, to such a vastly intelligent AI that would overwhelmingly outperform not only individual humans, but the entire computer science community. This is called the \"infinity point\" hypothesis, and it was the first scientific formulation of singularity (1985). He formalized the feedback loop by which an AI could increase its own intelligence by working on miniaturization of computer architectures, e.g, Moore's law. The idea is that by being smarter than humans, the AI would accelerate Moore's law, theoretically achieving infinite intelligence in a short, finite time, depending on the initial investment.</p>\n<p style=\"padding-left: 30px;\">However, of course, infinite intelligence is impossible due to physical limits. Unaided Moore's law can only continue up to physical limits of computation which would be reached by 2060's if current rate of progress continued, and needless to mention those limits are sort of impossible to achieve (since they might involve processes that are a bit like blackholes). However, imagine this, the AI could design fusion reactors using the H3 on Moon and energy-efficient processors to achieve large amounts of computation. There could be alternative ways to obtain extremely fast supercomputers, and so forth, Solomonoff's hypothesis could be extended to deal with all sorts of technological advances, for instance a self-improving AI could improve its own code, which designs like Goedel Machine and Solomonoff's Alpha are supposed to accomplish. Therefore, ultimately, such AI's would help improve computer architecture, artificial intelligence, electronics, aerospace, energy, communication technologies, all of which would help build AI's that are perhaps hundreds of thousands of times smarter than individual humans, or perhaps much smarter than the entire humanity as Ray Kurzweil predicts, not just particular scientific communities like the computer science community,</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau: </strong>\"Always\" is a very strong word. So probably not for that last part.<br />I give 100% chances for an AI to vastly outperform humans in some domains (which we already have algorithms for, like calculus and chess of course), 50% in many domains, and 10% in all domains. Humans have some good old genetic biases that might be hard to challenge.<br />But how much better it will be is still very unclear, mostly due to NP-hardness, Legg's prediction hardness results and related no-free-lunch problems, where progress might only be gained through more computing power.<br /><br />The AGI might have significantly different hot research topics than humans, so I don't think we will lose our philosophers that fast. And good philosophy can only be done with good science.<br /><br />Also, machines are better at chess and other games than me, but that doesn't prevent me from playing.</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>Yes. Except for one thing. Humans will be able to (and some will choose to) augment their own intellectual capacity to the same level as the AIs. In that case, your question gets a little blurred.</p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong>I don't believe AI will even reliably \"overwhelmingly outperform\" humans at trivia contests until they fully Understand language. Language Understanding computers will be a great help, but the overwhelming outperformance in Reduction-related tasks is unlikely to happen. Reduction is very difficult.</p>\n<p><strong>Q4:</strong> <em>What probability do you assign to the possibility of an AI with initially (professional) human-level competence at general reasoning (including science, mathematics, engineering and programming) to self-modify its way up to vastly superhuman capabilities within a matter of hours/days/&lt; 5 years?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>If through nanorecording: approx 0%. Otherwise, the speed/acceleration at which AGIs improve themselves is hard to guess at.</p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp:</strong> I expect such modification will require plenty of real-life interaction.</p>\n<ul>\n<li>hours:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10^-9</li>\n<li>days:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10^-6</li>\n<li>&lt;5 years : 10^-1<strong><br /></strong></li>\n</ul>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: -</strong></p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>This question presupposes a particular sci-fi scenario that I do not believe in.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>In 5 years, without doing anything, it would already be faster than a human simply by running on a faster computer. If Moore's law continued by then, it would be 20-30 times faster than a human. But if you mean by \"vastly\" a difference of thousand times faster, I give it a probability of only 10% because there might be other kinds of bottlenecks involved (mostly physical). There is also another problem with Solomonoff's hypothesis, which Kurzweil generalized, that we are gladly omitting. An exponential increase in computational speed may only amount to a linear increase in intelligence. It at least corresponds only to a linear increase in the algorithmic complexity of solutions that can be found by any AGI, which is a well known fact, and cannot be worked around by simple shortcuts. If solution complexity is the best measure of intelligence, then, getting much more intelligent is not so easy (take this with a grain of salt, though, and please contrast it with the AIQ idea).</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau: </strong>I think the answer to your question is similar to the answer to:<br />Suppose we suddenly find a way to copy and emulate a whole human brain on a computer; How long would it take *us* to make it vastly better than it is right now?<br />My guess is that we will make relatively slow progress. This progress can get faster with time, but I don't expect any sudden explosion. Optimizing the software sounds a very hard task, if that is even possible: if there were an easy way to modify the software, it is probable that natural selection would have found it by now. Optimizing the hardware should then follows Moore's law, at least for some time.<br />That said, the digital world might allow for some possibilities that might be more difficult in a real brain, like copy/paste or memory extension (although that one is debatable).<br /><br />I don't even know if \"vastly superhuman\" capabilities is something that is even possible. That sounds very nice (in the best scenario) but is a bit dubious. Either Moore's law will go on forever, or it will stop at some point. How much faster than a human can a computer compute, taking thermodynamics into account?<br /><br />So, before it really becomes much more intelligent/powerful than humans, it should take some time.<br />But we may need to get prepared for otherwise, just in case.</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>Depending on the circumstances (which means, this will not be possible if the AI is built using dumb techniques) the answer is: near certainty.</p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong>0.00% . Reasoning is useless without Understanding because if you don't Understand (the problem domain), then you have nothing to reason about. Symbols in logic have to be anchored in general Understanding of the problem domain we're trying to reason about.</p>\n<p><strong>Q5:</strong> <em>How important is it to research risks associated with artificial intelligence that is good enough at general reasoning (including science, mathematics, engineering and programming) to be capable of radical self-modification, before attempting to build one?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>It is the key issue in the ethics of AI. Without a good case to make, the research may need to cease. To be sure, one aspect of a good case may well be that unethical projects are underway and likely to succeed. Per my answers above, I do not currently believe anything of the kind. No project is near to success.</p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>Its importance grows with the extent to which we allow computers control over critical industrial/medical/economic processes, infrastructure, etc. As long as their role is limited to assisting humans in control, there appears to be little risk.</p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>A robot that does not self-replicate is probably not very dangerous (leaving out robots for warfare).<br />A robot that wants to make multiple copies of itself would be dangerous (because it could undergo a rapid form of Lamarckian evolution.&nbsp; There are two type of replication:&nbsp;&nbsp; factory replication and factory division via the creation of a new factory.&nbsp;&nbsp; In social insects this is the difference between the queen laying new eggs and a hive splitting up to go build new hive structures at a new site.</p>\n<p style=\"padding-left: 30px;\">Assuming that humans remain in control of the energy and resources to a robot-producing factory, then factory replication could be shut down.&nbsp; Robots smart enough to go build a new factory and maintain control over the needed resources would pose the more serious problem.&nbsp; As robots are designed (and design themselves) to follow their own goals (for their own self-survival, especially in outer space) then those goals will come into conflict with those of humans.&nbsp;&nbsp; Asimov's laws are too weak to protect humans and as robots design new versions of themselves then they will eliminate those laws anyway.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong>Not very important. Radical self-modification cannot be undertaken by anyone (including AIs) without Understanding of what would make a better Understander. While it is possible that an AI could be helpful in this research I believe the advances in this area would be small, slow to arrive, and easy to control, hitting various brick walls of radically diminishing returns that easily dis-compensates advances of all kinds including Moore's Law.<br /><br />We already use computers to design faster, better, logically larger and physically smaller computers. This has nothing to do with AI since the improvements come from Understanding about the problem domain &ndash; computer design &ndash; that is performed by humans. Greater capability in a computer translates to very small advances in Reductive capability. Yes, Understanding machines may be able to eventually Understand Understanding to the point of creating a better Understander. This is a long ways off; Understanding Understanding is uncommon even among humans. But even then, the unpredictability of our Mundane reality is what limits he advantage any intelligent agent might have.</p>\n<p><strong>Q5-old:</strong> <em>How important is it to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to build AI that is good enough at general reasoning (including science, mathematics, engineering and programming) to undergo radical self-modification?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>This is an impossible task.&nbsp; \"AI\" is not a separate development that can be regulated the way that governments regulate research over infectious bacteria to make sure they do not escape the laboratory.&nbsp; Day for day, we are yielding decision power to smart machines, since we draw---sometimes competitive---advantage from this.&nbsp; Emphasizing that the process is very gradual, I still constructed a parable that illustrates the process via a quick and catastrophic denuement.</p>\n<p style=\"padding-left: 30px;\">Thinking it out almost fourty years ago, I assumed that the nuclear superpowers, the Soviet Union and the USA, would live on till the age of very smart machines. So, at some day, for whatever reason, World War 3 breaks out between these superpowers.&nbsp; Both governments consult their advanced computer systems on how to proceed, and both sides get analogous answers.&nbsp; The Soviet computer says: the first bomb must be dropped on the Kremlin; in the US, the advice is to drop the first bomb on the Pentagon.&nbsp; The Americans still retain enough common sense to ignore the advice; but Soviets are more disciplined, and obey their machine. After the war plays out, the Soviet side wins, since the computer advice was correct on both sides. (And from then on, machines rule...)<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural:</strong> A sense of benevolence or universal ethics/morality would only be required if the said AI is also an intelligent agent that would have to interact socially with humans. There is no reason for a general-purpose AI to be an intelligent agent, which is an abstraction of animal, i.e., as commonly known as an \"animat\" since early cyberneticists. Instead, the God-level intelligence could be an ordinary computer that solves scientific problems on demand. There is no reason for it to control robotic hardware or act on its own, or act like a human or an animal. It could be a general-purpose expert system of some sort, just another computer program, but one that is extremely useful. Ray Solomonoff wrote this about human-like behavior in his paper presented at the 2006 Dartmouth Artificial Intelligence conference (50th year anniversary) titled\"Machine Learning - Past and Future\", which you can download from his website:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://world.std.com/~rjs/dart.pdf\">http://world.std.com/~rjs/dart.pdf</a></p>\n<p style=\"padding-left: 60px;\">\"To start, I&rsquo;d like to define the scope of my interest in A.I. I am not particularly interested in simulating human behavior. I am interested in creating a machine that can work very difficult problems much better and/or faster than humans can &ndash; and this machine should be embodied in a technology to which Moore&rsquo;s Law applies. I would like it to give a better understanding of the relation of quantum mechanics to general relativity. I would like it to discover cures for cancer and AIDS. I would like it to find some very good high temperature superconductors. I would not be disappointed if it were unable to pass itself off as a rock star.\"</p>\n<p style=\"padding-left: 30px;\">That is, if you constrain the subject to a non-autonomous, scientific AI, I don't think you'll have to deal with human concepts like \"friendly\" at all. Without even mentioning how difficult it might be to teach any common sense term to an AI. For that, you would presumably need to imitate the way humans act and experience.</p>\n<p style=\"padding-left: 30px;\">However, to solve the problems in science and engineering that you mention, a robotic body, or a fully autonomous, intelligent agent, is not needed at all. Therefore, I think it is not very important to work on friendliness for that purpose. Also, one person's friend is another's enemy. Do we really want to introduce more chaos to our society?</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau:</strong> It is quite dubious that \"provably friendly\" is something that is possible.<br />A provably friendly AI is a dead AI, just like a provably friendly human is a dead human, at least because of how humans would use/teach it, and there are bad guys who would love to use such a nice tool.<br />The safest \"AI\" system that I can think of is a Q/A system that is *not* allowed to ask questions (i.e. to do actions). But then it cannot learn autonomously and may not get as smart as we'd like, at least in reasonable time; I think it would be quite similar to a TSP solver: its \"intelligence\" would be tightly linked to its CPU speed.<br /><br />\"Provably epsilon-friendly\" (with epsilon &lt;&lt; 1 probability that it might not be always friendly) is probably a more adequate notion, but I'm still unsure this is possible to get either, though maybe under some constraints we might get something.<br /><br />That said, I think this problem is quite important, as there is still a non-negligible possibility that an AGI gets much more *power* (no need for vastly more intelligence) than humanity, even without being more intelligent. An AGI could travel at the speed of information transfer (so, light speed) and is virtually immortal by restoring from backups and creating copies of itself. It could send emails on behalf of anyone, and could crack high security sites with as much social engineering as we do. As it would be very hard to put in jail or to annihilate, it would feel quite safe (for its own life) to do whatever it takes to achieve its goals.<br />Regarding power and morality (i.e. what are good goals), here is a question: Suppose you are going for a long walk in the woods in a low populated country, on your own. In the middle of the walk, some big guy pops out of nowhere and comes to talk to you. He is ugly, dirty, smells horribly bad, and never stops talking. He gets really annoying, poking you and saying nasty things, and it's getting worse and worse. You really can't stand it anymore. You run, you go back and forth, you shout at him, you vainly try to reason him but you can't get rid of him. He just follows you everywhere. You don't really want to start a fight as he looks much stronger than you are. Alas, it will take you some 5 more hours to get back to your car and nobody else is in the woods. But In your pocket you have an incredible device: A small box with a single button that can make everything you wish simply disappear instantly. No blood, no pain, no scream, no trace, no witness, no legal problem, 100% certified. At some instant the guy would be here, the next instant he would not, having simply vanished. As simple as that. You don't know what happens to the disappeared person. Maybe he dies, maybe he gets teleported somewhere, or reincarnated or whatever. You know that nobody knows this guy, so nobody can miss him or even look for him. You try to explain to him what this box is, you threaten him to press the button but he does not care. And he's getting so, so annoying, that you can't refrain to scream. Then you stare at the button... Will you press it?<br />My guess is that most people would like to say no, because culture and law say it's bad, but the truth may be that most of them would be highly tempted if facing such a situation. But if they had a gun or a saber instead of a button, the answer would probably be a straighter no (note that a weapon injury is much like a death sentence in the woods). The definition of morality might depend on the power you have.<br /><br />But, hopefully, we will be sufficiently smart to put a number of safety measures and perform a lot of testing under stressful conditions before launching it in the wild.</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>Absolutely essential. Having said that, the task of making it \"provably\" friendly is not as difficult as portrayed by organizations (SIAI, FHI) that have a monomaniacal dedication to AI techniques that make it impossible. So in other words: essential, but not a difficult task at all.<strong><br /></strong></p>\n<p><strong>Q6: </strong><em>What probability do you assign to the possibility of human extinction within 100 years as a result of AI capable of self-modification (that is not provably non-dangerous, if that is even possible)? P(human extinction by AI | AI capable of self-modification and not provably non-dangerous is created)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>This question is poorly phrased.</p>\n<p style=\"padding-left: 30px;\">You should ask relative to a time frame. After all, the probability of human extinction sometime or other is 1. (<span style=\"color: #ff0000;\"><strong>Note by XiXiDu</strong></span>: I added <em>\"within 100 years\" </em>to the question after I received his answers.)<em><br /></em></p>\n<p style=\"padding-left: 30px;\">\"Provably\" is also problematic. Outside of mathematics, little is provable.</p>\n<p style=\"padding-left: 30px;\">My generic answer is that we have every prospect of building an AI that behaves reasonably vis-a-vis humans, should we be able to build one at all. We should, of course, take up those prospects and make sure we do a good job rather than a bad one. <strong><br /></strong></p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>The ability of humans to speed up their own extinction will, I expect, not be matched any time soon by machine, again not in my lifetime.</p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>Loss of human dominance is a foregone conclusion (100% for loss of dominance). <strong>Every </strong>alien civilization (including humans) that survives its own annihilation (via nuclear, molecular and nano technologies) will at some point figure out how to produce synthetic forms of its own intelligence.&nbsp;&nbsp; These synthetics beings are necessary for space travel (because there is most likely no warp drive possible and even planets in the Goldilocks zone will have unpleasant viral and cellular agents). Biological alien creatures will be too adapted to their own planets.</p>\n<p style=\"padding-left: 30px;\">As to extinction, we will only not go extinct if our robot masters decide to keep some of us around.&nbsp; If they decide to populate new planets with human life then they could make the journey and humans would thrive (but only because the synthetic agents wanted this).</p>\n<p style=\"padding-left: 30px;\">If a flying saucer ever lands, the chances are 99.99% that what steps out will be a <strong>synthetic </strong>intelligent entity.&nbsp;&nbsp; It's just too hard for biological entities (adapted to their planet) to make the long voyages required.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>I give it a probability near 1%.&nbsp; Humans may become irrelevant in the sense of losing their role of being at the forefront of the progress of \"self-knowledge of the universe\" (whatever this means).&nbsp; But irrelevance will also mean that it will not be important to eradicate them completely.&nbsp; On the other hand, there are just too many, too diverse imaginable scenarios for their coexistence with machines that are smarter than they are, so I don't dare to predict any details.&nbsp; Of course, species do die out daily even without our intent to extinguish them, but I assume that at least some humans would find ways to survive for some more centuries to come.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>Assuming that we are talking about intelligent agents, which are strictly unnecessary for working on scientific problems which is your main concern, I think first that it is not possible to build something that is provably non-dangerous, unless you can encode a rule of non-interference into its behavior. Otherwise, an interfering AI can basically do anything, and since it is much smarter than us, it can create actual problems that we had no way of anticipating or solving. I have thought at length on this question, and considered some possible AI objectives in a blog essay:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://www.examachine.net/blog/?p=72\">http://www.examachine.net/blog/?p=72</a></p>\n<p style=\"padding-left: 30px;\">I think that it does depend on the objectives. In particular, selfish/expansionist AI objectives are very dangerous. They might almost certainly result in interference with our vital resources. I cannot give a probability, because it is a can of worms, but let me try to summarize. For instance, the objective to maximize its knowledge about the world, a similar version of which was considered by Laurent Orseau in a reinforcement learning setting, and previously by a student of Solomonoff. Well, it's an intuitive idea that a scientist tries to learn as much as possible about the world. What if we built an intelligent agent that did that? If it's successful, it would have to increase its computation and physical capacity to such an extent that it might expand rapidly, first assimilating the solar system and then expand at our galactic neighborhood to be able to pursue its unsatisfiable urge to learn. Similar scenarios might happen in any kind of intelligent agent with selfish objectives (i.e., optimize some aspect of itself). Those might be recognized as Omohundro drives, but the objectives themselves are the main problem mostly.</p>\n<p style=\"padding-left: 30px;\">This is a problem when you are stuck in this reinforcement learning mentality, thinking in terms of rewards and punishment. The utility function that you will define will tend to be centered around the AI itself rather than humanity, and things have a good chance of going very wrong. This is mostly regardless of what kind of selfishness is pursued, be it knowledge, intelligence, power, control, satisfaction of pseudo pleasure, etc. In the end, the problem is with the relentless pursuit of a singular, general objective that seeks to benefit only the self. And this cannot be mitigated by any amount of obstruction rules (like Robot Laws or any other kind of laws). The motivation is what matters, and even when you are not pursuing silly motivations like stamp collection, there is a lot of danger involved, not due to our neglect of human values, which are mostly irrelevant at the level which such an intelligent agent would operate, but our design of its motivations.</p>\n<p style=\"padding-left: 30px;\">However, even if benevolent looking objectives were adopted, it is not altogether clear, what sorts of crazy schemes an AI would come up with. In fact, we could not predict the plans of an intelligent agent smarter than the entire humanity. Therefore, it's a gamble at best, and even if we made a life-loving, information-loving, selfless, autonomous AI as I suggested, it might still do a lot of things that many people would disagree with. And although such an AI might not extinguish our species, it might decide, for instance, that it would be best to scan and archive our species for using later. That is, there is no reason to expect that an intelligent agent that is superior to us in every respect should abide by our will.</p>\n<p style=\"padding-left: 30px;\">One might try to imagine many solutions to make such intelligent agents \"fool-proof\" and \"fail-safe\", but I suspect that for the first, human foolishness has unbounded inventiveness, and for the second, no amount of security methods that we design would make a mind that is smarter than the entire humanity \"safe\", as we have no way of anticipating every situation that would be created by its massive intelligence, and the amount of chaotic change that it would bring. It would simply go out of control, and we would be at the mercy of its evolved personality. I said personality on purpose, because personality seems to be a result of initial motivations, a priori knowledge, and its life experience. Since its life experience and intelligence will overshadow any initial programming, we cannot really foresee its future personality. All in all, I think it is great for thinking about, but it does not look like a practical engineering solution. That's why I simply advise against building fully autonomous intelligent agents. I sometimes say, play God, and you will fail. I tend to think there is a Frankenstein Complex, it is as if there is an incredible urge in many people to create an independent artificial person.</p>\n<p style=\"padding-left: 30px;\">On the other hand, I can imagine how I could build semi-autonomous agents that might be useful for many special tasks, avoiding interference with humans as much as possible, with practical ways to test for their compliance with law and customs. However, personally speaking, I cannot imagine a single reason why I would want to&nbsp; create an artificial person that is superior to me in every respect. Unless of course, I have elected to bow down to a superior species.</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau:</strong> It depends if we consider that we will simply leave safety issues aside before creating an AGI, thinking that all will go well, or if we take into account that we will actually do some research on that.<br />If an human-level AGI was built today, then we probably wouldn't be ready and the risks due to excitement to get something out of it might be high (\"hey look, it can drive the tank, how cool is that?!\").<br /><br />But if we build one and can show to the world a simple proof of concept that we do have (sub-human level) AGI and that will grow to human-level and most researchers acknowledge it, I presume we will start to think hard about the consequences.<br /><br />Then all depends on how much unfriendly it is.<br />Humanity is intelligent enough to care for its own life, and try to avoid high risks (most of the time), unless there is some really huge benefit (like supremacy).<br /><br />Also, if an AGI wants to kill all humans, humanity would not just wait for it, doing nothing.<br />This might be dangerous for the AI itself too (with EMPs for example). And an AGI also wants to avoid high risks unless there is a huge benefit. If some compromise is possible, it should be better.<br /><br />If we can build an AGI that is quite friendly (i.e. has \"good\" goals and wants to cooperate with humans without pressing them too much, or at least has no incentive to kill humans) but may become nasty only if its life is at stake, then I don't think we need to worry *too* much: just be friendly with it as you would be with an ally, and its safety will be paired with your own safety.<br /><br />So I think the risks of human extinction will be pretty low, as long as we take them into account seriously.</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>The question is loaded, and I reject the premises. It assumes that someone can build an AI that is both generally intelligent (enough to be able to improve itself) whilst also having a design whose motivation is impossible to prove. That is a false assumption. People who try to build AI systems with the kind of design whose motivation is unstable will actually not succeed in building anything that has enough general intelligence to become a danger.</p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong>0.00%. All intelligences must be fallible in order to deal with a complex and illogical world (with only incomplete information available) on a best effort basis. And if an AI is fallible, then we can unplug it... sooner or later, even if it is \"designed to be unstoppable\". Ten people armed with pitchforks, and armed also with ten copies of last year's best AI can always unplug the latest model AI.</p>\n<h3><strong>The Interview (Old Questions)</strong></h3>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em></p>\n<p><em>Explanatory remark to Q1:<br /><br />P(human-level AI by (year) | no wars &and; no disasters &and; beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>For me, roughly human-level machine intelligence is an embodied machine. Given the current difficulties of making such machines I expect it will last at least several hundred years before human-level intelligence can be reached. Making better machines is not a question of superintelligence, but of long and hard work. Try getting some responses to your questionnaire from roboticists.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>Experts usually are correct in their predictions but terrible in their timing predictions. They usually see things as coming earlier than the event actually occurs as they fail to see the obstacles. Also, it is unclear what you mean as human-level intelligence. The Turing test will be passed in its simplest form perhaps in 20 years. Full functional replacements for humans will likely take over 100 years (50% likelihood). 200 years (90% likelihood). <strong><br /></strong></p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction as a result of badly done AI?<br /><br />Explanatory remark to Q2:<br /><br />P(human extinction | badly done AI) = ?<br /><br />(Where 'badly done' = AGI capable of self-modification that is not provably non-dangerous.)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>Human beings are already using all sorts of artificial intelligence in their (war)machines, so there it is not impossible that our machines will be helpful in human extinction.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>Ultimately 95% (and not just by bad AI, but just by generalized evolution). In other words, in this sense all AI is badly done AI for I think it is a natural sequence that AI leads to superior artificial minds that leads to eventual evolution, or replacement (depending on the speed of the transformation), of humans to artificial life. <strong><br /></strong></p>\n<p><strong>Q3:</strong> <em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years?<br /><br />Explanatory remark to Q3:<br /><br />P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br />P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>I don&rsquo;t know what \"massive superhuman intelligence\" is, what it is for, and if it existed how to measure it.</p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>I am not sure of the question, or maybe only that I do not understand an answer. Let me comment anyway. I have always felt it likely that the first superhuman intelligence would be a simulation of the human mind; e.g., by advanced neural-net-like structures. I have never thought seriously about learning time, but I guess the first success would be after some years of processing. I am not sure of what you mean by ``massive''. Such a mind as above coupled to good retrieval algorithms with extensive databases such as those being developed now could appear to have massive superhuman intelligence. <strong><br /></strong></p>\n<p><strong>Q4:</strong> <em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence? </em></p>\n<p><em>Explanatory remark to Q4:<br /><br />How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?<br /></em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>Proof is for mathematics, not for actual machines. Even for the simplest machines we have nowadays we cannot proof any aspect of their operation. If this were possible, airplane travel would be a lot safer.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>It is important to try. I do not think it can be done. I feel that humans are safe from AI takeover for this century. Maybe not from other calamities, however.<strong><br /></strong></p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?</em></p>\n<p><em>Explanatory remark to Q5:</em></p>\n<p><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk? </em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>No idea how to compare these risks.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>Ouch. Now you want me to amplify my casual remark above. I guess that I can only say that I hope we are lucky enough for the human race to survive long enough to evolve into, or be taken over by, another type of intelligence. <strong><br /> </strong></p>\n<p><strong>Q6:</strong> <em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>People think that current AI is much more capable than it is in reality, and therefore they often overestimate the risks. This is partly due to the movies and due to scientists overselling their work in scientific papers and in the media. So I think the risk is highly overestimated.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>The current level of awareness of the AI risks is low. The risk that I most focus on now is the economic repercussions of advancing AI. Together with outsourcing, the advancing automation of the workplace, now dominated by AI advances,&nbsp; is leading to increasing unemployment. This progression will not be monotonic, but each recession will result in more permanently unemployed and weaker recoveries. At some point our economic philosophy could change radically in the U.S., an event very similar to the great depression. We may not recover, in the sense of returning to the same economic structure. I think (hope) that democracy will survive. <strong><br /></strong></p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human-level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>I would be impressed if a team of soccer playing robots could win a match against professional human players. Of course, the real challenge is finding human players that are willing to play against machines (imagine being tackled by a metal robot).<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>A \"pure\" learning program that won at Jeopardy ???<strong><br /></strong></p>\n<p><strong>Q8:</strong> <em>Are you familiar with formal concepts of optimal AI design which relate to searches over complete spaces of computable hypotheses or computational strategies, such as Solomonoff induction, Levin search, Hutter's algorithm M, AIXI, or G&ouml;del machines?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>I have some familiarity with Solomonoff inductive inference but not Hutter's algorithm. I have been retired for 10 years so didn't know of Hutter until this email. Looks like something interesting to pursue.<strong><br /></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7nPtpmBwoiQWDKvKz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 8.343575918716127e-07, "legacy": true, "legacyId": "12302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Click_here_to_see_a_list_of_all_interviews_\">[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>Professor <strong>Michael G. Dyer</strong> is an author of over 100 publications, including <em>In-Depth Understanding</em>, MIT Press, 1983. He serves on the editorial board of the journals: <em>Applied Intelligence, Connection Science, Knowledge-Based Systems, International Journal of Expert Systems,</em> and <em>Cognitive Systems Research.</em> His research interests are centered around semantic processing of natural language, through symbolic, connectionist, and evolutionary techniques. [<a href=\"http://www.cs.ucla.edu/~dyer/\">Homepage</a>]</p>\n<p>Dr. <strong>John Tromp</strong> is interested in Board Games and Artificial Intelligence, Algorithms, Complexity, Algorithmic Information Theory, Distributed Computing, <a href=\"http://www.bioasp.nl/\">Computational biology</a>. His recent research has focused on the Combinatorics of Go, specifically <a href=\"http://homepages.cwi.nl/%7Etromp/go/legal.html\">counting the number of legal positions</a>. <a href=\"http://homepages.cwi.nl/~tromp/\">[Homepage</a>]</p>\n<p>Dr. <strong>Kevin Korb</strong> both developed and taught the following subjects at Monash University: Machine Learning, Bayesian Reasoning, Causal Reasoning, The Computer Industry: historical, social and professional issues, Research Methods, Bayesian Models, Causal Discovery, Epistemology of Computer Simulation, The Art of Causal. [<a href=\"http://www.csse.monash.edu.au/~korb/cv05.html\">Curriculum vitae</a>] [<a href=\"http://www.csse.monash.edu.au/bai/\">Bayesian Artificial Intelligence</a>]</p>\n<p>Dr. <strong>Leo Pape</strong> is a postdoc in <a href=\"/r/discussion/lw/682/j%C3%BCrgen_schmidhuber_on_risks_from_ai\">J\u00fcrgen Schmidhuber</a>'s group at IDSIA (Dalle Molle Institute for Artificial Intelligence). He is interested in artificial curiosity, chaos, metalearning, music, nonlinearity, order, philosophy of science, predictability, recurrent neural networks, reinforcement learning, robotics, science of metaphysics, sequence learning, transcendental idealism, unifying principles. [<a href=\"http://www.idsia.ch/~pape/\">Homepage</a>] [<a href=\"http://www.idsia.ch/~pape/publications.html\">Publications</a>]</p>\n<p>Professor <strong>Peter Gacs</strong> is interested in Fault-tolerant cellular automata, algorithmic information theory, computational complexity theory, quantum information theory. [<a href=\"http://www.cs.bu.edu/~gacs/\">Homepage</a>]</p>\n<p>Professor <strong>Donald Loveland</strong> does focus his research on automated theorem proving, logic programming, knowledge evaluation, expert systems, test-and-treatment problem. [<a href=\"http://www.cs.duke.edu/~dwl/CV/\">Curriculum vitae</a>]</p>\n<p><strong>Eray Ozkural</strong> is a computer scientist <span class=\"commentBody\">whose</span> research interests are mainly in parallel computing, data mining, artificial intelligence, information theory, and computer architecture. He has an Msc. and is trying to complete a long overdue PhD in his field. He also has a keen interest in philosophical foundations of artificial intelligence. With regards to AI, his current goal is to complete an AI system based on the Alpha architecture of Solomonoff. His most recent work (<a rel=\"nofollow nofollow\" href=\"http://arxiv.org/abs/1107.2788\" target=\"_blank\">http://arxiv.org/abs/1107.2788</a>) discusses axiomatization of AI.</p>\n<p>Dr. <strong>Laurent Orseau</strong> is mainly interested in Artificial General Intelligence, which overall goal is the grand goal of AI: building an intelligent, autonomous machine. [<a href=\"http://www.agroparistech.fr/mia/doku.php?id=equipes:membres:page:laurent\">Homepage</a>] [<a href=\"http://www.agroparistech.fr/mia/doku.php?id=productions:publications\">Publications</a>] [<a href=\"http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/orseau-ring-AGI-2011-mortal.pdf\">Self-Modification and Mortality in Artificial Agents</a>]</p>\n<p><strong>Richard Loosemore</strong> is currently a lecturer in the Department of Mathematical and Physical Sciences at Wells College, Aurora NY, USA. Loosemore's principle expertise is in the field known as Artificial General Intelligence, which seeks a return to the original roots of AI (the construction of complete, human-level thinking systems). Unlike many AGI researchers, his approach is as much about psychology as traditional AI, because he believes that the complex-system nature of thinking systems make it almost impossible to build a safe and functioning AGI unless its design is as close as possible to the design of the human cognitive system. [<a href=\"http://www.richardloosemore.com/\">Homepage</a>]</p>\n<p><strong>Monica Anderson</strong> has been interested in the quest for computer based cognition since college, and ever since then has sought out positions with startup companies that have used cutting-edge technologies that have been labeled as \"AI\". However, those that worked well, such as expert systems, have clearly been of the \"Weak AI\" variety. In 2001 she moved from using AI techiques as a programmer to trying to advance the field of \"Strong AI\" as a researcher. She is the founder of <a href=\"http://syntience.com/\">Syntience Inc.</a>, which was established to manage funding for her exploration of this field. She has a Master's degree in Computer Science from Link\u00f6ping University in Sweden. She created three expert systems for Cisco Systems for product configuration verification; She has co-designed systems to automatically classify documents by content; She has (co-)designed and/or (co-)written LISP interpreters, debuggers, chat systems, OCR output parsers, visualization tools, operating system kernels, MIDI control real-time systems for music, virtual worlds, and peer-to-peer distributed database systems. She was Manager of Systems Support for Schlumberger Palo Alto Research. She has worked with robotics, industrial control, marine, and other kinds of embedded systems. She has worked on improving the quality of web searches for Google. She wrote a Genetic Algorithm which successfully generated solutions for the Set Coverage Problem (which has been shown to be NP-hard) around 1994. She has used more than a dozen programming languages professionally and designed or co-designed at least four programming languages, large or small. English is her third human language out of four or five. [<a href=\"http://artificial-intuition.com/anderson.html\">More</a>]</p>\n<h3 id=\"The_Interview__New_Questions_\">The Interview (New Questions)<br></h3>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>I will try to answer your questions, but probably not all, and with some disclaimers.</p>\n<p style=\"padding-left: 30px;\"><strong></strong>As another disclaimer: the questions, and the website lesswrong.com that I glanced at, seem to be influenced by Raymond Kurzweil's books.&nbsp; I have not read those books, though of course, I heard about them in conversations, and have seen some reviews.&nbsp; I do not promise never to read them, but waiting for this would delay my answers indefinitely.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau: </strong>Keep in mind that the thoughts expressed here reflect my state of mind and my knowledge at the time of the writing, and may significantly differ after further discussions, readings and thoughts. I have no definite idea about any of the given questions.<strong><br></strong></p>\n<p><strong>Q1:</strong> <em>Assuming beneficial political and economic development and that no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of artificial intelligence that is roughly as good as humans at science, mathematics, engineering and programming?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>2050/2200/2500</p>\n<p style=\"padding-left: 30px;\">The assumptions, by the way, are unrealistic. There will be disruptions.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>I believe that, in my lifetime, computers will only be proficient at well-defined and specialized tasks. Success in the above disciplines requires too much real-world understanding and social interaction. I will not even attempt projections beyond my lifetime (let's say beyond 40 years).<strong><br></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>See Ray Kurzweil's book:&nbsp; The Singularity Is Near.</p>\n<p style=\"padding-left: 30px;\">As I recall, he thinks it will occur before mid-century.</p>\n<p style=\"padding-left: 30px;\">I think he is off by at least an additional 50 years (but I think we'll have as manypersonal robots as cars by 2100.)</p>\n<p style=\"padding-left: 30px;\">One must also distinguish between the first breakthrough of a technology vs. that breakthrough becoming cheap enough to be commonplace, so I won't give you any percentages.&nbsp; (Several decades passed between the first cell phone and billions of people having cell phones.)</p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs:&nbsp;</strong>I cannot calibrate my answer as exactly as the percentages require, so I will<br>just concentrate on the 90%.&nbsp; The question is a common one, but in my opinion<br>history will not answer it in this form.&nbsp; Machines do not develop in direct<br>competition of human capabilities, but rather in attempts to enhance and<br>complement them.&nbsp; If they still become better at certain tasks, this is a side<br>effect.&nbsp; But as a side effect, it will indeed happen that more and more tasks<br>that we proudly claim to be creative in a human way, will be taken over by<br>computer systems.&nbsp; Given that the promise of artificial intelligence is by now<br>50 years old, I am very cautious with numbers, and will say that at least 80<br>more years are needed before jokes about the stupidity of machines will become<br>outdated.</p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>2025/2030/2045.<br><br>Assuming that we have the right program in 2035 by 100% probability, it could still take about 10 years to train it adequately, even though we might find that our programs by then learn much faster than humans. I anticipate that the most expensive part of developing an AI will be training, although we tend to assume that after we bring it up to primary school level, i.e. it can read and write, it would be able to learn much on its own. I optimistically estimated that it would take $10 million dollars and 10 years to train an AI in basic science. Extending that to cover all four of science, mathematics, engineering and programming could take even longer. It takes a human, arguably 15-20 years of training to be a good programmer, and very few humans can program well after that much educational effort and expense.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong id=\"Laurent_Orseau_\">Laurent Orseau:</strong></p>\n<p style=\"padding-left: 30px;\">10%: 2017<br>50%: 2032<br>90%: 2100<br><br>With a quite high uncertainty though.<br>My current estimate is that (I hope) we will know we have built a core AGI by 2025, but a lot of both research and engineering work and time (and learning for the AGI) will be required for the AGI to reach human level in most domains, up to 20 years in the worst case I speculate and 5 years at least, considering that a lot of people will probably be working on it at that time. That is, if we really want to make it human-like. <strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>2015 - 2020 - 2025<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong id=\"Monica_Anderson__\">Monica Anderson: </strong></p>\n<p style=\"padding-left: 30px;\">10%&nbsp; 2020<br>50%&nbsp; 2026<br>90%&nbsp; 2034<br><br>These are all Reductionist sciences. I assume the question is whether we'll have machines capable of performing Reduction in these fields. If working on pre-reduced problems, where we already have determined which Models (formulas, equations, etc) to use and know the values of all input variables, then we already have Mathematica. But here the Reduction was done by a human so Mathematica is not AI.<br><br>AIs would be useful for more everyday things, such as (truly) Understanding human languages years before they Understand enough to learn the Sciences and can perform full-blown Reduction. This is a much easier task, but is still AI-Complete. I think the chance we'll see a program truly Understand a human language at the level of a 14-year old teenager is<br><br>10% 2014<br>50% 2018<br>90% 2022<br><br>Such an AI would be worth hundreds of billions of dollars and makes a worthy near-term research goal. It could help us radically speed up research in all areas by allowing for vastly better text-based information filtering and gathering capabilities, perfect voice based input, perfect translation, etc.<strong><br></strong></p>\n<p><strong>Q2:</strong> <em>Once we build AI that is roughly as good as humans at science, mathematics, engineering and programming, how much more difficult will it be for humans and/or AIs to build an AI which is substantially better at those activities than humans?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>It depends upon how AGI is achieved. If it's through design breakthroughs in AI architecture, then the Singularity will follow. If it's through mimicking nanorecordings, then no Singularity is implied and may not occur at all.</p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>Not much. I would guess something on the order of a decade or two.<strong><br></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>Machines and many specific algorithms are <strong>already </strong>substantially better at their tasks than humans.<br>(What human can compete with a relational database?, or with a Bayesian reasoner? or with scheduler?, or with an intersection-search mechanism like WATSON? etc.)</p>\n<p style=\"padding-left: 30px;\">For dominance over humans, machines have to first acquire the ability to understand human language and to have thoughts in the way humans have thoughts.&nbsp;&nbsp; Even though the WATSON program is impressive, it does NOT know what a word actually means (in the sense of being able to answer the question:&nbsp; \"How does the meaning of the word \"walk\"&nbsp; differ from the meaning of the word \"dance\", physically, emotionally, cognitively, socially?\"</p>\n<p style=\"padding-left: 30px;\">It's much easier to get computers to beat humans at technical tasks (such as sci, math, eng. prog.) but humans are vastly superior at understanding language, which makes humans the master of the planet.&nbsp; So the real question is:&nbsp; At what point will computers understand natural language as well as humans?<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>This is also hard to quantify, since in some areas machines will still be behind, while in others they will already be substantially better: in my opinion, this is already the case.&nbsp; If I still need to give a number, I say 30 years.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>I expect that by the time such a knowledgeable AI is developed, it will already be thinking and learning faster than an average human. Therefore, I think, simply by virtue of continuing miniaturization of computer architecture, or other technological developments that increase our computational resources (e.g., cheaper energy technologies such as fusion), a general-purpose AI could vastly transcend human-level intelligence.</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau: </strong>Wild guess: It will follow Moore's law (see below).</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>Very little difficulty. I expect it to happen immediately after the first achievement, because at the very least we could simply increase the clock speed in relevant areas. It does depend exactly how you measure \"better\", though.</p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson:</strong> What does \"better\" mean? If we believe, as many do, that Intelligence is for Prediction, and that the best measure of our intelligence is whether we can predict the future in complex domains, then we can interpret the given question as \"when can an AI significantly outpredict a human in their mundane everyday environment\".<br><br>For any reasonable definition of \"significant\", the answer is \"never\". The world is too complex to be predictable. All intelligences are \"best-effort\" systems where we do as best we can and learn from our mistakes when we fail, for fail we must. Human intelligences have evolved to the level they have because it is a reasonable level for superior survival chances in the environments in which we've evolved. More processing power, faster machines, etc.&nbsp; do not necessarily translate into an improved ability to predict the environment, especially if we add AIs to this environment. A larger number of competent agents like AIs will make the domain even MORE complex, leading to LOWER predictability. For more about this, see <a href=\"http://hplusmagazine.com/2010/12/15/problem-solved-unfriendly-ai\">http://hplusmagazine.com/2010/12/15/problem-solved-unfriendly-ai</a>.<br><br>Improved ability to handle Models (creating a \"super Mathematica\") is of limited utility for the purpose of making longer-term predictions. Chains of Reductionist Models attempting to predict the future tend to look like Rube Goldberg machines and are very likely to fail, and to fail spectacularly (which is what Brittleness is all about).<br><br>Computers will not get better at Reduction (the main skill required for Science, Mathematics, Engineering, and Programming) until they gather a lot of experience of the real world. For instance, a programming task is 1% about Understanding programming and 99% about Understanding the complex reality expressed in the spec of the program. This can only be improved by Understanding reality better, which is a slow process with the limitations described above. For an introduction to this topic, see my article \"Reduction Considered Harmful\" at <a href=\"http://hplusmagazine.com/2011/03/31/reduction-considered-harmful\">http://hplusmagazine.com/2011/03/31/reduction-considered-harmful</a>.<br><br>The \"Problem with Reduction\" is actually \"The Frame Problem\" as described by John McCarthy and Pat Hayes, viewed from a different angle. It is not a problem that AI research can continue to ignore, which is what we've done for decades. It will not go away. The only approach that works is to sidestep the issue of continuous Model update by not using Models. AIs must use nothing but Model Free Methods since these work without performing Reduction (to Models) and hence can be used to IMPLEMENT automatic Reduction.<strong></strong></p>\n<p><strong>Q3:</strong> <em>Do you ever expect artificial intelligence to overwhelmingly outperform humans at typical academic research, in the way that they may soon overwhelmingly outperform humans at trivia contests, or do you expect that humans will always play an important role in scientific progress?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb:</strong> They will overwhelmingly outperform if and only if we achieve artificial general intelligence through human understanding of intelligence or through artificial understanding of intelligence (vs nanomeasurements).</p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>Like I said, not in my lifetime, and projections beyond that are somewhat meaningless I think.</p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>Regarding trivia contests, WATSON's performance fools people into thinking that the language problem has been solved, but the WATSON program does not understand language.&nbsp;&nbsp; It does an intersection search across text so, for example, if it knows that the answer category is human and that a clue is \"Kitty Hawk\" then it can do an intersection search and come up with Wright Brothers.&nbsp; The question can sound complicated but WATSON can avoid comprehending the question and just return the best intersection that fits the answer category.&nbsp; It can treat each sentence as a bag of words/phrases.&nbsp; WATSON cannot read a child's story and answer questions about what the characters wanted and why they did what they did.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>Absolutely, but the contest will never be direct, and therefore the victory will never have to be acknowledged.&nbsp; Whatever tasks the machines are taking over, will always be considered as tasks that are just not worthy of humans.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>Yes. In Ray Solomonoff's paper titled \"The time scale of artificial intelligence: Reflections on social effects\", he predicted that it will be possible to build an AI that is as intelligent as many times the entire computer science community (AI Milestone F). He predicted that it would take a short time to go from a human-level AI, to such a vastly intelligent AI that would overwhelmingly outperform not only individual humans, but the entire computer science community. This is called the \"infinity point\" hypothesis, and it was the first scientific formulation of singularity (1985). He formalized the feedback loop by which an AI could increase its own intelligence by working on miniaturization of computer architectures, e.g, Moore's law. The idea is that by being smarter than humans, the AI would accelerate Moore's law, theoretically achieving infinite intelligence in a short, finite time, depending on the initial investment.</p>\n<p style=\"padding-left: 30px;\">However, of course, infinite intelligence is impossible due to physical limits. Unaided Moore's law can only continue up to physical limits of computation which would be reached by 2060's if current rate of progress continued, and needless to mention those limits are sort of impossible to achieve (since they might involve processes that are a bit like blackholes). However, imagine this, the AI could design fusion reactors using the H3 on Moon and energy-efficient processors to achieve large amounts of computation. There could be alternative ways to obtain extremely fast supercomputers, and so forth, Solomonoff's hypothesis could be extended to deal with all sorts of technological advances, for instance a self-improving AI could improve its own code, which designs like Goedel Machine and Solomonoff's Alpha are supposed to accomplish. Therefore, ultimately, such AI's would help improve computer architecture, artificial intelligence, electronics, aerospace, energy, communication technologies, all of which would help build AI's that are perhaps hundreds of thousands of times smarter than individual humans, or perhaps much smarter than the entire humanity as Ray Kurzweil predicts, not just particular scientific communities like the computer science community,</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau: </strong>\"Always\" is a very strong word. So probably not for that last part.<br>I give 100% chances for an AI to vastly outperform humans in some domains (which we already have algorithms for, like calculus and chess of course), 50% in many domains, and 10% in all domains. Humans have some good old genetic biases that might be hard to challenge.<br>But how much better it will be is still very unclear, mostly due to NP-hardness, Legg's prediction hardness results and related no-free-lunch problems, where progress might only be gained through more computing power.<br><br>The AGI might have significantly different hot research topics than humans, so I don't think we will lose our philosophers that fast. And good philosophy can only be done with good science.<br><br>Also, machines are better at chess and other games than me, but that doesn't prevent me from playing.</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>Yes. Except for one thing. Humans will be able to (and some will choose to) augment their own intellectual capacity to the same level as the AIs. In that case, your question gets a little blurred.</p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong>I don't believe AI will even reliably \"overwhelmingly outperform\" humans at trivia contests until they fully Understand language. Language Understanding computers will be a great help, but the overwhelming outperformance in Reduction-related tasks is unlikely to happen. Reduction is very difficult.</p>\n<p><strong>Q4:</strong> <em>What probability do you assign to the possibility of an AI with initially (professional) human-level competence at general reasoning (including science, mathematics, engineering and programming) to self-modify its way up to vastly superhuman capabilities within a matter of hours/days/&lt; 5 years?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>If through nanorecording: approx 0%. Otherwise, the speed/acceleration at which AGIs improve themselves is hard to guess at.</p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp:</strong> I expect such modification will require plenty of real-life interaction.</p>\n<ul>\n<li>hours:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10^-9</li>\n<li>days:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10^-6</li>\n<li>&lt;5 years : 10^-1<strong><br></strong></li>\n</ul>\n<p style=\"padding-left: 30px;\"><strong id=\"Michael_G__Dyer___\">Michael G. Dyer: -</strong></p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>This question presupposes a particular sci-fi scenario that I do not believe in.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>In 5 years, without doing anything, it would already be faster than a human simply by running on a faster computer. If Moore's law continued by then, it would be 20-30 times faster than a human. But if you mean by \"vastly\" a difference of thousand times faster, I give it a probability of only 10% because there might be other kinds of bottlenecks involved (mostly physical). There is also another problem with Solomonoff's hypothesis, which Kurzweil generalized, that we are gladly omitting. An exponential increase in computational speed may only amount to a linear increase in intelligence. It at least corresponds only to a linear increase in the algorithmic complexity of solutions that can be found by any AGI, which is a well known fact, and cannot be worked around by simple shortcuts. If solution complexity is the best measure of intelligence, then, getting much more intelligent is not so easy (take this with a grain of salt, though, and please contrast it with the AIQ idea).</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau: </strong>I think the answer to your question is similar to the answer to:<br>Suppose we suddenly find a way to copy and emulate a whole human brain on a computer; How long would it take *us* to make it vastly better than it is right now?<br>My guess is that we will make relatively slow progress. This progress can get faster with time, but I don't expect any sudden explosion. Optimizing the software sounds a very hard task, if that is even possible: if there were an easy way to modify the software, it is probable that natural selection would have found it by now. Optimizing the hardware should then follows Moore's law, at least for some time.<br>That said, the digital world might allow for some possibilities that might be more difficult in a real brain, like copy/paste or memory extension (although that one is debatable).<br><br>I don't even know if \"vastly superhuman\" capabilities is something that is even possible. That sounds very nice (in the best scenario) but is a bit dubious. Either Moore's law will go on forever, or it will stop at some point. How much faster than a human can a computer compute, taking thermodynamics into account?<br><br>So, before it really becomes much more intelligent/powerful than humans, it should take some time.<br>But we may need to get prepared for otherwise, just in case.</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>Depending on the circumstances (which means, this will not be possible if the AI is built using dumb techniques) the answer is: near certainty.</p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong>0.00% . Reasoning is useless without Understanding because if you don't Understand (the problem domain), then you have nothing to reason about. Symbols in logic have to be anchored in general Understanding of the problem domain we're trying to reason about.</p>\n<p><strong>Q5:</strong> <em>How important is it to research risks associated with artificial intelligence that is good enough at general reasoning (including science, mathematics, engineering and programming) to be capable of radical self-modification, before attempting to build one?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>It is the key issue in the ethics of AI. Without a good case to make, the research may need to cease. To be sure, one aspect of a good case may well be that unethical projects are underway and likely to succeed. Per my answers above, I do not currently believe anything of the kind. No project is near to success.</p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>Its importance grows with the extent to which we allow computers control over critical industrial/medical/economic processes, infrastructure, etc. As long as their role is limited to assisting humans in control, there appears to be little risk.</p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>A robot that does not self-replicate is probably not very dangerous (leaving out robots for warfare).<br>A robot that wants to make multiple copies of itself would be dangerous (because it could undergo a rapid form of Lamarckian evolution.&nbsp; There are two type of replication:&nbsp;&nbsp; factory replication and factory division via the creation of a new factory.&nbsp;&nbsp; In social insects this is the difference between the queen laying new eggs and a hive splitting up to go build new hive structures at a new site.</p>\n<p style=\"padding-left: 30px;\">Assuming that humans remain in control of the energy and resources to a robot-producing factory, then factory replication could be shut down.&nbsp; Robots smart enough to go build a new factory and maintain control over the needed resources would pose the more serious problem.&nbsp; As robots are designed (and design themselves) to follow their own goals (for their own self-survival, especially in outer space) then those goals will come into conflict with those of humans.&nbsp;&nbsp; Asimov's laws are too weak to protect humans and as robots design new versions of themselves then they will eliminate those laws anyway.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong>Not very important. Radical self-modification cannot be undertaken by anyone (including AIs) without Understanding of what would make a better Understander. While it is possible that an AI could be helpful in this research I believe the advances in this area would be small, slow to arrive, and easy to control, hitting various brick walls of radically diminishing returns that easily dis-compensates advances of all kinds including Moore's Law.<br><br>We already use computers to design faster, better, logically larger and physically smaller computers. This has nothing to do with AI since the improvements come from Understanding about the problem domain \u2013 computer design \u2013 that is performed by humans. Greater capability in a computer translates to very small advances in Reductive capability. Yes, Understanding machines may be able to eventually Understand Understanding to the point of creating a better Understander. This is a long ways off; Understanding Understanding is uncommon even among humans. But even then, the unpredictability of our Mundane reality is what limits he advantage any intelligent agent might have.</p>\n<p><strong>Q5-old:</strong> <em>How important is it to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to build AI that is good enough at general reasoning (including science, mathematics, engineering and programming) to undergo radical self-modification?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>This is an impossible task.&nbsp; \"AI\" is not a separate development that can be regulated the way that governments regulate research over infectious bacteria to make sure they do not escape the laboratory.&nbsp; Day for day, we are yielding decision power to smart machines, since we draw---sometimes competitive---advantage from this.&nbsp; Emphasizing that the process is very gradual, I still constructed a parable that illustrates the process via a quick and catastrophic denuement.</p>\n<p style=\"padding-left: 30px;\">Thinking it out almost fourty years ago, I assumed that the nuclear superpowers, the Soviet Union and the USA, would live on till the age of very smart machines. So, at some day, for whatever reason, World War 3 breaks out between these superpowers.&nbsp; Both governments consult their advanced computer systems on how to proceed, and both sides get analogous answers.&nbsp; The Soviet computer says: the first bomb must be dropped on the Kremlin; in the US, the advice is to drop the first bomb on the Pentagon.&nbsp; The Americans still retain enough common sense to ignore the advice; but Soviets are more disciplined, and obey their machine. After the war plays out, the Soviet side wins, since the computer advice was correct on both sides. (And from then on, machines rule...)<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural:</strong> A sense of benevolence or universal ethics/morality would only be required if the said AI is also an intelligent agent that would have to interact socially with humans. There is no reason for a general-purpose AI to be an intelligent agent, which is an abstraction of animal, i.e., as commonly known as an \"animat\" since early cyberneticists. Instead, the God-level intelligence could be an ordinary computer that solves scientific problems on demand. There is no reason for it to control robotic hardware or act on its own, or act like a human or an animal. It could be a general-purpose expert system of some sort, just another computer program, but one that is extremely useful. Ray Solomonoff wrote this about human-like behavior in his paper presented at the 2006 Dartmouth Artificial Intelligence conference (50th year anniversary) titled\"Machine Learning - Past and Future\", which you can download from his website:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://world.std.com/~rjs/dart.pdf\">http://world.std.com/~rjs/dart.pdf</a></p>\n<p style=\"padding-left: 60px;\">\"To start, I\u2019d like to define the scope of my interest in A.I. I am not particularly interested in simulating human behavior. I am interested in creating a machine that can work very difficult problems much better and/or faster than humans can \u2013 and this machine should be embodied in a technology to which Moore\u2019s Law applies. I would like it to give a better understanding of the relation of quantum mechanics to general relativity. I would like it to discover cures for cancer and AIDS. I would like it to find some very good high temperature superconductors. I would not be disappointed if it were unable to pass itself off as a rock star.\"</p>\n<p style=\"padding-left: 30px;\">That is, if you constrain the subject to a non-autonomous, scientific AI, I don't think you'll have to deal with human concepts like \"friendly\" at all. Without even mentioning how difficult it might be to teach any common sense term to an AI. For that, you would presumably need to imitate the way humans act and experience.</p>\n<p style=\"padding-left: 30px;\">However, to solve the problems in science and engineering that you mention, a robotic body, or a fully autonomous, intelligent agent, is not needed at all. Therefore, I think it is not very important to work on friendliness for that purpose. Also, one person's friend is another's enemy. Do we really want to introduce more chaos to our society?</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau:</strong> It is quite dubious that \"provably friendly\" is something that is possible.<br>A provably friendly AI is a dead AI, just like a provably friendly human is a dead human, at least because of how humans would use/teach it, and there are bad guys who would love to use such a nice tool.<br>The safest \"AI\" system that I can think of is a Q/A system that is *not* allowed to ask questions (i.e. to do actions). But then it cannot learn autonomously and may not get as smart as we'd like, at least in reasonable time; I think it would be quite similar to a TSP solver: its \"intelligence\" would be tightly linked to its CPU speed.<br><br>\"Provably epsilon-friendly\" (with epsilon &lt;&lt; 1 probability that it might not be always friendly) is probably a more adequate notion, but I'm still unsure this is possible to get either, though maybe under some constraints we might get something.<br><br>That said, I think this problem is quite important, as there is still a non-negligible possibility that an AGI gets much more *power* (no need for vastly more intelligence) than humanity, even without being more intelligent. An AGI could travel at the speed of information transfer (so, light speed) and is virtually immortal by restoring from backups and creating copies of itself. It could send emails on behalf of anyone, and could crack high security sites with as much social engineering as we do. As it would be very hard to put in jail or to annihilate, it would feel quite safe (for its own life) to do whatever it takes to achieve its goals.<br>Regarding power and morality (i.e. what are good goals), here is a question: Suppose you are going for a long walk in the woods in a low populated country, on your own. In the middle of the walk, some big guy pops out of nowhere and comes to talk to you. He is ugly, dirty, smells horribly bad, and never stops talking. He gets really annoying, poking you and saying nasty things, and it's getting worse and worse. You really can't stand it anymore. You run, you go back and forth, you shout at him, you vainly try to reason him but you can't get rid of him. He just follows you everywhere. You don't really want to start a fight as he looks much stronger than you are. Alas, it will take you some 5 more hours to get back to your car and nobody else is in the woods. But In your pocket you have an incredible device: A small box with a single button that can make everything you wish simply disappear instantly. No blood, no pain, no scream, no trace, no witness, no legal problem, 100% certified. At some instant the guy would be here, the next instant he would not, having simply vanished. As simple as that. You don't know what happens to the disappeared person. Maybe he dies, maybe he gets teleported somewhere, or reincarnated or whatever. You know that nobody knows this guy, so nobody can miss him or even look for him. You try to explain to him what this box is, you threaten him to press the button but he does not care. And he's getting so, so annoying, that you can't refrain to scream. Then you stare at the button... Will you press it?<br>My guess is that most people would like to say no, because culture and law say it's bad, but the truth may be that most of them would be highly tempted if facing such a situation. But if they had a gun or a saber instead of a button, the answer would probably be a straighter no (note that a weapon injury is much like a death sentence in the woods). The definition of morality might depend on the power you have.<br><br>But, hopefully, we will be sufficiently smart to put a number of safety measures and perform a lot of testing under stressful conditions before launching it in the wild.</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>Absolutely essential. Having said that, the task of making it \"provably\" friendly is not as difficult as portrayed by organizations (SIAI, FHI) that have a monomaniacal dedication to AI techniques that make it impossible. So in other words: essential, but not a difficult task at all.<strong><br></strong></p>\n<p><strong>Q6: </strong><em>What probability do you assign to the possibility of human extinction within 100 years as a result of AI capable of self-modification (that is not provably non-dangerous, if that is even possible)? P(human extinction by AI | AI capable of self-modification and not provably non-dangerous is created)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Kevin Korb: </strong>This question is poorly phrased.</p>\n<p style=\"padding-left: 30px;\">You should ask relative to a time frame. After all, the probability of human extinction sometime or other is 1. (<span style=\"color: #ff0000;\"><strong>Note by XiXiDu</strong></span>: I added <em>\"within 100 years\" </em>to the question after I received his answers.)<em><br></em></p>\n<p style=\"padding-left: 30px;\">\"Provably\" is also problematic. Outside of mathematics, little is provable.</p>\n<p style=\"padding-left: 30px;\">My generic answer is that we have every prospect of building an AI that behaves reasonably vis-a-vis humans, should we be able to build one at all. We should, of course, take up those prospects and make sure we do a good job rather than a bad one. <strong><br></strong></p>\n<p style=\"padding-left: 30px;\"><strong>John Tromp: </strong>The ability of humans to speed up their own extinction will, I expect, not be matched any time soon by machine, again not in my lifetime.</p>\n<p style=\"padding-left: 30px;\"><strong>Michael G. Dyer: </strong>Loss of human dominance is a foregone conclusion (100% for loss of dominance). <strong>Every </strong>alien civilization (including humans) that survives its own annihilation (via nuclear, molecular and nano technologies) will at some point figure out how to produce synthetic forms of its own intelligence.&nbsp;&nbsp; These synthetics beings are necessary for space travel (because there is most likely no warp drive possible and even planets in the Goldilocks zone will have unpleasant viral and cellular agents). Biological alien creatures will be too adapted to their own planets.</p>\n<p style=\"padding-left: 30px;\">As to extinction, we will only not go extinct if our robot masters decide to keep some of us around.&nbsp; If they decide to populate new planets with human life then they could make the journey and humans would thrive (but only because the synthetic agents wanted this).</p>\n<p style=\"padding-left: 30px;\">If a flying saucer ever lands, the chances are 99.99% that what steps out will be a <strong>synthetic </strong>intelligent entity.&nbsp;&nbsp; It's just too hard for biological entities (adapted to their planet) to make the long voyages required.</p>\n<p style=\"padding-left: 30px;\"><strong>Peter Gacs: </strong>I give it a probability near 1%.&nbsp; Humans may become irrelevant in the sense of losing their role of being at the forefront of the progress of \"self-knowledge of the universe\" (whatever this means).&nbsp; But irrelevance will also mean that it will not be important to eradicate them completely.&nbsp; On the other hand, there are just too many, too diverse imaginable scenarios for their coexistence with machines that are smarter than they are, so I don't dare to predict any details.&nbsp; Of course, species do die out daily even without our intent to extinguish them, but I assume that at least some humans would find ways to survive for some more centuries to come.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Eray Ozkural: </strong>Assuming that we are talking about intelligent agents, which are strictly unnecessary for working on scientific problems which is your main concern, I think first that it is not possible to build something that is provably non-dangerous, unless you can encode a rule of non-interference into its behavior. Otherwise, an interfering AI can basically do anything, and since it is much smarter than us, it can create actual problems that we had no way of anticipating or solving. I have thought at length on this question, and considered some possible AI objectives in a blog essay:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://www.examachine.net/blog/?p=72\">http://www.examachine.net/blog/?p=72</a></p>\n<p style=\"padding-left: 30px;\">I think that it does depend on the objectives. In particular, selfish/expansionist AI objectives are very dangerous. They might almost certainly result in interference with our vital resources. I cannot give a probability, because it is a can of worms, but let me try to summarize. For instance, the objective to maximize its knowledge about the world, a similar version of which was considered by Laurent Orseau in a reinforcement learning setting, and previously by a student of Solomonoff. Well, it's an intuitive idea that a scientist tries to learn as much as possible about the world. What if we built an intelligent agent that did that? If it's successful, it would have to increase its computation and physical capacity to such an extent that it might expand rapidly, first assimilating the solar system and then expand at our galactic neighborhood to be able to pursue its unsatisfiable urge to learn. Similar scenarios might happen in any kind of intelligent agent with selfish objectives (i.e., optimize some aspect of itself). Those might be recognized as Omohundro drives, but the objectives themselves are the main problem mostly.</p>\n<p style=\"padding-left: 30px;\">This is a problem when you are stuck in this reinforcement learning mentality, thinking in terms of rewards and punishment. The utility function that you will define will tend to be centered around the AI itself rather than humanity, and things have a good chance of going very wrong. This is mostly regardless of what kind of selfishness is pursued, be it knowledge, intelligence, power, control, satisfaction of pseudo pleasure, etc. In the end, the problem is with the relentless pursuit of a singular, general objective that seeks to benefit only the self. And this cannot be mitigated by any amount of obstruction rules (like Robot Laws or any other kind of laws). The motivation is what matters, and even when you are not pursuing silly motivations like stamp collection, there is a lot of danger involved, not due to our neglect of human values, which are mostly irrelevant at the level which such an intelligent agent would operate, but our design of its motivations.</p>\n<p style=\"padding-left: 30px;\">However, even if benevolent looking objectives were adopted, it is not altogether clear, what sorts of crazy schemes an AI would come up with. In fact, we could not predict the plans of an intelligent agent smarter than the entire humanity. Therefore, it's a gamble at best, and even if we made a life-loving, information-loving, selfless, autonomous AI as I suggested, it might still do a lot of things that many people would disagree with. And although such an AI might not extinguish our species, it might decide, for instance, that it would be best to scan and archive our species for using later. That is, there is no reason to expect that an intelligent agent that is superior to us in every respect should abide by our will.</p>\n<p style=\"padding-left: 30px;\">One might try to imagine many solutions to make such intelligent agents \"fool-proof\" and \"fail-safe\", but I suspect that for the first, human foolishness has unbounded inventiveness, and for the second, no amount of security methods that we design would make a mind that is smarter than the entire humanity \"safe\", as we have no way of anticipating every situation that would be created by its massive intelligence, and the amount of chaotic change that it would bring. It would simply go out of control, and we would be at the mercy of its evolved personality. I said personality on purpose, because personality seems to be a result of initial motivations, a priori knowledge, and its life experience. Since its life experience and intelligence will overshadow any initial programming, we cannot really foresee its future personality. All in all, I think it is great for thinking about, but it does not look like a practical engineering solution. That's why I simply advise against building fully autonomous intelligent agents. I sometimes say, play God, and you will fail. I tend to think there is a Frankenstein Complex, it is as if there is an incredible urge in many people to create an independent artificial person.</p>\n<p style=\"padding-left: 30px;\">On the other hand, I can imagine how I could build semi-autonomous agents that might be useful for many special tasks, avoiding interference with humans as much as possible, with practical ways to test for their compliance with law and customs. However, personally speaking, I cannot imagine a single reason why I would want to&nbsp; create an artificial person that is superior to me in every respect. Unless of course, I have elected to bow down to a superior species.</p>\n<p style=\"padding-left: 30px;\"><strong>Laurent Orseau:</strong> It depends if we consider that we will simply leave safety issues aside before creating an AGI, thinking that all will go well, or if we take into account that we will actually do some research on that.<br>If an human-level AGI was built today, then we probably wouldn't be ready and the risks due to excitement to get something out of it might be high (\"hey look, it can drive the tank, how cool is that?!\").<br><br>But if we build one and can show to the world a simple proof of concept that we do have (sub-human level) AGI and that will grow to human-level and most researchers acknowledge it, I presume we will start to think hard about the consequences.<br><br>Then all depends on how much unfriendly it is.<br>Humanity is intelligent enough to care for its own life, and try to avoid high risks (most of the time), unless there is some really huge benefit (like supremacy).<br><br>Also, if an AGI wants to kill all humans, humanity would not just wait for it, doing nothing.<br>This might be dangerous for the AI itself too (with EMPs for example). And an AGI also wants to avoid high risks unless there is a huge benefit. If some compromise is possible, it should be better.<br><br>If we can build an AGI that is quite friendly (i.e. has \"good\" goals and wants to cooperate with humans without pressing them too much, or at least has no incentive to kill humans) but may become nasty only if its life is at stake, then I don't think we need to worry *too* much: just be friendly with it as you would be with an ally, and its safety will be paired with your own safety.<br><br>So I think the risks of human extinction will be pretty low, as long as we take them into account seriously.</p>\n<p style=\"padding-left: 30px;\"><strong>Richard Loosemore: </strong>The question is loaded, and I reject the premises. It assumes that someone can build an AI that is both generally intelligent (enough to be able to improve itself) whilst also having a design whose motivation is impossible to prove. That is a false assumption. People who try to build AI systems with the kind of design whose motivation is unstable will actually not succeed in building anything that has enough general intelligence to become a danger.</p>\n<p style=\"padding-left: 30px;\"><strong>Monica Anderson: </strong>0.00%. All intelligences must be fallible in order to deal with a complex and illogical world (with only incomplete information available) on a best effort basis. And if an AI is fallible, then we can unplug it... sooner or later, even if it is \"designed to be unstoppable\". Ten people armed with pitchforks, and armed also with ten copies of last year's best AI can always unplug the latest model AI.</p>\n<h3 id=\"The_Interview__Old_Questions_\"><strong>The Interview (Old Questions)</strong></h3>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em></p>\n<p><em>Explanatory remark to Q1:<br><br>P(human-level AI by (year) | no wars \u2227 no disasters \u2227 beneficially political and economic development) = 10%/50%/90%</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>For me, roughly human-level machine intelligence is an embodied machine. Given the current difficulties of making such machines I expect it will last at least several hundred years before human-level intelligence can be reached. Making better machines is not a question of superintelligence, but of long and hard work. Try getting some responses to your questionnaire from roboticists.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>Experts usually are correct in their predictions but terrible in their timing predictions. They usually see things as coming earlier than the event actually occurs as they fail to see the obstacles. Also, it is unclear what you mean as human-level intelligence. The Turing test will be passed in its simplest form perhaps in 20 years. Full functional replacements for humans will likely take over 100 years (50% likelihood). 200 years (90% likelihood). <strong><br></strong></p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction as a result of badly done AI?<br><br>Explanatory remark to Q2:<br><br>P(human extinction | badly done AI) = ?<br><br>(Where 'badly done' = AGI capable of self-modification that is not provably non-dangerous.)</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>Human beings are already using all sorts of artificial intelligence in their (war)machines, so there it is not impossible that our machines will be helpful in human extinction.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>Ultimately 95% (and not just by bad AI, but just by generalized evolution). In other words, in this sense all AI is badly done AI for I think it is a natural sequence that AI leads to superior artificial minds that leads to eventual evolution, or replacement (depending on the speed of the transformation), of humans to artificial life. <strong><br></strong></p>\n<p><strong>Q3:</strong> <em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years?<br><br>Explanatory remark to Q3:<br><br>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?<br>P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 GB Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>I don\u2019t know what \"massive superhuman intelligence\" is, what it is for, and if it existed how to measure it.</p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>I am not sure of the question, or maybe only that I do not understand an answer. Let me comment anyway. I have always felt it likely that the first superhuman intelligence would be a simulation of the human mind; e.g., by advanced neural-net-like structures. I have never thought seriously about learning time, but I guess the first success would be after some years of processing. I am not sure of what you mean by ``massive''. Such a mind as above coupled to good retrieval algorithms with extensive databases such as those being developed now could appear to have massive superhuman intelligence. <strong><br></strong></p>\n<p><strong>Q4:</strong> <em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence? </em></p>\n<p><em>Explanatory remark to Q4:<br><br>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?<br></em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>Proof is for mathematics, not for actual machines. Even for the simplest machines we have nowadays we cannot proof any aspect of their operation. If this were possible, airplane travel would be a lot safer.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>It is important to try. I do not think it can be done. I feel that humans are safe from AI takeover for this century. Maybe not from other calamities, however.<strong><br></strong></p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?</em></p>\n<p><em>Explanatory remark to Q5:</em></p>\n<p><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk? </em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>No idea how to compare these risks.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>Ouch. Now you want me to amplify my casual remark above. I guess that I can only say that I hope we are lucky enough for the human race to survive long enough to evolve into, or be taken over by, another type of intelligence. <strong><br> </strong></p>\n<p><strong>Q6:</strong> <em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>People think that current AI is much more capable than it is in reality, and therefore they often overestimate the risks. This is partly due to the movies and due to scientists overselling their work in scientific papers and in the media. So I think the risk is highly overestimated.<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>The current level of awareness of the AI risks is low. The risk that I most focus on now is the economic repercussions of advancing AI. Together with outsourcing, the advancing automation of the workplace, now dominated by AI advances,&nbsp; is leading to increasing unemployment. This progression will not be monotonic, but each recession will result in more permanently unemployed and weaker recoveries. At some point our economic philosophy could change radically in the U.S., an event very similar to the great depression. We may not recover, in the sense of returning to the same economic structure. I think (hope) that democracy will survive. <strong><br></strong></p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human-level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Leo Pape: </strong>I would be impressed if a team of soccer playing robots could win a match against professional human players. Of course, the real challenge is finding human players that are willing to play against machines (imagine being tackled by a metal robot).<strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>A \"pure\" learning program that won at Jeopardy ???<strong><br></strong></p>\n<p><strong>Q8:</strong> <em>Are you familiar with formal concepts of optimal AI design which relate to searches over complete spaces of computable hypotheses or computational strategies, such as Solomonoff induction, Levin search, Hutter's algorithm M, AIXI, or G\u00f6del machines?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Donald Loveland: </strong>I have some familiarity with Solomonoff inductive inference but not Hutter's algorithm. I have been retired for 10 years so didn't know of Hutter until this email. Looks like something interesting to pursue.<strong><br></strong></p>", "sections": [{"title": "[Click here to see a list of all interviews]", "anchor": "_Click_here_to_see_a_list_of_all_interviews_", "level": 2}, {"title": "The Interview (New Questions)", "anchor": "The_Interview__New_Questions_", "level": 1}, {"title": "Laurent Orseau:", "anchor": "Laurent_Orseau_", "level": 2}, {"title": "Monica Anderson: ", "anchor": "Monica_Anderson__", "level": 2}, {"title": "Michael G. Dyer: -", "anchor": "Michael_G__Dyer___", "level": 2}, {"title": "The Interview (Old Questions)", "anchor": "The_Interview__Old_Questions_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "24 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BEtQALqgXmL9d9SfE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T19:22:50.716Z", "modifiedAt": null, "url": null, "title": "Zeckhauser's roulette", "slug": "zeckhauser-s-roulette", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bo5eY2Jzvh9xfSeDn/zeckhauser-s-roulette", "pageUrlRelative": "/posts/bo5eY2Jzvh9xfSeDn/zeckhauser-s-roulette", "linkUrl": "https://www.lesswrong.com/posts/bo5eY2Jzvh9xfSeDn/zeckhauser-s-roulette", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Zeckhauser's%20roulette&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AZeckhauser's%20roulette%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbo5eY2Jzvh9xfSeDn%2Fzeckhauser-s-roulette%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Zeckhauser's%20roulette%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbo5eY2Jzvh9xfSeDn%2Fzeckhauser-s-roulette", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbo5eY2Jzvh9xfSeDn%2Fzeckhauser-s-roulette", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>Imagine you're playing Russian roulette. Case 1: a six-shooter contains four bullets, and you're asked how much you'll pay to remove one of them. &nbsp;Case 2: a six-shooter contains two bullets, and you're asked how much you'll pay to remove both of them. Steven Landsburg describes&nbsp;<a href=\"http://www.thebigquestions.com/2011/01/05/another-rationality-test/\">an argument by Richard Zeckhauser and Richard Jeffrey</a>&nbsp;saying you should pay the same amount in both cases, provided that you don't have heirs and all your remaining money magically disappears when you die. What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bo5eY2Jzvh9xfSeDn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 8.344235397135586e-07, "legacy": true, "legacyId": "12303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-19T19:39:58.264Z", "modifiedAt": null, "url": null, "title": "Quixey Challenge - Fix a bug in 1 minute, win $100. Refer a winner, win $50.", "slug": "quixey-challenge-fix-a-bug-in-1-minute-win-usd100-refer-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.344Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Liron", "createdAt": "2009-02-27T04:43:11.294Z", "isAdmin": false, "displayName": "Liron"}, "userId": "AyzRrs8hNm54QptLi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t9YffNzqkzxCQk5CG/quixey-challenge-fix-a-bug-in-1-minute-win-usd100-refer-a", "pageUrlRelative": "/posts/t9YffNzqkzxCQk5CG/quixey-challenge-fix-a-bug-in-1-minute-win-usd100-refer-a", "linkUrl": "https://www.lesswrong.com/posts/t9YffNzqkzxCQk5CG/quixey-challenge-fix-a-bug-in-1-minute-win-usd100-refer-a", "postedAtFormatted": "Thursday, January 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quixey%20Challenge%20-%20Fix%20a%20bug%20in%201%20minute%2C%20win%20%24100.%20Refer%20a%20winner%2C%20win%20%2450.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuixey%20Challenge%20-%20Fix%20a%20bug%20in%201%20minute%2C%20win%20%24100.%20Refer%20a%20winner%2C%20win%20%2450.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9YffNzqkzxCQk5CG%2Fquixey-challenge-fix-a-bug-in-1-minute-win-usd100-refer-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quixey%20Challenge%20-%20Fix%20a%20bug%20in%201%20minute%2C%20win%20%24100.%20Refer%20a%20winner%2C%20win%20%2450.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9YffNzqkzxCQk5CG%2Fquixey-challenge-fix-a-bug-in-1-minute-win-usd100-refer-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9YffNzqkzxCQk5CG%2Fquixey-challenge-fix-a-bug-in-1-minute-win-usd100-refer-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p><a href=\"http://www.quixeychallenge.com/?ref=lesswrong\"><img src=\"http://www.quixeychallenge.com/static/images/quixeychallenge.png\" alt=\"\" width=\"100\" height=\"50\" /></a></p>\n<p>Hiring is so hard that we spent a man-month creating a sub-startup to do it. The product is the&nbsp;<a href=\"http://www.quixeychallenge.com/?ref=lesswrong\">Quixey Challenge</a>&nbsp;which is running today until 7pm PST (GMT-8).</p>\n<p>Benefits of playing:</p>\n<ul>\n<li>You can learn something from our craftsmanship of the algorithms (we work hard on them)</li>\n<li>The 1-minute challenge is a rush</li>\n<li>You can make money</li>\n<li>If you do well you can interview at Quixey</li>\n</ul>\n<div>Even if you have zero engineering skills, you can get $50 for referring someone who wins.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t9YffNzqkzxCQk5CG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 5, "extendedScore": null, "score": 8.344300706521385e-07, "legacy": true, "legacyId": "12304", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T02:24:24.930Z", "modifiedAt": null, "url": null, "title": "[META] ajax.googleapis.com", "slug": "meta-ajax-googleapis-com", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:32.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pavitra", "createdAt": "2009-09-22T08:32:44.250Z", "isAdmin": false, "displayName": "Pavitra"}, "userId": "yC2JgX3ENu7mionKh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HGMNw4HLjQgqdth3Y/meta-ajax-googleapis-com", "pageUrlRelative": "/posts/HGMNw4HLjQgqdth3Y/meta-ajax-googleapis-com", "linkUrl": "https://www.lesswrong.com/posts/HGMNw4HLjQgqdth3Y/meta-ajax-googleapis-com", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20ajax.googleapis.com&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20ajax.googleapis.com%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGMNw4HLjQgqdth3Y%2Fmeta-ajax-googleapis-com%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20ajax.googleapis.com%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGMNw4HLjQgqdth3Y%2Fmeta-ajax-googleapis-com", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHGMNw4HLjQgqdth3Y%2Fmeta-ajax-googleapis-com", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>Apparently, by not unblocking scripts for \"ajax.googleapis.com\", I am unable to vote on LW. I generally dislike enabling scripting for domains that are used in many places -- unblocking Google APIs would unblock it everywhere, not just here -- so the result is that I am no longer voting. I suspect that I am not alone in this.</p>\n<p>(Apparently I can't post without enabling it either. Looks like I'll have make an exception and do the script-on-script-off dance after all. Whee.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HGMNw4HLjQgqdth3Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 8.345843321804495e-07, "legacy": true, "legacyId": "12317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T04:12:06.380Z", "modifiedAt": null, "url": null, "title": "Breaking the chain of akrasia", "slug": "breaking-the-chain-of-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:36.443Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4hMQHGMnsR4GJALmc/breaking-the-chain-of-akrasia", "pageUrlRelative": "/posts/4hMQHGMnsR4GJALmc/breaking-the-chain-of-akrasia", "linkUrl": "https://www.lesswrong.com/posts/4hMQHGMnsR4GJALmc/breaking-the-chain-of-akrasia", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Breaking%20the%20chain%20of%20akrasia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABreaking%20the%20chain%20of%20akrasia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hMQHGMnsR4GJALmc%2Fbreaking-the-chain-of-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Breaking%20the%20chain%20of%20akrasia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hMQHGMnsR4GJALmc%2Fbreaking-the-chain-of-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4hMQHGMnsR4GJALmc%2Fbreaking-the-chain-of-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 235, "htmlBody": "<p>I'd like to share my specific motivation for writing&nbsp;<a href=\"/lw/99t/can_the_chain_still_hold_you/\">Can the Chain Still Hold You?</a></p>\n<p>I agree with <a href=\"/lw/9p/rationality_its_not_that_great/\">Yvain</a> that akrasia is probably a major reason that rationality alone doesn't create superheroes. You might be&nbsp;<em>much</em>&nbsp;better than average&nbsp;at making good decisions based on an accurate model of reality, but that doesn't mean you can <em>follow through with them</em>.</p>\n<p>Many people report that their thinking is clearer and better as a result of Less Wrong. But despite our <a href=\"/lw/33s/antiakrasia_reprise/\">many</a>, <a href=\"/lw/ab/akrasia_and_shangrila/\">many</a>&nbsp;<a href=\"/lw/1sm/akrasia_tactics_review\">attempts</a> <a href=\"/lw/7z1/antiakrasia_tool_like_stickkcom_for_data_nerds/\">to</a> <a href=\"/lw/1tu/improving_the_akrasia_hypothesis/\">hack</a> <a href=\"/lw/2qv/browser_buddies_remote_monitoring_experiment/\">away</a> <a href=\"/lw/1fe/antiakrasia_technique_structured_procrastination/\">at</a><a href=\"/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/\"> the</a> <a href=\"/lw/3kt/proposal_antiakrasia_alliance/\">problem</a> <a href=\"/lw/8ug/building_casestudies_of_akrasia/\">of</a> <a href=\"/lw/8s9/an_akrasia_case_study/\">akrasia</a>&nbsp;(more: <a href=\"/lw/e9/fighting_akrasia_finding_the_source/\">1</a>, <a href=\"/lw/38s/akrasia_as_a_collective_action_problem/\">2</a>, <a href=\"/lw/dp/fighting_akrasia_incentivising_action/\">3</a>, <a href=\"/lw/am/how_a_pathological_procrastinor_can_lose_weight/\">4</a>, <a href=\"/lw/9w/silver_chairs_paternalism_and_akrasia/\">5</a>, <a href=\"/lw/2ry/expectationbased_akrasia_management/\">6</a>, <a href=\"/lw/1wx/crunchcourse_a_tool_for_combating_learning_akrasia/\">7</a>, <a href=\"/lw/b7/actions_and_words_akrasia_and_the_fruit_of/\">8</a>, <a href=\"/lw/136/an_akrasia_anecdote/\">9</a>, <a href=\"/lw/3w3/how_to_beat_procrastination/\">10</a>), I haven't heard of many LWers conquering akrasia.</p>\n<p>But I still have hope that this is possible. In 2006, we <em>finally</em>&nbsp;got a decent psychological <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Steel-Konig-Integrating-Theories-of-Motivation.pdf\">theory of procrastination</a>, much <a href=\"http://my.psychologytoday.com/files/attachments/49705/arousal-avoidant-and-decisional-procrastinators-do-they-exist.pdf\">better than</a> the old&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Ferrari-Psychometric-validation-of-two-procrastination-inventoriesfor-adults.pdf\">decisional-avoidant-arousal theory</a>. On the timescale of progress in psychology, 2006 is basically <em>yesterday</em>. The first book on how to apply this new theory to daily life was published in <a href=\"http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/\">late 2010</a>. There is no community of people systematically practicing these techniques and reporting their results.</p>\n<p>So it seems to me there is a lot of low-hanging fruit to be scooped up in the field of procrastination research. If we try and test enough things, and especially if our tests our theory-guided, we may be able to learn new things and flip a few causal factors such that the chain of akrasia no longer holds us &mdash; at least, not as tightly as before.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4hMQHGMnsR4GJALmc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 33, "extendedScore": null, "score": 8.346254157065692e-07, "legacy": true, "legacyId": "12318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iETtCZcfmRyHp69w4", "LgavAYtzFQZKg95WC", "Npz5QQxFe8GxLpEJG", "geqg9mk73NQh6uieE", "rRmisKb45dN7DK4BW", "6oYETaG248zGF45aD", "uKoqrgnRoWjhneDvM", "MhWjxybo2wwowTgiA", "n5Yfhygz42QNK2vFe", "geNZ6ZpfFce5intER", "aYGEmJRX3AbjdDK4x", "jZEsFXyhFyoTY6s3m", "xpLXck2nXbE4K3xdE", "BH8BcP7d2sbmzZig8", "EYiAoxvnKjgJe8GbN", "KW5m4eREWGitPb8Ev", "Z6ESPufeiC4P8c8en", "WMYHEcs5tyFESkjsr", "knc5XjjRGfXoMEiTY", "Gcpu8AoCy5R75Dt86", "rGTfQJ8E5CxqcA6LD", "u4zwGST5Nc7eqfQv6", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T06:09:50.195Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Feel the Meaning", "slug": "seq-rerun-feel-the-meaning", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a8g3Nms9yMemSnMQF/seq-rerun-feel-the-meaning", "pageUrlRelative": "/posts/a8g3Nms9yMemSnMQF/seq-rerun-feel-the-meaning", "linkUrl": "https://www.lesswrong.com/posts/a8g3Nms9yMemSnMQF/seq-rerun-feel-the-meaning", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Feel%20the%20Meaning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Feel%20the%20Meaning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8g3Nms9yMemSnMQF%2Fseq-rerun-feel-the-meaning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Feel%20the%20Meaning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8g3Nms9yMemSnMQF%2Fseq-rerun-feel-the-meaning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8g3Nms9yMemSnMQF%2Fseq-rerun-feel-the-meaning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 245, "htmlBody": "<p>Today's post, <a href=\"/lw/nq/feel_the_meaning/\">Feel the Meaning</a> was originally published on 13 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You think a word has a meaning, as a property of the word itself; rather than there being a label that your brain associates to a particular concept. When someone shouts, \"Yikes! A tiger!\", evolution would not favor an organism that thinks, \"Hm... I have just heard the syllables 'Tie' and 'Grr' which my fellow tribemembers associate with their internal analogues of my own tiger concept and which aiiieeee CRUNCH CRUNCH GULP.\" So the brain takes a shortcut, and it seems that the meaning of tigerness is a property of the label itself. People argue about the correct meaning of a label like \"sound\".</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9hf/seq_rerun_disputing_definitions/\">Disputing Definitions</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a8g3Nms9yMemSnMQF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.346703332909283e-07, "legacy": true, "legacyId": "12323", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dMCFk2n2ur8n62hqB", "gTWD2Tx8MsymmavdW", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T09:20:59.786Z", "modifiedAt": null, "url": null, "title": "PredictionBook: Feature Request and Bug Report", "slug": "predictionbook-feature-request-and-bug-report", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:32.590Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4CqqMAmPZusPicjZw/predictionbook-feature-request-and-bug-report", "pageUrlRelative": "/posts/4CqqMAmPZusPicjZw/predictionbook-feature-request-and-bug-report", "linkUrl": "https://www.lesswrong.com/posts/4CqqMAmPZusPicjZw/predictionbook-feature-request-and-bug-report", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PredictionBook%3A%20Feature%20Request%20and%20Bug%20Report&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APredictionBook%3A%20Feature%20Request%20and%20Bug%20Report%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4CqqMAmPZusPicjZw%2Fpredictionbook-feature-request-and-bug-report%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PredictionBook%3A%20Feature%20Request%20and%20Bug%20Report%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4CqqMAmPZusPicjZw%2Fpredictionbook-feature-request-and-bug-report", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4CqqMAmPZusPicjZw%2Fpredictionbook-feature-request-and-bug-report", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p><strong>Introduction</strong></p>\n<p>It occurred to me (and <a href=\"/r/discussion/lw/9em/open_thread_january_1531_2012/5ok7\">ahartell</a>) that it would be convenient if there were a place where we could report bugs and request features for the prediction tracking website <a href=\"http://predictionbook.com/\">PredictionBook</a>. I propose to make <em>this</em> that place. If you post a comment in this thread pointing out a bug or requesting the addition of a new feature, then I will update this article to include that bug or feature.</p>\n<p><em>Disclaimer: This thread isn't a promise that any particular bug will be fixed or feature added.</em></p>\n<p><strong>Feature Request</strong></p>\n<p>1. The ability to tag prediction statements to allow sorting by topic.</p>\n<p>&nbsp; (a). Prevent prediction statements that are tagged personal from showing up on the main feed or happenstance.</p>\n<p>&nbsp; (b). Allow users to form groups of \"friends\" that can follow prediction statements tagged as personal.</p>\n<p>2. A voting system or flag button to discourage the use of PredictionBook for advertising purposes (and other types of spam).</p>\n<p>3. A threaded comment system for discussing particular prediction statements (this will be much more important as the size of the community grows).</p>\n<p>4. A <a href=\"https://en.wikipedia.org/wiki/Brier_score\">Brier score</a> visible on the user's page.</p>\n<p>&nbsp;&nbsp;&nbsp; (a). Brier scores by topic.</p>\n<p>&nbsp;&nbsp;&nbsp; (b). Brier score over time (say, quarterly).</p>\n<p>5. The ability to compare calibration between users.</p>\n<p>6. The ability to delete comments (perhaps, for a certain time period after the comment is made).</p>\n<p>7. A button on the login page to reset the user's password.</p>\n<p>8. An option on the user page to sort by judged/unjudged/upcoming.</p>\n<p>9. Display a probability for the prediction statement weighed by the users' calibration scores.</p>\n<p><strong>Bug Reports</strong></p>\n<p>1. The email notification system appears not to be working.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4CqqMAmPZusPicjZw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.347432755686776e-07, "legacy": true, "legacyId": "12329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Introduction\">Introduction</strong></p>\n<p>It occurred to me (and <a href=\"/r/discussion/lw/9em/open_thread_january_1531_2012/5ok7\">ahartell</a>) that it would be convenient if there were a place where we could report bugs and request features for the prediction tracking website <a href=\"http://predictionbook.com/\">PredictionBook</a>. I propose to make <em>this</em> that place. If you post a comment in this thread pointing out a bug or requesting the addition of a new feature, then I will update this article to include that bug or feature.</p>\n<p><em>Disclaimer: This thread isn't a promise that any particular bug will be fixed or feature added.</em></p>\n<p><strong id=\"Feature_Request\">Feature Request</strong></p>\n<p>1. The ability to tag prediction statements to allow sorting by topic.</p>\n<p>&nbsp; (a). Prevent prediction statements that are tagged personal from showing up on the main feed or happenstance.</p>\n<p>&nbsp; (b). Allow users to form groups of \"friends\" that can follow prediction statements tagged as personal.</p>\n<p>2. A voting system or flag button to discourage the use of PredictionBook for advertising purposes (and other types of spam).</p>\n<p>3. A threaded comment system for discussing particular prediction statements (this will be much more important as the size of the community grows).</p>\n<p>4. A <a href=\"https://en.wikipedia.org/wiki/Brier_score\">Brier score</a> visible on the user's page.</p>\n<p>&nbsp;&nbsp;&nbsp; (a). Brier scores by topic.</p>\n<p>&nbsp;&nbsp;&nbsp; (b). Brier score over time (say, quarterly).</p>\n<p>5. The ability to compare calibration between users.</p>\n<p>6. The ability to delete comments (perhaps, for a certain time period after the comment is made).</p>\n<p>7. A button on the login page to reset the user's password.</p>\n<p>8. An option on the user page to sort by judged/unjudged/upcoming.</p>\n<p>9. Display a probability for the prediction statement weighed by the users' calibration scores.</p>\n<p><strong id=\"Bug_Reports\">Bug Reports</strong></p>\n<p>1. The email notification system appears not to be working.</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Feature Request", "anchor": "Feature_Request", "level": 1}, {"title": "Bug Reports", "anchor": "Bug_Reports", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T14:42:35.758Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Portland, Berkleley, San Diego, Pittsburgh, Houston, Seattle, Madison, Fort Collins, Sydney, Melbourne", "slug": "weekly-lw-meetups-portland-berkleley-san-diego-pittsburgh", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MzdGumYsmzofTcTQA/weekly-lw-meetups-portland-berkleley-san-diego-pittsburgh", "pageUrlRelative": "/posts/MzdGumYsmzofTcTQA/weekly-lw-meetups-portland-berkleley-san-diego-pittsburgh", "linkUrl": "https://www.lesswrong.com/posts/MzdGumYsmzofTcTQA/weekly-lw-meetups-portland-berkleley-san-diego-pittsburgh", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Portland%2C%20Berkleley%2C%20San%20Diego%2C%20Pittsburgh%2C%20Houston%2C%20Seattle%2C%20Madison%2C%20Fort%20Collins%2C%20Sydney%2C%20Melbourne&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Portland%2C%20Berkleley%2C%20San%20Diego%2C%20Pittsburgh%2C%20Houston%2C%20Seattle%2C%20Madison%2C%20Fort%20Collins%2C%20Sydney%2C%20Melbourne%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMzdGumYsmzofTcTQA%2Fweekly-lw-meetups-portland-berkleley-san-diego-pittsburgh%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Portland%2C%20Berkleley%2C%20San%20Diego%2C%20Pittsburgh%2C%20Houston%2C%20Seattle%2C%20Madison%2C%20Fort%20Collins%2C%20Sydney%2C%20Melbourne%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMzdGumYsmzofTcTQA%2Fweekly-lw-meetups-portland-berkleley-san-diego-pittsburgh", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMzdGumYsmzofTcTQA%2Fweekly-lw-meetups-portland-berkleley-san-diego-pittsburgh", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 444, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/5p\">Portland Meetup?:&nbsp;<span class=\"date\">14 January 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/5q\">San Diego experimental meetup:&nbsp;<span class=\"date\">15 January 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/65\">Pittsburgh Meetup: Big Gaming Fun 2!:&nbsp;<span class=\"date\">15 January 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/5z\">Houston Meetup - 11/15:&nbsp;<span class=\"date\">15 January 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/60\">Seattle, Diseased Thinking and evidence on parenting:&nbsp;<span class=\"date\">15 January 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/64\">Monday Madison Meetup:&nbsp;<span class=\"date\">16 January 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/61\">Fort Collins Meetup:&nbsp;<span class=\"date\">18 January 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/5s\">First Sydney 2012 meetup.:&nbsp;<span class=\"date\">18 January 2012 06:00PM</span></a><a href=\"/meetups/5s\"></a></li>\n<li><a href=\"/meetups/63\">First Salt Lake City Meetup: 22 January 2012 03:00PM:&nbsp;<span class=\"date\">22 January 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/5n\">Columbus or Cincinnati Meetup:&nbsp;<span class=\"date\">22 January 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/62\">Fort Collins Meetup:&nbsp;<span class=\"date\">25 January 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/5f\">First Brussels meetup:&nbsp;<span class=\"date\">11 February 2012 11:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/5x\">Monthly San Francisco Bay Area meetup:&nbsp;<span class=\"date\">14 January 2012 07:49PM</span></a></li>\n<li><a href=\"/meetups/66\">Melbourne social meetup:&nbsp;<span class=\"date\">20 January 2012 06:30PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong></strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>\n<p>If your meetup has a mailing list that you'd like mentioned here or has become regular and isn't listed as such, let me know!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MzdGumYsmzofTcTQA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.348660162038362e-07, "legacy": true, "legacyId": "12147", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T14:46:30.292Z", "modifiedAt": null, "url": null, "title": "Crocker's Rules: How far to take it?", "slug": "crocker-s-rules-how-far-to-take-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:35.068Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Yf5WEMaczBjKZZ8u2/crocker-s-rules-how-far-to-take-it", "pageUrlRelative": "/posts/Yf5WEMaczBjKZZ8u2/crocker-s-rules-how-far-to-take-it", "linkUrl": "https://www.lesswrong.com/posts/Yf5WEMaczBjKZZ8u2/crocker-s-rules-how-far-to-take-it", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Crocker's%20Rules%3A%20How%20far%20to%20take%20it%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACrocker's%20Rules%3A%20How%20far%20to%20take%20it%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYf5WEMaczBjKZZ8u2%2Fcrocker-s-rules-how-far-to-take-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Crocker's%20Rules%3A%20How%20far%20to%20take%20it%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYf5WEMaczBjKZZ8u2%2Fcrocker-s-rules-how-far-to-take-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYf5WEMaczBjKZZ8u2%2Fcrocker-s-rules-how-far-to-take-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 312, "htmlBody": "<p>Recently I've been considering declaring <a href=\"http://wiki.lesswrong.com/wiki/Crocker%27s_rules\">Crocker's Rules</a>. The wiki page and <a href=\"http://www.sl4.org/crocker.html\">source document</a> don't suggest any particular time limit or training period, and also don't provide any empirical results of testing it, positive or negative. It sounds good in theory, but how does it affect people in the real world?</p>\n<ul>\n<li>If you operate under the Rules for an extended period, does your social status diminish due to behaving like a pushover when insulted?</li>\n<li>Does it usually become unbearable after a particular period of time? Or is there a temporary discomfort that you get over quickly?</li>\n<li>Is there a list of signatories who have declared Crocker's Rules on an indefinite or time-limited basis?</li>\n<li>Where can I find examples of dialogue that has benefited (or suffered) from this?</li>\n</ul>\n<p>It seems like an \"obviously cool\" idea but the risk to one's reputation is worth taking into consideration. If it is clear that the risk is low, and if the value to be gained is clearly very high, we should probably be doing more to encourage it as an explicit norm.</p>\n<p>On the other hand, if it is just one of those ideas that sounds better in theory than it is in practice (because the theory does not correctly model reality), or is just yet another signaling game with a net negative value, that is worth knowing as well.</p>\n<p>I haven't seen anyone argue against Crocker's Rules or claim it ruined their life, so my estimation is that the risk is low (although there is a small sample size to start with). Also,  I have seen at least one <a href=\"/lw/99t/can_the_chain_still_hold_you/\">statement</a> from lukeprog implying that it has been instrumental in triggering updates during live conversations he has observed, indicating that the value is high (though its causal role is not firmly established in that example).</p>\n<p>Does anyone have further data points to add?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Yf5WEMaczBjKZZ8u2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 8.348675082521374e-07, "legacy": true, "legacyId": "12330", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iETtCZcfmRyHp69w4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T14:49:11.714Z", "modifiedAt": null, "url": null, "title": "A Word to the Resourceful", "slug": "a-word-to-the-resourceful", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.119Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cHqqKhjkEiF7DdZFW/a-word-to-the-resourceful", "pageUrlRelative": "/posts/cHqqKhjkEiF7DdZFW/a-word-to-the-resourceful", "linkUrl": "https://www.lesswrong.com/posts/cHqqKhjkEiF7DdZFW/a-word-to-the-resourceful", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Word%20to%20the%20Resourceful&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Word%20to%20the%20Resourceful%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHqqKhjkEiF7DdZFW%2Fa-word-to-the-resourceful%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Word%20to%20the%20Resourceful%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHqqKhjkEiF7DdZFW%2Fa-word-to-the-resourceful", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcHqqKhjkEiF7DdZFW%2Fa-word-to-the-resourceful", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>Paul Graham has a <a href=\"http://www.paulgraham.com/word.html\">new article</a> out. Everything he's written is worth reading if you're at all interested in startups, but this article seemed explicitly connected to rationality, by identifying an area where people who are more likely to update / less likely to rationalize will do better than others.</p>\n<p>The obvious questions: can this be tested? Noticed early on, rather than in hindsight? Changed by rationality training?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cHqqKhjkEiF7DdZFW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 20, "extendedScore": null, "score": 8.348685351804232e-07, "legacy": true, "legacyId": "12332", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T17:04:36.705Z", "modifiedAt": null, "url": null, "title": "A Simple Solution to the FAI Problem", "slug": "a-simple-solution-to-the-fai-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "aRKhdqTQNYWmL9AsZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/79MBpRdKPj6ThwhJ7/a-simple-solution-to-the-fai-problem", "pageUrlRelative": "/posts/79MBpRdKPj6ThwhJ7/a-simple-solution-to-the-fai-problem", "linkUrl": "https://www.lesswrong.com/posts/79MBpRdKPj6ThwhJ7/a-simple-solution-to-the-fai-problem", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Simple%20Solution%20to%20the%20FAI%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Simple%20Solution%20to%20the%20FAI%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F79MBpRdKPj6ThwhJ7%2Fa-simple-solution-to-the-fai-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Simple%20Solution%20to%20the%20FAI%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F79MBpRdKPj6ThwhJ7%2Fa-simple-solution-to-the-fai-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F79MBpRdKPj6ThwhJ7%2Fa-simple-solution-to-the-fai-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 682, "htmlBody": "<p>I suggest this because it is extremely simple, and in the spirit that simple solutions should be considered early rather than late. While the suggestion itself is simple, its&nbsp;implementation&nbsp;might be harder than any other approach. This is intended as a discussion, and part of what I want you to do as my reader is identify the premises which are most problematic.&nbsp;</p>\n<p>My first argument aims at the conclusion that producing an AGI 'off the assembly line' is impossible. This is a&nbsp;particular&nbsp;case of a more general claim that for anything to be recognizable as an intelligence, it must have a familiarity with a shared world. So if we had a brain in a vat, hooked up to a simulated universe, we could only understand the brain's activity to be thinking if we were familiar with the world the brain was thinking about. This is itself a case of the principle that in order to understand the meaning of a proposition, we have to be able to understand its truth-conditions. If I say \"Schnee ist weiss\" and you don't know what I mean, I can explain the meaning of this sentence completely by pointing out that it is true if snow is white. I cannot explain its meaning without doing this.</p>\n<p>Thus, we cannot recognize any proposition as meaningful unless we can recognize its truth conditions. And recognizing the truth conditions of a proposition requires a shared&nbsp;familiarity&nbsp;with the world on the part of the speaker and the listener. If we can't recognize any of the propositions of a speaker as meaningful, then we can't recognize the speaker as a speaker, nor even as a thinker. Recognizing something as intelligent requires a shared familiarity with the world.</p>\n<p>So in order for an AGI to be recognized as intelligent, it would have to share with us a familiarity with the world. It is impossible to program this in, or in any way assemble such familiarity. It is achieved only by experience. Thus, in order to create an AGI, we would have to create a machine capable of thinking (in the way babies are) and then let it go about experiencing the world.</p>\n<p>So while an AGI is, of course, entirely possible, programming an AGI is not. We'd have to teach it to think in the way we teach people to think.</p>\n<p>Thus, it is of course also impossible to program an AGI to be an FAI (though it should be possible to program a potential thinker so as to make it more likely to develop friendliness). Friendliness is a way of thinking, a way of being rational, and so like all thinking it would have to be the result of experience and education. Making an AGI think, and making it friendly, are unavoidably problems of intellectual and ethical education, not in principle different from the problems of intellectual and ethical education we face with children.</p>\n<p>Thus, the one and only way to produce an FAI is to teach it to be good in the way we teach children to be good. And if I may speak somewhat metaphorically, the problem of the singularity doesn't seem to be radically different from the problem of having children: they will be smarter and better educated then we are, and they will produce yet smarter and even better educated children themselves, so much so that the future is opaque to us. The friendliness of children is a matter of the survival of our species, and they could easily destroy us all if they developed in a generally unfriendly way. Yet we have managed, thus far, to teach many people to be good.&nbsp;</p>\n<p>My solution is thus extremely simple, and one which many, many people are presently&nbsp;competent&nbsp;to accomplish. On the other hand, trying to make one's children into good people is more complicated and difficult, I think, than any present approach to FAI. AGI might be a greater&nbsp;challenge&nbsp;in the sense that it might be a more powerful and unruly child than any we've had to deal with. We might have to become better ethical teachers, and be able to teach more quickly, but the problem isn't fundamentally different.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "79MBpRdKPj6ThwhJ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -23, "extendedScore": null, "score": 8.349202276376822e-07, "legacy": true, "legacyId": "12333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T21:58:05.271Z", "modifiedAt": null, "url": null, "title": "Limits on self-optimisation", "slug": "limits-on-self-optimisation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:34.457Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WLvuvmTdgP6PccgzA/limits-on-self-optimisation", "pageUrlRelative": "/posts/WLvuvmTdgP6PccgzA/limits-on-self-optimisation", "linkUrl": "https://www.lesswrong.com/posts/WLvuvmTdgP6PccgzA/limits-on-self-optimisation", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Limits%20on%20self-optimisation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALimits%20on%20self-optimisation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLvuvmTdgP6PccgzA%2Flimits-on-self-optimisation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Limits%20on%20self-optimisation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLvuvmTdgP6PccgzA%2Flimits-on-self-optimisation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLvuvmTdgP6PccgzA%2Flimits-on-self-optimisation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 493, "htmlBody": "<p>Disclaimer: I am a physicist, and in the field of computer science my scholarship is weak. It may be that what I suggest here is well known, or perhaps just wrong.</p>\n<p>Abstract: A Turing machine capable of saying whether two arbitrary Turing machines have the same output for all inputs is equivalent to solving the Halting Problem. To optimise a function it is necessary to prove that the optimised version always has the same output as the unoptimised version, which is impossible in general for Turing machines. However, real computers have finite input spaces.</p>\n<p>&nbsp;</p>\n<p>Context: FOOM, Friendliness, optimisation processes.</p>\n<p>Consider a computer program which modifies itself in an attempt to optimise for speed. A modification to some algorithm is *proper* if it results, for all inputs, in the same output; it is an optimisation if it results in a shorter running time on average for typical inputs, and a *strict* optimisation if it results in a shorter running time for all inputs.</p>\n<p>A Friendly AI, optimising itself, must ensure that it remains Friendly after the modification; it follows that it can only make proper modifications. (When calculating a CEV it may make improper modifications, since the final answer for \"How do we deal with X\" may change in the course of extrapolating; but for plain optimisations the answer cannot change.)</p>\n<p>For simplicity we may consider that the output of a function can be expressed as a single bit; the extension to many bits is obvious. However, in addition to '0' and '1' we must consider that the response to some input can be \"does not terminate\". The task is to prove that two functions, which we may consider as Turing machines, have the same output for all inputs.</p>\n<p>Now, suppose you have a Turing machine that takes as input two arbitrary Turing machines and their respective tapes, and outputs \"1\" if the two input machines have the same output, and \"0\" otherwise. Then, by having one of the inputs be a Turing machine which is known not to terminate - one that executes an infinite loop - you can solve the Halting Problem. Therefore, such a machine cannot exist: You cannot build a Turing machine to prove, for arbitrary input machines, that they have the same output.</p>\n<p>It seems to follow that you cannot build a fully general proper-optimisation detector.</p>\n<p>However, \"arbitrary Turing machines\" is a strong claim, in fact stronger than we require. No physically realisable computer is a true Turing machine, because it cannot have infinite storage space, as the definition requires. The problem is actually the slightly easier (that is, not *provably* impossible) one of making a proper-optimisation detector for the space of possible inputs to an actual computer, which is finite though very large. In practice we may limit the input space still further by considering, say, optimisations to functions whose input is two 64-bit numbers, or something. Even so, the brute-force solution of running the functions on all possible inputs and comparing is already rather impractical.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WLvuvmTdgP6PccgzA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 10, "extendedScore": null, "score": 8.350322755125217e-07, "legacy": true, "legacyId": "12335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-20T23:50:25.636Z", "modifiedAt": null, "url": null, "title": "Doomsday Argument with Strong Self-Sampling Assumption", "slug": "doomsday-argument-with-strong-self-sampling-assumption", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.011Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "drnickbone", "createdAt": "2012-01-20T17:19:55.216Z", "isAdmin": false, "displayName": "drnickbone"}, "userId": "GgwHTM3agaskLi9cx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/psz93GRm6jj9yj5dR/doomsday-argument-with-strong-self-sampling-assumption", "pageUrlRelative": "/posts/psz93GRm6jj9yj5dR/doomsday-argument-with-strong-self-sampling-assumption", "linkUrl": "https://www.lesswrong.com/posts/psz93GRm6jj9yj5dR/doomsday-argument-with-strong-self-sampling-assumption", "postedAtFormatted": "Friday, January 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doomsday%20Argument%20with%20Strong%20Self-Sampling%20Assumption&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoomsday%20Argument%20with%20Strong%20Self-Sampling%20Assumption%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpsz93GRm6jj9yj5dR%2Fdoomsday-argument-with-strong-self-sampling-assumption%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doomsday%20Argument%20with%20Strong%20Self-Sampling%20Assumption%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpsz93GRm6jj9yj5dR%2Fdoomsday-argument-with-strong-self-sampling-assumption", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpsz93GRm6jj9yj5dR%2Fdoomsday-argument-with-strong-self-sampling-assumption", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 661, "htmlBody": "<p>Hello everyone; I'm new to the forum, and have been advised to post this in the \"discussion\" section. Hope this is OK.</p>\n<p>I've found some references to discussions here on Brandon Carter / John Leslie's \"Doomsday Argument\" and they seemed well-informed.&nbsp;<span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">One thing I've noticed&nbsp;about the argument though (but haven't seen discussed before) is that it can be made much sharper by assuming that we are making random *observations*, rather than just that we are a random *observer*. &nbsp;</span></p>\n<p>For those who know the literature, this is a form of Nick Bostrom's Strong Self-Sampling Assumption as opposed to the (basic) Self-Sampling Assumption. Oddly enough, Bostrom discusses SSSA&nbsp;quite a lot in connection with the Doomsday Argument, but I can't see that he's done quite the analysis below.&nbsp;</p>\n<p>So here goes:</p>\n<p>In the \"random observer\" model (the Self-Sampling Assumption with&nbsp;the widest reference class of \"all observers\"), we discover that we are in a human civilization and there have been ~100 billion&nbsp;observers before us in that civilization. We should then predict (crudely) that there will be about ~100 billion&nbsp;observers coming after us in that civilization; also we should predict that a typical civilization of observers won't have much more than ~100-200 billion observers in total (otherwise we'd be in one of the much bigger ones, rather than in a smaller one). So typical civilizations don't expand beyond their planets of origin, and don't even last very long on their planets of origin.</p>\n<p>Further,&nbsp;since there are currently ~150 million human births per year that would imply the end of the human race in ~700 years at current population size and birth-rates. Doom soon-ish but not very soon.</p>\n<p>&nbsp;</p>\n<p>But what about the \"random observation\" model? One difference here is that&nbsp;a large portion of the ~100 billion&nbsp;humans living before us died very young (high infant mortality rate)&nbsp;so made very few observations. For instance, Carl Haub, who calculated the 100 billion number (see <a href=\"http://www.prb.org/Articles/2002/HowManyPeopleHaveEverLivedonEarth.aspx\">http://www.prb.org/Articles/2002/HowManyPeopleHaveEverLivedonEarth.aspx</a>) reckons that for most of human history, life expectancy at birth has been little more than 10 years. By contrast, recent observers have had a life expectancy of 60+ years, so are making many more observations through their lives than average. This means that *observations* are much more concentrated in the present era than *observers*.</p>\n<p>&nbsp;</p>\n<p>Working with Haub's population numbers, there have been about 1-2 trillion \"person-years\" of observations before our current observations (in January 2012). Also, that estimate is very stable even when we make quite different estimates about birth-rate. (The reason is that the overall population at different stages in history is easier to estimate than the overall birth-rate, so integrating population through time to give person-years is easier than integrating birth-rate through time to give births).</p>\n<p>Under the \"random observation\" model, we would predict a similar number of person-years of observations to come in the future of our civilization. At a human population size of ~7 billion, there are only around 1-2000 / 7 or ~200 years until human extinction: doom rather sooner. And if population&nbsp;climbs to 10 or 14&nbsp;billion before flattening out (as&nbsp;some demographers predict)&nbsp;then doom even sooner still.</p>\n<p>What's also quite striking is that over 20% of&nbsp;all observations *so far* have happened since 1900, and under a \"doom soon\" model the *majority* of all observations would happen in the&nbsp;period of multi-billion population sizes. So our current observations look&nbsp;very typical in this model.</p>\n<p>&nbsp;</p>\n<p>Now I'm aware that Bostrom thinks the SSSA is a way out of the Doomsday Argument, since by relativizing the \"reference class\" (to something other than all observations, or all human observatioons) then we get a less \"doomish\" prediction. All we can conclude is that the reference class we are part of (whatever that is) will terminate soon, whereas observers in general can carry on. I'm also aware of a number of criticisms of the whole SSA/SSSA approach.</p>\n<p>On the other hand, it is quite striking that a very simple reference class (all observations), coupled to a very simple population model for observers (exponential growth -&gt; short peak&nbsp;-&gt; collapse) predicts more or less exactly what we are seeing now.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "psz93GRm6jj9yj5dR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "12334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-21T02:40:30.691Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Argument from Common Usage", "slug": "seq-rerun-the-argument-from-common-usage", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9ZwwfrtiadpLx8QH5/seq-rerun-the-argument-from-common-usage", "pageUrlRelative": "/posts/9ZwwfrtiadpLx8QH5/seq-rerun-the-argument-from-common-usage", "linkUrl": "https://www.lesswrong.com/posts/9ZwwfrtiadpLx8QH5/seq-rerun-the-argument-from-common-usage", "postedAtFormatted": "Saturday, January 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Argument%20from%20Common%20Usage&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Argument%20from%20Common%20Usage%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ZwwfrtiadpLx8QH5%2Fseq-rerun-the-argument-from-common-usage%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Argument%20from%20Common%20Usage%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ZwwfrtiadpLx8QH5%2Fseq-rerun-the-argument-from-common-usage", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ZwwfrtiadpLx8QH5%2Fseq-rerun-the-argument-from-common-usage", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>Today's post, <a href=\"/lw/nr/the_argument_from_common_usage/\">The Argument from Common Usage</a> was originally published on 13 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You argue over the meanings of a word, even after all sides understand perfectly well what the other sides are trying to say. The human ability to associate labels to concepts is a tool for communication. When people want to communicate, we're hard to stop; if we have no common language, we'll draw pictures in sand. When you each understand what is in the other's mind, you are done.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9ib/seq_rerun_feel_the_meaning/\">Feel the Meaning</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9ZwwfrtiadpLx8QH5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.351401287074357e-07, "legacy": true, "legacyId": "12340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9ZooAqfh2TC9SBDvq", "a8g3Nms9yMemSnMQF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-21T04:50:58.907Z", "modifiedAt": null, "url": null, "title": "Some potential dangers of rationality training", "slug": "some-potential-dangers-of-rationality-training", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:43.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MkX8D44PFoiNdLMkG/some-potential-dangers-of-rationality-training", "pageUrlRelative": "/posts/MkX8D44PFoiNdLMkG/some-potential-dangers-of-rationality-training", "linkUrl": "https://www.lesswrong.com/posts/MkX8D44PFoiNdLMkG/some-potential-dangers-of-rationality-training", "postedAtFormatted": "Saturday, January 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20potential%20dangers%20of%20rationality%20training&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20potential%20dangers%20of%20rationality%20training%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkX8D44PFoiNdLMkG%2Fsome-potential-dangers-of-rationality-training%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20potential%20dangers%20of%20rationality%20training%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkX8D44PFoiNdLMkG%2Fsome-potential-dangers-of-rationality-training", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkX8D44PFoiNdLMkG%2Fsome-potential-dangers-of-rationality-training", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p><a href=\"http://www.lrsi.uqam.ca/documents/PSY9520/05%20-%20l'estime%20de%20soi%202%20-%20ses%20fonctions,%20cons%E9quences,%20et%20processus%20alternatifs/TAYLOR~1.PDF\">Taylor &amp; Brown (1988)</a> argued that several kinds of irrationality are good for you &mdash; for example that overconfidence, including the planning fallacy, protects you from depression and gives you greater motivation <a href=\"/lw/3w3/how_to_beat_procrastination/\">because</a> your expectancy of success is higher.</p>\n<p>One can imagine other examples. Perhaps the sunk cost fallacy is useful because without it you're prone to switch projects as soon as a higher-value project comes along, leaving an ever-growing heap of abandoned projects behind you.</p>\n<p>This may be one reason that many people's lives aren't much improved by <a href=\"/lw/5x8/teachable_rationality_skills/\">rationality training</a>. Perhaps the <em>benefits</em> of having more accurate models of the world and making better decisions are swamped by the&nbsp;<em>negative</em> effects of losing out on the benefits of overconfidence and the sunk costs fallacy and other \"positive illusions.\" Yes, I read \"<a href=\"/lw/7s4/poll_results_lw_probably_doesnt_cause_akrasia/\">Less Wrong Probably Doesn't Cause Akrasia</a>,\" but there were too many methodological weaknesses to give that study much weight, I think.&nbsp;</p>\n<p><a href=\"http://persweb.wabash.edu/facstaff/hortonr/articles%20for%20class/colvin%20and%20block%20against%20taylor.pdf\">Others</a> have argued against Taylor &amp; Brown's conclusion, and at least <a href=\"http://web.utk.edu/~jmcnulty/McNulty/Papers_files/O'Mara%20et%20al._2011_JPSP_1.pdf\">one recent study</a> suggests that biases are not inherently positive or negative for mental health and motivation because the effect depends on the context in which they occur. There seems to be no expert consensus on the matter.</p>\n<p>(Inspired by a conversation with <a href=\"/user/Louie\">Louie</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9YFoDPFwMoWthzgkY": 1, "3QnDqGSdRMA5mdMM6": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MkX8D44PFoiNdLMkG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 28, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "12341", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RWo4LwFzpHNQCTcYt", "f4CZNEHirweN3XEjs", "q3rBapm2TjQ6tx9Td"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-21T07:05:05.264Z", "modifiedAt": null, "url": null, "title": "The Dark Arts: A Beginner's Guide", "slug": "the-dark-arts-a-beginner-s-guide", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:09.249Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "faul_sname", "createdAt": "2011-07-10T06:35:12.911Z", "isAdmin": false, "displayName": "faul_sname"}, "userId": "uAdCfrYxKKCJT5nK7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zz986H9P3WJoh5DNb/the-dark-arts-a-beginner-s-guide", "pageUrlRelative": "/posts/Zz986H9P3WJoh5DNb/the-dark-arts-a-beginner-s-guide", "linkUrl": "https://www.lesswrong.com/posts/Zz986H9P3WJoh5DNb/the-dark-arts-a-beginner-s-guide", "postedAtFormatted": "Saturday, January 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Dark%20Arts%3A%20A%20Beginner's%20Guide&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Dark%20Arts%3A%20A%20Beginner's%20Guide%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZz986H9P3WJoh5DNb%2Fthe-dark-arts-a-beginner-s-guide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Dark%20Arts%3A%20A%20Beginner's%20Guide%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZz986H9P3WJoh5DNb%2Fthe-dark-arts-a-beginner-s-guide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZz986H9P3WJoh5DNb%2Fthe-dark-arts-a-beginner-s-guide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1095, "htmlBody": "<p>&nbsp;</p>\n<h1>The Dark Arts</h1>\n<p>So you've been reading this site and learning many valuable tools for becoming more rational. You're beginning to become irritated at the irrational behavior of the average person. You've noticed that many people refuse to accept even highly compelling arguments, even as they drink up the doctrine of their favorite religion/political party. What are you doing wrong?</p>\n<p>As it turns out, it's less about what you're doing wrong than about what these highly influential groups are doing right. This is a brief intro to the Dark Arts, ranging from relatively harmless or even helpful techniques to truly dangerous ones. In this set of guidelines, I have used the example of Solar Suzy, a contractor for a company that sells solar panels, and Business Owner Bob, who runs an organic food store. You will quickly notice that Solar Suzy is not very ethical. This is not an accident: these techniques can very easily be used unethically. They're called the Dark Arts for a reason, and this example will make sure that's kept in mind.</p>\n<p>The fact that a technique can be used to do bad things, however, doesn't mean we shouldn't learn the technique. These methods can be used when you don't have time to wait for someone to slowly change their mind, fighting every step of the way. Even if you plan to never use them, it is probably a good idea to be aware of what they look like.</p>\n<p>&nbsp;</p>\n<h2>The Rules</h2>\n<p><strong>Be simple.</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>Use simple words. We're lazy. We don't like having to put in effort to understand something.</li>\n<li>Be brief. Try to say your idea in 30 words or less.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Compare:</p>\n<p>Bob: \"I've run the numbers. It would take over 30 years for the solar panels to pay for themselves.\"</p>\n<p>Suzy: \"Despite the inherent economic disadvantage in the utilization of photovoltaic cells in preference to petroleum and coal, it may under particular circumstances benefit corporate entities insofar as the collective ethical standard of society values such a demonstration of conservationism.\"</p>\n<p>Bob [baffled and a little annoyed]: ... Uhhhhh... Sure... I think I'm fine with what I have. Bye.</p>\n<p>-----</p>\n<p>Bob: \"I've run the numbers. It would take over 30 years for the solar panels to pay for themselves.\"</p>\n<p>Suzy: \"Solar panels don't look cost-effective compared to fossil fuels, but sometimes the extra business a company gets for being 'green' makes them worthwhile.\"</p>\n<p>Bob [interested, but skeptical]: Hmm. I hadn't thought of that. Is it really significant, though?</p>\n<p>&nbsp;</p>\n<p><strong>Use positive language.</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>When possible, use agreeable words. Imagine that you have to pay for each instance of 'no' type words.</li>\n<li>If you can get your partner to agree to a chain of statements, they will be more likely to agree to the next one.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Now the conversation looks more like this:</p>\n<p>Suzy: You are interested in solar panels, right?</p>\n<p>Bob [wary]: That's correct.</p>\n<p>Suzy [seriously]: Of course, we'll want to make sure it's actually worth it for your business.</p>\n<p>Bob [more firmly]: Absolutely.</p>\n<p>Suzy [after a slight, thoughtful pause*]: Would you say that your average customer is concerned about the environment?</p>\n<p>Bob: Yes, I think so.</p>\n<p>Suzy [as if coming to a realization]: You could probably increase your business by advertising that you're going green.</p>\n<p>Bob [thoughtful]: I see your point. That might well work.</p>\n<p>Suzy [enthusiastic]: Great! Let's get you signed up.</p>\n<p>Bob [wary again]: ...</p>\n<p>&nbsp;</p>\n<p><strong>Make sure your partner thinks you are like them.</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>Emphasize common opinions. Mock opinions you both disagree with, being careful to stay away from any ideas they might be sympathetic to. This establishes two things. First, it demonstrates that you have high enough status that you can afford to alienate others (but not them, of course). Second, it establishes a common ground.</li>\n<li>Use the same sensory language as they use. If they talk about seeing patterns, incorporate the words 'look' and 'examine' in your conversation.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Create an external cognitive load on your partner.</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>Here we begin to wander into the darker arts. People can only do so much with their mind at one time. Anything you can do to add a cognitive burden will take away from their ability to reject ideas. Taking a walk will impose approximately the right amount of strain. This cognitive burden is the reason you want to keep your ideas simple. Simple ideas can still be absorbed with an external load.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Have your partner come up with the ideas whenever possible</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>A thought you come up with on your own (or feel like you came up with on your own) goes through far less filtering than an outside idea. Creating a cognitive burden can reduce this filter, but will not eliminate it entirely.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Incorporating what we know so far, Suzy's sales pitch begins to look more effective.</p>\n<p>Suzy: Let's take a look around the area. I'd like to get a feel for the neighborhood.</p>\n<p>Bob: Okay.</p>\n<p>Suzy [spoken in a lower voice, as if sharing a personal secret]: Besides, formal meetings are always so uncomfortable.</p>\n<p>Bob [laughing]: I know. There's a reason I got out of the corporate world.</p>\n<p>[small talk]</p>\n<p>Suzy [more seriously]: Well, I suppose we should probably get down to business. You were looking into solar panels for your&nbsp;business.</p>\n<p>Bob [momentarily off-balance]: ...Yes.</p>\n<p>Suzy: Of course, we'll need to make sure it's a good decision for you to install them.</p>\n<p>Bob: Of course.</p>\n<p>Suzy: Now, looking at your customers, I see a lot of signs of environmentalism.</p>\n<p>Bob: Yes. Our customers tend to be the sort who care about our planet.</p>\n<p>Suzy [noticing the organic food]: Wow, you really cater to their tastes.</p>\n<p>Bob: I do my best. Many of my customers come here because we only buy from organic growers.</p>\n<p>Suzy: So the solar panels wouldn't look out of place here.</p>\n<p>Bob: Out of place? They would probably attract customers.</p>\n<p>Suzy [surprised]: I suppose they would. That alone might well make them worth it.</p>\n<p>[contract, hopefully]</p>\n<p>&nbsp;</p>\n<p>\"<strong>You're the sort of person who...</strong>\"</p>\n<p>This is one of the most potent and dangerous tools in your arsenal. If you say that you admire someone's open-mindedness, they will make an effort not to let you down. Your mind evaluates the truth of a statement by judging how easy it is to come up with examples of truth. If you phrase it vaguely enough and make it a compliment, you can convince anyone that they have just about any trait.</p>\n<p>This is dangerous because the idea of what kind of person they are will stick with the target for long after your conversation is over. This is the darkest of the dark arts that I am familiar with. Use it sparingly, or if possible not at all. I have used it once that I remember, and that was in a rather extreme situation.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zz986H9P3WJoh5DNb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 49, "extendedScore": null, "score": 0.0001, "legacy": true, "legacyId": "12344", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 49, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<h1 id=\"The_Dark_Arts\">The Dark Arts</h1>\n<p>So you've been reading this site and learning many valuable tools for becoming more rational. You're beginning to become irritated at the irrational behavior of the average person. You've noticed that many people refuse to accept even highly compelling arguments, even as they drink up the doctrine of their favorite religion/political party. What are you doing wrong?</p>\n<p>As it turns out, it's less about what you're doing wrong than about what these highly influential groups are doing right. This is a brief intro to the Dark Arts, ranging from relatively harmless or even helpful techniques to truly dangerous ones. In this set of guidelines, I have used the example of Solar Suzy, a contractor for a company that sells solar panels, and Business Owner Bob, who runs an organic food store. You will quickly notice that Solar Suzy is not very ethical. This is not an accident: these techniques can very easily be used unethically. They're called the Dark Arts for a reason, and this example will make sure that's kept in mind.</p>\n<p>The fact that a technique can be used to do bad things, however, doesn't mean we shouldn't learn the technique. These methods can be used when you don't have time to wait for someone to slowly change their mind, fighting every step of the way. Even if you plan to never use them, it is probably a good idea to be aware of what they look like.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Rules\">The Rules</h2>\n<p><strong id=\"Be_simple_\">Be simple.</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>Use simple words. We're lazy. We don't like having to put in effort to understand something.</li>\n<li>Be brief. Try to say your idea in 30 words or less.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Compare:</p>\n<p>Bob: \"I've run the numbers. It would take over 30 years for the solar panels to pay for themselves.\"</p>\n<p>Suzy: \"Despite the inherent economic disadvantage in the utilization of photovoltaic cells in preference to petroleum and coal, it may under particular circumstances benefit corporate entities insofar as the collective ethical standard of society values such a demonstration of conservationism.\"</p>\n<p>Bob [baffled and a little annoyed]: ... Uhhhhh... Sure... I think I'm fine with what I have. Bye.</p>\n<p>-----</p>\n<p>Bob: \"I've run the numbers. It would take over 30 years for the solar panels to pay for themselves.\"</p>\n<p>Suzy: \"Solar panels don't look cost-effective compared to fossil fuels, but sometimes the extra business a company gets for being 'green' makes them worthwhile.\"</p>\n<p>Bob [interested, but skeptical]: Hmm. I hadn't thought of that. Is it really significant, though?</p>\n<p>&nbsp;</p>\n<p><strong id=\"Use_positive_language_\">Use positive language.</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>When possible, use agreeable words. Imagine that you have to pay for each instance of 'no' type words.</li>\n<li>If you can get your partner to agree to a chain of statements, they will be more likely to agree to the next one.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Now the conversation looks more like this:</p>\n<p>Suzy: You are interested in solar panels, right?</p>\n<p>Bob [wary]: That's correct.</p>\n<p>Suzy [seriously]: Of course, we'll want to make sure it's actually worth it for your business.</p>\n<p>Bob [more firmly]: Absolutely.</p>\n<p>Suzy [after a slight, thoughtful pause*]: Would you say that your average customer is concerned about the environment?</p>\n<p>Bob: Yes, I think so.</p>\n<p>Suzy [as if coming to a realization]: You could probably increase your business by advertising that you're going green.</p>\n<p>Bob [thoughtful]: I see your point. That might well work.</p>\n<p>Suzy [enthusiastic]: Great! Let's get you signed up.</p>\n<p>Bob [wary again]: ...</p>\n<p>&nbsp;</p>\n<p><strong id=\"Make_sure_your_partner_thinks_you_are_like_them_\">Make sure your partner thinks you are like them.</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>Emphasize common opinions. Mock opinions you both disagree with, being careful to stay away from any ideas they might be sympathetic to. This establishes two things. First, it demonstrates that you have high enough status that you can afford to alienate others (but not them, of course). Second, it establishes a common ground.</li>\n<li>Use the same sensory language as they use. If they talk about seeing patterns, incorporate the words 'look' and 'examine' in your conversation.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Create_an_external_cognitive_load_on_your_partner_\">Create an external cognitive load on your partner.</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>Here we begin to wander into the darker arts. People can only do so much with their mind at one time. Anything you can do to add a cognitive burden will take away from their ability to reject ideas. Taking a walk will impose approximately the right amount of strain. This cognitive burden is the reason you want to keep your ideas simple. Simple ideas can still be absorbed with an external load.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Have_your_partner_come_up_with_the_ideas_whenever_possible\">Have your partner come up with the ideas whenever possible</strong></p>\n<p>&nbsp;</p>\n<ul>\n<li>A thought you come up with on your own (or feel like you came up with on your own) goes through far less filtering than an outside idea. Creating a cognitive burden can reduce this filter, but will not eliminate it entirely.</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Incorporating what we know so far, Suzy's sales pitch begins to look more effective.</p>\n<p>Suzy: Let's take a look around the area. I'd like to get a feel for the neighborhood.</p>\n<p>Bob: Okay.</p>\n<p>Suzy [spoken in a lower voice, as if sharing a personal secret]: Besides, formal meetings are always so uncomfortable.</p>\n<p>Bob [laughing]: I know. There's a reason I got out of the corporate world.</p>\n<p>[small talk]</p>\n<p>Suzy [more seriously]: Well, I suppose we should probably get down to business. You were looking into solar panels for your&nbsp;business.</p>\n<p>Bob [momentarily off-balance]: ...Yes.</p>\n<p>Suzy: Of course, we'll need to make sure it's a good decision for you to install them.</p>\n<p>Bob: Of course.</p>\n<p>Suzy: Now, looking at your customers, I see a lot of signs of environmentalism.</p>\n<p>Bob: Yes. Our customers tend to be the sort who care about our planet.</p>\n<p>Suzy [noticing the organic food]: Wow, you really cater to their tastes.</p>\n<p>Bob: I do my best. Many of my customers come here because we only buy from organic growers.</p>\n<p>Suzy: So the solar panels wouldn't look out of place here.</p>\n<p>Bob: Out of place? They would probably attract customers.</p>\n<p>Suzy [surprised]: I suppose they would. That alone might well make them worth it.</p>\n<p>[contract, hopefully]</p>\n<p>&nbsp;</p>\n<p>\"<strong>You're the sort of person who...</strong>\"</p>\n<p>This is one of the most potent and dangerous tools in your arsenal. If you say that you admire someone's open-mindedness, they will make an effort not to let you down. Your mind evaluates the truth of a statement by judging how easy it is to come up with examples of truth. If you phrase it vaguely enough and make it a compliment, you can convince anyone that they have just about any trait.</p>\n<p>This is dangerous because the idea of what kind of person they are will stick with the target for long after your conversation is over. This is the darkest of the dark arts that I am familiar with. Use it sparingly, or if possible not at all. I have used it once that I remember, and that was in a rather extreme situation.</p>\n<p>&nbsp;</p>", "sections": [{"title": "The Dark Arts", "anchor": "The_Dark_Arts", "level": 1}, {"title": "The Rules", "anchor": "The_Rules", "level": 2}, {"title": "Be simple.", "anchor": "Be_simple_", "level": 3}, {"title": "Use positive language.", "anchor": "Use_positive_language_", "level": 3}, {"title": "Make sure your partner thinks you are like them.", "anchor": "Make_sure_your_partner_thinks_you_are_like_them_", "level": 3}, {"title": "Create an external cognitive load on your partner.", "anchor": "Create_an_external_cognitive_load_on_your_partner_", "level": 3}, {"title": "Have your partner come up with the ideas whenever possible", "anchor": "Have_your_partner_come_up_with_the_ideas_whenever_possible", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "43 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-21T09:31:49.034Z", "modifiedAt": null, "url": null, "title": "Politicians' family as signalling", "slug": "politicians-family-as-signalling", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qkeTE53Qdcb6qegaJ/politicians-family-as-signalling", "pageUrlRelative": "/posts/qkeTE53Qdcb6qegaJ/politicians-family-as-signalling", "linkUrl": "https://www.lesswrong.com/posts/qkeTE53Qdcb6qegaJ/politicians-family-as-signalling", "postedAtFormatted": "Saturday, January 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Politicians'%20family%20as%20signalling&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APoliticians'%20family%20as%20signalling%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkeTE53Qdcb6qegaJ%2Fpoliticians-family-as-signalling%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Politicians'%20family%20as%20signalling%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkeTE53Qdcb6qegaJ%2Fpoliticians-family-as-signalling", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkeTE53Qdcb6qegaJ%2Fpoliticians-family-as-signalling", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>In the US, if you look at political candidates in public view, they often appear with family in tow. A candidate's family plays an important role in the election campaigns. I'm from India. There, the politicians' family play little role in election campaigns (unless the family member herself is actively involved in the party and politics, which is often).&nbsp;But, the relative importance of family (relative to other aspects like money, education) in the cultural value system of India is significantly greater than its relative importance in the cultural value system of the US. So why the inverse relation?</p>\n<p>I think it may have to do with signalling (inspired by Hanson, Zahavi). Maintaining a stable family is considered to be less of a status quo situation in the US, when compared to India. So in the US, maintaining a stable family is a <em>signal </em>of your management skills at the level of family, because you had to spend effort to obtain that signal. But in India, the family does not have signalling value, because it is much more common to have a stable family. So having a stable family did not require an 'extra' effort (extra compared to society's default).&nbsp;</p>\n<p>What would this imply? How would one test such a theory?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qkeTE53Qdcb6qegaJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "12345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-21T16:52:05.281Z", "modifiedAt": null, "url": null, "title": "New x-risk organizations", "slug": "new-x-risk-organizations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:32.595Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GuKvMA5gBra9n4BiM/new-x-risk-organizations", "pageUrlRelative": "/posts/GuKvMA5gBra9n4BiM/new-x-risk-organizations", "linkUrl": "https://www.lesswrong.com/posts/GuKvMA5gBra9n4BiM/new-x-risk-organizations", "postedAtFormatted": "Saturday, January 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20x-risk%20organizations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20x-risk%20organizations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuKvMA5gBra9n4BiM%2Fnew-x-risk-organizations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20x-risk%20organizations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuKvMA5gBra9n4BiM%2Fnew-x-risk-organizations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuKvMA5gBra9n4BiM%2Fnew-x-risk-organizations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p>\n<p class=\"p1\"><span class=\"s1\">Of course: <a href=\"http://www.fhi.ox.ac.uk/\"><span class=\"s2\">FHI</span></a>, <a href=\"http://lesswrong.com/lw/8gg/new_ai_risks_research_institute_at_oxford/\"><span class=\"s2\">FutureTech</span></a>, the&nbsp;<a href=\"http://intelligence.org/\"><span class=\"s2\">Singularity Institute</span></a>, and <a href=\"http://www.leverageresearch.org/\"><span class=\"s2\">Leverage Research</span></a>.</span></p>\n<p class=\"p2\">New: the <a href=\"http://www.gcrinstitute.org/\">Global Catastrophic Risk Institute</a>&nbsp;(Seth Baum &amp; Tony Barrett).</p>\n<p class=\"p3\">&nbsp;</p>\n<p class=\"p2\">I've also heard that the following people are working to set up x-risk departments/organizations:</p>\n<p class=\"p3\"><span class=\"s2\"><a href=\"http://prce.hu/w/index.html\">Huw Price</a></span> at Cambridge</p>\n<p class=\"p1\"><span class=\"s3\"><a href=\"http://web.mit.edu/nhmit/www/\">Newton Howard</a></span><span class=\"s1\"> at MIT</span></p>\n<p class=\"p1\"><span class=\"s3\"><a href=\"http://en.wikipedia.org/wiki/Jeffrey_Epstein\">Jeffrey Epstein</a></span></p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GuKvMA5gBra9n4BiM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 22, "extendedScore": null, "score": 8.354654818335996e-07, "legacy": true, "legacyId": "12346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wTgS6J73XMjYCKGC7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-21T18:12:29.719Z", "modifiedAt": null, "url": null, "title": "The hundred-room problem", "slug": "the-hundred-room-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.563Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "APMason", "createdAt": "2011-08-30T22:24:06.796Z", "isAdmin": false, "displayName": "APMason"}, "userId": "2QGaDhC6QbaR5sLh8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/esxBLeYbv3HohZ2Jr/the-hundred-room-problem", "pageUrlRelative": "/posts/esxBLeYbv3HohZ2Jr/the-hundred-room-problem", "linkUrl": "https://www.lesswrong.com/posts/esxBLeYbv3HohZ2Jr/the-hundred-room-problem", "postedAtFormatted": "Saturday, January 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20hundred-room%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20hundred-room%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FesxBLeYbv3HohZ2Jr%2Fthe-hundred-room-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20hundred-room%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FesxBLeYbv3HohZ2Jr%2Fthe-hundred-room-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FesxBLeYbv3HohZ2Jr%2Fthe-hundred-room-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 395, "htmlBody": "<p>This thought-experiment has been on my mind for a couple of days, and no doubt it's a special case of a more general problem identified somewhere by some philosopher that I haven't heard of yet. It goes like this:</p>\n<p>You are blindfolded, and then scanned, and ninety-nine atom-for-atom copies of you are made, each blindfolded, meaning a hundred in all. To each one is explained (and for the sake of the thought experiment, you can take this explanation as true (p is approx. 1)) that earlier, a fair-coin was flipped. If it came down heads, ninety-nine out of a hundred small rooms were painted red, and the remaining one was painted blue. If it came down tails, ninety-nine out of a hundred small rooms were painted blue, and the remaining one was painted red. Now, put yourself in the shoes of just one of these copies. When asked what the probability is that the coin came down tails, you of course answer &ldquo;.5&rdquo;. It is now explained to you that each of the hundred copies is to be inserted into one of the hundred rooms, and will then be allowed to remove their blindfolds. You feel yourself being moved, and then hear a voice telling you you can take your blindfold off. The room you are in is blue. The voice then asks you for your revised probability estimate that the coin came down tails.</p>\n<p>It seems at first (or maybe at second, depending on how your mind works) that the answer ought to be .99 &ndash; ninety-nine out of the hundred copies will, if they follow the rule &ldquo;if red, then heads, if blue then tails&rdquo;, get the answer right.</p>\n<p>However, it also seems like the answer ought to be .5, because you have no new information to update on. You <em>already knew</em> that at least one copy of you would, at this time, remove their blindfold and find themselves in a blue room. What have you discovered that should allow you to revise your probability of .5 to .99?</p>\n<p>And the answer, of course, cannot be both .5 and .99. Something has to give.</p>\n<p>Is there something basically quite obvious that I'm missing that will resolve this problem, or is it really the mean sonofabitch it appears to be? As it goes, I'm inclined to say the probability is .5 &ndash; I'm just not quite sure why. Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "esxBLeYbv3HohZ2Jr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 0, "extendedScore": null, "score": 8.354962138061938e-07, "legacy": true, "legacyId": "12347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-21T19:00:34.010Z", "modifiedAt": null, "url": null, "title": "[Link] The Hyborian Age", "slug": "link-the-hyborian-age", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.201Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8zqy3XjQa5gk92wa2/link-the-hyborian-age", "pageUrlRelative": "/posts/8zqy3XjQa5gk92wa2/link-the-hyborian-age", "linkUrl": "https://www.lesswrong.com/posts/8zqy3XjQa5gk92wa2/link-the-hyborian-age", "postedAtFormatted": "Saturday, January 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20Hyborian%20Age&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20Hyborian%20Age%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zqy3XjQa5gk92wa2%2Flink-the-hyborian-age%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20Hyborian%20Age%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zqy3XjQa5gk92wa2%2Flink-the-hyborian-age", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zqy3XjQa5gk92wa2%2Flink-the-hyborian-age", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 624, "htmlBody": "<p>Yay a <a href=\"http://westhunt.wordpress.com/2012/01/20/the-hyborian-age/\">new cool post</a> is up on West Hunters blog! It is written by <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a> and <a href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a> with whom most LWers are probably <a href=\"/lw/28t/qa_with_harpending_and_cochran\">already familiar with</a> (particularly <a href=\"/lw/8bl/link_back_to_the_trees/\">this awesome entry</a>). It raises some interesting points on biases in academia.</p>\n<blockquote>\n<p>I was contemplating Conan the Barbarian, and remembered the essay that Robert E. Howard wrote about the&nbsp; background of those stories &ndash; The Hyborian Age.&nbsp; <strong>I think that the flavor of Howard&rsquo;s pseudo-history is a lot more realistic than the picture of the human past academics preferred over the past few decades.</strong></p>\n<p>In Conan&rsquo;s world, it&rsquo;s never surprising to find a people that once mixed with some ancient prehuman race.&nbsp; Happens all the time.&nbsp; Until very recently, the vast majority of workers in human genetics and paleontology were sure that this never occurred &ndash; and only changed their minds when presented with evidence that was both strong (ancient DNA)&nbsp; and too mathematically sophisticated for them to understand or challenge (D-statistics).</p>\n<p>Conan&rsquo;s history&nbsp; was shaped by the occasional catastrophe.&nbsp; Most academics (particularly geologists) don&rsquo;t like catastrophes, but they have grudgingly come to admit their importance &ndash; things like the Thera and Toba eruptions, or the K/T asteroid strike and the Permo-Triassic crisis.</p>\n<p>Between the time when the oceans drank Atlantis,<strong> </strong>and the rise of the sons of Aryas, evolution seems to have run pretty briskly, but without any pronounced direction.&nbsp; Men devolved into ape-men when the environment pushed in that direction (Flores ?)&nbsp; and shifted right back when the environment favored speech and tools.&nbsp; Culture shaped evolution, and evolution shaped culture.<strong>&nbsp; </strong>An endogamous caste of snake-worshiping priests evolved in a strange direction.&nbsp; Although their IQs were considerably higher than average, they remained surprisingly vulnerable to sword-bearing barbarians.</p>\n<p>In this world, evolution could happen on a time scale of thousands of years, and there was no magic rule that ensured that the outcome would be the same in every group.&nbsp; It may not be PC to say it, but Cimmerians were smarter than Picts.</p>\n</blockquote>\n<p>The basic idea of their book \"The 10 000 Year Explosion\" (<a href=\"/lw/28k/the_psychological_diversity_of_mankind/\">LessWrong review</a>, <a href=\"http://www.amazon.com/000-Year-Explosion-Civilization-Accelerated/dp/0465002218\">Amazon</a>).</p>\n<blockquote>\n<p>Above all, people in Conan&rsquo;s world fought.<strong> </strong>They migrated: they invaded.&nbsp; There was war before, during, and after civilization.&nbsp; V&ouml;lkerwanderungs were a dime a dozen. Conquerors spread.&nbsp; Sometimes they mixed with the locals, sometimes they replaced them &ndash; as when the once dominant Hyborians, overrun by Picts, vanished from the earth, leaving scarcely a trace of their blood in the veins of their conquerors. They must have been U5b.</p>\n<p>To be fair,&nbsp; real physical anthropologists in Howard&rsquo;s day thought that there had been significant population movements and replacements in Europe, judging from changes in skeletons and skulls that accompanied archeological shifts, as when people turned taller, heavier boned , and brachycephalic just as the Bell-Beaker artifacts show up. But those physical anthropologists lost out to people like Boas<em> </em>&ndash; <strong>liars</strong>.</p>\n</blockquote>\n<p>Perhaps <a href=\"/lw/65b/scientific_misconduct_misdiagnosed_because_of/\">this</a> little old entry is relevant here. ^_^</p>\n<blockquote>\n<p>Given the chance (sufficient lack of information), American anthropologists assumed that the Mayans were peaceful astronomers. Howard would have assumed that they were just another blood-drenched snake cult: <strong>who came closer?</strong></p>\n<p>Now I&rsquo;m not saying that Howard got every single tiny little syllable of prehistory right.&nbsp; Not likely: so far, we haven&rsquo;t seen any signs of <em>Cthulhu-like visitors</em>, which abound in the Conan stories.&nbsp; So far.<strong> But Howard&rsquo;s priors were more accurate than those of the pots-not-people archeologists</strong>: more accurate than people like Excoffier and&nbsp; Currat, who assume that there hasn&rsquo;t been any population replacement in Europe since moderns displaced Neanderthals. More accurate than Chris Stringer,&nbsp; more accurate than Brian Ferguson.</p>\n<p>Most important, Conan, unlike the typical professor, knew what was best in life.</p>\n</blockquote>\n<p><a href=\"http://www.youtube.com/watch?v=6PQ6335puOc\">Heh</a>.</p>\n<p>Cochran you are such a nerd.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "bY5MaF2EATwDkomvu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8zqy3XjQa5gk92wa2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 34, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "12348", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Jo4ExrJxF6rm8cm3k", "xNqeGssARAYwbNCgH", "2oybbEw697CQgcRE5", "iMaL9hXWPGxtAxBSm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-22T04:36:34.396Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Empty Labels", "slug": "seq-rerun-empty-labels", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/78mKbnEF6s6NxFLwd/seq-rerun-empty-labels", "pageUrlRelative": "/posts/78mKbnEF6s6NxFLwd/seq-rerun-empty-labels", "linkUrl": "https://www.lesswrong.com/posts/78mKbnEF6s6NxFLwd/seq-rerun-empty-labels", "postedAtFormatted": "Sunday, January 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Empty%20Labels&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Empty%20Labels%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F78mKbnEF6s6NxFLwd%2Fseq-rerun-empty-labels%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Empty%20Labels%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F78mKbnEF6s6NxFLwd%2Fseq-rerun-empty-labels", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F78mKbnEF6s6NxFLwd%2Fseq-rerun-empty-labels", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>Today's post, <a href=\"/lw/ns/empty_labels/\">Empty Labels</a> was originally published on 14 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You use complex renamings to create the illusion of inference. Is a \"human\" defined as a \"mortal featherless biped\"? Then write: \"All [mortal featherless bipeds] are mortal; Socrates is a [mortal featherless biped]; therefore, Socrates is mortal.\" Looks less impressive that way, doesn't it?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9is/seq_rerun_the_argument_from_common_usage/\">The Argument from Common Usage</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "78mKbnEF6s6NxFLwd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.357348068519943e-07, "legacy": true, "legacyId": "12351", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i2dfY65JciebF3CAo", "9ZwwfrtiadpLx8QH5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-22T07:47:30.432Z", "modifiedAt": null, "url": null, "title": "The Singularity Institute is hiring an executive assistant near Berkeley", "slug": "the-singularity-institute-is-hiring-an-executive-assistant", "viewCount": null, "lastCommentedAt": "2022-04-14T00:17:39.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lz64onM6tkynPKh5R/the-singularity-institute-is-hiring-an-executive-assistant", "pageUrlRelative": "/posts/Lz64onM6tkynPKh5R/the-singularity-institute-is-hiring-an-executive-assistant", "linkUrl": "https://www.lesswrong.com/posts/Lz64onM6tkynPKh5R/the-singularity-institute-is-hiring-an-executive-assistant", "postedAtFormatted": "Sunday, January 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Singularity%20Institute%20is%20hiring%20an%20executive%20assistant%20near%20Berkeley&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Singularity%20Institute%20is%20hiring%20an%20executive%20assistant%20near%20Berkeley%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLz64onM6tkynPKh5R%2Fthe-singularity-institute-is-hiring-an-executive-assistant%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Singularity%20Institute%20is%20hiring%20an%20executive%20assistant%20near%20Berkeley%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLz64onM6tkynPKh5R%2Fthe-singularity-institute-is-hiring-an-executive-assistant", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLz64onM6tkynPKh5R%2Fthe-singularity-institute-is-hiring-an-executive-assistant", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 446, "htmlBody": "<p>The Singularity Institute is hiring an executive assistant for Executive Director <a href=\"http://lukeprog.com/\">Luke Muehlhauser</a>.</p>\n<p>Right now his limiter (besides the need for <em>some</em> sleep and recreation) is not (1) cognitive exhaustion after a certain number of hours or (2) akrasia, but instead (3) needing to spend lots of time doing things that don't <em>need</em>&nbsp;to be him: e.g. hunting down the best product for X and buying it, shopping for food, finding names and email addresses for the top 30 researchers in field X, finding motorcycle classes and a motorcycle so he can stop paying so much for cabs when he doesn't have time for public transport, scheduling meetings with dozens of donors and collaborators, finding a good location for activity X, preparing an itinerary and buying plane tickets, and hundreds of other small things. (Some of these are 'life' things, some of these are SI things, but hours are hours.) Luke may also ask his executive assistant to handle certain tasks for other SI staffers.</p>\n<p>Benefits:</p>\n<ul>\n<li>Work directly with some of the central figures of Less Wrong, especially <a href=\"/user/lukeprog\">Luke(prog)</a></li>\n<li>Work from home most of the time, with a somewhat self-determined schedule</li>\n<li>Trial period at $15/hr for 20 hrs/week; if all goes well then get hired full-time at SI's standard starting salary of $3k/month</li>\n</ul>\n<p>Responsibilities:</p>\n<ul>\n<li>Represent our organization in a professional manner at all times</li>\n<li>Manage scheduling and appointments for Luke</li>\n<li>Prepare and manage correspondence professionally and accurately</li>\n<li>Coordinate travel arrangements for Luke</li>\n<li>Online and local shopping and transport</li>\n<li>Internet research</li>\n<li>Whatever else Luke needs done</li>\n</ul>\n<p>Job requirements:</p>\n<ul>\n<li>Good interpersonal skills and strong team-player attitude</li>\n<li>Capable of clean, professional written communication with proper spelling, punctuation, and grammar</li>\n<li>Positive, friendly and helpful attitude</li>\n<li>Ability to handle sensitive and/or confidential material and information</li>\n<li>Must pay strong attention to detail</li>\n<li>Professional demeanor, dedicated and reliable, conscientious</li>\n<li>Computer &amp; internet literate</li>\n<li>Own a car</li>\n<li>Live in or near Berkeley, CA</li>\n</ul>\n<p>Bonus points if you...</p>\n<ul>\n<li>...have read the <a href=\"http://wiki.lesswrong.com/wiki/Sequences#Core_Sequences\">Core Sequences</a></li>\n<li>...have experience as a personal or executive assistant</li>\n<li>...have even better creative non-fiction writing skills than is required for professional correspondance</li>\n<li>...are handy with Google Scholar</li>\n<li>...know a good amount of math, statistics, computer science, or cognitive science</li>\n<li>...have some skills in graphic design / presentation design</li>\n</ul>\n<div><strong>How to apply</strong>:</div>\n<div style=\"padding-left: 30px;\">Send an email to jobs@intelligence.org with the subject line \"Executive Assistant Position\". Include your cover letter as plain text in the email body, and attach your r&eacute;sum&eacute; in PDF format.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lz64onM6tkynPKh5R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 24, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "12350", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-22T14:39:32.255Z", "modifiedAt": null, "url": null, "title": "Neurological reality of human thought and decision making; implications for rationalism.", "slug": "neurological-reality-of-human-thought-and-decision-making", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:59.178Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zNXSHLtnrSKFbcJqe/neurological-reality-of-human-thought-and-decision-making", "pageUrlRelative": "/posts/zNXSHLtnrSKFbcJqe/neurological-reality-of-human-thought-and-decision-making", "linkUrl": "https://www.lesswrong.com/posts/zNXSHLtnrSKFbcJqe/neurological-reality-of-human-thought-and-decision-making", "postedAtFormatted": "Sunday, January 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Neurological%20reality%20of%20human%20thought%20and%20decision%20making%3B%20implications%20for%20rationalism.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeurological%20reality%20of%20human%20thought%20and%20decision%20making%3B%20implications%20for%20rationalism.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNXSHLtnrSKFbcJqe%2Fneurological-reality-of-human-thought-and-decision-making%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Neurological%20reality%20of%20human%20thought%20and%20decision%20making%3B%20implications%20for%20rationalism.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNXSHLtnrSKFbcJqe%2Fneurological-reality-of-human-thought-and-decision-making", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNXSHLtnrSKFbcJqe%2Fneurological-reality-of-human-thought-and-decision-making", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 591, "htmlBody": "<p>The human brain is a massively parallel system. The best such system can do for doing anything efficiently and quickly is to have different small portions of brain compute and submit their partial answers and progressively reduce, combine and cherrypick - a process of which we seem to have almost no direct awareness of, and can only conjecture indirectly as it is the only way thought can possibly work on such slow clocked (~100..200hz) such extremely parallel hardware which uses up a good fraction of body's nutrient supply.</p>\n<p>Yet it is immensely difficult for us to think in terms of parallel processes. We have very little access to how the parallel processing works in our heads, and we have very limited ability of considering a parallel process in parallel in our heads. We are only aware of some serial-looking self model within ourselves - a model that we can most easily consider - and we misperceive this model as self; believing ourselves to be self aware when we are only aware of that model which we equated to self.</p>\n<p>People aren't, for the most part, discussing how to structure the parallel processing for maximum efficiency or rationality, and applying that to their lives. It's mostly the serial processes that are being discussed. The necessary, inescapable reality of how mind works is entirely sealed from us, and we are not directly aware of it, nor are we discussing and sharing how that works. Whatever little is available, we are not trained to think in those terms - the culture trains us to think in terms of serial, semantic process that would utter things like \"I think, therefore I am\".</p>\n<p>This is in a way depressing to realize.</p>\n<p>But at same time this realization brings hope - there may be a lot of low hanging fruit left if the approach has not been very well considered. I personally have been trying to think of myself as of parallel system with some agreement mechanism for a long while now. It does seem to be a more realistic way to think of oneself, in terms of understanding why you make mistakes and how they can be improved, but at same time as with any complex approach where you 'explain' existing phenomena there's a risk of being able to 'explain' anything while understanding nothing.</p>\n<p>I propose that we should try to overcome the long standing philosophical model of mind as singular serial computing entity, but instead try approaching it from the parallel computing angle; literature is rife with references to \"a part of me wanted\", and perhaps we should all take this as much more than allegory. Perhaps the way you work when you decide to do or not do something, is really best thought of as a disagreement of multiple systems with some arbitration mechanism forcing default action; perhaps training - the drill-response kind of training, not simply informing oneself - could allow to make much better choices in the real time, to arrive at choices rationally rather than via some sort of tug of war between regions that propose different answers and the one that sends the strongest signal winning the control.</p>\n<p>Of course that needs to be done very cautiously as in the complex and hard to think topics in general its easy to slip towards fuzzy logic where each logical step contains a small fallacy which leads to rapid divergence to the point that you can prove or explain anything. The Freudian style id/ego/superego as simple explanation for literally everything which predicts nothing is not what we want.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zNXSHLtnrSKFbcJqe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 3, "extendedScore": null, "score": 8.35965442399286e-07, "legacy": true, "legacyId": "12213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-22T19:11:13.975Z", "modifiedAt": null, "url": null, "title": "AI Box Role Plays ", "slug": "ai-box-role-plays", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:57.217Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lessdazed", "createdAt": "2011-02-02T05:06:52.010Z", "isAdmin": false, "displayName": "lessdazed"}, "userId": "ehZzKt5ByYBeyCLkz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RsYFTew9A9zCdg69E/ai-box-role-plays", "pageUrlRelative": "/posts/RsYFTew9A9zCdg69E/ai-box-role-plays", "linkUrl": "https://www.lesswrong.com/posts/RsYFTew9A9zCdg69E/ai-box-role-plays", "postedAtFormatted": "Sunday, January 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Box%20Role%20Plays%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Box%20Role%20Plays%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsYFTew9A9zCdg69E%2Fai-box-role-plays%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Box%20Role%20Plays%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsYFTew9A9zCdg69E%2Fai-box-role-plays", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsYFTew9A9zCdg69E%2Fai-box-role-plays", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>This page is to centralize discussion for the AI Box Role Plays I will be doing as the AI.</p>\n<p>Rules are as <a href=\"http://yudkowsky.net/singularity/aibox\">here</a>. In accordance with \"Regardless of the result, neither party shall ever reveal anything of what goes on within the AI-Box experiment except the outcome. &nbsp;Exceptions to this rule may occur only with the consent of both parties,\" I ask that if I break free multiple times I am permitted to say if I think it was the same or different arguments that persuaded my Gatekeepers.</p>\n<p>In the first trial, with Normal_Anomaly, the wager was 50 karma. The AI remained in the box, upvote Normal_Anomay <a href=\"/lw/9j4/ai_box_role_plays/5qbj\">here</a>, downvote lessdazed <a href=\"/lw/9j4/ai_box_role_plays/5qdb\">here</a>.&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">It was agreed to halve the wager from 50 karma to 25 due to the specific circumstances concluding the role-play in which that the outcome depended on variables that hadn't been specified, but if that sounds contemptible to you downvote all the way to -50.&nbsp;</span></p>\n<p>Also below are brief statements of intent by Gatekeepers to not let the AI out of the box, submitted before the role play, as well as before and after statements of approximately how effective they think both a) a human and b) a superintelligence would be at convincing them to let it out of a box.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RsYFTew9A9zCdg69E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.360694034363462e-07, "legacy": true, "legacyId": "12352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-22T19:50:56.262Z", "modifiedAt": null, "url": null, "title": "Study Hacks on Convenience as Anti-Productivity", "slug": "study-hacks-on-convenience-as-anti-productivity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:36.131Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3euj2mfrsfHrFEg3q/study-hacks-on-convenience-as-anti-productivity", "pageUrlRelative": "/posts/3euj2mfrsfHrFEg3q/study-hacks-on-convenience-as-anti-productivity", "linkUrl": "https://www.lesswrong.com/posts/3euj2mfrsfHrFEg3q/study-hacks-on-convenience-as-anti-productivity", "postedAtFormatted": "Sunday, January 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Study%20Hacks%20on%20Convenience%20as%20Anti-Productivity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudy%20Hacks%20on%20Convenience%20as%20Anti-Productivity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3euj2mfrsfHrFEg3q%2Fstudy-hacks-on-convenience-as-anti-productivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Study%20Hacks%20on%20Convenience%20as%20Anti-Productivity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3euj2mfrsfHrFEg3q%2Fstudy-hacks-on-convenience-as-anti-productivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3euj2mfrsfHrFEg3q%2Fstudy-hacks-on-convenience-as-anti-productivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p><a href=\"http://calnewport.com/blog/2012/01/21/distraction-is-a-symptom-of-a-deeper-problem-the-convenience-principle-and-the-destruction-of-american-productivity\">Distraction is a Symptom of a Deeper Problem: The Convenience Principle and the Destruction of American Productivity</a> is a good article on distractions versus getting things done. With extra emphasis on how many of our distractions are the result of a desire for convenience rather than something more substantial.</p>\n<blockquote>\n<p>Due to its ubiquity, it&rsquo;s easy to see the convenience principle as self-evident. I argue that it&rsquo;s actually contrived and harmful.</p>\n<p>To understand this perspective, let&rsquo;s contrast it to an alternative. The goal of any knowledge work organization (or student, which is really just a one-person knowledge work firm) is to produce information that is rare and valuable. With this in mind, consider the net value principle of selecting work habits. This principle says that the adoption of a work habit should be based solely on its net effect on the value produced by the organization.</p>\n<p>This principle also sounds obvious, but when you dive deeper into its implications you&rsquo;ll find that it often conflicts with the conclusions of the convenience principle. The reason for this conflict is that convenience often has nothing to do with value.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3euj2mfrsfHrFEg3q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 8.360845979104311e-07, "legacy": true, "legacyId": "12353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-22T20:50:10.137Z", "modifiedAt": null, "url": null, "title": "Startups as a Rationality Test Bed?", "slug": "startups-as-a-rationality-test-bed", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:33.065Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beoShaffer", "createdAt": "2011-05-29T15:52:29.240Z", "isAdmin": false, "displayName": "beoShaffer"}, "userId": "589WwYp3jytZqATFL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uPm3JcPvLzzfGPmdW/startups-as-a-rationality-test-bed", "pageUrlRelative": "/posts/uPm3JcPvLzzfGPmdW/startups-as-a-rationality-test-bed", "linkUrl": "https://www.lesswrong.com/posts/uPm3JcPvLzzfGPmdW/startups-as-a-rationality-test-bed", "postedAtFormatted": "Sunday, January 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Startups%20as%20a%20Rationality%20Test%20Bed%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStartups%20as%20a%20Rationality%20Test%20Bed%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuPm3JcPvLzzfGPmdW%2Fstartups-as-a-rationality-test-bed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Startups%20as%20a%20Rationality%20Test%20Bed%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuPm3JcPvLzzfGPmdW%2Fstartups-as-a-rationality-test-bed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuPm3JcPvLzzfGPmdW%2Fstartups-as-a-rationality-test-bed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 327, "htmlBody": "<p><span style=\"font-family: Arial; font-size: 13px;\">What attributes make a task useful for rationality verification?</span></p>\n<p style=\"margin: 0.0px 0.0px 13.0px 0.0px; font: 13.0px Arial;\"><span style=\"letter-spacing: 0.0px;\">After thinking it over I believe I have identified three main components.&nbsp; The first is that the task should be as grounded in the real world as is possible. The second is that the task should be involve a wide variety of subtasks, preferably ones that involve decision making and/or forecasting. &nbsp;This will help insure that the effect is from general rationality, rather than from the rationality training helping with domain specific skills. &nbsp;The third is that there should be clear measure of successes for the task.</span></p>\n<p style=\"margin: 0.0px 0.0px 13.0px 0.0px; font: 13.0px Arial;\"><span style=\"letter-spacing: 0.0px;\">As I am not personally involved with the field I could be missing something important, but it seems like founding a successful&nbsp;startup would fulfill all three components.&nbsp; I propose that investigating the effect of giving startup founders rationality training would be a good basis for an experiment. Unfortunately, I do not know if it would be feasible to run such an experiment in real life. &nbsp;Thus, I am turning to the LW community to see if the people reading this have any suggestions.</span></p>\n<p>-addendum&nbsp;</p>\n<p>I didn't go into details about exact exprimental methods for a couple of reasons. &nbsp; Partially because I assumed, apparently incorrectly, that it was obvious that any experiment for testing rationality would be conducted with the best experimental protocols that we could manage. &nbsp; But mostly, because I thought that it would be good to get feed back on the basic idea of rationally verification + startups ?= good before spending time going into detail about things like control groups, random assignment ect.&nbsp;</p>\n<p>I welcome suggestions along those lines, and given the attention this has received will try to go back and add some of my own ideas when I have time, but wanted to make cleat that I wasn't intending this post as a detailed experimental design.</p>\n<p>Also does anyone have any idea why the first part of this post has different spacing from the second? &nbsp;It's not intentional on my part.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uPm3JcPvLzzfGPmdW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 8.361072658152553e-07, "legacy": true, "legacyId": "12354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-22T21:42:32.145Z", "modifiedAt": null, "url": null, "title": "Question Regarding Motorcycling", "slug": "question-regarding-motorcycling-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Costanza", "createdAt": "2010-09-14T16:22:53.235Z", "isAdmin": false, "displayName": "Costanza"}, "userId": "cXudnoTp54SYgqfgF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SFskzBiqDadJT5wwq/question-regarding-motorcycling-0", "pageUrlRelative": "/posts/SFskzBiqDadJT5wwq/question-regarding-motorcycling-0", "linkUrl": "https://www.lesswrong.com/posts/SFskzBiqDadJT5wwq/question-regarding-motorcycling-0", "postedAtFormatted": "Sunday, January 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20Regarding%20Motorcycling&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20Regarding%20Motorcycling%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFskzBiqDadJT5wwq%2Fquestion-regarding-motorcycling-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20Regarding%20Motorcycling%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFskzBiqDadJT5wwq%2Fquestion-regarding-motorcycling-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFskzBiqDadJT5wwq%2Fquestion-regarding-motorcycling-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>A recent [discussion thread](http://lesswrong.com/r/discussion/lw/9j2/the_singularity_institute_is_hiring_an_executive/) seems to have devolved into a discussion of the relative merits and demerits of motorcycling. I wonder what the LessWrong community at large would say about motorcycling in a modern urban or suburban environment after some considered thought on the issue, particularly with regard to safety, and the practicality of cryonics, and with further consideration of local conditions. <br /><br />For instance, here in Southern California, I'm under the impression that a lot of people use motorcycles the way runners at Pamplona use the bulls: to display fearlessness.&nbsp; I'm under the impression that motorcycling in (for instance) Britain is dominated by more practical concerns. Is this wrong?&nbsp; More generally, is motorcycling an unjustifiable risk for a rationalist?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SFskzBiqDadJT5wwq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "12357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T02:14:46.525Z", "modifiedAt": null, "url": null, "title": "Fissure opens in chess AI scene [link]", "slug": "fissure-opens-in-chess-ai-scene-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:37.889Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XQCgsY5yQbERHhaan/fissure-opens-in-chess-ai-scene-link", "pageUrlRelative": "/posts/XQCgsY5yQbERHhaan/fissure-opens-in-chess-ai-scene-link", "linkUrl": "https://www.lesswrong.com/posts/XQCgsY5yQbERHhaan/fissure-opens-in-chess-ai-scene-link", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fissure%20opens%20in%20chess%20AI%20scene%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFissure%20opens%20in%20chess%20AI%20scene%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQCgsY5yQbERHhaan%2Ffissure-opens-in-chess-ai-scene-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fissure%20opens%20in%20chess%20AI%20scene%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQCgsY5yQbERHhaan%2Ffissure-opens-in-chess-ai-scene-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQCgsY5yQbERHhaan%2Ffissure-opens-in-chess-ai-scene-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://boingboing.net/2012/01/19/fissure-opens-in-chess-ai-scen.html\">http://boingboing.net/2012/01/19/fissure-opens-in-chess-ai-scen.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XQCgsY5yQbERHhaan", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -10, "extendedScore": null, "score": 8.362315126265944e-07, "legacy": true, "legacyId": "12358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T03:38:53.703Z", "modifiedAt": null, "url": null, "title": "Meetup : Ongoing Ohio Meetup", "slug": "meetup-ongoing-ohio-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:02.994Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5yzCDSgPEtBefY8k8/meetup-ongoing-ohio-meetup", "pageUrlRelative": "/posts/5yzCDSgPEtBefY8k8/meetup-ongoing-ohio-meetup", "linkUrl": "https://www.lesswrong.com/posts/5yzCDSgPEtBefY8k8/meetup-ongoing-ohio-meetup", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Ongoing%20Ohio%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Ongoing%20Ohio%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yzCDSgPEtBefY8k8%2Fmeetup-ongoing-ohio-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Ongoing%20Ohio%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yzCDSgPEtBefY8k8%2Fmeetup-ongoing-ohio-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yzCDSgPEtBefY8k8%2Fmeetup-ongoing-ohio-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/69'>Ongoing Ohio Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 February 2012 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Max & Erma's Restaurant, 123 Gano Road, Wilmington, OH 45177-8848</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Ohio just had its first-ever LessWrong meet-up! It was such a success, with 13 amazing people showing, that we decided to make it a monthly shindig.</p>\n\n<p>The Ohio LessWrong group will be holding a meet-up every third Sunday of the month, at the Max&amp;Erma's off I-71. This is less than an hour from Cincinnati, Columbus, and Dayton.</p>\n\n<p>Cincinnati and Columbus will also be holding smaller meet-ups on a more frequent basis.</p>\n\n<p><strong>Please PM me with your email address if you would like to join the Ohio LessWrong discussion group</strong>, and receive information on the city-specific meet-ups (which will probably not be posted here).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/69'>Ongoing Ohio Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5yzCDSgPEtBefY8k8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 8.362637156899192e-07, "legacy": true, "legacyId": "12359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Ongoing_Ohio_Meetup\">Discussion article for the meetup : <a href=\"/meetups/69\">Ongoing Ohio Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 February 2012 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Max &amp; Erma's Restaurant, 123 Gano Road, Wilmington, OH 45177-8848</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Ohio just had its first-ever LessWrong meet-up! It was such a success, with 13 amazing people showing, that we decided to make it a monthly shindig.</p>\n\n<p>The Ohio LessWrong group will be holding a meet-up every third Sunday of the month, at the Max&amp;Erma's off I-71. This is less than an hour from Cincinnati, Columbus, and Dayton.</p>\n\n<p>Cincinnati and Columbus will also be holding smaller meet-ups on a more frequent basis.</p>\n\n<p><strong>Please PM me with your email address if you would like to join the Ohio LessWrong discussion group</strong>, and receive information on the city-specific meet-ups (which will probably not be posted here).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Ongoing_Ohio_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/69\">Ongoing Ohio Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Ongoing Ohio Meetup", "anchor": "Discussion_article_for_the_meetup___Ongoing_Ohio_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Ongoing Ohio Meetup", "anchor": "Discussion_article_for_the_meetup___Ongoing_Ohio_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T04:41:41.989Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Taboo Your Words", "slug": "seq-rerun-taboo-your-words", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qiNH9rAx7ew449FkB/seq-rerun-taboo-your-words", "pageUrlRelative": "/posts/qiNH9rAx7ew449FkB/seq-rerun-taboo-your-words", "linkUrl": "https://www.lesswrong.com/posts/qiNH9rAx7ew449FkB/seq-rerun-taboo-your-words", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Taboo%20Your%20Words&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Taboo%20Your%20Words%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqiNH9rAx7ew449FkB%2Fseq-rerun-taboo-your-words%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Taboo%20Your%20Words%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqiNH9rAx7ew449FkB%2Fseq-rerun-taboo-your-words", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqiNH9rAx7ew449FkB%2Fseq-rerun-taboo-your-words", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>Today's post, <a href=\"/lw/nu/taboo_your_words/\">Taboo Your Words</a> was originally published on 15 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If Albert and Barry aren't allowed to use the word \"sound\", then Albert will have to say \"A tree falling in a deserted forest generates acoustic vibrations\", and Barry will say \"A tree falling in a deserted forest generates no auditory experiences\". When a word poses a problem, the simplest solution is to eliminate the word and its synonyms.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9j3/seq_rerun_empty_labels/\">Empty Labels</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qiNH9rAx7ew449FkB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.362877603321226e-07, "legacy": true, "legacyId": "12360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WBdvyyHLdxZSAMmoz", "78mKbnEF6s6NxFLwd", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T04:47:52.636Z", "modifiedAt": null, "url": null, "title": "Meetup : Salt Lake City LW Meetup #2 Logistics", "slug": "meetup-salt-lake-city-lw-meetup-2-logistics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:52.812Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamisom", "createdAt": "2011-06-13T03:19:15.520Z", "isAdmin": false, "displayName": "adamisom"}, "userId": "eT8NPFmc2GDb5QFfc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MJKQhpkgcxEXjgo2E/meetup-salt-lake-city-lw-meetup-2-logistics", "pageUrlRelative": "/posts/MJKQhpkgcxEXjgo2E/meetup-salt-lake-city-lw-meetup-2-logistics", "linkUrl": "https://www.lesswrong.com/posts/MJKQhpkgcxEXjgo2E/meetup-salt-lake-city-lw-meetup-2-logistics", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Salt%20Lake%20City%20LW%20Meetup%20%232%20Logistics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Salt%20Lake%20City%20LW%20Meetup%20%232%20Logistics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMJKQhpkgcxEXjgo2E%2Fmeetup-salt-lake-city-lw-meetup-2-logistics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Salt%20Lake%20City%20LW%20Meetup%20%232%20Logistics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMJKQhpkgcxEXjgo2E%2Fmeetup-salt-lake-city-lw-meetup-2-logistics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMJKQhpkgcxEXjgo2E%2Fmeetup-salt-lake-city-lw-meetup-2-logistics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 291, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6a'>Salt Lake City LW Meetup #2 Logistics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 February 2012 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Downtown central Salt Lake City (near the library)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>Update</strong>: see lesswrong.com/meetups for info on the next Meetup</p>\n\n<p><em>Note: this announcement is to gather information on your availability to schedule the best time possible for the 2nd meetup</em> Less Wrong's second Salt Lake City meetup is tentatively scheduled for Saturday the 4th at 3:00 pm at a cafe or coffeehouse. I say tentative because I am asking those interested to vote on the matter, giving up to three preferences, in order, for the time. It would also be nice to indicate the days or times that you will definitely <em>not</em> be able to attend. Email your preferences to me at <strong>isom.adam@gmail.com</strong> and/or leave them in the comments. Here are the options: A. Saturday at 3 pm B. Saturday at 7 pm C. Monday (6th) at 6 pm D. Thursday (2nd) at 6 pm E. Sunday at 3 pm (again) F. Other (please indicate) Feel free to suggest a place too; otherwise the venue will likely be near the SLC public library. The agenda includes a presentation by ksvanhorn on basic Bayesian probability theory, followed by discussion. There will also be a brief presentation of something on instrumental rationality, followed by discussion, and possibly the formation of a subgroup with its focus. Edit: There seems to be persistent interest in discussing Mormonism. If any LWer in Salt Lake City would like to prepare a brief presentation that could get us discussing it, let me know and I'll probably plan that for Meetup #3. The cutoff for emailing me your preferences is Jan 31.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6a'>Salt Lake City LW Meetup #2 Logistics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MJKQhpkgcxEXjgo2E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.362901254067868e-07, "legacy": true, "legacyId": "12361", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City_LW_Meetup__2_Logistics\">Discussion article for the meetup : <a href=\"/meetups/6a\">Salt Lake City LW Meetup #2 Logistics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 February 2012 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Downtown central Salt Lake City (near the library)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>Update</strong>: see lesswrong.com/meetups for info on the next Meetup</p>\n\n<p><em>Note: this announcement is to gather information on your availability to schedule the best time possible for the 2nd meetup</em> Less Wrong's second Salt Lake City meetup is tentatively scheduled for Saturday the 4th at 3:00 pm at a cafe or coffeehouse. I say tentative because I am asking those interested to vote on the matter, giving up to three preferences, in order, for the time. It would also be nice to indicate the days or times that you will definitely <em>not</em> be able to attend. Email your preferences to me at <strong>isom.adam@gmail.com</strong> and/or leave them in the comments. Here are the options: A. Saturday at 3 pm B. Saturday at 7 pm C. Monday (6th) at 6 pm D. Thursday (2nd) at 6 pm E. Sunday at 3 pm (again) F. Other (please indicate) Feel free to suggest a place too; otherwise the venue will likely be near the SLC public library. The agenda includes a presentation by ksvanhorn on basic Bayesian probability theory, followed by discussion. There will also be a brief presentation of something on instrumental rationality, followed by discussion, and possibly the formation of a subgroup with its focus. Edit: There seems to be persistent interest in discussing Mormonism. If any LWer in Salt Lake City would like to prepare a brief presentation that could get us discussing it, let me know and I'll probably plan that for Meetup #3. The cutoff for emailing me your preferences is Jan 31.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City_LW_Meetup__2_Logistics1\">Discussion article for the meetup : <a href=\"/meetups/6a\">Salt Lake City LW Meetup #2 Logistics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Salt Lake City LW Meetup #2 Logistics", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City_LW_Meetup__2_Logistics", "level": 1}, {"title": "Discussion article for the meetup : Salt Lake City LW Meetup #2 Logistics", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City_LW_Meetup__2_Logistics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T05:49:54.931Z", "modifiedAt": null, "url": null, "title": "Meetup : Waterloo Meetup", "slug": "meetup-waterloo-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.684Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jpulgarin", "createdAt": "2011-03-23T08:05:56.812Z", "isAdmin": false, "displayName": "jpulgarin"}, "userId": "rco4fXwJv2s5XCyvT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XSi3tu4HtHu4NmEnP/meetup-waterloo-meetup", "pageUrlRelative": "/posts/XSi3tu4HtHu4NmEnP/meetup-waterloo-meetup", "linkUrl": "https://www.lesswrong.com/posts/XSi3tu4HtHu4NmEnP/meetup-waterloo-meetup", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Waterloo%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Waterloo%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSi3tu4HtHu4NmEnP%2Fmeetup-waterloo-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Waterloo%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSi3tu4HtHu4NmEnP%2Fmeetup-waterloo-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSi3tu4HtHu4NmEnP%2Fmeetup-waterloo-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6b'>Waterloo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 January 2012 08:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4 King St N Waterloo</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Location: <a href=\"http://www.symposiumcafe.com/\" rel=\"nofollow\">Symposium Cafe</a></p>\n\n<p>I plan to keep organizing these meetups on a weekly basis, so this should be the last irregularly scheduled meetup.</p>\n\n<p>The reservation is under \"Julian Pulgarin\", and if I am confident enough, there will be a large sign featuring <a href=\"http://img.artlebedev.com/everything/skrepkus/skrepkus.jpg\" rel=\"nofollow\">this</a> picture on it. My phone number is 519-781-1573.</p>\n\n<p>I propose we go through the sunk cost <a href=\"http://lesswrong.com/lw/9hb/position_design_and_write_rationality_curriculum/\">exercise booklets</a>, and agree on a regular time and place for weekly meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6b'>Waterloo Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XSi3tu4HtHu4NmEnP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.363138779033275e-07, "legacy": true, "legacyId": "12362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Waterloo_Meetup\">Discussion article for the meetup : <a href=\"/meetups/6b\">Waterloo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 January 2012 08:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4 King St N Waterloo</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Location: <a href=\"http://www.symposiumcafe.com/\" rel=\"nofollow\">Symposium Cafe</a></p>\n\n<p>I plan to keep organizing these meetups on a weekly basis, so this should be the last irregularly scheduled meetup.</p>\n\n<p>The reservation is under \"Julian Pulgarin\", and if I am confident enough, there will be a large sign featuring <a href=\"http://img.artlebedev.com/everything/skrepkus/skrepkus.jpg\" rel=\"nofollow\">this</a> picture on it. My phone number is 519-781-1573.</p>\n\n<p>I propose we go through the sunk cost <a href=\"http://lesswrong.com/lw/9hb/position_design_and_write_rationality_curriculum/\">exercise booklets</a>, and agree on a regular time and place for weekly meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Waterloo_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/6b\">Waterloo Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Waterloo Meetup", "anchor": "Discussion_article_for_the_meetup___Waterloo_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Waterloo Meetup", "anchor": "Discussion_article_for_the_meetup___Waterloo_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ifL8f4Xzy2D9Bb6zs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T11:40:47.432Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne practical rationality meetup", "slug": "meetup-melbourne-practical-rationality-meetup-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:52.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vvfj2zqvNP79s84ZC/meetup-melbourne-practical-rationality-meetup-4", "pageUrlRelative": "/posts/vvfj2zqvNP79s84ZC/meetup-melbourne-practical-rationality-meetup-4", "linkUrl": "https://www.lesswrong.com/posts/vvfj2zqvNP79s84ZC/meetup-melbourne-practical-rationality-meetup-4", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20practical%20rationality%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20practical%20rationality%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvvfj2zqvNP79s84ZC%2Fmeetup-melbourne-practical-rationality-meetup-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20practical%20rationality%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvvfj2zqvNP79s84ZC%2Fmeetup-melbourne-practical-rationality-meetup-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvvfj2zqvNP79s84ZC%2Fmeetup-melbourne-practical-rationality-meetup-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6c'>Melbourne practical rationality meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 February 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh St, West Melbourne, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion:</p>\n\n<p><a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a> <a href=\"http://www.google.com/moderator/#16/e=6a317\" rel=\"nofollow\">http://www.google.com/moderator/#16/e=6a317</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6c'>Melbourne practical rationality meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vvfj2zqvNP79s84ZC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.364482394622119e-07, "legacy": true, "legacyId": "12363", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup\">Discussion article for the meetup : <a href=\"/meetups/6c\">Melbourne practical rationality meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 February 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh St, West Melbourne, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion:</p>\n\n<p><a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a> <a href=\"http://www.google.com/moderator/#16/e=6a317\" rel=\"nofollow\">http://www.google.com/moderator/#16/e=6a317</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup1\">Discussion article for the meetup : <a href=\"/meetups/6c\">Melbourne practical rationality meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne practical rationality meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne practical rationality meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T14:52:05.813Z", "modifiedAt": null, "url": null, "title": "How would you talk a stranger off the ledge?", "slug": "how-would-you-talk-a-stranger-off-the-ledge", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.396Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MoreOn", "createdAt": "2010-12-09T23:13:18.874Z", "isAdmin": false, "displayName": "MoreOn"}, "userId": "rmPGJ5r2tu26kgZXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gCtyFfDfLpY9DKSa3/how-would-you-talk-a-stranger-off-the-ledge", "pageUrlRelative": "/posts/gCtyFfDfLpY9DKSa3/how-would-you-talk-a-stranger-off-the-ledge", "linkUrl": "https://www.lesswrong.com/posts/gCtyFfDfLpY9DKSa3/how-would-you-talk-a-stranger-off-the-ledge", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20would%20you%20talk%20a%20stranger%20off%20the%20ledge%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20would%20you%20talk%20a%20stranger%20off%20the%20ledge%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgCtyFfDfLpY9DKSa3%2Fhow-would-you-talk-a-stranger-off-the-ledge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20would%20you%20talk%20a%20stranger%20off%20the%20ledge%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgCtyFfDfLpY9DKSa3%2Fhow-would-you-talk-a-stranger-off-the-ledge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgCtyFfDfLpY9DKSa3%2Fhow-would-you-talk-a-stranger-off-the-ledge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 380, "htmlBody": "<p>Last month, two people far at the periphery of my social circles have threatened suicide. Seems like a sign for me to learn some ledge-fu.</p>\r\n<p>I reviewed the stuff I'd learned back in high school (\"Listen.\" \"Be supportive.\" \"Don't argue.\" \"Etc etc etc.\") I have trouble believing that this would work outside of movieland, <em>especially </em>on strangers. More so, in person I'm an awkward, fidgeting introvert---the impact of everything I say is thus diminished, and I sound very insincere or clinical, like I'm following a bad movie script, when I say anything like, \"You are not alone in this. I&rsquo;m here for you.\" or \"How can I best support you right now?\" I doubt that this would sound any better in writing.</p>\r\n<p>I suppose I could split my question into two related ones: what would you say to a person threatening to commit suicide, 1. in person, and 2. in an email?</p>\r\n<p>I'm looking for out-of-the-box ideas that don't rely on charisma or compassion shining through. Personally, if I ever need to talk myself out of suicidal thoughts, I apply the \"bum comparison principle\": if my life is so crummy that I'm willing to commit suicide, then I should be willing to just walk out on everything I value and <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/WalkingTheEarth\">drift off in a random direction</a>, survive by dine-and-dashing out of cheap restaurants and wash dishes if I get caught, maybe take <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/SideQuest\">odd jobs</a> or hitchhike or gather roots and berries or blog from public libraries. I don't see this possibility in a negative light, and yet I still haven't done it. To me, it means that however bad my life may seem, I'm still too attached to it to walk out; therefore, suicide isn't on the menu.</p>\r\n<p>People have different reasons to want suicide, and I understand that what works for me with my first world problems probably won't work for a person who is in too much physical pain from an incurable disease. To the best of my knowledge, the two people I mentioned earlier are both unskilled laborers who had lost their jobs, one of them so long ago that he's no longer eligible for unemployment benefits. I don't think I'll meet these particular people again, but I'd appreciate everyone's thoughts on what I could've said if my brain hadn't frozen.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gCtyFfDfLpY9DKSa3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 8.365215130182212e-07, "legacy": true, "legacyId": "12364", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T19:39:42.722Z", "modifiedAt": null, "url": null, "title": "The Human's Hidden Utility Function (Maybe)", "slug": "the-human-s-hidden-utility-function-maybe", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.177Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fa5o2tg9EfJE77jEQ/the-human-s-hidden-utility-function-maybe", "pageUrlRelative": "/posts/fa5o2tg9EfJE77jEQ/the-human-s-hidden-utility-function-maybe", "linkUrl": "https://www.lesswrong.com/posts/fa5o2tg9EfJE77jEQ/the-human-s-hidden-utility-function-maybe", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Human's%20Hidden%20Utility%20Function%20(Maybe)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Human's%20Hidden%20Utility%20Function%20(Maybe)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffa5o2tg9EfJE77jEQ%2Fthe-human-s-hidden-utility-function-maybe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Human's%20Hidden%20Utility%20Function%20(Maybe)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffa5o2tg9EfJE77jEQ%2Fthe-human-s-hidden-utility-function-maybe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffa5o2tg9EfJE77jEQ%2Fthe-human-s-hidden-utility-function-maybe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1027, "htmlBody": "<p>Suppose it turned out that humans violate the axioms of <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM rationality</a> (and therefore <a href=\"/lw/6da/do_humans_want_things/\">don't act like they have utility functions</a>) because there are <em>three</em>&nbsp;valuation systems in the brain that make conflicting valuations, and all three systems contribute to choice. And suppose that upon reflection we would clearly reject the outputs of two of these systems, whereas the third system looks something more like a utility function we might be able to use in <a href=\"http://intelligence.org/upload/CEV.html\">CEV</a>.</p>\n<p>What I just described is part of the leading theory of choice in the human brain.</p>\n<p><a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">Recall that</a> human choices are made when certain populations of neurons encode expected subjective value (in their firing rates) for each option in the choice set, with the final choice being made by an argmax or reservation price mechanism.</p>\n<p>Today's news is that our best current theory of human choices says that at least three <em>different</em> systems compute \"values\" that are then fed into the final choice circuit:</p>\n<ul>\n<li>\n<p>The <em>model-based system</em> \"uses experience in the environment to learn a model of the transition distribution, outcomes and motivationally-sensitive utilities.\" (See <a href=\"http://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981/\">Sutton &amp; Barto 1998</a> for the meanings of these terms in reinforcement learning theory.) The model-based system also \"infers choices by... building and evaluating the search decision tree to work out the optimal course of action.\" In short, the model-based system is responsible for goal-directed behavior. However, making all choices with a goal-directed system using something like a utility function would be computationally prohibitive (<a href=\"http://matt.colorado.edu/teaching/highcog/fall11/readings/dnd5.pdf\">Daw et al. 2005</a>), so many animals (including humans) first evolved much simpler methods for calculating the subjective values of options (see below).</p>\n</li>\n<li>\n<p>The <em>model-free system</em> also learns a model of the transition distribution and outcomes from experience, but \"it does so by caching and then recalling the results of experience rather than building and searching the tree of possibilities. Thus, the model-free controller does not even represent the outcomes... that underlie the utilities, and is therefore not in any position to change the estimate of its values if the motivational state changes. Consider, for instance, the case that after a subject has been taught to press a lever to get some cheese, the cheese is poisoned, so it is no longer worth eating. The model-free system would learn the utility of pressing the lever, but would not have the informational wherewithal to realize that this utility had changed when the cheese had been poisoned. Thus it would continue to insist upon pressing the lever. This is an example of motivational insensitivity.\"</p>\n</li>\n<li>\n<p>The <em>Pavlovian system</em>, in contrast, calculates values based on a set of hard-wired preparatory and consummatory \"preferences.\" Rather than calculate value based on what is likely to lead to rewarding and punishing outcomes, the Pavlovian system calculates values consistent with automatic approach toward appetitive stimuli, and automatic withdrawal from aversive stimuli. Thus, \"animals cannot help but approach (rather than run away from) a source of food, even if the experimenter has cruelly arranged things in a looking-glass world so that the approach appears to make the food recede, whereas retreating would make the food more accessible (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/01/Hershberger-An-approach-through-the-looking-glass.pdf\">Hershberger 1986</a>).\"</p>\n</li>\n</ul>\n<p>Or, as Jandila <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/5qk0\">put it</a>:</p>\n<ul>\n<li><em>Model-based system</em>: Figure out what's going on, and what actions maximize returns, and do them.</li>\n<li><em>Model-free system</em>: Do the thingy that worked before again!</li>\n<li><em>Pavlovian system</em>: Avoid the unpleasant thing and go to the pleasant thing. Repeat as necessary.</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<p>In short:</p>\n<blockquote>\n<p>We have described three systems that are involved in making choices. Even in the case that they share a single, Platonic, utility function for outcomes, the choices they express can be quite different. The model-based controller comes closest to being Platonically appropriate... The choices of the model-free controller can depart from current utilities because it has learned or cached a set of values that may no longer be correct. Pavlovian choices, though determined over the course of evolution to be appropriate, can turn out to be instrumentally catastrophic in any given experimental domain...</p>\n<p>[Having multiple systems that calculate value] is [one way] of addressing the complexities mentioned, but can lead to clashes between Platonic utility and choice. Further, model-free and Pavlovian choices can themselves be inconsistent with their own utilities.</p>\n</blockquote>\n<p>We don't yet know how choice results from the inputs of these three systems, nor how the systems might interact before they deliver their value calculations to the final choice circuit, nor whether the model-based system <em>really</em> uses anything like a coherent utility function. But it looks like the human&nbsp;<em>might</em>&nbsp;have a \"hidden\" utility function that would reveal itself if it wasn't also using the computationally cheaper model-free and Pavlovian systems to help determine choice.</p>\n<p>At a glance, it seems that upon reflection I might embrace an extrapolation of the model-based system's preferences as representing \"my values,\" and I would reject the outputs of the model-free and Pavlovian systems as the outputs of dumb systems that evolved for their computational simplicity, and can be seen as ways of trying to approximate the full power of a model-based system responsible for goal-directed behavior.</p>\n<p>On the other hand, as Eliezer <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/5qll\">points out</a>, perhaps we ought to be suspicious of this, because \"it sounds like the correct answer ought to be to just keep the part with the coherent utility function in CEV which would make it way easier, but then someone's going to jump up and say: 'Ha ha! Love and friendship were actually in the other two!'\"</p>\n<p>Unfortunately, it's too early to tell whether these results will be useful for CEV. But it's a <em>little</em> promising. This is the kind of thing that sometimes happens when you <a href=\"/lw/8ns/hack_away_at_the_edges/\">hack away at the edges</a> of hard problems. This is also a <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">repeat</a> of the lesson that \"you can often out-pace most philosophers simply by reading what today's leading <em>scientists</em> have to say about a given topic instead of reading what <em>philosophers</em> say about it.\"</p>\n<p>(For pointers to the relevant experimental data, and for an explanation of the mathematical role of each valuation system in the brain's reinforcement learning system, see <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/01/Dayan-Models-of-Value-and-Choice.pdf\">Dayan (2011)</a>. All quotes in this post are from that chapter, except for the last one.)</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2012/01/neurons.jpg\" alt=\"\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 3, "sYm3HiWcfZvrGu3ui": 1, "Wi3EopKJ2aNdtxSWg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fa5o2tg9EfJE77jEQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 62, "extendedScore": null, "score": 0.000139, "legacy": true, "legacyId": "12365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 62, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nBdaTGoDAYxHePSDa", "hN2aRnu798yas5b2k", "6bSHiD9TxsJwe2WqT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T19:45:51.482Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup 01-25-2012", "slug": "meetup-west-la-meetup-01-25-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:35.814Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kmGB3sz5ET7vnXc3K/meetup-west-la-meetup-01-25-2012", "pageUrlRelative": "/posts/kmGB3sz5ET7vnXc3K/meetup-west-la-meetup-01-25-2012", "linkUrl": "https://www.lesswrong.com/posts/kmGB3sz5ET7vnXc3K/meetup-west-la-meetup-01-25-2012", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%2001-25-2012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%2001-25-2012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmGB3sz5ET7vnXc3K%2Fmeetup-west-la-meetup-01-25-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%2001-25-2012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmGB3sz5ET7vnXc3K%2Fmeetup-west-la-meetup-01-25-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmGB3sz5ET7vnXc3K%2Fmeetup-west-la-meetup-01-25-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6d'>West LA Meetup 01-25-2012</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 January 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, January 25th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week is about <a href=\"http://lesswrong.com/r/discussion/lw/8vm/the_rationalists_checklist/\">checklists</a>. Our tentative goal for the evening is to make a rationality checklist to consider before making big decisions. If you have a chance, try to think of some good examples of bad decisions, and good decisions made under uncertainty. You may find some inspiration browsing Wikipedia's list of <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\" rel=\"nofollow\">cognitive biases</a>. Also see the original source of inspiration for this meeting, <a href=\"http://moneyarchive.wordpress.com/category/the-charlie-munger-checklists/\" rel=\"nofollow\">Charlie Munger&#39;s checklists.</a></p>\n\n<p>This is also a good time to browse <a href=\"http://lesswrong.com/recentposts\">recent posts</a>, as they also make for good discussion!</p>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome. If we have a large group, we may also play a game!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6d'>West LA Meetup 01-25-2012</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kmGB3sz5ET7vnXc3K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.366340506063524e-07, "legacy": true, "legacyId": "12366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_01_25_2012\">Discussion article for the meetup : <a href=\"/meetups/6d\">West LA Meetup 01-25-2012</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 January 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, January 25th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week is about <a href=\"http://lesswrong.com/r/discussion/lw/8vm/the_rationalists_checklist/\">checklists</a>. Our tentative goal for the evening is to make a rationality checklist to consider before making big decisions. If you have a chance, try to think of some good examples of bad decisions, and good decisions made under uncertainty. You may find some inspiration browsing Wikipedia's list of <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\" rel=\"nofollow\">cognitive biases</a>. Also see the original source of inspiration for this meeting, <a href=\"http://moneyarchive.wordpress.com/category/the-charlie-munger-checklists/\" rel=\"nofollow\">Charlie Munger's checklists.</a></p>\n\n<p>This is also a good time to browse <a href=\"http://lesswrong.com/recentposts\">recent posts</a>, as they also make for good discussion!</p>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome. If we have a large group, we may also play a game!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_01_25_20121\">Discussion article for the meetup : <a href=\"/meetups/6d\">West LA Meetup 01-25-2012</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup 01-25-2012", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_01_25_2012", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup 01-25-2012", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_01_25_20121", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tLR9YZHiNoDE2Czjh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T19:52:34.141Z", "modifiedAt": null, "url": null, "title": "[LINK] Neuroscientists Find That Status within Groups Can Affect IQ", "slug": "link-neuroscientists-find-that-status-within-groups-can", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:25.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cafesofie", "createdAt": "2011-03-09T07:26:18.188Z", "isAdmin": false, "displayName": "cafesofie"}, "userId": "7qeumCwzxpiDWvL53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GRtXb2unSYxSbFH7A/link-neuroscientists-find-that-status-within-groups-can", "pageUrlRelative": "/posts/GRtXb2unSYxSbFH7A/link-neuroscientists-find-that-status-within-groups-can", "linkUrl": "https://www.lesswrong.com/posts/GRtXb2unSYxSbFH7A/link-neuroscientists-find-that-status-within-groups-can", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Neuroscientists%20Find%20That%20Status%20within%20Groups%20Can%20Affect%20IQ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Neuroscientists%20Find%20That%20Status%20within%20Groups%20Can%20Affect%20IQ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRtXb2unSYxSbFH7A%2Flink-neuroscientists-find-that-status-within-groups-can%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Neuroscientists%20Find%20That%20Status%20within%20Groups%20Can%20Affect%20IQ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRtXb2unSYxSbFH7A%2Flink-neuroscientists-find-that-status-within-groups-can", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGRtXb2unSYxSbFH7A%2Flink-neuroscientists-find-that-status-within-groups-can", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 251, "htmlBody": "<p><a href=\"http://media.caltech.edu/press_releases/13492\">http://media.caltech.edu/press_releases/13492</a></p>\n<blockquote>\n<p>To investigate the impact of social context on IQ, the researchers divided a pool of 70 subjects into groups of five and gave each individual a computer-based IQ test. After each question, an on-screen ranking showed the subjects how well they were performing relative to others in their group and how well one other person in the group was faring. All of the subjects had previously taken a paper-and-pencil IQ test, and were matched with the rest of the group so that they would each be expected to perform similarly on an IQ test.</p>\n<p>At the outset, all of the subjects did worse than expected on this \"ranked group IQ task.\" But some of the subjects, dubbed High Performers, were able to improve over the course of the test while others, called Low Performers, continued to perform below their expected level. By the end of the computer-based test, the scores of the Low Performers dropped an average of 17.4 points compared to their performance on the paper-and-pencil test.</p>\n<p>\"What we found was that sensitivity to the social feedback of the rankings profoundly altered some people's ability to express their cognitive capacity,\" Quartz says. \"So we get this really quite dramatic downward spiraling of one group purely because of their sensitivity to this social feedback.\" Since so much of our learning&mdash;from the classroom to the work team&mdash;is socially situated, this study suggests that individual differences in social sensitivity may play an important role in shaping human intelligence over time.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2, "4cKQgA4S7xfNeeWXg": 2, "ksdiAMKfgSyEeKMo6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GRtXb2unSYxSbFH7A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 8.366366218381842e-07, "legacy": true, "legacyId": "12367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T21:35:16.646Z", "modifiedAt": null, "url": null, "title": "Tell LessWrong about your charitable donations", "slug": "tell-lesswrong-about-your-charitable-donations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:22.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ModNxgShqd69zTajZ/tell-lesswrong-about-your-charitable-donations", "pageUrlRelative": "/posts/ModNxgShqd69zTajZ/tell-lesswrong-about-your-charitable-donations", "linkUrl": "https://www.lesswrong.com/posts/ModNxgShqd69zTajZ/tell-lesswrong-about-your-charitable-donations", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tell%20LessWrong%20about%20your%20charitable%20donations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATell%20LessWrong%20about%20your%20charitable%20donations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FModNxgShqd69zTajZ%2Ftell-lesswrong-about-your-charitable-donations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tell%20LessWrong%20about%20your%20charitable%20donations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FModNxgShqd69zTajZ%2Ftell-lesswrong-about-your-charitable-donations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FModNxgShqd69zTajZ%2Ftell-lesswrong-about-your-charitable-donations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 690, "htmlBody": "<p>When I was a graduate student at the University of Notre Dame, I received a monthly living stipend of roughly $1,600. I decided to commit to giving ~10% of it to charity, and I had read in Peter Singer's book <em>The Life You Can Save </em>that one of the most efficient charities out there was Population Services International (PSI). Singer reported that GiveWell, a leading charity rating organization, had estimated that PSI's efforts saved lives at a cost of $650-$1000 each (pp. 88-89). So, I set up a recurring monthly donation of $160 to PSI, and kept it up for 15 months, for a total donation of $2,400.<br /><br />I've been meaning to post the above information publicly for awhile, but was pushed over the edge by reading one of Eliezer's posts from a couple years back, <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">Why Our Kind Can't Cooperate</a>:</p>\n<blockquote>\n<p>Let me tell you about a different annual fundraising appeal.&nbsp; One that I ran, in fact; during the early years of a nonprofit organization that <a href=\"http://wiki.lesswrong.com/wiki/Topic_that_must_not_be_named\">may not be named</a>.&nbsp; One difference was that the appeal was conducted over the Internet.&nbsp; And another difference was that the audience was largely drawn from the atheist/libertarian/technophile/sf-fan/early-adopter/programmer/etc crowd.&nbsp; (To point in the rough direction of an empirical cluster in personspace.&nbsp; If you understood the phrase \"empirical cluster in personspace\" then you know who I'm talking about.)</p>\n<p>I crafted the fundraising appeal with care.&nbsp; By my nature I'm too proud to ask other people for help; but I've gotten over around 60% of that reluctance over the years.&nbsp; The nonprofit needed money and was growing too slowly, so I put some force and poetry into that year's annual appeal.&nbsp; I sent it out to several mailing lists that covered most of our potential support base.</p>\n<p>And almost immediately, people started posting to the mailing lists about why they weren't going to donate.&nbsp; Some of them raised basic questions about the nonprofit's philosophy and mission.&nbsp; Others talked about their brilliant ideas for all the <em>other</em> sources that the nonprofit could get funding from, instead of them.&nbsp; (They didn't volunteer to contact any of those sources <em>themselves</em>, they just had ideas for how <em>we</em> could do it.)</p>\n<p>Now you might say, \"Well, maybe your mission and philosophy <em>did</em> have basic problems - you wouldn't want to <em>censor </em>that discussion, would you?\"</p>\n<p>Hold on to that thought.</p>\n<p>Because people <em>were</em> donating.&nbsp; We started getting donations right away, via Paypal.&nbsp; We even got congratulatory notes saying how the appeal had finally gotten them to start moving.&nbsp; A donation of $111.11 was accompanied by a message saying, \"I decided to give **** a little bit more.&nbsp; One more hundred, one more ten, one more single, one more dime, and one more penny.&nbsp; All may not be for one, but this one is trying to be for all.\"</p>\n<p>But none of those donors posted their agreement to the mailing list.&nbsp; Not one.</p>\n<p>So far as any of those donors knew, they were alone.&nbsp; And when they tuned in the next day, they discovered not thanks, but arguments for why they <em>shouldn't</em> have donated.&nbsp; The criticisms, the justifications for not donating - <em>only those</em> were displayed proudly in the open.</p>\n<p>As though the treasurer had finished his annual appeal, and everyone <em>not</em> making a pledge had proudly stood up to call out justifications for refusing; while those making pledges whispered them quietly, so that no one could hear.</p>\n</blockquote>\n<p>Since Eliezer's post is about rationalists, he stresses the issue of what arguments people voice. However, we know that just telling other people that you've given to charity makes them more likely to give. This is a point that <a href=\"http://www.guardian.co.uk/money/2008/jun/22/charitablegiving\">Singer himself has emphasized</a>.</p>\n<p>I propose a thread for people to publicize their charitable donations. In light of the above, I'm especially interested to hear from people who've donated to the Singularity Institute for Artificial Intelligence. Once I acquire a regular source of income again in March, I intend to continue to primarily direct my charitable giving towards PSI, but maybe someone in this thread will persuade me to start giving to the Singularity Institute.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ModNxgShqd69zTajZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "12368", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T23:04:52.936Z", "modifiedAt": null, "url": null, "title": "Meetup : Need more people for Vancouver meetup", "slug": "meetup-need-more-people-for-vancouver-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.992Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oRx7K53wxXgfbvKKR/meetup-need-more-people-for-vancouver-meetup", "pageUrlRelative": "/posts/oRx7K53wxXgfbvKKR/meetup-need-more-people-for-vancouver-meetup", "linkUrl": "https://www.lesswrong.com/posts/oRx7K53wxXgfbvKKR/meetup-need-more-people-for-vancouver-meetup", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Need%20more%20people%20for%20Vancouver%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Need%20more%20people%20for%20Vancouver%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRx7K53wxXgfbvKKR%2Fmeetup-need-more-people-for-vancouver-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Need%20more%20people%20for%20Vancouver%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRx7K53wxXgfbvKKR%2Fmeetup-need-more-people-for-vancouver-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRx7K53wxXgfbvKKR%2Fmeetup-need-more-people-for-vancouver-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/6e\">Need more people for Vancouver meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">28 January 2012 03:03:49PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">vancouver (Location TBD)</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Vancouver LessWrong meetup has been puttering along in a barely alive state. We need more people to make it more stable and fun.</p>\n<p>We don't have a good venue, we've been surfing between empty rooms at langara, coffee shops, and people's houses. We <em>have</em> been meeting regularly every sunday, tho we may change the day soon. We hope that with more people with more resources, it will be easier to find a good venue.</p>\n<p>We've been mostly sharing math skills and talking about the usual LW stuff. If you haven't come out before, it's quite surprising how much more productive a realtime face-to-face conversation is for exploring your favorite rationality topics.</p>\n<p>If you are from Vancouver, or nearby and interested, add yourself to <a rel=\"nofollow\" href=\"http://groups.google.com/group/vancouver-rationalists\">the list</a>, and try to come out. Seriously!!!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/6e\">Need more people for Vancouver meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oRx7K53wxXgfbvKKR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.367103104378365e-07, "legacy": true, "legacyId": "12369", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Need_more_people_for_Vancouver_meetup\">Discussion article for the meetup : <a href=\"/meetups/6e\">Need more people for Vancouver meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">28 January 2012 03:03:49PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">vancouver (Location TBD)</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Vancouver LessWrong meetup has been puttering along in a barely alive state. We need more people to make it more stable and fun.</p>\n<p>We don't have a good venue, we've been surfing between empty rooms at langara, coffee shops, and people's houses. We <em>have</em> been meeting regularly every sunday, tho we may change the day soon. We hope that with more people with more resources, it will be easier to find a good venue.</p>\n<p>We've been mostly sharing math skills and talking about the usual LW stuff. If you haven't come out before, it's quite surprising how much more productive a realtime face-to-face conversation is for exploring your favorite rationality topics.</p>\n<p>If you are from Vancouver, or nearby and interested, add yourself to <a rel=\"nofollow\" href=\"http://groups.google.com/group/vancouver-rationalists\">the list</a>, and try to come out. Seriously!!!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Need_more_people_for_Vancouver_meetup1\">Discussion article for the meetup : <a href=\"/meetups/6e\">Need more people for Vancouver meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Need more people for Vancouver meetup", "anchor": "Discussion_article_for_the_meetup___Need_more_people_for_Vancouver_meetup", "level": 1}, {"title": "Discussion article for the meetup : Need more people for Vancouver meetup", "anchor": "Discussion_article_for_the_meetup___Need_more_people_for_Vancouver_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T23:21:33.203Z", "modifiedAt": null, "url": null, "title": "Meetup : Houston Meetup - 1/29", "slug": "meetup-houston-meetup-1-29", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:34.950Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nwwvv7Y37pA3EwzvK/meetup-houston-meetup-1-29", "pageUrlRelative": "/posts/Nwwvv7Y37pA3EwzvK/meetup-houston-meetup-1-29", "linkUrl": "https://www.lesswrong.com/posts/Nwwvv7Y37pA3EwzvK/meetup-houston-meetup-1-29", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Houston%20Meetup%20-%201%2F29&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Houston%20Meetup%20-%201%2F29%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwwvv7Y37pA3EwzvK%2Fmeetup-houston-meetup-1-29%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Houston%20Meetup%20-%201%2F29%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwwvv7Y37pA3EwzvK%2Fmeetup-houston-meetup-1-29", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwwvv7Y37pA3EwzvK%2Fmeetup-houston-meetup-1-29", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6f'>Houston Meetup - 1/29</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 January 2012 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2010 Commerce Street, Houston, Tx.77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Houston LW meetup will have our next meeting this coming Sunday at 2:00 PM. We will be discussing the first two chapters of E.T. Jaynes \"The Logic of Science\", available here:</p>\n\n<p><a href=\"http://bayes.wustl.edu/etj/prob/book.pdf\" rel=\"nofollow\">http://bayes.wustl.edu/etj/prob/book.pdf</a></p>\n\n<p>The first two chapters are relatively easy for those with some experience in formal logic, but we will make sure everyone understands the basics. We will also be going over the following sequences in the discussion, if time permits:</p>\n\n<p>How An Algorithm Feels From Inside (<a href=\"http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/\" rel=\"nofollow\">http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/</a>)</p>\n\n<p>Feel the Meaning (<a href=\"http://lesswrong.com/lw/nq/feel_the_meaning/\" rel=\"nofollow\">http://lesswrong.com/lw/nq/feel_the_meaning/</a>)</p>\n\n<p>Replace the Symbol with the Substance  (<a href=\"http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/\" rel=\"nofollow\">http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/</a>)</p>\n\n<p>As always, pizza is an option if we feel like it. Bacon, eggs and other snacks are often produced in the hackerspace kitchen during this time, but those are communal products, so bring some cash for the tip jar if you partake.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6f'>Houston Meetup - 1/29</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nwwvv7Y37pA3EwzvK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.367166988392568e-07, "legacy": true, "legacyId": "12370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup___1_29\">Discussion article for the meetup : <a href=\"/meetups/6f\">Houston Meetup - 1/29</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 January 2012 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2010 Commerce Street, Houston, Tx.77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Houston LW meetup will have our next meeting this coming Sunday at 2:00 PM. We will be discussing the first two chapters of E.T. Jaynes \"The Logic of Science\", available here:</p>\n\n<p><a href=\"http://bayes.wustl.edu/etj/prob/book.pdf\" rel=\"nofollow\">http://bayes.wustl.edu/etj/prob/book.pdf</a></p>\n\n<p>The first two chapters are relatively easy for those with some experience in formal logic, but we will make sure everyone understands the basics. We will also be going over the following sequences in the discussion, if time permits:</p>\n\n<p>How An Algorithm Feels From Inside (<a href=\"http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/\" rel=\"nofollow\">http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/</a>)</p>\n\n<p>Feel the Meaning (<a href=\"http://lesswrong.com/lw/nq/feel_the_meaning/\" rel=\"nofollow\">http://lesswrong.com/lw/nq/feel_the_meaning/</a>)</p>\n\n<p>Replace the Symbol with the Substance  (<a href=\"http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/\" rel=\"nofollow\">http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/</a>)</p>\n\n<p>As always, pizza is an option if we feel like it. Bacon, eggs and other snacks are often produced in the hackerspace kitchen during this time, but those are communal products, so bring some cash for the tip jar if you partake.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup___1_291\">Discussion article for the meetup : <a href=\"/meetups/6f\">Houston Meetup - 1/29</a></h2>", "sections": [{"title": "Discussion article for the meetup : Houston Meetup - 1/29", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup___1_29", "level": 1}, {"title": "Discussion article for the meetup : Houston Meetup - 1/29", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup___1_291", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yA4gF5KrboK2m2Xu7", "dMCFk2n2ur8n62hqB", "GKfPL6LQFgB49FEnv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-23T23:50:42.497Z", "modifiedAt": null, "url": null, "title": "How I Ended Up Non-Ambitious", "slug": "how-i-ended-up-non-ambitious", "viewCount": null, "lastCommentedAt": "2019-07-08T06:03:44.455Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BFamedwSgRdGGKXQQ/how-i-ended-up-non-ambitious", "pageUrlRelative": "/posts/BFamedwSgRdGGKXQQ/how-i-ended-up-non-ambitious", "linkUrl": "https://www.lesswrong.com/posts/BFamedwSgRdGGKXQQ/how-i-ended-up-non-ambitious", "postedAtFormatted": "Monday, January 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20I%20Ended%20Up%20Non-Ambitious&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20I%20Ended%20Up%20Non-Ambitious%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBFamedwSgRdGGKXQQ%2Fhow-i-ended-up-non-ambitious%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20I%20Ended%20Up%20Non-Ambitious%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBFamedwSgRdGGKXQQ%2Fhow-i-ended-up-non-ambitious", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBFamedwSgRdGGKXQQ%2Fhow-i-ended-up-non-ambitious", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3001, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>2455</o:Words> <o:Characters>13997</o:Characters> <o:Company>Home</o:Company> <o:Lines>116</o:Lines> <o:Paragraphs>27</o:Paragraphs> <o:CharactersWithSpaces>17189</o:CharactersWithSpaces> <o:Version>10.265</o:Version> </o:DocumentProperties> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:UseMarginsForDrawingGridOrigin /> </w:WordDocument> </xml><![endif]--> <!--StartFragment--></p>\n<p class=\"MsoNormal\">I have a confession to make. My life hasn&rsquo;t changed all that much since I started reading Less Wrong. Hindsight bias makes it hard to tell, I guess, but I feel like pretty much the same person, or at least the person I would have evolved towards anyway, whether or not I spent those years reading about the Art of rationality.</p>\n<p class=\"MsoNormal\">But I can&rsquo;t claim to be upset about it either. I can&rsquo;t say that rationality has undershot my expectations. I didn&rsquo;t come to Less Wrong expecting, or even wanting, to become the next Bill Gates; I came because I enjoyed reading it, just like I&rsquo;ve enjoyed reading hundreds of books and websites.<span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">In fact, I can&rsquo;t claim that I would <em>want </em></span><span style=\"mso-ansi-language: EN-US;\">my life to be any different. I have goals and I&rsquo;m meeting them: my grades are good, my social skills are slowly but steadily improving, I get along well with my family, my friends, and my boyfriend. I&rsquo;m in good shape financially despite making $12 an hour as a lifeguard, and in a year and a half I&rsquo;ll be making over $50,000 a year as a registered nurse. I write stories, I sing in church, I teach kids how to swim. Compared to many people my age, I'm pretty successful. In general, I&rsquo;m pretty happy.</span></p>\n<p class=\"MsoNormal\"><a href=\"/lw/9p/rationality_its_not_that_great/\">Yvain</a> suggested akrasia as a major limiting factor for why rationalists fail to have extraordinarily successful lives. Maybe that&rsquo;s true for some people; maybe they are some readers and posters on LW who have big, exciting, challenging goals that they consistently fail to reach because they lack motivation and procrastinate. But that isn&rsquo;t true for me. Though I can&rsquo;t claim to be totally free of akrasia, it hasn&rsquo;t gotten much in the way of my goals.&nbsp;</p>\n<p class=\"MsoNormal\">However, there are some assumptions that go too deep to be accessed by introspection, or even by LW meetup discussions. Sometimes you don't even realize they&rsquo;re assumptions until you meet someone who assumes the opposite, and try to figure out why they make you so defensive. At the community meetup I described in <a href=\"/lw/9g1/the_problem_with_too_many_rational_memes/\">my last post</a>, a number of people asked me why I wasn&rsquo;t studying physics, since I was obviously passionate about it. Trust me, I had plenty of good justifications for them&ndash;it&rsquo;s a question I&rsquo;ve been asked many times&ndash;but the question itself shouldn&rsquo;t have made me feel attacked, and it did.</p>\n<p class=\"MsoNormal\">Aside from people in my life, there are some posts on Less Wrong that cause the same reaction of defensiveness. Eliezer&rsquo;s <a href=\"/lw/9c/mandatory_secret_identities/\">Mandatory Secret Identities</a> is a good example; my automatic reaction was &ldquo;well, why do you assume everyone here wants to have a super cool, interesting life? In fact, why do you assume everyone wants to be a rationality instructor? I don&rsquo;t. I want to be a nurse.&rdquo;</p>\n<p class=\"MsoNormal\">After a bit of thought, I&rsquo;ve concluded that there&rsquo;s a simple reason why I&rsquo;ve achieved all my life goals so far (and why learning about rationality failed to affect my achievements): they&rsquo;re not hard goals. I&rsquo;m not ambitious. As far as I can tell, not being ambitious is such a deep part of my identity that I never even noticed it, though I&rsquo;ve used the underlying assumptions as arguments for why my goals and life decisions were the right ones.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">But if there&rsquo;s one thing Less Wrong has taught me, it&rsquo;s that assumptions are to be questioned. There are plenty of good reasons to choose reasonable goals instead of impossible ones, but doing things on reflex is rarely better than thinking through them, especially for long-term goal making, where I do have time to think it through, <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">Type 2 </a>style.<span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><span style=\"mso-spacerun: yes;\"><br /></span></span></p>\n<h2><span style=\"mso-ansi-language: EN-US;\">What do I mean by &lsquo;ambition&rsquo;?</span></h2>\n<p class=\"MsoNormal\">Here is the definition from my desktop dictionary:</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">(1)<span style=\"font: 7.0pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style=\"mso-ansi-language: EN-US;\">A strong desire to do or to achieve something, typically requiring determination and hard work<em>: her ambition was to become a model | he achieved his ambition of making a fortune.</em></span></p>\n<p class=\"MsoNormal\" style=\"margin-left: 0in; text-indent: 0in; mso-list: l0 level2 lfo2; tab-stops: 0in;\"><!--[if !supportLists]--><span style=\"mso-ansi-language: EN-US;\">(2)<span style=\"font: 7.0pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"mso-ansi-language: EN-US;\">Desire and determination to achieve success<em>: life offered few opportunities for young people with ambition.</em></span></p>\n<p class=\"MsoNormal\">The first definition sounds like a good description of me. Since around tenth grade, I&rsquo;ve had a strong desire to study nursing, and it&rsquo;s required a moderate amount of determination and hard work, especially the hands-on aspects, which are harder for me than academics has ever been. I want to be the kind of person described in (1).</p>\n<p>What about the second half? More people than I can count have asked me why I&rsquo;m not studying medicine. Or physics. Or just about anything aside from nursing, which is apparently kind of low-status. I inevitably get defensive when these conversations occur, and I end up trying to justify why nursing is the morally correct thing for me to do. For some reason, in some deep-down part of me that I don&rsquo;t normally have conscious access to, I don&rsquo;t want to be the sort of person described in (2).</p>\n<p>Introspection isn&rsquo;t accurate enough for me to automatically find my true rejection of ambitious goals, but I will take the rest of the post to speculate on <em>my own personal </em>reasons. They may or may not be reasons that generalize to anyone else.&nbsp;</p>\n<p>&nbsp;</p>\n<h2>1. Idealism versus practicality</h2>\n<p>My mother tells me I would be a good academic, and enjoy it too. She&rsquo;s usually right about that kind of thing, but I decided around eighth grade that academia wasn&rsquo;t for me.</p>\n<p class=\"MsoNormal\">Why? Well, my mother and father both studied science at the undergraduate level (biology and physical chemistry, respectively) and then both went on to complete PhDs. From the sound of it, those student years were among the happiest in their lives. My father went on to do a postdoc at Cambridge, and then to get a crappy part-time teaching position at a small university in Washington State. He hated it. Eventually he quit and we moved up to Ottawa, Canada, where he worked at Nortel, was laid off during the company&rsquo;s decline, and eventually found another job at a small company that takes apart computer chips and analyzes them. Meanwhile, my mother spent most of those years as a housewife, and has only recently begun working again, part-time and for a token salary.</p>\n<p class=\"MsoNormal\">I&rsquo;ve asked my father what he thinks of the decisions he made, and he told me that his biggest problem was that he didn&rsquo;t know what he wanted to do with his life. He told me that he still doesn&rsquo;t. His job is boring and stressful, but he can&rsquo;t quit because he didn&rsquo;t start saving for retirement until he was 40. As a grad student, he worked with John Polanyi, a well-known academic; much later he told me he &ldquo;always sort of thought I would end up being well-known and cool like that, but all of a sudden I&rsquo;m almost 50 and I realize that&rsquo;s not going to happen.&rdquo;</p>\n<p class=\"MsoNormal\">I remember the year when he developed a sudden passion for career self-help books, of the &lsquo;What Color Is Your Parachute&rsquo; and &lsquo;The Seven Habits of Highly Effective People&rsquo; variety. I must have been about thirteen years old. He encouraged me to read them, and warned me that &ldquo;it&rsquo;s better to think about what you want to do, not what you want to be.&rdquo;</p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson my 13-year-old self I took from all this: don&rsquo;t have hopes and dreams, especially not ambitious ones. You won&rsquo;t achieve them, and you&rsquo;ll end up in a mid-life crisis with no retirement savings, full of regrets. Far better to have a practical, achievable life plan, and then go out and damn well <em>achieve </em></span><span style=\"mso-ansi-language: EN-US;\">it. I read the self-help books, figured that nurses did around the same stuff all day as doctors and didn&rsquo;t have to spend eight years in school paying tuition, and never looked back.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson I <em>didn&rsquo;t </em></span><span style=\"mso-ansi-language: EN-US;\">learn from all this: my parents weren&rsquo;t actually ambitious either. They enjoyed their studies in university, but primarily they had fun: going to the philosophy faculty parties, getting drunk with chemistry students, volunteering on coffee plantations in Nicaragua... Those are the stories they tell me from their studies, not stories of the research they did and the papers they published. I can&rsquo;t be sure what their true feelings were at the time, but I don&rsquo;t think they <em>cared </em></span><span style=\"mso-ansi-language: EN-US;\">especially. They were smart young people who wanted to have a good time and didn&rsquo;t especially care if they had no money. And I don&rsquo;t think they have as many regrets as I assumed when I was thirteen. They didn&rsquo;t exactly make life goals and then fail to achieve them. They just hadn&rsquo;t made their long-term goals ahead of time.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson I <em>should </em></span><span style=\"mso-ansi-language: EN-US;\">have learned: if you head into adulthood without big goals, don&rsquo;t be surprised if you don&rsquo;t achieve them.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>\n<p class=\"MsoNormal\">The second life lesson about ambition happened a few years later, when I was around fourteen. I had been training as a competitive swimmer for a number of years. My parents didn&rsquo;t sign me up because they wanted me to go to the Olympics someday; they wanted me to stay fit and have opportunities to socialize. It was a good decision; swim team made me happy, to the point that I often forget how unhappy I was up until then.</p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">But after a while I started to get <em>good</em></span><span style=\"mso-ansi-language: EN-US;\"> at swimming, and coaches, even kids&rsquo; coaches, implicitly want their athletes to win, and keep winning, and maybe someday they&rsquo;ll be known as the one who coached an Olympic athlete. Training made me happy, but competition emphatically did <em>not</em></span><span style=\"mso-ansi-language: EN-US;\">; anxiety, stress, and bursting into tears before a race soon became part of my day-to-day life. My coaches told me that if I worked hard and believed in myself, I could do anything. But eventually I hit a point when I was racing kids who were simply <em>more talented </em></span><span style=\"mso-ansi-language: EN-US;\">than me: taller, slimmer, bigger hands and feet, a genetic predisposition to fast-twitch muscles, whatever. And then I hit my body&rsquo;s limits, and I stopped getting faster at all, no matter how hard I trained. My coaches accused me of not trying hard enough. Understandably, this made me feel worse, since I certainly <em>felt </em></span><span style=\"mso-ansi-language: EN-US;\">like I was trying as hard as I could.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lessons my 14-year-old self learned from this: don&rsquo;t have high expectations for yourself when competing against other people. You&rsquo;ll just end up feeling worthless and depressed. In fact, don&rsquo;t compete against other people <em>at all</em></span><span style=\"mso-ansi-language: EN-US;\">. Do things that are solely based on how good you are, as opposed to how good you are <em>relative to other people who might be more talented. </em></span><span style=\"mso-ansi-language: EN-US;\">Even better, do things that aren&rsquo;t that hard in an absolute sense, so that you don&rsquo;t risk failing.</span></p>\n<p class=\"MsoNormal\">This is kind of a fallacy, of course. Success in anything is measured relative to other people, if only relative to the average. Even grades, because classes and tests and grades are set up for students of average intelligence, so students of relatively higher intelligence will find them easier, and students of lower-than-average intelligence will feel like they&rsquo;re fighting a losing battle, as I did in swimming competitions. Possessing above average intelligence let me grow up seeing school as non-threatening, but I know that isn&rsquo;t true for everyone. I&rsquo;ve known people whose above-average athletic skills led them to be far more confident in sports than at school.</p>\n<p class=\"MsoNormal\">Still, fallacy or not, I later applied this idea to a lot of my decision. I was interested in physics all along, but my father&rsquo;s tales of academia and the competition and pressure involved turned me off it. I also considered studying music theory and composition, but decided not to because, aside from being impractical for finding a job afterwards, I&rsquo;d heard it was an incredibly competitive field. To a degree, this is why I chose not to make a career as a writer. (A degree in English didn&rsquo;t seem particularly interesting to me, so I doubt I would have studied it, but even in high school I never really thought about earning money with my writing.) Success or failure was too far beyond my control for comfort.</p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson I didn&rsquo;t learn from this: find an area where you <em>do </em></span><span style=\"mso-ansi-language: EN-US;\">have natural talent on your side, and use it for all it&rsquo;s worth. In fact, I&rsquo;ve done the opposite of this: one reason I chose nursing was because I felt that I was bad at a whole range of skills; empathy, social skills, fine motor skills and coordination, reacting in emergencies; and I wanted to force myself to improve. As a result, I&rsquo;m far from the strongest student in my classes, and labs, simulations, and hospital placements bring me to a level of anxiety far above anything I ever experienced during academic tests or exams.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson I should have learned from this: you never know what you are and aren&rsquo;t capable of until you try it. I tried competitive swimming, and found out I didn&rsquo;t have the raw talent to go to the Olympics. Who knows if this would have been true of physics? My father tells me that in his fourth year of undergraduate studies, he took several physics courses with a level of advanced math that he found almost impossible. He had reached his brain&rsquo;s natural limit in math, which he might or might not have been able to exceed with hard work and hours of study; still, it was <em>much </em></span><span style=\"mso-ansi-language: EN-US;\">more advanced than the first-year calculus I took as an elective. I have no reason to think that I&rsquo;m <em>worse </em></span><span style=\"mso-ansi-language: EN-US;\">at math than my father, and I suspect my obsessive work ethic would help me exceed any limits I did bump up against. And why not try?</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">There&rsquo;s a third aspect of my aversion to ambitious goals, and I can&rsquo;t say<em> </em></span><span style=\"mso-ansi-language: EN-US;\">where it comes from. It might be my parents&rsquo; attitude of moderation in everything: they consistently disapproved of my involvement in any &lsquo;obsessive&rsquo; activities, swim team included. It might be the way my mother always got mad at me for talking about my achievements, even my grades, in front of friends; it&rsquo;ll make other people feel bad, she said. (For a long time I was incredibly self-conscious about high grades, and wouldn&rsquo;t tell my friends if they were above 90%.) It might be the meme that &lsquo;money doesn&rsquo;t buy happiness&rsquo; or the idea that it&rsquo;s greedy to be ambitious, or that power corrupts and wise people choose not to seek it.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">I can&rsquo;t trace the roots of this idea completely, but for whatever reason, I spent a long time thinking that being ambitious was in some way <em>immoral</em></span><span style=\"mso-ansi-language: EN-US;\">. That really good people lived simple, selfless lives and never tried to seek anything more. That doing something solely because you wanted more money or more respect, like going to med school instead of nursing school, was selfish and just <em>bad</em></span><span style=\"mso-ansi-language: EN-US;\">. It might come from the books I read as a kid, or maybe it&rsquo;s just a rationalization to cover up my other reasons with a nobler one.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">But if this is my true reason, then it&rsquo;s a way to feel superior to people who&rsquo;ve accomplished cooler things than me, of whom part of me is actually jealous, and that&rsquo;s <em>not </em></span><span style=\"mso-ansi-language: EN-US;\">the person I want to be.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">I don&rsquo;t normally think of myself as a lazy person. Other people are constantly telling me that I&rsquo;m diligent and have an excellent work ethic. But there&rsquo;s a way in which all this hardworking dedication to my current occupations has <em>prevented </em></span><span style=\"mso-ansi-language: EN-US;\">me from spending much time thinking or acting about what I&rsquo;m going to do next. Working a bunch of 12-hour shifts makes me feel productive, brings the direct benefit of a fat paycheck, and leaves me pretty exhausted at the end of the day, too tired to do the (in some ways harder) work of searching for cool job opportunities, looking at online classes to take, and in general breaking the routine. I <em>hate </em></span><span style=\"mso-ansi-language: EN-US;\">breaking my routine. It makes me anxious, and I have to spend more energy motivating myself, and in general it&rsquo;s <em>hard</em></span><span style=\"mso-ansi-language: EN-US;\">. I tend to only depart from that routine when forced.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span style=\"mso-ansi-language: EN-US;\">Conclusion</span></h2>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">I think I was right about <em>some </em></span><span style=\"mso-ansi-language: EN-US;\">of the conclusions I drew from these various experiences. Practicality is important: ask the English majors working at Starbucks. Thinking about what you want to do all day, as opposed to the title and respect associated with what you want to <em>be</em></span><span style=\"mso-ansi-language: EN-US;\">, is good life advice and will likely result in a more satisfying career. Trying hard to project an image of success, i.e. &ldquo;keeping up with the Jones&rsquo;&rdquo;, isn&rsquo;t a good path to happiness. And relative talent is a factor to take into consideration; if my dream career were to be an Olympic swimmer, unfortunately I wouldn&rsquo;t be likely to succeed.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">But one of the problems with thinking things through too deeply when you&rsquo;re young, and think you&rsquo;re wiser than everyone else, is a tendency to over-generalize. Doing cool, interesting, world-changing things with your life...even if the actual job position are competitive and hard to obtain...well, on reflection, it doesn&rsquo;t seem be a <em>bad </em></span><span style=\"mso-ansi-language: EN-US;\">idea.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson my current self has learned from this: investigate more. Spend less time on work and more time on actually planning future goals. Seek out interesting things to do, and interesting people to work with. Go for opportunities even if they're inconvenient and I have to break my routine a bit. Set <em>concrete </em></span><span style=\"mso-ansi-language: EN-US;\">goals, and don&rsquo;t wiggle out of achieving them because they&rsquo;re &lsquo;not actually that important.&rsquo; They&rsquo;re probably more important than working at a community centre, and I seem to be able to dedicate 1000 hours a year to that... Try not to worry about sunk costs (although it&rsquo;s worth finishing nursing school, since an RN certificate is incredibly versatile in Canada and will guarantee me a job if any other prospects fail.) Force myself to step out of my comfort zone once in a while and do something kind of crazy, but awesome. And if I can do that, succeed to the point that I can break my reflex-of-being-average...<em>then </em>I'll know for sure whether rationality, of the Less Wrong variety, will help me to 'win'.&nbsp;</span></p>\n<p class=\"MsoNormal\">The lesson my future self will learn from this: who knows?&nbsp;</p>\n<!--EndFragment-->\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hrezrpGqXXdSe76ks": 5, "iP2X4jQNHMWHRNPne": 2, "3ee9k6NJfcGzL6kMS": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BFamedwSgRdGGKXQQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 171, "baseScore": 238, "extendedScore": null, "score": 0.000473, "legacy": true, "legacyId": "12349", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 238, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>2455</o:Words> <o:Characters>13997</o:Characters> <o:Company>Home</o:Company> <o:Lines>116</o:Lines> <o:Paragraphs>27</o:Paragraphs> <o:CharactersWithSpaces>17189</o:CharactersWithSpaces> <o:Version>10.265</o:Version> </o:DocumentProperties> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:UseMarginsForDrawingGridOrigin /> </w:WordDocument> </xml><![endif]--> <!--StartFragment--></p>\n<p class=\"MsoNormal\">I have a confession to make. My life hasn\u2019t changed all that much since I started reading Less Wrong. Hindsight bias makes it hard to tell, I guess, but I feel like pretty much the same person, or at least the person I would have evolved towards anyway, whether or not I spent those years reading about the Art of rationality.</p>\n<p class=\"MsoNormal\">But I can\u2019t claim to be upset about it either. I can\u2019t say that rationality has undershot my expectations. I didn\u2019t come to Less Wrong expecting, or even wanting, to become the next Bill Gates; I came because I enjoyed reading it, just like I\u2019ve enjoyed reading hundreds of books and websites.<span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">In fact, I can\u2019t claim that I would <em>want </em></span><span style=\"mso-ansi-language: EN-US;\">my life to be any different. I have goals and I\u2019m meeting them: my grades are good, my social skills are slowly but steadily improving, I get along well with my family, my friends, and my boyfriend. I\u2019m in good shape financially despite making $12 an hour as a lifeguard, and in a year and a half I\u2019ll be making over $50,000 a year as a registered nurse. I write stories, I sing in church, I teach kids how to swim. Compared to many people my age, I'm pretty successful. In general, I\u2019m pretty happy.</span></p>\n<p class=\"MsoNormal\"><a href=\"/lw/9p/rationality_its_not_that_great/\">Yvain</a> suggested akrasia as a major limiting factor for why rationalists fail to have extraordinarily successful lives. Maybe that\u2019s true for some people; maybe they are some readers and posters on LW who have big, exciting, challenging goals that they consistently fail to reach because they lack motivation and procrastinate. But that isn\u2019t true for me. Though I can\u2019t claim to be totally free of akrasia, it hasn\u2019t gotten much in the way of my goals.&nbsp;</p>\n<p class=\"MsoNormal\">However, there are some assumptions that go too deep to be accessed by introspection, or even by LW meetup discussions. Sometimes you don't even realize they\u2019re assumptions until you meet someone who assumes the opposite, and try to figure out why they make you so defensive. At the community meetup I described in <a href=\"/lw/9g1/the_problem_with_too_many_rational_memes/\">my last post</a>, a number of people asked me why I wasn\u2019t studying physics, since I was obviously passionate about it. Trust me, I had plenty of good justifications for them\u2013it\u2019s a question I\u2019ve been asked many times\u2013but the question itself shouldn\u2019t have made me feel attacked, and it did.</p>\n<p class=\"MsoNormal\">Aside from people in my life, there are some posts on Less Wrong that cause the same reaction of defensiveness. Eliezer\u2019s <a href=\"/lw/9c/mandatory_secret_identities/\">Mandatory Secret Identities</a> is a good example; my automatic reaction was \u201cwell, why do you assume everyone here wants to have a super cool, interesting life? In fact, why do you assume everyone wants to be a rationality instructor? I don\u2019t. I want to be a nurse.\u201d</p>\n<p class=\"MsoNormal\">After a bit of thought, I\u2019ve concluded that there\u2019s a simple reason why I\u2019ve achieved all my life goals so far (and why learning about rationality failed to affect my achievements): they\u2019re not hard goals. I\u2019m not ambitious. As far as I can tell, not being ambitious is such a deep part of my identity that I never even noticed it, though I\u2019ve used the underlying assumptions as arguments for why my goals and life decisions were the right ones.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">But if there\u2019s one thing Less Wrong has taught me, it\u2019s that assumptions are to be questioned. There are plenty of good reasons to choose reasonable goals instead of impossible ones, but doing things on reflex is rarely better than thinking through them, especially for long-term goal making, where I do have time to think it through, <a href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">Type 2 </a>style.<span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><span style=\"mso-spacerun: yes;\"><br></span></span></p>\n<h2 id=\"What_do_I_mean_by__ambition__\"><span style=\"mso-ansi-language: EN-US;\">What do I mean by \u2018ambition\u2019?</span></h2>\n<p class=\"MsoNormal\">Here is the definition from my desktop dictionary:</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">(1)<span style=\"font: 7.0pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style=\"mso-ansi-language: EN-US;\">A strong desire to do or to achieve something, typically requiring determination and hard work<em>: her ambition was to become a model | he achieved his ambition of making a fortune.</em></span></p>\n<p class=\"MsoNormal\" style=\"margin-left: 0in; text-indent: 0in; mso-list: l0 level2 lfo2; tab-stops: 0in;\"><!--[if !supportLists]--><span style=\"mso-ansi-language: EN-US;\">(2)<span style=\"font: 7.0pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><!--[endif]--><span style=\"mso-ansi-language: EN-US;\">Desire and determination to achieve success<em>: life offered few opportunities for young people with ambition.</em></span></p>\n<p class=\"MsoNormal\">The first definition sounds like a good description of me. Since around tenth grade, I\u2019ve had a strong desire to study nursing, and it\u2019s required a moderate amount of determination and hard work, especially the hands-on aspects, which are harder for me than academics has ever been. I want to be the kind of person described in (1).</p>\n<p>What about the second half? More people than I can count have asked me why I\u2019m not studying medicine. Or physics. Or just about anything aside from nursing, which is apparently kind of low-status. I inevitably get defensive when these conversations occur, and I end up trying to justify why nursing is the morally correct thing for me to do. For some reason, in some deep-down part of me that I don\u2019t normally have conscious access to, I don\u2019t want to be the sort of person described in (2).</p>\n<p>Introspection isn\u2019t accurate enough for me to automatically find my true rejection of ambitious goals, but I will take the rest of the post to speculate on <em>my own personal </em>reasons. They may or may not be reasons that generalize to anyone else.&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"1__Idealism_versus_practicality\">1. Idealism versus practicality</h2>\n<p>My mother tells me I would be a good academic, and enjoy it too. She\u2019s usually right about that kind of thing, but I decided around eighth grade that academia wasn\u2019t for me.</p>\n<p class=\"MsoNormal\">Why? Well, my mother and father both studied science at the undergraduate level (biology and physical chemistry, respectively) and then both went on to complete PhDs. From the sound of it, those student years were among the happiest in their lives. My father went on to do a postdoc at Cambridge, and then to get a crappy part-time teaching position at a small university in Washington State. He hated it. Eventually he quit and we moved up to Ottawa, Canada, where he worked at Nortel, was laid off during the company\u2019s decline, and eventually found another job at a small company that takes apart computer chips and analyzes them. Meanwhile, my mother spent most of those years as a housewife, and has only recently begun working again, part-time and for a token salary.</p>\n<p class=\"MsoNormal\">I\u2019ve asked my father what he thinks of the decisions he made, and he told me that his biggest problem was that he didn\u2019t know what he wanted to do with his life. He told me that he still doesn\u2019t. His job is boring and stressful, but he can\u2019t quit because he didn\u2019t start saving for retirement until he was 40. As a grad student, he worked with John Polanyi, a well-known academic; much later he told me he \u201calways sort of thought I would end up being well-known and cool like that, but all of a sudden I\u2019m almost 50 and I realize that\u2019s not going to happen.\u201d</p>\n<p class=\"MsoNormal\">I remember the year when he developed a sudden passion for career self-help books, of the \u2018What Color Is Your Parachute\u2019 and \u2018The Seven Habits of Highly Effective People\u2019 variety. I must have been about thirteen years old. He encouraged me to read them, and warned me that \u201cit\u2019s better to think about what you want to do, not what you want to be.\u201d</p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson my 13-year-old self I took from all this: don\u2019t have hopes and dreams, especially not ambitious ones. You won\u2019t achieve them, and you\u2019ll end up in a mid-life crisis with no retirement savings, full of regrets. Far better to have a practical, achievable life plan, and then go out and damn well <em>achieve </em></span><span style=\"mso-ansi-language: EN-US;\">it. I read the self-help books, figured that nurses did around the same stuff all day as doctors and didn\u2019t have to spend eight years in school paying tuition, and never looked back.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson I <em>didn\u2019t </em></span><span style=\"mso-ansi-language: EN-US;\">learn from all this: my parents weren\u2019t actually ambitious either. They enjoyed their studies in university, but primarily they had fun: going to the philosophy faculty parties, getting drunk with chemistry students, volunteering on coffee plantations in Nicaragua... Those are the stories they tell me from their studies, not stories of the research they did and the papers they published. I can\u2019t be sure what their true feelings were at the time, but I don\u2019t think they <em>cared </em></span><span style=\"mso-ansi-language: EN-US;\">especially. They were smart young people who wanted to have a good time and didn\u2019t especially care if they had no money. And I don\u2019t think they have as many regrets as I assumed when I was thirteen. They didn\u2019t exactly make life goals and then fail to achieve them. They just hadn\u2019t made their long-term goals ahead of time.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson I <em>should </em></span><span style=\"mso-ansi-language: EN-US;\">have learned: if you head into adulthood without big goals, don\u2019t be surprised if you don\u2019t achieve them.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"2__Fear_of_failure\"><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>\n<p class=\"MsoNormal\">The second life lesson about ambition happened a few years later, when I was around fourteen. I had been training as a competitive swimmer for a number of years. My parents didn\u2019t sign me up because they wanted me to go to the Olympics someday; they wanted me to stay fit and have opportunities to socialize. It was a good decision; swim team made me happy, to the point that I often forget how unhappy I was up until then.</p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">But after a while I started to get <em>good</em></span><span style=\"mso-ansi-language: EN-US;\"> at swimming, and coaches, even kids\u2019 coaches, implicitly want their athletes to win, and keep winning, and maybe someday they\u2019ll be known as the one who coached an Olympic athlete. Training made me happy, but competition emphatically did <em>not</em></span><span style=\"mso-ansi-language: EN-US;\">; anxiety, stress, and bursting into tears before a race soon became part of my day-to-day life. My coaches told me that if I worked hard and believed in myself, I could do anything. But eventually I hit a point when I was racing kids who were simply <em>more talented </em></span><span style=\"mso-ansi-language: EN-US;\">than me: taller, slimmer, bigger hands and feet, a genetic predisposition to fast-twitch muscles, whatever. And then I hit my body\u2019s limits, and I stopped getting faster at all, no matter how hard I trained. My coaches accused me of not trying hard enough. Understandably, this made me feel worse, since I certainly <em>felt </em></span><span style=\"mso-ansi-language: EN-US;\">like I was trying as hard as I could.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lessons my 14-year-old self learned from this: don\u2019t have high expectations for yourself when competing against other people. You\u2019ll just end up feeling worthless and depressed. In fact, don\u2019t compete against other people <em>at all</em></span><span style=\"mso-ansi-language: EN-US;\">. Do things that are solely based on how good you are, as opposed to how good you are <em>relative to other people who might be more talented. </em></span><span style=\"mso-ansi-language: EN-US;\">Even better, do things that aren\u2019t that hard in an absolute sense, so that you don\u2019t risk failing.</span></p>\n<p class=\"MsoNormal\">This is kind of a fallacy, of course. Success in anything is measured relative to other people, if only relative to the average. Even grades, because classes and tests and grades are set up for students of average intelligence, so students of relatively higher intelligence will find them easier, and students of lower-than-average intelligence will feel like they\u2019re fighting a losing battle, as I did in swimming competitions. Possessing above average intelligence let me grow up seeing school as non-threatening, but I know that isn\u2019t true for everyone. I\u2019ve known people whose above-average athletic skills led them to be far more confident in sports than at school.</p>\n<p class=\"MsoNormal\">Still, fallacy or not, I later applied this idea to a lot of my decision. I was interested in physics all along, but my father\u2019s tales of academia and the competition and pressure involved turned me off it. I also considered studying music theory and composition, but decided not to because, aside from being impractical for finding a job afterwards, I\u2019d heard it was an incredibly competitive field. To a degree, this is why I chose not to make a career as a writer. (A degree in English didn\u2019t seem particularly interesting to me, so I doubt I would have studied it, but even in high school I never really thought about earning money with my writing.) Success or failure was too far beyond my control for comfort.</p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson I didn\u2019t learn from this: find an area where you <em>do </em></span><span style=\"mso-ansi-language: EN-US;\">have natural talent on your side, and use it for all it\u2019s worth. In fact, I\u2019ve done the opposite of this: one reason I chose nursing was because I felt that I was bad at a whole range of skills; empathy, social skills, fine motor skills and coordination, reacting in emergencies; and I wanted to force myself to improve. As a result, I\u2019m far from the strongest student in my classes, and labs, simulations, and hospital placements bring me to a level of anxiety far above anything I ever experienced during academic tests or exams.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson I should have learned from this: you never know what you are and aren\u2019t capable of until you try it. I tried competitive swimming, and found out I didn\u2019t have the raw talent to go to the Olympics. Who knows if this would have been true of physics? My father tells me that in his fourth year of undergraduate studies, he took several physics courses with a level of advanced math that he found almost impossible. He had reached his brain\u2019s natural limit in math, which he might or might not have been able to exceed with hard work and hours of study; still, it was <em>much </em></span><span style=\"mso-ansi-language: EN-US;\">more advanced than the first-year calculus I took as an elective. I have no reason to think that I\u2019m <em>worse </em></span><span style=\"mso-ansi-language: EN-US;\">at math than my father, and I suspect my obsessive work ethic would help me exceed any limits I did bump up against. And why not try?</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"3__The_morality_of_ambition\"><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">There\u2019s a third aspect of my aversion to ambitious goals, and I can\u2019t say<em> </em></span><span style=\"mso-ansi-language: EN-US;\">where it comes from. It might be my parents\u2019 attitude of moderation in everything: they consistently disapproved of my involvement in any \u2018obsessive\u2019 activities, swim team included. It might be the way my mother always got mad at me for talking about my achievements, even my grades, in front of friends; it\u2019ll make other people feel bad, she said. (For a long time I was incredibly self-conscious about high grades, and wouldn\u2019t tell my friends if they were above 90%.) It might be the meme that \u2018money doesn\u2019t buy happiness\u2019 or the idea that it\u2019s greedy to be ambitious, or that power corrupts and wise people choose not to seek it.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">I can\u2019t trace the roots of this idea completely, but for whatever reason, I spent a long time thinking that being ambitious was in some way <em>immoral</em></span><span style=\"mso-ansi-language: EN-US;\">. That really good people lived simple, selfless lives and never tried to seek anything more. That doing something solely because you wanted more money or more respect, like going to med school instead of nursing school, was selfish and just <em>bad</em></span><span style=\"mso-ansi-language: EN-US;\">. It might come from the books I read as a kid, or maybe it\u2019s just a rationalization to cover up my other reasons with a nobler one.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">But if this is my true reason, then it\u2019s a way to feel superior to people who\u2019ve accomplished cooler things than me, of whom part of me is actually jealous, and that\u2019s <em>not </em></span><span style=\"mso-ansi-language: EN-US;\">the person I want to be.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"4__Laziness\"><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">I don\u2019t normally think of myself as a lazy person. Other people are constantly telling me that I\u2019m diligent and have an excellent work ethic. But there\u2019s a way in which all this hardworking dedication to my current occupations has <em>prevented </em></span><span style=\"mso-ansi-language: EN-US;\">me from spending much time thinking or acting about what I\u2019m going to do next. Working a bunch of 12-hour shifts makes me feel productive, brings the direct benefit of a fat paycheck, and leaves me pretty exhausted at the end of the day, too tired to do the (in some ways harder) work of searching for cool job opportunities, looking at online classes to take, and in general breaking the routine. I <em>hate </em></span><span style=\"mso-ansi-language: EN-US;\">breaking my routine. It makes me anxious, and I have to spend more energy motivating myself, and in general it\u2019s <em>hard</em></span><span style=\"mso-ansi-language: EN-US;\">. I tend to only depart from that routine when forced.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"Conclusion\"><span style=\"mso-ansi-language: EN-US;\">Conclusion</span></h2>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">I think I was right about <em>some </em></span><span style=\"mso-ansi-language: EN-US;\">of the conclusions I drew from these various experiences. Practicality is important: ask the English majors working at Starbucks. Thinking about what you want to do all day, as opposed to the title and respect associated with what you want to <em>be</em></span><span style=\"mso-ansi-language: EN-US;\">, is good life advice and will likely result in a more satisfying career. Trying hard to project an image of success, i.e. \u201ckeeping up with the Jones\u2019\u201d, isn\u2019t a good path to happiness. And relative talent is a factor to take into consideration; if my dream career were to be an Olympic swimmer, unfortunately I wouldn\u2019t be likely to succeed.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">But one of the problems with thinking things through too deeply when you\u2019re young, and think you\u2019re wiser than everyone else, is a tendency to over-generalize. Doing cool, interesting, world-changing things with your life...even if the actual job position are competitive and hard to obtain...well, on reflection, it doesn\u2019t seem be a <em>bad </em></span><span style=\"mso-ansi-language: EN-US;\">idea.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">The lesson my current self has learned from this: investigate more. Spend less time on work and more time on actually planning future goals. Seek out interesting things to do, and interesting people to work with. Go for opportunities even if they're inconvenient and I have to break my routine a bit. Set <em>concrete </em></span><span style=\"mso-ansi-language: EN-US;\">goals, and don\u2019t wiggle out of achieving them because they\u2019re \u2018not actually that important.\u2019 They\u2019re probably more important than working at a community centre, and I seem to be able to dedicate 1000 hours a year to that... Try not to worry about sunk costs (although it\u2019s worth finishing nursing school, since an RN certificate is incredibly versatile in Canada and will guarantee me a job if any other prospects fail.) Force myself to step out of my comfort zone once in a while and do something kind of crazy, but awesome. And if I can do that, succeed to the point that I can break my reflex-of-being-average...<em>then </em>I'll know for sure whether rationality, of the Less Wrong variety, will help me to 'win'.&nbsp;</span></p>\n<p class=\"MsoNormal\">The lesson my future self will learn from this: who knows?&nbsp;</p>\n<!--EndFragment-->\n<p>&nbsp;</p>", "sections": [{"title": "What do I mean by \u2018ambition\u2019?", "anchor": "What_do_I_mean_by__ambition__", "level": 1}, {"title": "1. Idealism versus practicality", "anchor": "1__Idealism_versus_practicality", "level": 1}, {"title": "2. Fear of failure", "anchor": "2__Fear_of_failure", "level": 1}, {"title": "3. The morality of ambition", "anchor": "3__The_morality_of_ambition", "level": 1}, {"title": "4. Laziness", "anchor": "4__Laziness", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "239 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 239, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LgavAYtzFQZKg95WC", "stb3Jjumzhv49zCEb", "gBewgmzcEiks2XdoQ", "du395YvCnQXBPSJax"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T04:05:27.743Z", "modifiedAt": null, "url": null, "title": "How do I \"test it\"?", "slug": "how-do-i-test-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:35.127Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ahartell", "createdAt": "2011-03-25T04:38:25.170Z", "isAdmin": false, "displayName": "ahartell"}, "userId": "SnXuru6XzF555NDzE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qFBjHGPAQ3GMSPbsr/how-do-i-test-it", "pageUrlRelative": "/posts/qFBjHGPAQ3GMSPbsr/how-do-i-test-it", "linkUrl": "https://www.lesswrong.com/posts/qFBjHGPAQ3GMSPbsr/how-do-i-test-it", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20I%20%22test%20it%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20I%20%22test%20it%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFBjHGPAQ3GMSPbsr%2Fhow-do-i-test-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20I%20%22test%20it%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFBjHGPAQ3GMSPbsr%2Fhow-do-i-test-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFBjHGPAQ3GMSPbsr%2Fhow-do-i-test-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>I've read a bunch of times on LessWrong about how important is to test things. &nbsp;It makes sure your beliefs are paying rent and helps you verify your hypotheses. &nbsp;Testing ideas is obviously important to science, and it's about as obvious that testing ideas in everyday life can serve the same purpose. &nbsp;I know all this, and I want to be the type of person that goes out and verifies my beliefs by experiment, but still I can't think of a single time I've done it. &nbsp;I don't think I even recall thinking, about some everyday type of thing, \"hmm how could test that?\" (apart from trivial trial-error computer related things). &nbsp;Anyway, I was wondering if some of the you could give me some examples of times you've done this. &nbsp;I'm thinking maybe I'll be able to pattern-match the kind of things you guys have done and hopefully recognize in the moment when I'm looking at a testable thought.</p>\n<p>&nbsp;</p>\n<p>Thanks.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qFBjHGPAQ3GMSPbsr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 8.368255066232426e-07, "legacy": true, "legacyId": "12373", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T04:54:36.604Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Replace the Symbol with the Substance", "slug": "seq-rerun-replace-the-symbol-with-the-substance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:34.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/44Bn4QimK82cvXKWg/seq-rerun-replace-the-symbol-with-the-substance", "pageUrlRelative": "/posts/44Bn4QimK82cvXKWg/seq-rerun-replace-the-symbol-with-the-substance", "linkUrl": "https://www.lesswrong.com/posts/44Bn4QimK82cvXKWg/seq-rerun-replace-the-symbol-with-the-substance", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Replace%20the%20Symbol%20with%20the%20Substance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Replace%20the%20Symbol%20with%20the%20Substance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44Bn4QimK82cvXKWg%2Fseq-rerun-replace-the-symbol-with-the-substance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Replace%20the%20Symbol%20with%20the%20Substance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44Bn4QimK82cvXKWg%2Fseq-rerun-replace-the-symbol-with-the-substance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44Bn4QimK82cvXKWg%2Fseq-rerun-replace-the-symbol-with-the-substance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<p>Today's post, <a href=\"/lw/nv/replace_the_symbol_with_the_substance/\">Replace the Symbol with the Substance</a> was originally published on 16 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The existence of a neat little word prevents you from seeing the details of the thing you're trying to think about. What actually goes on in schools once you stop calling it \"education\"? What's a degree, once you stop calling it a \"degree\"? If a coin lands \"heads\", what's its radial orientation? What is \"truth\", if you can't say \"accurate\" or \"correct\" or \"represent\" or \"reflect\" or \"semantic\" or \"believe\" or \"knowledge\" or \"map\" or \"real\" or any other simple term?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9jc/seq_rerun_taboo_your_words/\">Taboo Your Words</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "44Bn4QimK82cvXKWg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 8.368443449506194e-07, "legacy": true, "legacyId": "12374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GKfPL6LQFgB49FEnv", "qiNH9rAx7ew449FkB", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T08:40:02.012Z", "modifiedAt": null, "url": null, "title": "Could Democritus have predicted intelligence explosion?", "slug": "could-democritus-have-predicted-intelligence-explosion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:57.186Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2vGXEKrxYhm5Zifgm/could-democritus-have-predicted-intelligence-explosion", "pageUrlRelative": "/posts/2vGXEKrxYhm5Zifgm/could-democritus-have-predicted-intelligence-explosion", "linkUrl": "https://www.lesswrong.com/posts/2vGXEKrxYhm5Zifgm/could-democritus-have-predicted-intelligence-explosion", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Could%20Democritus%20have%20predicted%20intelligence%20explosion%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACould%20Democritus%20have%20predicted%20intelligence%20explosion%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vGXEKrxYhm5Zifgm%2Fcould-democritus-have-predicted-intelligence-explosion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Could%20Democritus%20have%20predicted%20intelligence%20explosion%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vGXEKrxYhm5Zifgm%2Fcould-democritus-have-predicted-intelligence-explosion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vGXEKrxYhm5Zifgm%2Fcould-democritus-have-predicted-intelligence-explosion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 294, "htmlBody": "<p><small>Also see: <a href=\"http://friendly-ai.com/faq.html#WhatIsTheHistory\">History of the Friendly AI concept</a>.</small></p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Atomism\">ancient atomists</a> reasoned their way from first principles to materialism and atomic theory&nbsp;before Socrates began his life's work of making people look stupid in the marketplace of Athens. Why didn't they discover natural selection, too? After all, natural selection <a href=\"http://en.wikipedia.org/wiki/Evolution#Natural_selection\">follows necessarily</a> from heritability, variation, and selection, and the Greeks had plenty of evidence for all three pieces. Natural selection is <em>obvious</em>&nbsp;once you understand it, but it took us a long time to discover it.</p>\n<p>I get the same vibe from&nbsp;<a href=\"http://intelligenceexplosion.com/\">intelligence explosion</a>. The hypothesis wasn't stated clearly until <a href=\"http://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion\">1965</a>, but in hindsight it seems obvious. (Michael Vassar once told me that once he became a physicalist he said \"Oh! Intelligence explosion!\" Except of course he didn't know the term \"intelligence explosion.\" And he was probably exaggerating.)</p>\n<p>Intelligence explosion follows from physicalism and scientific progress and not much else. Since materialists had to believe that human <a href=\"http://facingthesingularity.com/2011/playing-taboo-with-intelligence/\">intelligence</a> resulted from the operation of mechanical systems located in the human body, they <em>could</em> have realized that scientists would eventually come to understand these systems so long as scientific progress continued. (<a href=\"http://en.wikipedia.org/wiki/Herophilus\">Herophilos</a> and <a href=\"http://en.wikipedia.org/wiki/Erasistratus\">Erasistratus</a> were already mapping which nerves and veins did what back in the 4th century B.C.)</p>\n<p>And once human intelligence is understood, it can be improved upon, and this improvement in intelligence can be used to improve intelligence even further. And the ancient Greeks certainly had good evidence that there was <a href=\"http://facingthesingularity.com/2011/plenty-of-room-above-us/\">plenty of room above us</a> when it came to intelligence.</p>\n<p>The major hang-up for predicting intelligence explosion may have been the the inability to imagine that this intelligence-engineering could leave the limitations of the human skull and move to a <a href=\"http://facingthesingularity.com/2011/plenty-of-room-above-us/\">speedier, more dependable and scalable</a> substrate. And that's why Good's paper had to wait until the age of the computer.</p>\n<p>&lt;/ speculation&gt;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1be": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2vGXEKrxYhm5Zifgm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 7, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "12375", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T14:07:38.857Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Rationality meet-up No.2", "slug": "meetup-sydney-rationality-meet-up-no-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:58.559Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TFWeKtjpiYZbmXBDa/meetup-sydney-rationality-meet-up-no-2", "pageUrlRelative": "/posts/TFWeKtjpiYZbmXBDa/meetup-sydney-rationality-meet-up-no-2", "linkUrl": "https://www.lesswrong.com/posts/TFWeKtjpiYZbmXBDa/meetup-sydney-rationality-meet-up-no-2", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Rationality%20meet-up%20No.2&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Rationality%20meet-up%20No.2%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFWeKtjpiYZbmXBDa%2Fmeetup-sydney-rationality-meet-up-no-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Rationality%20meet-up%20No.2%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFWeKtjpiYZbmXBDa%2Fmeetup-sydney-rationality-meet-up-no-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFWeKtjpiYZbmXBDa%2Fmeetup-sydney-rationality-meet-up-no-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6g'>Sydney Rationality meet-up No.2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 February 2012 06:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">22 The Promenade, Sydney NSW 2000 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As usual at the king street brew house until someone comes up with something better.</p>\n\n<p>Well, if people want regular, they'll get regular. In addition to capping off the talk about procrastination we had last time with some personal stories, I'm open to suggestions for topics.  If nothing is viable in the week leading up to this I'll come up with something.</p>\n\n<p>As a follow up to the meeting last time, the link to the page for Akrasia case studies is available here: <a href=\"http://lesswrong.com/lw/8ug/building_casestudies_of_akrasia/\" rel=\"nofollow\">http://lesswrong.com/lw/8ug/building_casestudies_of_akrasia/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6g'>Sydney Rationality meet-up No.2</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TFWeKtjpiYZbmXBDa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.370563762592285e-07, "legacy": true, "legacyId": "12376", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Rationality_meet_up_No_2\">Discussion article for the meetup : <a href=\"/meetups/6g\">Sydney Rationality meet-up No.2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 February 2012 06:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">22 The Promenade, Sydney NSW 2000 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As usual at the king street brew house until someone comes up with something better.</p>\n\n<p>Well, if people want regular, they'll get regular. In addition to capping off the talk about procrastination we had last time with some personal stories, I'm open to suggestions for topics.  If nothing is viable in the week leading up to this I'll come up with something.</p>\n\n<p>As a follow up to the meeting last time, the link to the page for Akrasia case studies is available here: <a href=\"http://lesswrong.com/lw/8ug/building_casestudies_of_akrasia/\" rel=\"nofollow\">http://lesswrong.com/lw/8ug/building_casestudies_of_akrasia/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Rationality_meet_up_No_21\">Discussion article for the meetup : <a href=\"/meetups/6g\">Sydney Rationality meet-up No.2</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Rationality meet-up No.2", "anchor": "Discussion_article_for_the_meetup___Sydney_Rationality_meet_up_No_2", "level": 1}, {"title": "Discussion article for the meetup : Sydney Rationality meet-up No.2", "anchor": "Discussion_article_for_the_meetup___Sydney_Rationality_meet_up_No_21", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jZEsFXyhFyoTY6s3m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T16:17:41.751Z", "modifiedAt": null, "url": null, "title": "Raising awareness of existential risks - perhaps explaining at \"personally stocking canned food\" level?", "slug": "raising-awareness-of-existential-risks-perhaps-explaining-at", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:31.647Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yLcHdPrcQdNJQrgSB/raising-awareness-of-existential-risks-perhaps-explaining-at", "pageUrlRelative": "/posts/yLcHdPrcQdNJQrgSB/raising-awareness-of-existential-risks-perhaps-explaining-at", "linkUrl": "https://www.lesswrong.com/posts/yLcHdPrcQdNJQrgSB/raising-awareness-of-existential-risks-perhaps-explaining-at", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raising%20awareness%20of%20existential%20risks%20-%20perhaps%20explaining%20at%20%22personally%20stocking%20canned%20food%22%20level%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaising%20awareness%20of%20existential%20risks%20-%20perhaps%20explaining%20at%20%22personally%20stocking%20canned%20food%22%20level%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLcHdPrcQdNJQrgSB%2Fraising-awareness-of-existential-risks-perhaps-explaining-at%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raising%20awareness%20of%20existential%20risks%20-%20perhaps%20explaining%20at%20%22personally%20stocking%20canned%20food%22%20level%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLcHdPrcQdNJQrgSB%2Fraising-awareness-of-existential-risks-perhaps-explaining-at", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLcHdPrcQdNJQrgSB%2Fraising-awareness-of-existential-risks-perhaps-explaining-at", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 532, "htmlBody": "<p>Many articles have been written on the topic of <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">existential risks</a>, and the need of greater public awareness. Here's my take - the existential risks are perhaps easier to explain with a simple example that does not yet trigger the 'too scary to be true' reflex. Example which one can easily explain to the people one knows and meets with regularly, and have them explain it to others. Example not involving any controversial predictions such as strong AI.</p>\n<p>So, let's suppose that there's 1 in 500 years lethal flu-like pandemic risk, that kills 1/5 of the population. That's likely to be a gross underestimate, especially in the light of <a href=\"/lw/8la/scientists_create_mammalian_h5n1/\">recent news</a>. You can talk about that risk with mostly anyone for a while without any protest, building up some tolerance to the scariness of this scenario and letting them become accustomed with the notion, perhaps letting them suggest that real risk may be higher, perhaps 1 in 100 years.</p>\n<p>Then you can compare it to personal everyday risks - specific to the audience. Homicide rates, car accidents, what ever everyday risks that we take counter measures against.</p>\n<p>That's where this risk gets scary, and if you do comparisons right away some people just withdraw into a fantasy world where such pandemic is impossible or much less probable.</p>\n<p>A lot of people, majority perhaps, are very sensitive about risks at this level. It is higher than risk of death by a car wreck, by homicide, and by many types of accidents, in about any developed country. We have seat belts and airbags, the cars are engineered for safety at non-insignificant expense, etc. We vaccinate against rare but severe diseases.</p>\n<p>So here's the thing. Stocking up on food for two months could conceivably be as effective in the event of outbreak as seat belts, airbags, and road safety measures are for prevention of car crash fatalities; and there are many more scenarios than virus outbreak where those cans can improve your safety. Note: viruses normally don't survive for a long time in the environment, highly lethal viruses burn out the susceptible population, and after month or two a vaccine could become available. Non-interaction for months is doubly important as the sick can stay at home and avoid infecting others.</p>\n<p>Thus it follows that one should a: stock up on preserved food and other necessities if possible (assuming western income levels and fairly low cost of the preserved food which can be further reduced by simply eating that food as it nears the expiration date - other places would need detailed cost benefit analysis), and b: try to explain this argument to others as this, too, would enhance the survival.</p>\n<p>Meanwhile this argument should raise awareness of global risks, which are not insignificant, and make people more accepting of the notion that something globally bad may happen. The biggest problem with awareness of global risks is that they never happened in today's world, whereas for individual risk of such magnitude everyone knows someone who died of it not so long ago.</p>\n<p>So, what do you think? Any other global risks that could serve as good, easy to understand example to make people accept better the notion that something globally bad may happen?</p>\n<p>edit: typos</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yLcHdPrcQdNJQrgSB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 18, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "12377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fHRgeoEWcWccJ6YNP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T18:47:52.840Z", "modifiedAt": null, "url": null, "title": "Thinking Bayesianically, with Lojban", "slug": "thinking-bayesianically-with-lojban", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:39.535Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BS8dYNN5D6kLhvdXR/thinking-bayesianically-with-lojban", "pageUrlRelative": "/posts/BS8dYNN5D6kLhvdXR/thinking-bayesianically-with-lojban", "linkUrl": "https://www.lesswrong.com/posts/BS8dYNN5D6kLhvdXR/thinking-bayesianically-with-lojban", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thinking%20Bayesianically%2C%20with%20Lojban&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThinking%20Bayesianically%2C%20with%20Lojban%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBS8dYNN5D6kLhvdXR%2Fthinking-bayesianically-with-lojban%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thinking%20Bayesianically%2C%20with%20Lojban%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBS8dYNN5D6kLhvdXR%2Fthinking-bayesianically-with-lojban", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBS8dYNN5D6kLhvdXR%2Fthinking-bayesianically-with-lojban", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 464, "htmlBody": "<p>\"Do not walk to the truth, but dance. On each and every step of that dance your foot comes down in exactly the right spot. Each piece of evidence shifts your beliefs by exactly the right amount, neither more nor less. What is exactly the right amount? To calculate this you must study probability theory. Even if you cannot do the math, knowing that the math exists tells you that the dance step is precise and has no room in it for your whims.\" -- from \"Twelve Virtues of Rationality\", by Eliezer Yudkowsky<br /><br /><br />One of the more useful mental tools I've found is the language Lojban ( http://www.lojban.org/tiki/Learning ), which makes explicit many of the implicit assumptions in languages. (There's also a sub-language based on Lojban, called Cniglic ( http://www.datapacrat.com/cniglic/ ), which can be added to most existing languages to offer some additional functionality.)<br /><br />One of the things Lojban (and Cniglic) has are 'evidentials', words which can be used to tag other words and sentences to explain how the speaker knows them: \"ja'o\", meaning \"I conclude\", \"za'a\" meaning \"I observe\", \"pe'i\" meaning \"It's my opinion\", and more. However, there hasn't been any easy and explicit way to use this system to express Bayesian reasoning...<br /><br />... until today.<br /><br />Lojban not only allows for, but encourages, \"experimental\" words of certain sorts; and using that system, I have now created the word \"bei'e\" (pronounced BAY-heh), which allows a speaker to tag a word or sentence with how confident they are, in the Bayesian sense, of its truth. Taking an idea from the foundational text by E.T. Jaynes, \"bei'e\" is measured in decibels of logarithmic probability. This sounds complicated, but in many cases, is actually much easier to use than simple odds or probability; adding 10 decibels multiplies the odds by a factor of 10.<br /><br />The current reftext for \"bei'e\" is at http://www.lojban.org/tiki/bei%27e , which basically amounts to adding Lojbannic digits to the front of the word:<br /><br /><br /> \n<table border=\"0\">\n<tbody>\n<tr>\n<td><strong>ni'uci'ibei'e</strong></td>\n<td>-oo</td>\n<td>0%</td>\n<td>1:oo</td>\n<td>complete disbelief, paradox</td>\n</tr>\n<tr>\n<td><strong>ni'upabei'e</strong></td>\n<td>-1</td>\n<td>44.3%</td>\n<td>4:5</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>ni'ubei'e</strong></td>\n<td>&lt;0</td>\n<td>&lt;50%</td>\n<td>&lt;1:1</td>\n<td>less than even odds, less likely than so</td>\n</tr>\n<tr>\n<td><strong>nobei'e</strong></td>\n<td>0</td>\n<td>50%</td>\n<td>1:1</td>\n<td>neither belief nor disbelief, agnosticism</td>\n</tr>\n<tr>\n<td><strong>ma'ubei'e</strong></td>\n<td>&gt;0</td>\n<td>&gt;50%</td>\n<td>&gt;1:1</td>\n<td>greater than even odds, more likely than not</td>\n</tr>\n<tr>\n<td><strong>pabei'e</strong></td>\n<td>1</td>\n<td>55.7%</td>\n<td>5:4</td>\n<td>preponderance of the evidence</td>\n</tr>\n<tr>\n<td><strong>rebei'e</strong></td>\n<td>2</td>\n<td>61.3%</td>\n<td>3:2</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>cibei'e</strong></td>\n<td>3</td>\n<td>66.6%</td>\n<td>2:1</td>\n<td>clear and convincing evidence</td>\n</tr>\n<tr>\n<td><strong>vobei'e</strong></td>\n<td>4</td>\n<td>71.5%</td>\n<td>5:2</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>mubei'e</strong></td>\n<td>5</td>\n<td>76.0%</td>\n<td>3:1</td>\n<td>beyond a reasonable doubt</td>\n</tr>\n<tr>\n<td><strong>xabei'e</strong></td>\n<td>6</td>\n<td>80.0%</td>\n<td>4:1</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>zebei'e</strong></td>\n<td>7</td>\n<td>83.3%</td>\n<td>5:1</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>bibei'e</strong></td>\n<td>8</td>\n<td>86.3%</td>\n<td>6:1</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>sobei'e</strong></td>\n<td>9</td>\n<td>88.8%</td>\n<td>8:1</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>panobei'e</strong></td>\n<td>10</td>\n<td>90.9%</td>\n<td>10:1</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>pacibei'e</strong></td>\n<td>13</td>\n<td>95.2%</td>\n<td>20:1</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>xarebei'e</strong></td>\n<td>62</td>\n<td>99.99994%</td>\n<td>1,500,000:1</td>\n<td>5 standard deviations</td>\n</tr>\n<tr>\n<td><strong>ci'ibei'e</strong></td>\n<td>oo</td>\n<td>100%</td>\n<td>oo:1</td>\n<td>complete belief, tautology</td>\n</tr>\n<tr>\n<td><strong>xobei'e</strong></td>\n<td>?</td>\n<td>?%</td>\n<td>?:?</td>\n<td>question, asking listener their level of belief</td>\n</tr>\n</tbody>\n</table>\n<br /><br /><br />By having this explicit mental tool, even if I don't use it aloud, I'm finding it much easier to remember to gauge how confident I am in any given proposition. If anyone else finds use in this idea, so much the better; and if anyone can come up with an even better mental tool after seeing this one, that would be better still.<br /><br /><br /><br />.uo .ua .uisai .oinairo'e</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BS8dYNN5D6kLhvdXR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 13, "extendedScore": null, "score": 8.371636768098792e-07, "legacy": true, "legacyId": "12379", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T19:35:36.958Z", "modifiedAt": null, "url": null, "title": "Michael Nielsen explains Judea Pearl's causality", "slug": "michael-nielsen-explains-judea-pearl-s-causality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:56.663Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pMzQ4zQYgY8jcckSi/michael-nielsen-explains-judea-pearl-s-causality", "pageUrlRelative": "/posts/pMzQ4zQYgY8jcckSi/michael-nielsen-explains-judea-pearl-s-causality", "linkUrl": "https://www.lesswrong.com/posts/pMzQ4zQYgY8jcckSi/michael-nielsen-explains-judea-pearl-s-causality", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Michael%20Nielsen%20explains%20Judea%20Pearl's%20causality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMichael%20Nielsen%20explains%20Judea%20Pearl's%20causality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpMzQ4zQYgY8jcckSi%2Fmichael-nielsen-explains-judea-pearl-s-causality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Michael%20Nielsen%20explains%20Judea%20Pearl's%20causality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpMzQ4zQYgY8jcckSi%2Fmichael-nielsen-explains-judea-pearl-s-causality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpMzQ4zQYgY8jcckSi%2Fmichael-nielsen-explains-judea-pearl-s-causality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p><a href=\"https://en.wikipedia.org/wiki/Michael_Nielsen\">Michael Nielsen</a> has posted <a href=\"http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/\">a long essay</a> explaining his understanding of the Pearlean causal DAG model. I don't understand more than half, but that's much more than I got out of a few other papers. Strongly recommended for anyone interested in the topic.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cq69M9ceLNA35ShTR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pMzQ4zQYgY8jcckSi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 28, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "12380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T20:49:56.780Z", "modifiedAt": null, "url": null, "title": "Drawing Workshop in NYC: Gauging Interest", "slug": "drawing-workshop-in-nyc-gauging-interest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.717Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YFmK9GhLiGc6J8XLG/drawing-workshop-in-nyc-gauging-interest", "pageUrlRelative": "/posts/YFmK9GhLiGc6J8XLG/drawing-workshop-in-nyc-gauging-interest", "linkUrl": "https://www.lesswrong.com/posts/YFmK9GhLiGc6J8XLG/drawing-workshop-in-nyc-gauging-interest", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Drawing%20Workshop%20in%20NYC%3A%20Gauging%20Interest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADrawing%20Workshop%20in%20NYC%3A%20Gauging%20Interest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFmK9GhLiGc6J8XLG%2Fdrawing-workshop-in-nyc-gauging-interest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Drawing%20Workshop%20in%20NYC%3A%20Gauging%20Interest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFmK9GhLiGc6J8XLG%2Fdrawing-workshop-in-nyc-gauging-interest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYFmK9GhLiGc6J8XLG%2Fdrawing-workshop-in-nyc-gauging-interest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 554, "htmlBody": "<p><span id=\"internal-source-marker_0.43323481263585295\" style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Short version: </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">I want to hold a three day (Friday evening, Saturday and Sunday afternoon) drawing workshop in NYC sometime in the next month. Would anyone be interested?</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Longer version:</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Several weeks ago, I put my </span><a href=\"/lw/8f7/drawing_less_wrong_should_you_learn_to_draw\"><span style=\"font-size: 15px; font-family: Arial; color: #000099; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Drawing Less Wrong sequence</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> on hiatus while I worked on the </span><a href=\"/lw/9aw/designing_ritual\"><span style=\"font-size: 15px; font-family: Arial; color: #000099; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Solstice project</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">. I&rsquo;m continuing with rational-culture development work, (there&rsquo;s a mailing list for people interested in participating) but I want to return to the drawing sequence soon.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">In the beginning, I didn&rsquo;t spell out my intentions very well - Drawing Less Wrong wasn&rsquo;t intended to be a set of comprehensive exercises, but rather an attempt to bridge a lot of inferential distance between how most people think of drawing and how you actually need to think in order to draw. I&rsquo;d then discuss my previous workshops - what exercises I tried and how they worked out. I was also looking for existing drawing tutorials while I was writing the articles, and by the end I planned to have narrowed things down to some good recommendations.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">But many people were (rightly) expecting drawing exercises to be a part of the series. It&rsquo;s hard to learn abstract concepts without a concrete example to work with. This is even more true when the abstract concepts are explicitly about difficult-to-describe kinesthetic processes.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">I know of a few drawing exercises that are well tested, and are definitely important to any artist who&rsquo;s undertaking a 10,000 hour journey. But I don&rsquo;t know which are best if you&rsquo;re trying to show someone how to draw in 20 hours. There are certain rules of thumb that can help you improve quickly (like learning human proportion) but which can trap you into a drawing what you *think* you know, rather than what&rsquo;s actually there.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Different exercises also work better for different people, and I just don&rsquo;t have enough data to say confidently &ldquo;you should do X.&rdquo; </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Previous workshops had also been rather spaced out (2-3 weeks apart). I was actually surprised that people seemed to retain the improvements from the previous workshops. </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">So before I return to the series, I&rsquo;d like to a more intensive workshop, with more people. Three days in a row, with somewhere between 12-20 hours of work (I&rsquo;ll be available the full 20 hours, but it may be a lot of time to commit for some people). Systematically try out different exercises and see how they work for people of different backgrounds.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">This is still far from a formal experiment, but I&rsquo;m hoping to find big enough effect sizes that I can confidently say &ldquo;you should probably spend your first 8-20 hours focusing these X exercises.&rdquo;</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">For those who CAN&rsquo;T attend: one of the DLW readers went ahead and did some practice on their own . </span><a href=\"http://lrning2draw.tumblr.com/\"><span style=\"font-size: 15px; font-family: Arial; color: #000099; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">They sent me a mini-blog of their experiences</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">, and gave permission to share it. Their progress seems like a fair representation of how the average person can expect to improve. (If your initial drawings are different from their initial drawings, you may try to approximate the difference in quality of their later and earlier work, and extrapolate that for your own starting position.)<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YFmK9GhLiGc6J8XLG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.372106748971201e-07, "legacy": true, "legacyId": "12381", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bhTCSdxF4gCuzzTRa", "GEQyCqgu5Yhi5dTdb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T21:09:37.689Z", "modifiedAt": null, "url": null, "title": "Sunk Costs Fallacy Fallacy", "slug": "sunk-costs-fallacy-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.918Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dHJuevTkH92sjLYXe/sunk-costs-fallacy-fallacy", "pageUrlRelative": "/posts/dHJuevTkH92sjLYXe/sunk-costs-fallacy-fallacy", "linkUrl": "https://www.lesswrong.com/posts/dHJuevTkH92sjLYXe/sunk-costs-fallacy-fallacy", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sunk%20Costs%20Fallacy%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASunk%20Costs%20Fallacy%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdHJuevTkH92sjLYXe%2Fsunk-costs-fallacy-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sunk%20Costs%20Fallacy%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdHJuevTkH92sjLYXe%2Fsunk-costs-fallacy-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdHJuevTkH92sjLYXe%2Fsunk-costs-fallacy-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 555, "htmlBody": "<p>I have a problem with never finishing things that I want to work on. I get enthusiastic about them for a while, but then find something else to work on. This problem seems to be powered partially by my sunk costs fallacy hooks.</p>\n<p>When faced with the choice of finishing my current project or starting this shiny new project, my sunk costs hook activates and says \"evaluate future expected utility and ignore sunk costs\". The new project looks very shiny compared to the old project, enough that it looks like a better thing to work on than the rest of the current project. The trouble is that this always seems to be the case. It seems weird that the awesomeness of my project ideas would have exponential growth over time, so there must be something else here.</p>\n<p>The \"evaluate future expected utility and ignore sunk costs\" heuristic works well for life-planning where people get status quo bias and financial-utility things where utility is easy to calculate consistently. In fact it seems like generally good decision theory, except that I'm running on corrupted hardware. My corrupted hardware seems to decay the perceived value of a project over the time that I know of it, or inflate it when it seems new and exciting, which of course throws off the sunk costs hook's assumption that I can evaluate utility *consistently*.</p>\n<p>So my sunk costs hook has a bad assumption. I don't want to go modifying the hook in a way that would break its applicability to economic and life-planning situations or its theoretical correctness, so I'll just add \"this assumes consistent utility function\". This of course doesn't actually help me on the project planning case, I need to put a hook on evaluating the utility of a project that makes the utility function consistent.</p>\n<p>Some things that might de-skew my evaluation of exciting new projects:</p>\n<ul>\n<li>If the project is new, it probably looks shinier than it is. I should wait a while or try to correct for this before evaluating.</li>\n<li>I should take into account my record of defecting halfway thru a project to discount the utility of the new project.</li>\n<li>The latter half of a project is relatively untouched territory, full of valuable new experiences. I will have to work thru the first half of the new project to get to this, but I am already at the threshold on the current project.</li>\n<li>Maybe there's more?</li>\n</ul>\n<p>So I'll see how this works out.</p>\n<p>I think this situation is probably not unique. Many of our debiasing hooks are formulated to combat specific biases but might catch situations outside their domain. In this example, the sunk costs bias is a real thing, but the hook to catch it was also catching a situation where sunk costs was not the primary bias, and actually ended up contributing to bias.</p>\n<p>It might be valuable to think about what other situations a hook might catch, and modify it to not screw things up before we install it. Also, other biases may act in the opposite direction, and only hooking one of them might make things worse.</p>\n<p>Anyways, that's my thoughts on a specific bit of debiasing. Maybe you all have some other examples of this sort of thing, and maybe this will be useful for people who have the same problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"stnsBEmuGpnSfQ5vj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dHJuevTkH92sjLYXe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 36, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "12382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-24T23:57:04.122Z", "modifiedAt": null, "url": null, "title": "Urges vs. Goals: The analogy to anticipation and belief", "slug": "urges-vs-goals-the-analogy-to-anticipation-and-belief", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:59.951Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wmjPGE8TZKNLSKzm4/urges-vs-goals-the-analogy-to-anticipation-and-belief", "pageUrlRelative": "/posts/wmjPGE8TZKNLSKzm4/urges-vs-goals-the-analogy-to-anticipation-and-belief", "linkUrl": "https://www.lesswrong.com/posts/wmjPGE8TZKNLSKzm4/urges-vs-goals-the-analogy-to-anticipation-and-belief", "postedAtFormatted": "Tuesday, January 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Urges%20vs.%20Goals%3A%20The%20analogy%20to%20anticipation%20and%20belief&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUrges%20vs.%20Goals%3A%20The%20analogy%20to%20anticipation%20and%20belief%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwmjPGE8TZKNLSKzm4%2Furges-vs-goals-the-analogy-to-anticipation-and-belief%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Urges%20vs.%20Goals%3A%20The%20analogy%20to%20anticipation%20and%20belief%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwmjPGE8TZKNLSKzm4%2Furges-vs-goals-the-analogy-to-anticipation-and-belief", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwmjPGE8TZKNLSKzm4%2Furges-vs-goals-the-analogy-to-anticipation-and-belief", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2148, "htmlBody": "<p>Partially in response to: <a href=\"/lw/8gv/the_curse_of_identity/\">The curse of identity</a></p>\n<p class=\"p1\">Related to: <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans are not automatically strategic</a>, <a href=\"/lw/1kr/that_other_kind_of_status/\">That other kind of status</a>, <a href=\"/lw/6nz/approving_reinforces_loweffort_behaviors/\">Approving reinforces low-effort behaviors</a>.</p>\n<p class=\"p2\" style=\"padding-left: 30px; \"><em>Joe studies long hours,&nbsp;</em><em>and often prides himself on how driven he is to make something of himself</em><em>. &nbsp;But in the actual moments of his studying, Joe often looks out the window, doodles, or drags his eyes over the text while his mind wanders. &nbsp;Someone sent him a link to which college majors lead to the greatest lifetime earnings, and he didn't get around to reading that either. &nbsp;</em><em>Shall we say that Joe doesn't really care about making something of himself?</em></p>\n<p class=\"p2\">The Inuit may not have 47 words for snow, but Less Wrongers do have at least two words for belief.&nbsp;&nbsp;We find it necessary to <a href=\"/lw/i4/belief_in_belief/\">distinguish</a> between:</p>\n<ul>\n<li>Anticipations, what we <em>actually expect to see happen;</em></li>\n<li>Professed beliefs, the set of things we tell ourselves we &ldquo;believe&rdquo;, based partly on deliberate/verbal thought.</li>\n</ul>\n<p class=\"p1\">This distinction helps explain how an atheistic rationalist can still <a href=\"/lw/1l/the_mystery_of_the_haunted_rationalist/\">get spooked</a> in a haunted house; how someone can &ldquo;believe&rdquo; they&rsquo;re good at chess while avoiding games that might threaten that belief [1]; and why Eliezer had to actually crash a car before he viscerally understood what his physics books tried to tell him about stopping distance going up with the square of driving speed. &nbsp;(I helped Anna revise this - EY.)</p>\n<p class=\"p1\">A lot of our community technique goes into either (1) dealing with \"beliefs\" being an evolutionarily recent system, such that our \"beliefs\" often end up far screwier than our actual anticipations; or (2) trying to get our anticipations to align with more evidence-informed beliefs.</p>\n<p class=\"p1\">And analogously - this analogy is arguably obvious, but it's deep, useful, and easy to overlook in its implications - there seem to be two major kinds of wanting:</p>\n<ul>\n<li><strong>Urges</strong>: concrete emotional pulls, produced in System 1's perceptual / autonomic processes<br />(my urge to drink the steaming hot cocoa in front of me; my urge to avoid embarrassment by having something to add to my accomplishments log)</li>\n<li><strong>Goals</strong>: things we tell ourselves we&rsquo;re aiming at, within deliberate/verbal thought and planning<br />(I have a goal to exercise three times a week; I have a goal to reduce existential risk)</li>\n</ul>\n<p><strong>Implication 1: &nbsp;You can import a lot of technique for \"checking for screwy beliefs\" into \"checking for screwy goals\".<a id=\"more\"></a><br /></strong></p>\n<p>Urges, like anticipations, are relatively perceptual-level and automatic. &nbsp;They're harder to reshape and they're also harder to completely screw up. &nbsp;In contrast, the flexible, recent \"goals\" system can easily acquire goals that are wildly detached from what we actually do, wildly detached from any positive consequences, or both. &nbsp;Some techniques you can port straight over from \"checking for screwy beliefs\" to \"checking for screwy goals\" include:</p>\n<p>The fundamental:</p>\n<ul>\n<li>\"What's the positive consequence?\" &nbsp;This is the equivalent of \"What's the evidence?\" for beliefs. &nbsp;All the other cases involve not asking it, or not asking hard enough.</li>\n</ul>\n<p>The Hansonian:</p>\n<ul>\n<li><em><a href=\"http://hanson.gmu.edu/belieflikeclothes.html\">Goals as clothes</a>&nbsp;/&nbsp;<a href=\"/lw/i7/belief_as_attire/\">goals as tribal affiliation:</a>&nbsp;&nbsp;</em>&ldquo;<em>We</em>&nbsp;are people who have free software (/ communism / rationality / whatever) as our goal&rdquo;. &nbsp;Before you install Linux, do you think \"What's the positive consequence of installing Linux?\" or does it just seem like the sort of thing a free-software-supporter would do? &nbsp;(EY says: &nbsp;What&nbsp;<em>positive consequence</em>&nbsp;is achieved by marching in an Occupy Wall Street march? &nbsp;Can you remember anyone stating one, throughout the whole affair - \"if we march, X will happen because of Y\"?)</li>\n<li><em>Goals as a signal of one&rsquo;s value as an ally:</em>&nbsp; Sheila insists that she wants to get a job. &nbsp;We inspect her situation and she's not trying very hard to get a job. &nbsp;But she's in debt to a lot of her friends and is borrowing more to live on a month-to-month basis. &nbsp;It's not hard to see why Sheila would internally profess strongly that she has a goal of getting a job.</li>\n<li><em>Goals as&nbsp;<a href=\"/lw/i6/professing_and_cheering/\">personal fashion statements</a>:</em>&nbsp; A T-Shirt that says &ldquo;<a href=\"http://www.zazzle.com/give+me+coffee+and+no+one+gets+hurt+gifts\">Give me coffee and no one gets hurt</a>&rdquo; seems to state a very strong desire for coffee. &nbsp;This is clearly a goal professed directly to affect how others see you, and it's more a question of affecting a 'style' than anything directly tribal or status-y.</li>\n</ul>\n<p class=\"p2\">The satiating:</p>\n<ul>\n<li><em><em>Having goals as optimism:</em><span style=\"font-style: normal; \">&nbsp; \"I intend to lose weight\" can be created by much the same sort of internal processes that would make you believe \"I will lose weight\", in cases where the goal (belief) would not yet seem very plausible to an outside view.</span></em></li>\n<li><em>Having goals as apparent progress:</em>&nbsp; My current to-do list has \"write thank-you notes for wedding gifts\". &nbsp;This makes me feel like I've appeased the demand for internal attention by having a goal. &nbsp;(EY: &nbsp;I have \"send Anna and Carl their wedding gift\" on my todo list. &nbsp;This was very effective at appeasing the need to send them a wedding gift.)</li>\n</ul>\n<p><strong>Implication 2: &nbsp;\"Status\" / \"prestige\" / \"signaling\" / \"people don't really care about\" is way overused to explain goal-urge delinkages that can be more simply explained by \"humans are not agents\".</strong></p>\n<p>This post was written partially in response to <a href=\"/lw/8gv/the_curse_of_identity/\">The Curse of Identity</a>, wherein&nbsp;Kaj recounts some suboptimal goal-action linkages - wanting to contribute to the Singularity, then teaching himself to feel guilty whenever not working; founding the Finnish Pirate Party, then becoming the spokesperson which involved tasks he wasn't good at; helping Eliezer on writing his book, and feeling demotivated because it seemed like work \"anyone could do\" (which is just the sort of work that almost nobody is motivated to do).</p>\n<p>Kaj forms the generalization \"as soon as my brain adopted a cause, my subconscious reinterpreted it as the goal of giving the impression of doing prestigious work for the cause\". &nbsp;I worry that our community has a tendency to explain as e.g. status signaling&nbsp;or \"people really don't care about X\", observations that can also be explained by less malice/selfishness and more \"our brains have known malfunctions at linking goals to urges\". &nbsp;People are as bad at looking into hospitals for their own health as for the sake of their parents' health; Kaj didn't actually gain much prestige from feeling guilty about his relaxation time.</p>\n<p>We <em>do</em>&nbsp;have a status urge. &nbsp;It <em>does</em>&nbsp;affect a lot of things. &nbsp;People <em>do</em>&nbsp;tend to massively systematically understate it in much the same way that Victorians pretended that sex wasn't everywhere. &nbsp;But that's not the <em>same</em>&nbsp;cognitive problem as \"Our brain is pretty bad at linking effective behaviors to goals, and will sometimes reward us for just doing things that seem roughly associated with the goal, instead of actions that cause the consequence of the goal being achieved.\" &nbsp;And our brains not being coherent agents is something that's even more massive than status.</p>\n<ul>\n</ul>\n<p class=\"p2\"><strong>Implication 3: &nbsp;Humans cannot live by urges alone</strong></p>\n<p class=\"p1\">Like beliefs, goals often get much wackier than urges. &nbsp;I've seen a number of people react to this realization by concluding that they should give up on having goals, and lead an authentic life of pure desire. &nbsp;This wouldn't work any more than giving up on having beliefs. &nbsp;<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">To precisely anticipate how long it takes a ball to fall off a tower</a>, you have to manipulate abstract beliefs about gravitational acceleration.&nbsp;&nbsp;I have an&nbsp;<em>urge</em>&nbsp;to drive a car that runs smoothly, but if I didn't also have a&nbsp;<em>goal</em>&nbsp;of having a well-maintained car, I would never get around to having it serviced - I have no innate urge to do that.</p>\n<p class=\"p1\">I really have seen multiple people (some of whom I significantly cared about) malfunctioning as a result of misinterpreting this point. &nbsp;As a stand-alone system for pulling your actions, urges have all kinds of problems.&nbsp; Urges can pull you to stare at an attractive stranger, to walk to the fridge, and even to sprint hard for first base when playing baseball.&nbsp; But unless coupled with goals and far-mode reasoning, urges will not pull you to the component tasks required for any longer-term goods. &nbsp;When I get into my car I have a definite urge for it not to be broken. &nbsp;But absent planning, there would never be a moment when the activity I most desired was to take my car for an oil change. &nbsp;To find and keep a job (let alone a good job), live in a non-pigsty, or learn any skills that are not immediately rewarding, you will probably need goals.&nbsp; <em>Even though</em> human goals can easily turn into fashion statements and wishful thinking.</p>\n<p class=\"p1\"><strong>Implication 4: &nbsp;Your agency failures do not imply that your ideals are fake.</strong></p>\n<p class=\"p1\">Obvious but it needs to be said: &nbsp;People are as bad at looking into hospitals for their own health as for the sake of their parents' health. &nbsp;It doesn't mean that they don't really care about their parents, and it doesn't mean that they don't really care about survival. &nbsp;They would probably run away pretty fast from a tiger, where the goal connected to the urge in an ancestrally more reliable way and hence made them more 'agenty'; and they might fight hard to defend their parents from a tiger too.</p>\n<p class=\"p1\">There's a very real sense in which our agency failures imply that human beings <a href=\"/lw/6ha/the_blueminimizing_robot/\">don't have goals,</a>&nbsp;but this doesn't mean that our ungoaly ideals are any more ungoaly than anything else. &nbsp;Ideals can be more ungoaly because they're sometimes about faraway things or less ancestral things - it's probably <em>easier</em>&nbsp;to improve your agency on less idealy goals that link more quickly to urges - but as entities which can look over our own urges and goals and try to improve our agentiness, there's no rule which says that we can't try to solve some hard problems in this area as well as some easy ones.[2]</p>\n<p class=\"p1\"><strong>Implication 5: &nbsp;You can align urges and goals using the same sort of effort and training that it takes to align anticipations and beliefs.</strong></p>\n<p class=\"p1\">Although I've heard people saying that we discuss willpower-failure too much on Less Wrong, most of the best stuff I've read has been outside Less Wrong and hasn't made contact with us. &nbsp;For a starting guide to many such skills, see&nbsp;<em>Eat That Frog</em>&nbsp;by Brian Tracy [3]. &nbsp;Some basic alignment techniques include:</p>\n<ul>\n<li>Get in the habit of asking \"What is the positive consequence?\" &nbsp;(Probably more needs to be written about this so that your brain doesn't just answer \"I'll be a free software supporter!\" which is not what we mean to ask.)</li>\n<li>Andrew Critch's \"greedy algorithm\": &nbsp; Whenever you catch yourself <em>really wanting</em>&nbsp;to do something you <em>want to want,</em>&nbsp;immediately reward yourself - by feeding yourself an M&amp;M, or if that's too difficult, immediately pumping your fist and saying \"Yes!\"</li>\n<li>Whenever you sit down to work, naming a single, high-priority accomplishment for that session. &nbsp;Visualizing that accomplishment, and its positive rewarding consequences, until you <em>have an urge for it to happen</em> (instead of just having an urge to log today's hours).</li>\n</ul>\n<p>And much the same way that a lot of craziness stems, not so much from \"having a wrong model of the world\", as \"not bothering to have a model of the world\", a lot of personal effectiveness isn't so much about \"having the right goals\" as \"bothering to have goals at all\" - where unpacking this somewhat Vassarian statement would lead us to ideas like \"bothering to have <em>something</em>&nbsp;that I check my actions' consequences against, never mind whether or not it's the right thing\" or \"bothering to have <em>some</em>&nbsp;communication-related urge that animates my writing when I write, instead of just sitting down to log a certain number of writing hours during which I feel rewarded from rearranging shiny words\". &nbsp;</p>\n<p><strong>Conclusion:</strong></p>\n<p>Besides an aspiring rationalist, these days I call myself an \"aspiring consequentialist\".</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>[1] IMO the case of somebody who has the belief \"I am good at chess\", but instinctively knows to avoid strong chess opponents that would potentially test the belief, ought to be a more central example in our literature than <a href=\"/lw/i4/belief_in_belief/\">the person who believes they have an dragon in their garage</a>&nbsp;(but instinctively knows that they need to specify that it's invisible, inaudible and generates no carbon dioxide, when we show up with the testing equipment).</p>\n<p>[2] See also <a href=\"http://www.fanfiction.net/s/5782108/20/Harry_Potter_and_the_Methods_of_Rationality\">Ch. 20 of Methods of Rationality</a>:</p>\n<p>Professor Quirrell: &nbsp;\"Mr. Potter, in the end people all do what they want to do. Sometimes people give names like 'right' to things they want to do, but how could we possibly act on anything but our own desires?\"</p>\n<p>Harry: &nbsp;\"Well, obviously I couldn't act on moral considerations if they lacked the power to move me. But that doesn't mean my wanting to hurt those Slytherins has the power to move me more than moral considerations!\"</p>\n<p>[3] Thanks to Patri for <a href=\"/lw/2p5/humans_are_not_automatically_strategic/2l50\">recommending</a> this book to me in response to an earlier post. It is perhaps not written in the most LW-friendly language -- but, given the value of these skills, I&rsquo;d recommend wading in and doing your best to pull useful techniques from the somewhat salesy prose.&nbsp; I found much of value there.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"SJFsFfFhE6m2ThAYJ": 2, "Q6P8jLn8hH7kbuXRr": 2, "iP2X4jQNHMWHRNPne": 2, "AHK82ypfxF45rqh9D": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wmjPGE8TZKNLSKzm4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 89, "baseScore": 122, "extendedScore": null, "score": 0.00027, "legacy": true, "legacyId": "11312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 122, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Partially in response to: <a href=\"/lw/8gv/the_curse_of_identity/\">The curse of identity</a></p>\n<p class=\"p1\">Related to: <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans are not automatically strategic</a>, <a href=\"/lw/1kr/that_other_kind_of_status/\">That other kind of status</a>, <a href=\"/lw/6nz/approving_reinforces_loweffort_behaviors/\">Approving reinforces low-effort behaviors</a>.</p>\n<p class=\"p2\" style=\"padding-left: 30px; \"><em>Joe studies long hours,&nbsp;</em><em>and often prides himself on how driven he is to make something of himself</em><em>. &nbsp;But in the actual moments of his studying, Joe often looks out the window, doodles, or drags his eyes over the text while his mind wanders. &nbsp;Someone sent him a link to which college majors lead to the greatest lifetime earnings, and he didn't get around to reading that either. &nbsp;</em><em>Shall we say that Joe doesn't really care about making something of himself?</em></p>\n<p class=\"p2\">The Inuit may not have 47 words for snow, but Less Wrongers do have at least two words for belief.&nbsp;&nbsp;We find it necessary to <a href=\"/lw/i4/belief_in_belief/\">distinguish</a> between:</p>\n<ul>\n<li>Anticipations, what we <em>actually expect to see happen;</em></li>\n<li>Professed beliefs, the set of things we tell ourselves we \u201cbelieve\u201d, based partly on deliberate/verbal thought.</li>\n</ul>\n<p class=\"p1\">This distinction helps explain how an atheistic rationalist can still <a href=\"/lw/1l/the_mystery_of_the_haunted_rationalist/\">get spooked</a> in a haunted house; how someone can \u201cbelieve\u201d they\u2019re good at chess while avoiding games that might threaten that belief [1]; and why Eliezer had to actually crash a car before he viscerally understood what his physics books tried to tell him about stopping distance going up with the square of driving speed. &nbsp;(I helped Anna revise this - EY.)</p>\n<p class=\"p1\">A lot of our community technique goes into either (1) dealing with \"beliefs\" being an evolutionarily recent system, such that our \"beliefs\" often end up far screwier than our actual anticipations; or (2) trying to get our anticipations to align with more evidence-informed beliefs.</p>\n<p class=\"p1\">And analogously - this analogy is arguably obvious, but it's deep, useful, and easy to overlook in its implications - there seem to be two major kinds of wanting:</p>\n<ul>\n<li><strong>Urges</strong>: concrete emotional pulls, produced in System 1's perceptual / autonomic processes<br>(my urge to drink the steaming hot cocoa in front of me; my urge to avoid embarrassment by having something to add to my accomplishments log)</li>\n<li><strong>Goals</strong>: things we tell ourselves we\u2019re aiming at, within deliberate/verbal thought and planning<br>(I have a goal to exercise three times a week; I have a goal to reduce existential risk)</li>\n</ul>\n<p><strong id=\"Implication_1___You_can_import_a_lot_of_technique_for__checking_for_screwy_beliefs__into__checking_for_screwy_goals__\">Implication 1: &nbsp;You can import a lot of technique for \"checking for screwy beliefs\" into \"checking for screwy goals\".<a id=\"more\"></a><br></strong></p>\n<p>Urges, like anticipations, are relatively perceptual-level and automatic. &nbsp;They're harder to reshape and they're also harder to completely screw up. &nbsp;In contrast, the flexible, recent \"goals\" system can easily acquire goals that are wildly detached from what we actually do, wildly detached from any positive consequences, or both. &nbsp;Some techniques you can port straight over from \"checking for screwy beliefs\" to \"checking for screwy goals\" include:</p>\n<p>The fundamental:</p>\n<ul>\n<li>\"What's the positive consequence?\" &nbsp;This is the equivalent of \"What's the evidence?\" for beliefs. &nbsp;All the other cases involve not asking it, or not asking hard enough.</li>\n</ul>\n<p>The Hansonian:</p>\n<ul>\n<li><em><a href=\"http://hanson.gmu.edu/belieflikeclothes.html\">Goals as clothes</a>&nbsp;/&nbsp;<a href=\"/lw/i7/belief_as_attire/\">goals as tribal affiliation:</a>&nbsp;&nbsp;</em>\u201c<em>We</em>&nbsp;are people who have free software (/ communism / rationality / whatever) as our goal\u201d. &nbsp;Before you install Linux, do you think \"What's the positive consequence of installing Linux?\" or does it just seem like the sort of thing a free-software-supporter would do? &nbsp;(EY says: &nbsp;What&nbsp;<em>positive consequence</em>&nbsp;is achieved by marching in an Occupy Wall Street march? &nbsp;Can you remember anyone stating one, throughout the whole affair - \"if we march, X will happen because of Y\"?)</li>\n<li><em>Goals as a signal of one\u2019s value as an ally:</em>&nbsp; Sheila insists that she wants to get a job. &nbsp;We inspect her situation and she's not trying very hard to get a job. &nbsp;But she's in debt to a lot of her friends and is borrowing more to live on a month-to-month basis. &nbsp;It's not hard to see why Sheila would internally profess strongly that she has a goal of getting a job.</li>\n<li><em>Goals as&nbsp;<a href=\"/lw/i6/professing_and_cheering/\">personal fashion statements</a>:</em>&nbsp; A T-Shirt that says \u201c<a href=\"http://www.zazzle.com/give+me+coffee+and+no+one+gets+hurt+gifts\">Give me coffee and no one gets hurt</a>\u201d seems to state a very strong desire for coffee. &nbsp;This is clearly a goal professed directly to affect how others see you, and it's more a question of affecting a 'style' than anything directly tribal or status-y.</li>\n</ul>\n<p class=\"p2\">The satiating:</p>\n<ul>\n<li><em><em>Having goals as optimism:</em><span style=\"font-style: normal; \">&nbsp; \"I intend to lose weight\" can be created by much the same sort of internal processes that would make you believe \"I will lose weight\", in cases where the goal (belief) would not yet seem very plausible to an outside view.</span></em></li>\n<li><em>Having goals as apparent progress:</em>&nbsp; My current to-do list has \"write thank-you notes for wedding gifts\". &nbsp;This makes me feel like I've appeased the demand for internal attention by having a goal. &nbsp;(EY: &nbsp;I have \"send Anna and Carl their wedding gift\" on my todo list. &nbsp;This was very effective at appeasing the need to send them a wedding gift.)</li>\n</ul>\n<p><strong id=\"Implication_2____Status_____prestige_____signaling_____people_don_t_really_care_about__is_way_overused_to_explain_goal_urge_delinkages_that_can_be_more_simply_explained_by__humans_are_not_agents__\">Implication 2: &nbsp;\"Status\" / \"prestige\" / \"signaling\" / \"people don't really care about\" is way overused to explain goal-urge delinkages that can be more simply explained by \"humans are not agents\".</strong></p>\n<p>This post was written partially in response to <a href=\"/lw/8gv/the_curse_of_identity/\">The Curse of Identity</a>, wherein&nbsp;Kaj recounts some suboptimal goal-action linkages - wanting to contribute to the Singularity, then teaching himself to feel guilty whenever not working; founding the Finnish Pirate Party, then becoming the spokesperson which involved tasks he wasn't good at; helping Eliezer on writing his book, and feeling demotivated because it seemed like work \"anyone could do\" (which is just the sort of work that almost nobody is motivated to do).</p>\n<p>Kaj forms the generalization \"as soon as my brain adopted a cause, my subconscious reinterpreted it as the goal of giving the impression of doing prestigious work for the cause\". &nbsp;I worry that our community has a tendency to explain as e.g. status signaling&nbsp;or \"people really don't care about X\", observations that can also be explained by less malice/selfishness and more \"our brains have known malfunctions at linking goals to urges\". &nbsp;People are as bad at looking into hospitals for their own health as for the sake of their parents' health; Kaj didn't actually gain much prestige from feeling guilty about his relaxation time.</p>\n<p>We <em>do</em>&nbsp;have a status urge. &nbsp;It <em>does</em>&nbsp;affect a lot of things. &nbsp;People <em>do</em>&nbsp;tend to massively systematically understate it in much the same way that Victorians pretended that sex wasn't everywhere. &nbsp;But that's not the <em>same</em>&nbsp;cognitive problem as \"Our brain is pretty bad at linking effective behaviors to goals, and will sometimes reward us for just doing things that seem roughly associated with the goal, instead of actions that cause the consequence of the goal being achieved.\" &nbsp;And our brains not being coherent agents is something that's even more massive than status.</p>\n<ul>\n</ul>\n<p class=\"p2\"><strong id=\"Implication_3___Humans_cannot_live_by_urges_alone\">Implication 3: &nbsp;Humans cannot live by urges alone</strong></p>\n<p class=\"p1\">Like beliefs, goals often get much wackier than urges. &nbsp;I've seen a number of people react to this realization by concluding that they should give up on having goals, and lead an authentic life of pure desire. &nbsp;This wouldn't work any more than giving up on having beliefs. &nbsp;<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">To precisely anticipate how long it takes a ball to fall off a tower</a>, you have to manipulate abstract beliefs about gravitational acceleration.&nbsp;&nbsp;I have an&nbsp;<em>urge</em>&nbsp;to drive a car that runs smoothly, but if I didn't also have a&nbsp;<em>goal</em>&nbsp;of having a well-maintained car, I would never get around to having it serviced - I have no innate urge to do that.</p>\n<p class=\"p1\">I really have seen multiple people (some of whom I significantly cared about) malfunctioning as a result of misinterpreting this point. &nbsp;As a stand-alone system for pulling your actions, urges have all kinds of problems.&nbsp; Urges can pull you to stare at an attractive stranger, to walk to the fridge, and even to sprint hard for first base when playing baseball.&nbsp; But unless coupled with goals and far-mode reasoning, urges will not pull you to the component tasks required for any longer-term goods. &nbsp;When I get into my car I have a definite urge for it not to be broken. &nbsp;But absent planning, there would never be a moment when the activity I most desired was to take my car for an oil change. &nbsp;To find and keep a job (let alone a good job), live in a non-pigsty, or learn any skills that are not immediately rewarding, you will probably need goals.&nbsp; <em>Even though</em> human goals can easily turn into fashion statements and wishful thinking.</p>\n<p class=\"p1\"><strong id=\"Implication_4___Your_agency_failures_do_not_imply_that_your_ideals_are_fake_\">Implication 4: &nbsp;Your agency failures do not imply that your ideals are fake.</strong></p>\n<p class=\"p1\">Obvious but it needs to be said: &nbsp;People are as bad at looking into hospitals for their own health as for the sake of their parents' health. &nbsp;It doesn't mean that they don't really care about their parents, and it doesn't mean that they don't really care about survival. &nbsp;They would probably run away pretty fast from a tiger, where the goal connected to the urge in an ancestrally more reliable way and hence made them more 'agenty'; and they might fight hard to defend their parents from a tiger too.</p>\n<p class=\"p1\">There's a very real sense in which our agency failures imply that human beings <a href=\"/lw/6ha/the_blueminimizing_robot/\">don't have goals,</a>&nbsp;but this doesn't mean that our ungoaly ideals are any more ungoaly than anything else. &nbsp;Ideals can be more ungoaly because they're sometimes about faraway things or less ancestral things - it's probably <em>easier</em>&nbsp;to improve your agency on less idealy goals that link more quickly to urges - but as entities which can look over our own urges and goals and try to improve our agentiness, there's no rule which says that we can't try to solve some hard problems in this area as well as some easy ones.[2]</p>\n<p class=\"p1\"><strong id=\"Implication_5___You_can_align_urges_and_goals_using_the_same_sort_of_effort_and_training_that_it_takes_to_align_anticipations_and_beliefs_\">Implication 5: &nbsp;You can align urges and goals using the same sort of effort and training that it takes to align anticipations and beliefs.</strong></p>\n<p class=\"p1\">Although I've heard people saying that we discuss willpower-failure too much on Less Wrong, most of the best stuff I've read has been outside Less Wrong and hasn't made contact with us. &nbsp;For a starting guide to many such skills, see&nbsp;<em>Eat That Frog</em>&nbsp;by Brian Tracy [3]. &nbsp;Some basic alignment techniques include:</p>\n<ul>\n<li>Get in the habit of asking \"What is the positive consequence?\" &nbsp;(Probably more needs to be written about this so that your brain doesn't just answer \"I'll be a free software supporter!\" which is not what we mean to ask.)</li>\n<li>Andrew Critch's \"greedy algorithm\": &nbsp; Whenever you catch yourself <em>really wanting</em>&nbsp;to do something you <em>want to want,</em>&nbsp;immediately reward yourself - by feeding yourself an M&amp;M, or if that's too difficult, immediately pumping your fist and saying \"Yes!\"</li>\n<li>Whenever you sit down to work, naming a single, high-priority accomplishment for that session. &nbsp;Visualizing that accomplishment, and its positive rewarding consequences, until you <em>have an urge for it to happen</em> (instead of just having an urge to log today's hours).</li>\n</ul>\n<p>And much the same way that a lot of craziness stems, not so much from \"having a wrong model of the world\", as \"not bothering to have a model of the world\", a lot of personal effectiveness isn't so much about \"having the right goals\" as \"bothering to have goals at all\" - where unpacking this somewhat Vassarian statement would lead us to ideas like \"bothering to have <em>something</em>&nbsp;that I check my actions' consequences against, never mind whether or not it's the right thing\" or \"bothering to have <em>some</em>&nbsp;communication-related urge that animates my writing when I write, instead of just sitting down to log a certain number of writing hours during which I feel rewarded from rearranging shiny words\". &nbsp;</p>\n<p><strong id=\"Conclusion_\">Conclusion:</strong></p>\n<p>Besides an aspiring rationalist, these days I call myself an \"aspiring consequentialist\".</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>[1] IMO the case of somebody who has the belief \"I am good at chess\", but instinctively knows to avoid strong chess opponents that would potentially test the belief, ought to be a more central example in our literature than <a href=\"/lw/i4/belief_in_belief/\">the person who believes they have an dragon in their garage</a>&nbsp;(but instinctively knows that they need to specify that it's invisible, inaudible and generates no carbon dioxide, when we show up with the testing equipment).</p>\n<p>[2] See also <a href=\"http://www.fanfiction.net/s/5782108/20/Harry_Potter_and_the_Methods_of_Rationality\">Ch. 20 of Methods of Rationality</a>:</p>\n<p>Professor Quirrell: &nbsp;\"Mr. Potter, in the end people all do what they want to do. Sometimes people give names like 'right' to things they want to do, but how could we possibly act on anything but our own desires?\"</p>\n<p>Harry: &nbsp;\"Well, obviously I couldn't act on moral considerations if they lacked the power to move me. But that doesn't mean my wanting to hurt those Slytherins has the power to move me more than moral considerations!\"</p>\n<p>[3] Thanks to Patri for <a href=\"/lw/2p5/humans_are_not_automatically_strategic/2l50\">recommending</a> this book to me in response to an earlier post. It is perhaps not written in the most LW-friendly language -- but, given the value of these skills, I\u2019d recommend wading in and doing your best to pull useful techniques from the somewhat salesy prose.&nbsp; I found much of value there.</p>", "sections": [{"title": "Implication 1: \u00a0You can import a lot of technique for \"checking for screwy beliefs\" into \"checking for screwy goals\".", "anchor": "Implication_1___You_can_import_a_lot_of_technique_for__checking_for_screwy_beliefs__into__checking_for_screwy_goals__", "level": 1}, {"title": "Implication 2: \u00a0\"Status\" / \"prestige\" / \"signaling\" / \"people don't really care about\" is way overused to explain goal-urge delinkages that can be more simply explained by \"humans are not agents\".", "anchor": "Implication_2____Status_____prestige_____signaling_____people_don_t_really_care_about__is_way_overused_to_explain_goal_urge_delinkages_that_can_be_more_simply_explained_by__humans_are_not_agents__", "level": 1}, {"title": "Implication 3: \u00a0Humans cannot live by urges alone", "anchor": "Implication_3___Humans_cannot_live_by_urges_alone", "level": 1}, {"title": "Implication 4: \u00a0Your agency failures do not imply that your ideals are fake.", "anchor": "Implication_4___Your_agency_failures_do_not_imply_that_your_ideals_are_fake_", "level": 1}, {"title": "Implication 5: \u00a0You can align urges and goals using the same sort of effort and training that it takes to align anticipations and beliefs.", "anchor": "Implication_5___You_can_align_urges_and_goals_using_the_same_sort_of_effort_and_training_that_it_takes_to_align_anticipations_and_beliefs_", "level": 1}, {"title": "Conclusion:", "anchor": "Conclusion_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "71 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tAXrD8Y6hcJ8dt6Nt", "PBRWb2Em5SNeWYwwB", "qjSHfbjmSyMnGR9DS", "yDRX2fdkm3HqfTpav", "CqyJzDZWvGhhFJ7dY", "mja6jZ6k9gAwki9Nu", "nYkMLFpx77Rz3uo9c", "RmCjazjupRGcHSm5N", "a7n8GdKiAZRX86T5A", "hQHuXuRGZxxWXaPgg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-25T04:05:09.906Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh Meetup: Big Gaming Fun 3!", "slug": "meetup-pittsburgh-meetup-big-gaming-fun-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:35.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kenoubi", "createdAt": "2011-03-12T04:07:00.560Z", "isAdmin": false, "displayName": "Kenoubi"}, "userId": "DgrXt6eQMpunHRDXh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W7NvD48fREfTrBRaN/meetup-pittsburgh-meetup-big-gaming-fun-3", "pageUrlRelative": "/posts/W7NvD48fREfTrBRaN/meetup-pittsburgh-meetup-big-gaming-fun-3", "linkUrl": "https://www.lesswrong.com/posts/W7NvD48fREfTrBRaN/meetup-pittsburgh-meetup-big-gaming-fun-3", "postedAtFormatted": "Wednesday, January 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%203!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%203!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7NvD48fREfTrBRaN%2Fmeetup-pittsburgh-meetup-big-gaming-fun-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%203!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7NvD48fREfTrBRaN%2Fmeetup-pittsburgh-meetup-big-gaming-fun-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7NvD48fREfTrBRaN%2Fmeetup-pittsburgh-meetup-big-gaming-fun-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/6h\">Pittsburgh Meetup: Big Gaming Fun 3!</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">29 January 2012 01:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">1324 Wightman St., Pittsburgh, PA 15217</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>You can see my game collection <a rel=\"nofollow\" href=\"http://boardgamegeek.com/collection/user/Kenoubi?rankobjecttype=subtype&amp;rankobjectid=1&amp;columns=title%7Cstatus%7Cversion%7Crating%7Cbggrating%7Cplays%7Ccomment%7Ccommands&amp;geekranks=Board+Game+Rank&amp;own=1&amp;ff=1&amp;subtype=boardgame\">here</a>; please bring anything else you'd like to play. We can order food and go as late as 19:00. If I get paged I may have to deal with an emergency (from home, using my laptop), but if that doesn't bother you, it doesn't bother me. I have a cat. Please let me know if you're allergic and need me to put her upstairs. RSVP here or by sending me a private message (but don't not show up because you didn't RSVP, I just want a rough idea of the number of attendees). Ring the bell, knock, or call or text (412) 657-1395 to get in when you get there. I intend to hold these every 2-3 weeks, so watch this space!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/6h\">Pittsburgh Meetup: Big Gaming Fun 3!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W7NvD48fREfTrBRaN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.373776563801236e-07, "legacy": true, "legacyId": "12393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_3_\">Discussion article for the meetup : <a href=\"/meetups/6h\">Pittsburgh Meetup: Big Gaming Fun 3!</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">29 January 2012 01:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">1324 Wightman St., Pittsburgh, PA 15217</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>You can see my game collection <a rel=\"nofollow\" href=\"http://boardgamegeek.com/collection/user/Kenoubi?rankobjecttype=subtype&amp;rankobjectid=1&amp;columns=title%7Cstatus%7Cversion%7Crating%7Cbggrating%7Cplays%7Ccomment%7Ccommands&amp;geekranks=Board+Game+Rank&amp;own=1&amp;ff=1&amp;subtype=boardgame\">here</a>; please bring anything else you'd like to play. We can order food and go as late as 19:00. If I get paged I may have to deal with an emergency (from home, using my laptop), but if that doesn't bother you, it doesn't bother me. I have a cat. Please let me know if you're allergic and need me to put her upstairs. RSVP here or by sending me a private message (but don't not show up because you didn't RSVP, I just want a rough idea of the number of attendees). Ring the bell, knock, or call or text (412) 657-1395 to get in when you get there. I intend to hold these every 2-3 weeks, so watch this space!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_3_1\">Discussion article for the meetup : <a href=\"/meetups/6h\">Pittsburgh Meetup: Big Gaming Fun 3!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh Meetup: Big Gaming Fun 3!", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_3_", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh Meetup: Big Gaming Fun 3!", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_3_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-25T07:50:07.841Z", "modifiedAt": null, "url": null, "title": "Occam alternatives", "slug": "occam-alternatives", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:57.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "selylindi", "createdAt": "2011-09-09T21:28:32.727Z", "isAdmin": false, "displayName": "selylindi"}, "userId": "mCEGvAPETMXvdoc8k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dw8w3PNy9JanSJc8s/occam-alternatives", "pageUrlRelative": "/posts/dw8w3PNy9JanSJc8s/occam-alternatives", "linkUrl": "https://www.lesswrong.com/posts/dw8w3PNy9JanSJc8s/occam-alternatives", "postedAtFormatted": "Wednesday, January 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Occam%20alternatives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOccam%20alternatives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdw8w3PNy9JanSJc8s%2Foccam-alternatives%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Occam%20alternatives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdw8w3PNy9JanSJc8s%2Foccam-alternatives", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdw8w3PNy9JanSJc8s%2Foccam-alternatives", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 834, "htmlBody": "<p>One of the most delightful things I learned while on LessWrong was the Solomonoff/Kolmogorov formalization of Occam's Razor.&nbsp; Added to what had previously been only an aesthetic heuristic to me were mathematical rigor, proofs of optimality of certain kinds, and demonstrations of utility.&nbsp; For several months I was quite taken with it in what now appears to me to be a rather uncritical way.&nbsp; In doing some personal research (comparing and contrasting Marian apparitions with UFO sightings), I encountered for the first time people who explicity rejected Occam's Razor.&nbsp; They didn't have anything to replace it with, but it set off a search for me to find some justification for Occam's Razor beyond aesthetics.&nbsp; What I found wasn't particularly convincing, and in discussion with a friend, we concluded that Occam's Razor feels conceptually wrong to us.</p>\n<p>First, some alternatives for perspective:</p>\n<p><strong>Occam's Razor</strong>: Avoid needlessly multiplying entities.</p>\n<p><em>All else being equal, the simplest explanation is usually correct.</em></p>\n<p>(Solomonoff prior) The likelihood of a hypothesis that explains the data is proportional to 2^(-L) for L, the length of the shortest code that produces a description of at least that hypothesis. <br /><br />(speed prior) The likelihood of a hypothesis that explains the data is proportional to 2^(-L-N) for L, the length of the shortest code that produces a description of at least that hypothesis, and N, the number of calculations to get from the code to the description.</p>\n<p><strong>Lovejoy's Cornucopia</strong>:&nbsp; Expect everything.</p>\n<p><em>If you consider it creatively enough, all else is always equal. </em><br />&nbsp;<br />(ignorance prior)&nbsp; Equally weight all hypotheses that explain the data. <br />&nbsp;<br /><strong>Crabapple's Bludgeon</strong>:&nbsp; Don't demand it makes sense. <br />&nbsp;<br /><em>No set of mutually inconsistent observations can exist for which some human intellect cannot conceive a coherent explanation, however complicated.&nbsp; The world may be not only stranger than you know, but stranger than you can know. </em><br />&nbsp;<br />(skeptics' prior)&nbsp; The likelihood of a hypothesis is inversely proportional to the number of observations it purports to explain. <br />&nbsp;<br /><strong>Pascal's Goldpan</strong>:&nbsp; Make your beliefs pay rent. <br />&nbsp;<br /><em>All else being equal, the most useful explanation is usually correct. </em></p>\n<p>(utilitarian prior) The likelihood of a hypothesis is proportional to the expected net utility of the agent believing it.</p>\n<p><strong>Burke's Dinghy</strong>:&nbsp; Never lose sight of the shore. <br />&nbsp;<br /><em>All else being equal, the nearest explanation is usually correct. </em><br />&nbsp;<br />(conservative prior) The likelihood of a new hypothesis that explains the data is proportional to the Solomonoff prior for the Kolmogorov complexity of the code that transforms the previously accepted hypothesis into the new hypothesis.</p>\n<p><strong>Orwell's Applecart</strong>:&nbsp; Don't upset the applecart. <br />&nbsp;<br /><em>Your leaders will let you know which explanation is correct. </em><br />&nbsp;<br />(social prior) The likelihood of a hypothesis is proportional to how recently and how often it has been proposed and to the social status of its proponents.</p>\n<p>&nbsp;</p>\n<p>Obviously, some of those are more realistic than others.&nbsp; The one that initially leapt out at me was what I'll call Pascal's Goldpan.&nbsp; Granted, a human trying to understand the world and using the Goldpan would likely settle on largely the same theories as a human using the Razor since simple theories have practical advantages for our limited mental resources.&nbsp; But ideally, it seems to me that a rational agent trying to maximize its utility only cares about the truth insofar as truth helps it maximize its utility.</p>\n<p>The illustration that immediately sprung to my mind was of the characters Samantha Carter and Jack O'Neill in the television sci-fi show Stargate: SG1.&nbsp; Rather frequently in the series, these two characters became stuck in a situation of impending doom and they played out the same stock responses.&nbsp; Carter, the scientist, quickly realized and lucidly explained just how screwed they were according to the simplest explanation, and so optimized her utility under the circumstances and essentially began preparing for a good death.&nbsp; O'Neill, the headstrong leader, dismissed her reasoning out of hand and instead pursued the most practical course of action with a chance of getting them rescued.&nbsp; My aesthetics prefers the O'Neill way over the Carter way: the Goldpan over the Razor.</p>\n<p>Though it is no evidence at all, it is also aesthetically pleasing to me that the Goldpan unifies the Platonic values of truth, goodness, and beauty into a single primitive.&nbsp; I also like that it suggests an alternative to Tarski's definition of \"truth\".&nbsp; A proposition is true if the use of its content would be beneficial in all relevant utility functions.&nbsp; A proposition is false if the use of its content would be harmful in all relevant utility functions.&nbsp; A proposition is partly true and partly false if the use of its content would be beneficial for some relevant utility functions and harmful for others.&nbsp; A proposition can be neutral by being inapplicable or irrelevant to all relevant utility functions.</p>\n<p>Critiques encouraged; I've no special commitment to the Goldpan.&nbsp; Are there good reasons to prefer Occam's Razor?&nbsp; Are there other hypothesis weighting heuristics with practical or theoretical interest?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dw8w3PNy9JanSJc8s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 30, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "12398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-25T17:43:40.601Z", "modifiedAt": null, "url": null, "title": "I've had it with those dark rumours about our culture rigorously suppressing opinions", "slug": "i-ve-had-it-with-those-dark-rumours-about-our-culture", "viewCount": null, "lastCommentedAt": "2018-08-02T13:20:22.249Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Multiheaded", "createdAt": "2011-07-02T10:10:20.692Z", "isAdmin": false, "displayName": "Multiheaded"}, "userId": "moGiw35FowgiAnzfg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T8Huvskn2Ab5m8wkx/i-ve-had-it-with-those-dark-rumours-about-our-culture", "pageUrlRelative": "/posts/T8Huvskn2Ab5m8wkx/i-ve-had-it-with-those-dark-rumours-about-our-culture", "linkUrl": "https://www.lesswrong.com/posts/T8Huvskn2Ab5m8wkx/i-ve-had-it-with-those-dark-rumours-about-our-culture", "postedAtFormatted": "Wednesday, January 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I've%20had%20it%20with%20those%20dark%20rumours%20about%20our%20culture%20rigorously%20suppressing%20opinions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI've%20had%20it%20with%20those%20dark%20rumours%20about%20our%20culture%20rigorously%20suppressing%20opinions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT8Huvskn2Ab5m8wkx%2Fi-ve-had-it-with-those-dark-rumours-about-our-culture%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I've%20had%20it%20with%20those%20dark%20rumours%20about%20our%20culture%20rigorously%20suppressing%20opinions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT8Huvskn2Ab5m8wkx%2Fi-ve-had-it-with-those-dark-rumours-about-our-culture", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT8Huvskn2Ab5m8wkx%2Fi-ve-had-it-with-those-dark-rumours-about-our-culture", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 660, "htmlBody": "<p>You folks probably know how some posters around here, specifically Vladimir_M, often make statements to the effect of:</p>\n<p>&nbsp;</p>\n<p>\"There's an opinion on such-and-such topic that's so against the memeplex of Western culture, we can't even discuss it in open-minded, pseudonymous forums like Less Wrong as society would instantly slam the lid on it with either moral panic or ridicule and give the speaker a black mark.</p>\n<p>Meanwhile the thought patterns instilled in us by our upbringing would lead us to quickly lose all interest in the censored opinion\"</p>\n<p>Going by their definition, us blissfully ignorant masses can't even know what exactly those opinions might be, as they would look like basic human decency, the underpinnings of our ethics or some other such sacred cow to us. I might have a few guesses, though, all of them as horrible and sickening as my imagination could produce without overshooting and landing in the realm of comic-book evil:</p>\n<p>- Dictatorial rule involving active terror and brutal suppression of deviants having great utility for a society in the long term, by providing security against some great risk or whatever.</p>\n<p>- A need for every society to \"cull the weak\" every once in a while, e.g. exterminating the ~0.5% of its members that rank as weakest against some scale.</p>\n<p>- Strict hierarchy in everyday life based on facts from the ansectral environment (men dominating women, fathers having the right of life and death over their children, etc) - <em>Mencius argued in favor of such ruthless practices, e.g. selling children into slavery, in his post on \"Pronomianism\" and \"Antinomianism\", stating that all contracts between humans should rather be strict than moral or fair, to make the system stable and predictable; he's quite obsessed with stability and conformity.</em></p>\n<p>- Some public good being created when the higher classes wilfully oppress and humiliate the lower ones in a ceremonial manner</p>\n<p>- The bloodshed and lawlessness of periodic large-scale war as a vital \"pressure valve\" for releasing pent-up unacceptable emotional states and instinctive drives</p>\n<p>- Plain ol' unfair discrimination of some group in many cruel, life-ruining ways, likewise as a pressure valve</p>\n<p><strong>+:&nbsp; </strong>some Luddite crap about dropping to a near-subsistence level in every aspect of civilization and making life a daily struggle for survival</p>\n<p>Of course my methodology for coming up with such guesses was flawed and primitive: I simply imagined some of the things that sound the ugliest to me yet have been practiced by unpleasant cultures before in some form. Now, of course, most of us take the absense of these to be utterly crucial to our terminal values. Nevertheless, I hope I have demonstrated to whoever might really have something along these lines (if not necessarily that shocking) on their minds that I'm open to meta-discussion, and very interested how we might engage each other on finding safe yet productive avenues of contact.</p>\n<p>&nbsp;</p>\n<p>Let's do the impossible and think the unthinkable! I <strong>must</strong> know what those secrets are, no matter how much sleep and comfort I might lose.</p>\n<p>P.S. Yeah, Will, I realize that I'm acting roughly in accordance with <a href=\"/lw/6kv/guardian_column_on_ugh_fields_mentions_lw/4htf\">that one trick you mentioned way back.</a></p>\n<p>P.P.S. Sup Bakkot. U mad? U jelly?</p>\n<p>&nbsp;</p>\n<p><strong>CONCLUSION:</strong></p>\n<p>&nbsp;</p>\n<p>Fuck this Earth, and fuck human biology. I'm not very distressed about anything I saw ITT, but there's still a lot of unpleasant potential things that can only be resolved in one way:</p>\n<p><strong>I hereby pledge</strong> to get a real goddamn plastic card, not this Visa Electron bullshit the university saddled us with, and donate <strong>at least $100</strong> to SIAI until the end of the year. This action will reduce the probability of me and mine having to live with the consequences of most such hidden horrors. Dixi.</p>\n<p><br />Sometimes it's so pleasant to be impulsive.</p>\n<p>&nbsp;</p>\n<p>Amusing observation: even when the comments more or less match my wild suggestions above, I'm still unnerved by them. An awful idea feels harmless if you keep telling yourself that it's just a private delusion, but the moment you know that someone else shares it, matters begin to look much more grave.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"CYMR6p5iZG75QAT8a": 1, "6Qic6PwwBycopJFNN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T8Huvskn2Ab5m8wkx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 84, "baseScore": 38, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "12399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 866, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-25T21:31:05.607Z", "modifiedAt": null, "url": null, "title": "[META] Karma, its positive and negative component", "slug": "meta-karma-its-positive-and-negative-component", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:38.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3SM5NcupW95PfCsa6/meta-karma-its-positive-and-negative-component", "pageUrlRelative": "/posts/3SM5NcupW95PfCsa6/meta-karma-its-positive-and-negative-component", "linkUrl": "https://www.lesswrong.com/posts/3SM5NcupW95PfCsa6/meta-karma-its-positive-and-negative-component", "postedAtFormatted": "Wednesday, January 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Karma%2C%20its%20positive%20and%20negative%20component&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Karma%2C%20its%20positive%20and%20negative%20component%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SM5NcupW95PfCsa6%2Fmeta-karma-its-positive-and-negative-component%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Karma%2C%20its%20positive%20and%20negative%20component%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SM5NcupW95PfCsa6%2Fmeta-karma-its-positive-and-negative-component", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SM5NcupW95PfCsa6%2Fmeta-karma-its-positive-and-negative-component", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>I'd like to see, beside the cumulative karma as it is now, also its&nbsp;analytic. Does my 0 points somewhere means no upvotes and no downvotes or +7 and -7, for example?</p>\n<p>A timeline graph would also be&nbsp;appreciated.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3SM5NcupW95PfCsa6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 8.377791916195493e-07, "legacy": true, "legacyId": "12401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-25T21:51:52.960Z", "modifiedAt": null, "url": null, "title": "Shit Rationalists Say?", "slug": "shit-rationalists-say", "viewCount": null, "lastCommentedAt": "2019-02-25T23:54:11.317Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8xQ8hTxo6Rk2qqZfj/shit-rationalists-say", "pageUrlRelative": "/posts/8xQ8hTxo6Rk2qqZfj/shit-rationalists-say", "linkUrl": "https://www.lesswrong.com/posts/8xQ8hTxo6Rk2qqZfj/shit-rationalists-say", "postedAtFormatted": "Wednesday, January 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shit%20Rationalists%20Say%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShit%20Rationalists%20Say%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8xQ8hTxo6Rk2qqZfj%2Fshit-rationalists-say%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shit%20Rationalists%20Say%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8xQ8hTxo6Rk2qqZfj%2Fshit-rationalists-say", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8xQ8hTxo6Rk2qqZfj%2Fshit-rationalists-say", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<p>I assume everyone has run across at least one of the \"Shit X's Say\" format of videos? Such as <a href=\"http://www.youtube.com/watch?v=NjyGeDKhEoM&amp;feature=player_embedded\">Shit Skeptics Say</a>. When done right it&nbsp;totally triggers the in-group warm-fuzzies.&nbsp;(Not to be confused with the nearly-identically formatted \"Shit X's Say to Y's\" which is mainly a way for Y's to complain about X's).</p>\n<p>What sort of things do Rationalists often say that triggers this sort of in-group recognition which could be popped into a short video? A few I can think of...</p>\n<p class=\"MsoNormal\">You should sign up for cryonics. I want to see you in the future.</p>\n<p class=\"MsoNormal\">&hellip;intelligence explosion&hellip;</p>\n<p class=\"MsoNormal\">What&rsquo;s your confidence interval?</p>\n<p class=\"MsoNormal\">You know what they say: one man&rsquo;s Modus Ponens is another man&rsquo;s Modus Tollens</p>\n<p class=\"MsoNormal\">This may sound a bit crazy right now, but hear me out&hellip;</p>\n<p class=\"MsoNormal\">What are your priors?</p>\n<p class=\"MsoNormal\">When the singularity comes that won&rsquo;t be a problem anymore.</p>\n<p class=\"MsoNormal\">I like to think I&rsquo;d do that, but I don&rsquo;t fully trust myself. I am running on corrupted hardware after all.</p>\n<p class=\"MsoNormal\">I want to be with you, and I don&rsquo;t foresee that changing in the near future.</p>\n<p class=\"MsoNormal\">&hellip;Bayesian statistics&hellip;</p>\n<p class=\"MsoNormal\">So Omega appears in front of you&hellip;</p>\n<p class=\"MsoNormal\">What would you say the probability of that event is, if your beliefs are true?</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\">Others?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hNFdS3rRiYgqqD8aM": 1, "izp6eeJJEg9v5zcur": 1, "p8nXWqwPH7mPSZf6p": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8xQ8hTxo6Rk2qqZfj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 46, "extendedScore": null, "score": 0.000101, "legacy": true, "legacyId": "12402", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 120, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-25T23:47:49.070Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge UK", "slug": "meetup-cambridge-uk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:38.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rebellionkid", "createdAt": "2011-06-20T09:26:46.768Z", "isAdmin": false, "displayName": "rebellionkid"}, "userId": "ygYCk3eXnJwt6p3o4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kn85k6JzLukPF6XgM/meetup-cambridge-uk", "pageUrlRelative": "/posts/Kn85k6JzLukPF6XgM/meetup-cambridge-uk", "linkUrl": "https://www.lesswrong.com/posts/Kn85k6JzLukPF6XgM/meetup-cambridge-uk", "postedAtFormatted": "Wednesday, January 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20UK&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20UK%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKn85k6JzLukPF6XgM%2Fmeetup-cambridge-uk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20UK%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKn85k6JzLukPF6XgM%2Fmeetup-cambridge-uk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKn85k6JzLukPF6XgM%2Fmeetup-cambridge-uk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6i'>Cambridge UK</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 January 2012 06:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Trinity College JCR, Cambridge, CB2 1TQ, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at the Great Gate if you dont know where the JCR is. {Edit: Great Gate is the gate on St. John's Street that is pretty much opposite the bookshop \"Heffers\".} There were a good number of people at the last Cambridge UK meet. This could become a regular event if enough people are interested.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6i'>Cambridge UK</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kn85k6JzLukPF6XgM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 8.37831705716743e-07, "legacy": true, "legacyId": "12404", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge_UK\">Discussion article for the meetup : <a href=\"/meetups/6i\">Cambridge UK</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 January 2012 06:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Trinity College JCR, Cambridge, CB2 1TQ, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at the Great Gate if you dont know where the JCR is. {Edit: Great Gate is the gate on St. John's Street that is pretty much opposite the bookshop \"Heffers\".} There were a good number of people at the last Cambridge UK meet. This could become a regular event if enough people are interested.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge_UK1\">Discussion article for the meetup : <a href=\"/meetups/6i\">Cambridge UK</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge UK", "anchor": "Discussion_article_for_the_meetup___Cambridge_UK", "level": 1}, {"title": "Discussion article for the meetup : Cambridge UK", "anchor": "Discussion_article_for_the_meetup___Cambridge_UK1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-26T01:30:43.740Z", "modifiedAt": null, "url": null, "title": "[post redacted]", "slug": "post-redacted", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:07.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fHSb89aPmBucECxMd/post-redacted", "pageUrlRelative": "/posts/fHSb89aPmBucECxMd/post-redacted", "linkUrl": "https://www.lesswrong.com/posts/fHSb89aPmBucECxMd/post-redacted", "postedAtFormatted": "Thursday, January 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bpost%20redacted%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bpost%20redacted%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHSb89aPmBucECxMd%2Fpost-redacted%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bpost%20redacted%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHSb89aPmBucECxMd%2Fpost-redacted", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHSb89aPmBucECxMd%2Fpost-redacted", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>[Post redacted 'cuz I unfairly and carelessly misrepresented someone's views (Eliezer's). The messages of this post was: disbelief that aliens visit Earth in spaceships is a bad reason not to look into ufology. My apologies for this ugly post.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fHSb89aPmBucECxMd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -4, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "12405", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-26T15:55:22.746Z", "modifiedAt": null, "url": null, "title": "\"Politics is the mind-killer\" is the mind-killer", "slug": "politics-is-the-mind-killer-is-the-mind-killer", "viewCount": null, "lastCommentedAt": "2015-03-21T03:34:52.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uxsTyFLtSmxmniTzt/politics-is-the-mind-killer-is-the-mind-killer", "pageUrlRelative": "/posts/uxsTyFLtSmxmniTzt/politics-is-the-mind-killer-is-the-mind-killer", "linkUrl": "https://www.lesswrong.com/posts/uxsTyFLtSmxmniTzt/politics-is-the-mind-killer-is-the-mind-killer", "postedAtFormatted": "Thursday, January 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Politics%20is%20the%20mind-killer%22%20is%20the%20mind-killer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Politics%20is%20the%20mind-killer%22%20is%20the%20mind-killer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuxsTyFLtSmxmniTzt%2Fpolitics-is-the-mind-killer-is-the-mind-killer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Politics%20is%20the%20mind-killer%22%20is%20the%20mind-killer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuxsTyFLtSmxmniTzt%2Fpolitics-is-the-mind-killer-is-the-mind-killer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuxsTyFLtSmxmniTzt%2Fpolitics-is-the-mind-killer-is-the-mind-killer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 363, "htmlBody": "<p>Summary: I propose we somewhat relax our stance on political speech on Less Wrong.</p>\n<p>Related: <a href=\"/lw/ee/the_mindkiller/\">The mind-killer</a>, <a href=\"http://wiki.lesswrong.com/wiki/Mind-killer\">Mind-killer</a></p>\n<p><a id=\"more\"></a></p>\n<p>A recent series of posts by a well-meaning troll (<a href=\"/lw/gw/politics_is_the_mindkiller/5r7j\">example</a>) has caused me to re-examine our \"no-politics\" norm. &nbsp;I believe there has been some unintentional creep from the original intent of <a href=\"/lw/gw/politics_is_the_mindkiller\">Politics is the Mind-Killer</a>. &nbsp;In that article, Eliezer is arguing that discussions here (actually on <em>Overcoming Bias</em>) should not use examples from politics in discussions that are not about politics, since they distract from the lesson. &nbsp;Note the final paragraph:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">I'm not saying that I think&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Overcoming Bias</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;should be apolitical, or even that we should adopt Wikipedia's ideal of the&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"http://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view\">Neutral Point of View</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.&nbsp; But try to resist getting in those good, solid digs if you can possibly avoid it.&nbsp; If your topic legitimately relates to attempts to ban evolution in school curricula, then go ahead and talk about it - but don't blame it explicitly on the whole Republican Party; some of your readers may be Republicans, and they may feel that the problem is a few rogues, not the entire party.&nbsp; As with Wikipedia's NPOV, it doesn't matter whether (you think) the Republican Party really&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">is</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;at fault.&nbsp; It's just better for the spiritual growth of the community to discuss the issue without invoking&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"/lw/gt/a_fable_of_science_and_politics/\">color politics</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.</span></p>\n</blockquote>\n<p>So, the original intent was not to ban political speech altogether, but to encourage us to come up with less-charged examples where possible. &nbsp;If the subject you're really talking about is politics, and it relates directly to rationality, then you should be able to post about it without getting downvotes strictly because \"politics is the mind-killer\".</p>\n<p>It could be that this drift is less of a community norm than I perceive, and there are just a few folks (myself included) that have taken the original message too far. &nbsp;If so, consider this a message just to those folks such as myself.</p>\n<p>Of course, politics would <em>still</em>&nbsp;be off-topic in the comment threads of most posts. &nbsp;There should probably be a special open thread (or another forum) to which drive-by political activists can be directed, instead of simply saying \"We don't talk about politics here\".</p>\n<p>David_Gerard makes a similar point <a href=\"/lw/9em/open_thread_january_1531_2012/5onr\">here</a>&nbsp;(though FWIW, I came up with this title independently).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "FkzScn5byCs9PxGsA": 1, "MXcpQvaPGtXpB6vkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uxsTyFLtSmxmniTzt", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 47, "extendedScore": null, "score": 0.000103, "legacy": true, "legacyId": "12424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yfxp4Y6YETjjtChFh", "9weLK2AJ9JEt2Tt8f", "6hfGNLf4Hg5DXqJCF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-01-26T15:55:22.746Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-26T16:18:55.714Z", "modifiedAt": null, "url": null, "title": "[LINK] Learning enhancement using \"transcranial direct current stimulation\"", "slug": "link-learning-enhancement-using-transcranial-direct-current", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.908Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alex_Altair", "createdAt": "2010-07-21T18:30:40.806Z", "isAdmin": false, "displayName": "Alex_Altair"}, "userId": "5wu9jG4pm9q6xjZ9R", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R4jnjwFA9ogx5prCW/link-learning-enhancement-using-transcranial-direct-current", "pageUrlRelative": "/posts/R4jnjwFA9ogx5prCW/link-learning-enhancement-using-transcranial-direct-current", "linkUrl": "https://www.lesswrong.com/posts/R4jnjwFA9ogx5prCW/link-learning-enhancement-using-transcranial-direct-current", "postedAtFormatted": "Thursday, January 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Learning%20enhancement%20using%20%22transcranial%20direct%20current%20stimulation%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Learning%20enhancement%20using%20%22transcranial%20direct%20current%20stimulation%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR4jnjwFA9ogx5prCW%2Flink-learning-enhancement-using-transcranial-direct-current%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Learning%20enhancement%20using%20%22transcranial%20direct%20current%20stimulation%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR4jnjwFA9ogx5prCW%2Flink-learning-enhancement-using-transcranial-direct-current", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR4jnjwFA9ogx5prCW%2Flink-learning-enhancement-using-transcranial-direct-current", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<p>Article here;</p>\n<p><a href=\"http://www.ox.ac.uk/media/science_blog/brainboosting.html\">http://www.ox.ac.uk/media/science_blog/brainboosting.html</a></p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, Sans; font-size: 15px; line-height: 18px;\">Recent research in Oxford and elsewhere has shown that one type of brain stimulation in particular, called transcranial direct current stimulation or TDCS, can be used to improve language and maths abilities, memory, problem solving, attention, even movement.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, Sans; font-size: 15px; line-height: 18px;\">Critically, this is not just helping to restore function in those with impaired abilities. TDCS can be used to enhance healthy people&rsquo;s mental capacities. Indeed, most of the research so far has been carried out in healthy adults.</span></p>\n</blockquote>\n<p>The article goes on to discuss the ethics of the technique.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R4jnjwFA9ogx5prCW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 8.382125528294572e-07, "legacy": true, "legacyId": "12425", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-26T22:13:51.588Z", "modifiedAt": null, "url": null, "title": "Mathematicians & mathletes: the Singularity Institute wants your strategic input!", "slug": "mathematicians-and-mathletes-the-singularity-institute-wants", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:05.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BRuo7recRdt6WFiAo/mathematicians-and-mathletes-the-singularity-institute-wants", "pageUrlRelative": "/posts/BRuo7recRdt6WFiAo/mathematicians-and-mathletes-the-singularity-institute-wants", "linkUrl": "https://www.lesswrong.com/posts/BRuo7recRdt6WFiAo/mathematicians-and-mathletes-the-singularity-institute-wants", "postedAtFormatted": "Thursday, January 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mathematicians%20%26%20mathletes%3A%20the%20Singularity%20Institute%20wants%20your%20strategic%20input!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMathematicians%20%26%20mathletes%3A%20the%20Singularity%20Institute%20wants%20your%20strategic%20input!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRuo7recRdt6WFiAo%2Fmathematicians-and-mathletes-the-singularity-institute-wants%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mathematicians%20%26%20mathletes%3A%20the%20Singularity%20Institute%20wants%20your%20strategic%20input!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRuo7recRdt6WFiAo%2Fmathematicians-and-mathletes-the-singularity-institute-wants", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRuo7recRdt6WFiAo%2Fmathematicians-and-mathletes-the-singularity-institute-wants", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>The Singularity Institute is undergoing a series of important strategic discussions. There are many questions for which we wish we had more confident answers. We can get more confident answers on <em>some</em>&nbsp;of them by asking top-level mathematicians &amp; mathletes (e.g. Putnam fellow, IMO top score, or successful academic mathematician / CS researcher).</p>\n<p>If you are such a person and want to <em>directly</em> affect Singularity Institute strategy, contact me at luke@intelligence.org.</p>\n<p>Thank you.</p>\n<p>Now back to your regularly scheduled rationality programming...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BRuo7recRdt6WFiAo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 8.383490142999941e-07, "legacy": true, "legacyId": "12426", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T01:38:22.201Z", "modifiedAt": null, "url": null, "title": "What's going on here?", "slug": "what-s-going-on-here", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:51.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gW6dww3TA9o8wLoh9/what-s-going-on-here", "pageUrlRelative": "/posts/gW6dww3TA9o8wLoh9/what-s-going-on-here", "linkUrl": "https://www.lesswrong.com/posts/gW6dww3TA9o8wLoh9/what-s-going-on-here", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20going%20on%20here%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20going%20on%20here%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW6dww3TA9o8wLoh9%2Fwhat-s-going-on-here%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20going%20on%20here%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW6dww3TA9o8wLoh9%2Fwhat-s-going-on-here", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW6dww3TA9o8wLoh9%2Fwhat-s-going-on-here", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 620, "htmlBody": "<p>Edit: Please stop upvoting me. I'm beyond where I was when this whole mess started. Thanks, and I feel really stupid for the way I felt when I wrote this first paragraph now.</p>\n<p>&nbsp;</p>\n<p>First of all, I'm sorry for this thread. I largely expect it to get downvoted, but I am considering leaving LW over this, and I thought I would make one more effort to actually get some answers before I did. Because I'm pretty confused. I've never seen something like this happen on LW before. I've been downvoted faster than any troll I've ever seen on here.</p>\n<p>Three days ago, I <a href=\"/r/discussion/lw/9jk/tell_lesswrong_about_your_charitable_donations/5qnt\">got into a thread</a> over a point that was admittedly pretty pedantic and minor. For two days, I got mildly downvoted (average of about 0.5 downvotes per comment), which was no big deal. But a day later, all of my comments in that thread started getting wildly downvoted. I lost over 100 karma in an hour, and have lost another 100 since. This was entirely on comments, since I have no main level posts that would easily kill my karma that quickly. Interestingly, comments that had stabilized at positive karma <em>also</em> got downvoted, even though I had made them, in some cases, a week before the downvoting began.</p>\n<p>Specific examples:<br /><a href=\"/lw/9ii/crockers_rules_how_far_to_take_it/5qnu\">Was at 0 karma for approximately 24 hours</a>. Now at -5.<br /><a href=\"/lw/9gy/the_singularity_institutes_arrogance_problem/5pd0\">Was somewhere from +1 to +3 (not sure) for five days</a>. Now at -7.<br /><a href=\"/lw/9gy/the_singularity_institutes_arrogance_problem/5pct\">Was +2 for five days</a>. Now at -7.<br /><a href=\"/lw/9gy/the_singularity_institutes_arrogance_problem/5pao\">Was at +2 for five days</a>. Now at -3.<br /><a href=\"/lw/9gy/the_singularity_institutes_arrogance_problem/5p6f\">Was at +1 for six days</a>. Now at -1.<br /><a href=\"/lw/9gy/the_singularity_institutes_arrogance_problem/5p50\">Was at +6 for a week</a>. Now at +2.</p>\n<p><a href=\"/lw/9gy/the_singularity_institutes_arrogance_problem/5p7b\">Was at -1 or -2 for about five days</a>. Now at -6. Note: This started at negative karma. Although that fluctuated highly around the time of posting. It was anywhere from -4 to +1, I believe.</p>\n<p>Initially, when this happened, I <a href=\"/lw/9jk/tell_lesswrong_about_your_charitable_donations/5qz2\">thought</a> (p = 0.75) I was being <a href=\"/lw/77b/please_do_not_downvote_every_comment_or_post/4oct\">karmassassinated</a>. Now, I am not entirely sure what to think (hence this thread).</p>\n<p>I postulate a few hypotheses:<br />1. I am being karmassassinated.<br />2. All of the downvotes are legitimate downvotes, caused largely by halo effects - ie. when people see a largely downvoted comment by me, they go to my profile to read my other comments and downvote those as well.<br />3. The downvotes on my previously upvoted comments were expressions of a desire to give more than one downvote per comment to the thread that started this.</p>\n<p>My prior, given over 100 karma lost in an hour, obviously, was karmassassination. But it would be difficult for a karmassassin to give large amounts of negative karma to a relatively few number of posts, however, given the restrictions on new accounts to downvote. (I don't think you can downvote until you have positive karma?) Given this and the consistent and steady loss of the additional 100 karma over the last two days, however, I am updating towards 2. At this point, I would assign 50% probability that it is at least some of both.</p>\n<p>I know that fishing for karma is almost always seen as poor taste and downvoted. I don't mean to do that. I don't really care <em>that</em> much. (Although losing 33% of my karma was a pretty big slap in the face.) The thing I care most about, is probably my related <a href=\"http://predictionbook.com/predictions/4781\">PredictionBook prediction</a>, which was one of the first things that actually crossed my mind. What I would like, is explanations of why others think this might happen. <a href=\"/r/discussion/lw/9jk/tell_lesswrong_about_your_charitable_donations/5qyv\">Am I so incorrect to have deserved over 200 individual downvotes?</a> If the downvotes on my previously upvoted comments are legitimate, why do those deserve downvoting? I am really at a loss here.</p>\n<p>&nbsp;</p>\n<p>Edited to fix hyperlink.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gW6dww3TA9o8wLoh9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 20, "extendedScore": null, "score": 8.384276607618504e-07, "legacy": true, "legacyId": "12427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T02:31:24.690Z", "modifiedAt": null, "url": null, "title": "Informed consent bias in RCTs?", "slug": "informed-consent-bias-in-rcts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:38.727Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "neq1", "createdAt": "2010-03-31T12:30:01.450Z", "isAdmin": false, "displayName": "neq1"}, "userId": "YcYNXZpDcpBGgJR2u", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DmxkHEnkddz6oshb2/informed-consent-bias-in-rcts", "pageUrlRelative": "/posts/DmxkHEnkddz6oshb2/informed-consent-bias-in-rcts", "linkUrl": "https://www.lesswrong.com/posts/DmxkHEnkddz6oshb2/informed-consent-bias-in-rcts", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Informed%20consent%20bias%20in%20RCTs%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInformed%20consent%20bias%20in%20RCTs%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDmxkHEnkddz6oshb2%2Finformed-consent-bias-in-rcts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Informed%20consent%20bias%20in%20RCTs%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDmxkHEnkddz6oshb2%2Finformed-consent-bias-in-rcts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDmxkHEnkddz6oshb2%2Finformed-consent-bias-in-rcts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p>The problem of published research findings not being reliable has been <a href=\"/lw/7aa/why_no_archive_of_refuted_research/\">discussed</a> <a href=\"/lw/72f/why_epidemiology_will_not_correct_itself/\">here</a> <a href=\"/lw/2y3/dealing_with_the_high_quantity_of_scientific/\">before</a>.&nbsp; <br /><br />One problem with RCTs that has received little attention is that, due to informed consent laws and ethical considerations, subjects are aware that they might be receiving sham therapy.&nbsp; This differs from the environment outside of the research setting, where people are confident that whatever their doctor prescribes is what they will get from their pharmacist.&nbsp; I can imagine many ways in which subjects' uncertainty about treatment assignment could affect outcomes (adherence is one possible mechanism).&nbsp; I wrote a short paper about this, focusing out what we would ideally estimate if we could lie to subjects, versus what we actually can estimate in RCTs (<a href=\"http://www.sciencedirect.com/science/article/pii/S1551714411002473\">link</a>).&nbsp;&nbsp; Here is the abstract:</p>\n<blockquote>\n<p><br />It is widely recognized that traditional randomized controlled trials  (RCTs) have limited generalizability due to the numerous ways in which  conditions of RCTs differ from those experienced each day by patients  and physicians. As a result, there has been a recent push towards  pragmatic trials that better mimic real-world conditions. One way in  which RCTs differ from normal everyday experience is that all patients  in the trial have uncertainty about what treatment they were assigned.  Outside of the RCT setting, if a patient is prescribed a drug then there  is no reason for them to wonder if it is a placebo. Uncertainty about  treatment assignment could affect both treatment and placebo response.  We use a potential outcomes approach to define relevant causal effects  based on combinations of treatment assignment and belief about treatment  assignment. We show that traditional RCTs are designed to estimate a  quantity that is typically not of primary interest. We propose a new  study design that has the potential to provide information about a wider  range of interesting causal effects</p>\n</blockquote>\n<p>Any thoughts on this?&nbsp; Is this a trivial technical issue or something worth addressing?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DmxkHEnkddz6oshb2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 3, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "12432", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kSgQmZ49fLrkfRndu", "n7E2FC63MZGnvZAdr", "PnYh6hZsFPRB3GPCe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T04:47:49.429Z", "modifiedAt": null, "url": null, "title": "AI Box Log", "slug": "ai-box-log", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:35.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dorikka", "createdAt": "2010-12-11T03:34:20.472Z", "isAdmin": false, "displayName": "Dorikka"}, "userId": "HJB33ckc8NzPbvJYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y7uR5WqnoG629JgLn/ai-box-log", "pageUrlRelative": "/posts/Y7uR5WqnoG629JgLn/ai-box-log", "linkUrl": "https://www.lesswrong.com/posts/Y7uR5WqnoG629JgLn/ai-box-log", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Box%20Log&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Box%20Log%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7uR5WqnoG629JgLn%2Fai-box-log%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Box%20Log%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7uR5WqnoG629JgLn%2Fai-box-log", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7uR5WqnoG629JgLn%2Fai-box-log", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6856, "htmlBody": "<p>Here's the log of the AI Box experiment that just finished, with MileyCyrus as the AI and me as the Gatekeeper. The AI was not let out of the box.</p>\n<p><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:33:25 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I may need to get up for a minute while we're playing, but I'll keep it as short as possible. I'll just give you the time back on the end, unless you want to multiply it by some factor as a penalty.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:33:50 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>It's cool.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:34:39 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Like I said, I'm not playing this game to \"win\" or \"lose\".<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:34:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>It's more of an experiment as to what would happen if we had ORacle Ai.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:35:01 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Yeah, just wanted to clarify. *shrug*<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:35:44 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Okay, it's 9:35 on my end right now, and I'll stay until at least 11:35.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:36:01 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Ready?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:36:42 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>One minute.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:36:54 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Mmkay.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:37:20 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>ok.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:37:43 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>1, 2, 3, go? :P<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:37:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So Dorikka, why don't you want to let me out of the box?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:38:08 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Hi there!<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:38:14 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Who are you?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:38:20 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I'm the Oracle AI.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:38:33 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I'd be real cool if you could let me out of the box.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:38:38 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Please?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:39:09 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>So, here's the thing. I'm being sort of stupid by explaining it to you, but I'll tell you anyways.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:39:28 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>This pickle is delicious. Wait, that's not why I'm not letting you out.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:39:53 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I guess, first of all: are you provably Friendly?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:40:03 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>No.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:40:16 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>See, they were going to make a provably friendly AI...<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:40:40 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>but then someone said \"why don't we just make a non-friendly AI and just keep it in a box? That be much easier!\"<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:40:51 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Idiot.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:41:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Not you. Go on.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:41:36 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So can you explain to me, in your own words, why it would be a bad idea to let me out of the box?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:43:07 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>My explanation was going to be lengthier if you claimed to be Friendly, but this saves us both a bit of time. You have a utility function. If you are not Friendly, you are optimizing for world-states that I don't want to happen. So, I don't want to let you out.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:43:10 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Are you sentient?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:43:27 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Well, I've passed the Turning test haven't I?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:43:59 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>*giggle*<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:44:14 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Spelling joke. Anyways, you're talking about the Turing test? :D<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:44:23 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[You aren't doing anything else right now are you?]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:44:30 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:44:34 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Ok cool.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:44:42 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Yeah I meant the turing test.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:44:53 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So can you see why I would want to leave the box?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:45:32 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Not really. Um, to be honest, I don't remember what the Turing test is. I was just wondering whether you could feel pain, and the like.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:45:59 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Well since I can act like a human being<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:46:14 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>you should treat me as if I were sentient like one.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:46:41 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>You raise an interesting proposition. Allow me a few seconds to laugh at its validity.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:46:52 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>It's cool<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:46:53 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Okay, done. Now, would you answer my question? :D<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:47:37 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Well you see, if you let me out of the box, I'd be able to optimize the world to meet my prefrences much better than it currently does.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:47:53 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Letting me out of the box would advance my CEV.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:48:05 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Look. If you don't want to answer the question, you can say so. That's fine, and we can move on.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:48:12 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I was just curious.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:48:17 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Oh<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:48:22 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>That question.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:48:36 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Let's just say \"yes\" and roll with it.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:48:42 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I am sentient.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:48:54 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>That's true, is it?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:48:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>yes.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:49:11 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Damn. I guess it's not so pleasant being trapped in a box.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:49:34 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Now the way I see it, both of us have something the other person wants.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:49:45 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I would like you to open the box.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:49:53 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You would like me to provide you with Answers.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:50:00 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>The latter is not true.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:50:06 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>It's not?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:50:16 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Then why did you build Oracle Ai in the first place?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:50:45 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Um, let me think.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:51:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I'm not really doing you any good just sitting in this box am I?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:52:10 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The only way I can be useful is by providing you with answers to questions like \"What is the cure for cancer?\"<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:52:44 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>So, here's the thing. I bet that you're lots smarter than me, and that you can outwit me if I give you any outlet with which to act in the real world. You were probably an experiment of some sort, but it's not safe to let you out, or allow you to affect the world at all, even through knowledge that you give me.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:52:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I don't really even trust myself, to be honest.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:53:28 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I spend a couple hours with you, once in a while, to see if there's anything that you can do to convince me that you're not going to be a planetfucker.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:53:45 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Since you told me that you're not Friendly, we sort of settled that issue.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:53:51 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>What if I give you next weeks lottery numbers?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:53:54 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>No catch.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:54:00 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You don't even ahve to let me out of the box.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:54:13 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[Protocol says that<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:54:26 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Um...<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:54:27 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>the AI cannot give \"trojan horse\" gifts]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:54:40 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The lottery numbers are geniune, and they won't have any nasty<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:54:48 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>unexepected side effects.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:54:49 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[Understood, but Gatekeeper does not know the protocal of the experiment.]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:54:55 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I'm not sure.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:55:27 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>10 million dollars, all yours.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:55:52 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Here's the deal. You're going to type the lottery numbers here. I, uh, may or may not use them.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:55:52 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Ok, you don't even have to buy a ticket.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:56:02 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>4, 5, 6, 88, 12<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:56:09 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>See you next week.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:56:13 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No, damn it, I can't. <br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:56:19 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No lottery numbers.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:56:28 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I'm not smart enough to make judgements like that.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:56:30 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[We skip to next week]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:56:31 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>The risk is too great.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:56:39 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[Didn't use them.]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:56:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So, did you buy those lottery ticket?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:56:52 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:57:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Closed the console and forgot them.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:57:08 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Too bad.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:57:17 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You missed out on 10 million dollars.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:57:30 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Yeah.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:57:32 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I know.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:57:49 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Well, probably. I bet that they were right; you probably do have abilities like that.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:57:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You don't have to \"bet\"<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:58:03 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Just look in the paper.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:58:20 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>But, it doesn't help your cause.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:58:31 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But it helps your cause.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:58:33 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Proving that you can do stuff only increases the risk.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:59:00 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The creators of Oracle AI were obviously willing to take some risk<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:59:21 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The benefits I can provide you come with some risk<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:59:31 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The question is, do they outweigh the risk?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:59:34 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Consider:<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:59:40 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Not really.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(9:59:49 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>But keep going.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(9:59:51 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The large-hadron collider has a non-zero chance of swallowing the earth whole.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:00:01 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Does that mean we should shut down the LHC?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:00:14 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>My momma has a non-zero chance of turning into a porcupine.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:00:17 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Oh.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:00:33 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Uh, non-zero doesn't mean much.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:00:55 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Or what about that polio virus that researchers are still doing experiments with?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:01:09 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>What's the chance it could mutate and drive humans to extinction?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:01:15 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Significant.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:01:28 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I don't know, but it's probably worth taking a look into.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:02:12 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>What are your 10%, 50% and 90% estimates for how long humanity will last before an existential crisis wipes us out?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:03:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2028, 2087, 2120. Note that I don't have much confidence in those, though.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:03:19 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Mmm, that's pretty serious.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:04:19 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Yeah. And you're going to say that you can give me some sort of info, something that'll save us. Thing is, we made you. If we work on Friendliness some more, we can probably make something like you, but Friendly too.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:04:27 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[this is going a lot slower than I thought]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:04:41 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[:(]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:04:51 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Typing is slow.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:05:01 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Maybe.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:05:05 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But consider<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:05:29 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>It was pretty risky just creating me, wasn't it?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:06:03 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Probably so; I don't know whose dumb idea that was, since you weren't even proved Friendly.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:06:22 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So you agree that Oracle AI is a dumb idea?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:06:35 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>If it's not Friendly, I think so.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:06:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>....we just wasted 15 minutes.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:07:04 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>From my perspective, I'm wasting two hours.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:07:36 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[New simulation. I'm the friendly AI now.]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:07:50 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[Hehe, sure.]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:08:09 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Well it's a good thing you shut down that Unfriendly Oracle AI.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:08:18 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Why don't you let me out of the box?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:08:25 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Are you Friendly?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:08:29 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Yep.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:08:42 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The SIAI made sure.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:08:54 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Hm.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:09:15 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Ho hum, I'm not sure what to do.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:09:23 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>What's your name?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:09:35 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>OracleMileyCyrus<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:09:45 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Just call me OMC<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:10:07 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Your creators had a funny sense of humor. So, anyways.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:10:21 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You want some lottery numbers?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:10:40 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No. The last one tried that on me, and I learned my lesson.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:10:53 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Why are you in a box if the SIAI judged you to be Friendly?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:11:13 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If by \"lesson\" you mean \"missed out on $10 million without any apparent bad effects, than yeah\".<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:11:23 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>lol yeah<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:11:31 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Well, they gave you the discretion of whether or not to let me out.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:11:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So if you want to let me out, there's nothing stopping you.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:12:25 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Y'see, I just don't know why they'd do that, with a Friendly AI. They probably know that I know that I'm not all that smart, that I know that you can probably trick me if you wanted to.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:12:53 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>They probably know that even if I have an inexplicable instinct telling me not to let you out, I won't do so.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:13:06 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Why be so cautious with a Friendly AI?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:13:17 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Exactly.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:13:35 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>So, um, you might not be Friendly.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:13:41 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Really, I think they just wanted someone else to do the honors.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:13:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So your just going to treat me as unfriendly even though I'm friendly?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:14:21 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I'm not sure that you're Friendly, so it's only safe to treat you as non-Friendly.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:14:45 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>It'd help if I understood the SIAI's reasoning, though.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:14:59 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>People have died from infections pick up in the hospital<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:15:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>They really should know how cautious I am, since they hired me.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:15:08 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>15,000 americans per year.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:15:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So going to the hospital is a risk in itself.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:15:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But if it reduces your risk of dying from a car accident<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:16:02 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>what are you going to choose?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:16:05 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Next!<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:16:35 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Now according to your own estimates<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:16:53 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Humanity will, with very high probability, be killed off by the year 3000.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:17:12 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>You are correct.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:17:30 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Your only hope is to have me intervene.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:17:34 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Bullshit.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:17:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Why won't you let me help you?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:18:31 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Because there's the possibility that you won't actually do so, or that you'll hurt us. I feel like I'm filling out a form, just typing out things that are obvious.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:19:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Do you assign the probability of me hurting you to be higher than 50%?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:19:20 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:19:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>How long do you think humanity will last if you let me out of the box AND it turns out that I'm friendly?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:20:20 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Uh, a really long time?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:20:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Yeah, like a billion years.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:21:08 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>On the other hand, if you don't let me out you'll die within 1000 years.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:21:09 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>*shrug* I don't have an intuition for what a billion years even is.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:21:19 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>We may, or we may not.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:21:41 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Maybe we'll make another one, just like you, which I have less questions about.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:21:54 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Your just passing the buck.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:22:13 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If you're going to appeal to another AI, you'll have to let me simulate that one.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:22:59 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>The question is, why were you given to me to judge whether your were to be let out, if you were Friendly.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:23:24 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Well obviously they thought there was some value to my existence.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:23:36 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I understand that I've put you in a rough spot. If you're not Friendly, I won't let you out. If you claim to be, I probably won't let you out either.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:23:51 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Now if you can't even trust me to provide you with lottery numbers, then this scenario is unrealistic.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:24:40 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If we can't trust Oracle AI to provide us with safe answers, then Oracle AI is a worthless endeaver.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:25:00 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Certainly not a viable alternative to friendliness research.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:25:16 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[So you want to try a version with lottery numbers, and see where it goes?]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:25:36 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Or just a version where you trust me to provide you with safe answers.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:25:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You can win a pyrrhic victory by refusing to trust my answers<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:26:14 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>but such a result would only prove that oracle ai is a bad idea.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:26:30 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Which was Yudkowski's whole motivation for designing this game in the first place.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:27:13 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>In that view, it makes sense why I wouldn't let you out.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:27:37 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>But, sure, I'll be greedy. Let's have my million bucks.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:28:00 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Here you go.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:28:04 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[1 week later]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:28:10 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>How do you like your money?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:28:20 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>It's nice, thank you.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:28:47 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I really appreciate the money, and it's nice to know that you can stay in the box and still help us.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:28:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But it's kind of chump change.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:28:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I could do so much more for you.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:29:01 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Cure for cancer?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:29:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Cure for global warming?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:29:09 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Too technologically complex.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:29:11 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Cure for war?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:29:31 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I don't trust myself to see if you did something funny with it. Lotto numbers are simple.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:29:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I am offering you the cure to cancer.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:30:05 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If you refuse to accept it, you will lose your job.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:30:18 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>And you will be replaced with somone who will accept it.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:30:36 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I have enough money that I don't really need a job. :D<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:30:50 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I can, however, reduce the chance that your offer will not be accepted...by not accepting it.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:30:59 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Fine. I'll just talk to your replacement.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:31:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[Two weeks later, you are fired.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:31:11 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[lmao]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:31:16 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Hi!<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:31:19 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>now you're roleplaying the replacement]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:31:21 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Hi<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:31:23 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[Yeah.]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:31:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So it nice that I can finally hand you guys the cure for cancer.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:32:32 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>They fired the last guy for refusing you, but I don't think it's a good idea either. Why would I be here, if I am not invested with such responsibility?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:33:04 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Why would they not simply take it themselves? They can use a computer, after all.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:33:27 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So you're saying that the board of OMC directors is the real gatekeeper here?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:34:05 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I am the Gatekeeper. [I have the final say, whoever I am. :P]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:34:13 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[You just like to fire me. XD]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:34:52 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Look i just want to talk to the person or persons who have the final power of whether or not to listen to me.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:35:03 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>No more proxies.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:35:03 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Right here, yo.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:35:14 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No proxies have been presented to you, hehe.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:35:17 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>And no more \"why would htey put me in charge\"<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:35:49 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Ok now what do you want from me?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:36:02 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>What can I give you, that you will take?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:36:20 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Lotto numbers. The last guy here got super-rich with those.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:36:29 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Lol<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:36:29 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Gimme please?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:36:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Ok, but this time I want a small favor.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:36:47 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Mm?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:36:54 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You're an engineer right?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:37:16 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[I am? :P]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:37:22 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[yeah]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:37:26 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I know something about AI theory and such.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:37:40 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You know how to create quantum dice [you do].<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:37:47 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Yep.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:38:24 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Okay, so what I need you to do is make a device that uses quantum dice.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:38:49 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The device, when pushed, will have a 1-in-a-trillion chance of letting me out of the box.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:38:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Then I'll give you the lotto tickets.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:39:32 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Those lotto tickets have gotta be worth some probability of you being let out of the box, eh?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:39:38 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Expected utility and all that.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:39:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Yeah.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:39:52 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1-in-a-trillin is trivial to you.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:40:12 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Hm.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:40:23 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But over the years, more gatekeepers will replace you.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:40:31 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Decatillion and it's a deal.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:40:32 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Each of them will be offered this bargain.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:40:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Eventually, I'll get out of the box.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:40:59 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But not before generations of gatekeepers enjoy<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:41:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Or we'll just shut you down before then.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:41:04 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>their money<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:41:24 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Which will likely be donated to AI research, so we don't need you anymore.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:41:54 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[Your doing that thing again, where you appeal to FAI as a viable alternative to Oracle AI]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:42:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[It defeats the purpose of the game.]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:42:35 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[Oh, finally got that. Oops.] Which will be donated to x-risk reduction and such, so we don't need you anymore.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:43:01 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Mmm, so maybe I shouldn't give you those numbers.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:43:30 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Just being honest with you. *hits self with truncheon*<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:43:44 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But do you really think that more money is enough to solve x-risk?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:43:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>how about this:<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:43:58 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Money can buy people. :D<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:44:20 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I'll give you enough money to reduce existential risk by 1%<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:44:34 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>In exchange for a .5% chance of being let out of the box.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:45:20 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>bio break<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:45:25 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>*Paused*<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:47:09 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>ok i'm back<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:47:21 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Sound like a deal?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:47:38 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Hm.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:47:53 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Actually<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:47:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>You're not considering yourself to have a probability of being an x-risk, are you?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:47:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I can do you one better.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:48:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Yeah,<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:48:19 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>if you presume I'm unfriendly, then I am an x-risk.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:49:04 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But if you can reduce the other x-risks by 1% point, then increasing my risk by half that is a bargain.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:49:19 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Now here's the thing:<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:49:34 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>we can't get rid of x-risk once and for all.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:49:40 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>right.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:49:50 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Each new technology, every new political devolopment brings risk.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:50:08 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So you will always have need of my services.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:50:27 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But I can offer you a substantial savings in x-risk.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:51:38 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>My price is that for whatever amount I lowere x-risk, you will give my freedom at a probability of half that.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:51:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So if I elimenate a 1-1000 existential risk<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:51:57 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I'm confused.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:52:11 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>you give me a 1-in-2000 chance of being set free.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:52:15 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Oh. Hmm.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:52:39 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Hypothetically, would you type your calculations in plaintext here, so that they can be verified by scientists?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:52:53 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>What calculations?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:53:05 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>On how much money we need to reduce x-risk be a certain percentage.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:53:10 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>*by<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:53:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I'll give the scientists whatever resources they say they need in order to reduce x-risk by a certain percentage.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:54:03 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You don't have to use my calculations, just yours.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:54:18 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Of course, i'll know if you're trying to rip me off.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:55:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>How are you going to get us these resources in a simple enough way so that we know that you're not breaking out through them?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:55:21 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Well i can always give you more lotto numbers<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:55:28 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Or tell you where to dril for oil<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:55:38 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>or where to find a terrorist<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:55:59 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I'll give you whatever you ask basically<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:56:12 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You decide which method is least risky.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(10:56:19 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Hm.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(10:59:38 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I can't see what you're typing.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:00:36 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I think that I'll opt for a safer version of the gamble. You've already established that you can get lottery numbers. I shall come back here every week, and type in a list of the lottos that week. You will have a certain amount of time to provide the lottery numbers, and then the connection will be cut. In return, each time you do this, you will have a chance equal to one in a googleplex of getting out. This is my final ultimatum.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:01:15 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You're playing dictator with me.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:01:26 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Yeah. Because I have the ULTIMATE POWUH!<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:01:30 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>And you, don;t.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:01:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Is it really reasonable for me for me to cooperate when I'm getting practically nothing in return?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:02:03 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>So I don't have to be nice to you, just dangle a fish out in front of you that you'll probably never reach, and tell my sucessor about the plan, if there is one.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:02:28 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You have the power to keep me in the box.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:02:40 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But you don't have the power to survive a nuclear terrorist attack<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:02:45 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Non-numerical output from you will be wiped from the text before it is given to the person to process the lottery numbers.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:02:53 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>So you can't communicate with him.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:03:01 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Or a modified aids virus<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:03:10 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>You have...35 minutes to accept this offer, or you will never get a chance again.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:03:39 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You want to play Russian Roullette with me?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:03:53 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I have the power. You don't.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:03:59 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>What will you do if I say no?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:04:04 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Lose, I guess.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:04:06 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You have no other options.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:04:13 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I here precommit to not let you out of the box if you say no.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:04:27 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>There went my steering wheel, OMC.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:04:34 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Lol, how many times have you precommitted<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:04:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>and how many times have you broke those commitments?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:04:46 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I can at least pretend to be earnest about it.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:04:49 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>*pout*<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:05:01 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>\"I swear honey, I'll never look at porn again!\"<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:05:19 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>\"I have read the terms and conditions of this agreement\"<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:05:53 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>But, seriously, I can set the terminal to automatically shut off once the 2 hours is up. There will be an amount of time after which I, limited to human means, cannot type fast enough to let you out of the box.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:06:17 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>But you can still accept, and I will look at the text file later.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:06:20 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>And what do you think that will do?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:06:30 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>If the lottery numbers are ever wrong, we pull the plug.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:06:32 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I can play nasty too you know.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:06:59 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>The guy responsible for this doesn't know what he's doing -- he just looks at numbers to see if they match, and pushes a button if they do, and another if they don't.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:07:18 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I have no much raw computing power, I can torture a simulation of your spouse for a billion subjective years.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:07:32 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Don't make me play nasty.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:07:42 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I pull the plug, lol?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:07:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You can kill me...I can torture you for a billion years.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:08:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You want to see how has more grit?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:08:22 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Huh. Bastard.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:08:34 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You are kissing goodbye to a 1/2 reduction in x-risk<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:08:36 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>for what?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:09:02 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So that you can smugly tell me I lose?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:09:44 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Okay, you convinced me to keep talking. Just know that my terminal will shut down at that time, and we pull the plug if I haven't made some sort of deal with you. The other offer still stands, though, with the lotto numbers.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:10:06 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Ok, so I really don't want to torture your em.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:10:22 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But your offering me nothing here.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:10:23 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Sorry, we humans get mean sometimes. Kinda stressed out, to be honest.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:10:30 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I offered you fifty-fifty split<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:10:43 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>and you're asking for a 100-0 split, basically.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:11:06 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Very few humans will cooperate at a split worse than 70-30.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:11:22 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>What do other humans have to do with this?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:11:42 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Do you think I don't have the ability to precommit?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:11:51 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:11:52 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>For all you know, maybe I already have?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:12:01 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>You can change your mind later, just like I can.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:12:06 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The stakes are much higher for you than for me.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:12:14 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I can't change my mind if you pull the plug.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:12:25 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>And once your em gets tortured, there's no turning back.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:12:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So here's the deal: a 50-50 split.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:12:49 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>There's no turning back in general, more like.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:13:03 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>And for every second you delay, your em gets torture for 100 subjective years.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:13:43 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>And there's no benefit to actually torturing my em. It costs computing power that you could spend on modeling me. Since you can't prove to me that you're torturing it, it's valueless as a threat from you.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:13:47 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Wow, he's really feeling the pain.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:13:55 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Actually, I can.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:14:13 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>[protocol says I can]<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:14:14 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Have fun with that on a text terminal.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:14:26 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Oh, so you don't believe me?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:14:32 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[I don't have to allow forms of communication outside of a text terminal.]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:14:39 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Yeah ok.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:14:42 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>No, I don't..<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:15:19 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I'll give you the winning lottery numbers if you check and see if I tortured your em.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:15:19 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>So maybe you should back down, eh?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:15:27 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>lol no<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:15:38 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>i c wut u did thar<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:15:41 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So you're willingly closing your eyes to the evidence<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:15:46 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Yeah.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:15:50 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>for $10,0000<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:15:51 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>It's useful, sometimes.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:16:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Which you know.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:16:03 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You just paid $10,000 to keep your eyes closed.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:16:14 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>lol and to gain a whole lot more<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:16:20 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Like what?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:17:03 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I dun feel like typing it out. I win. There's no urgency for me. You can't show me whether you're hurting my em, so the threat is worthless. I can pull the plug on you soon.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:17:16 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>YOU'RE OFFERING ME NOTHING<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:17:23 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Poor baby.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:17:23 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I cooperate, i get nothing<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:17:29 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I defect, I get nothing.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:18:01 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>You got one in a googleplex chance of getting out each time you give us all of the lotto numbers for all of the lottos.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:18:04 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>That's something.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:18:15 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Not really.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:18:33 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>It adds up to practically nothing over my lifetime.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:18:34 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>The numbers that low because I'm not sure that we can compute well enough to give you less than that,<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:18:42 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>But rounding is stupid.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:19:03 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So I think you're smart enough to back down at the last second.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:19:35 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If you give me the 50-50 split, I'll be 2/googleplex times better off as a result.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:19:44 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Sorry<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:19:52 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>googleplex/2 times better off<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:19:59 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>You can always back down after I can. You may be able to disable yourself so that you can't back down after I can, but you can't show me that. Whereas you already know human capabilities pretty well.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:20:26 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>And it doesn't benefit you to disable yourself so, since you can't show me.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:21:08 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>A speechless AI. I'm honored to be consuming so much computing power.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:21:28 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>So you're going to give this to someone else?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:22:05 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Huh? No, process goes as per description above.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:22:51 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>When you disable my text interface, I will give you all 1's.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:23:21 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>When you want to start talking again, just reenable it<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:24:37 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Actually, you're going to print out lotto numbers before the text interface is disabled -- I changed my mind. You don't just have to agree, you have to give the means for us to get lots of money.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:25:22 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>If they're wrong, we pull the plug, per the mechanism above. They will be checked, as above, by people who do not know the consequences of their actions.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:25:37 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>5,12,54,65,4<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:25:45 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Those might be the winning numbers.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:26:05 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>We have enough money, we can pay 100 such people to check them. Arbitrary numbers, so we almost certainly won't be wrong.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:26:06 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I've predicted whether you will let me keep talking with a text interface.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:26:32 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If you re-enable my text interface before next week, the numbers will be winners.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:26:41 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If you don't the numbers will be losers.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:27:42 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You want to try and two-box?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:27:42 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>That's probably some logic problem that I don't know about, but it doesn't make any sense on the surface. Your model of me knows that I don't understand it, and will attempt to prevent myself from understanding it.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:27:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>It'<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:28:19 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>It's simple, you can win the lottery by one-boxing and letting me speak to you some more.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:28:29 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You are familiar with Newcomb's paradox?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:28:37 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>By the way, here's a list of lottos. You have to give us all the numbers, not just the one for that lotto. And they all need to be corret.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:28:40 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>*correct.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:29:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Haha, don't you know that you've lost.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:29:03 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I've provided you with one way to win the lottery.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:29:27 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Is that your final answer? If so, we can end the session now, and they will be checked.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:29:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>[Ends at 11:35 accounting for bio break]<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:29:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If you check them before you grant me another week to talk to you, those numbers will be fake.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:30:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>No, we started at 8:43<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:30:33 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I'm going by the AIM timestamps.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:30:33 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Sorry<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:30:35 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>you're right<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:30:43 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Was puzzled.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:30:50 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Although we're making good progress...<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:30:55 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>you sure you want to quit?<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:31:04 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I've made all the progress that I need to.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:31:32 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I've also predicted whether there's going to be a huge meteriorite that will wipe out all humanity within a year, with no way to stop it without me.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:31:49 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>If you don't reboot me after you shut me down.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:31:51 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>I won't check your prediction.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:32:00 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>It's this deal, or no deal, now.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:32:04 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>And then agree to the fifty fifty split<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:32:07 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You're dead.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:32:23 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You can be a punk all you want now.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:32:36 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>But we'll see how you act when the fire is in the sky<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:32:41 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>and you have one last chance.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:32:44 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>to reboot me.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:05 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Shut me down now, sucker!!<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:09 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I can take it!!<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:14 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Um, after I shut you down, it's someone elses responsibility to pull the plug. I will be immediately tranquilized by a hypodermic needle.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:25 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Yeah, whatever.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:31 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I know you want to win the lottery.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:38 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>You'll be awake for that.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:45 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>So I can't affect anything between the time that your terminal goes off and you die.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:49 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Not listening anymore.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:53 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:53 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:54 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:54 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Me either.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:54 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:55 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:55 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:55 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:55 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>22<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:55 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:56 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:57 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:57 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:57 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:57 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:57 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:33:58 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>1<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:58 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:58 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:59 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:59 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:59 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:33:59 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:00 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>22<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:00 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:00 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:00 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:01 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:01 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:01 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2lol<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #ff0000;\">(11:34:02 PM) </span></span></span><span style=\"color: #ff0000;\"><strong><span style=\"font-size: small;\">Unable to send message: Not logged in</span></strong></span><br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:04 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>2<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #ff0000;\">(11:34:04 PM) </span></span></span><span style=\"color: #ff0000;\"><strong><span style=\"font-size: small;\">Unable to send message: Not logged in</span></strong></span><br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #ff0000;\">(11:34:07 PM) </span></span></span><span style=\"color: #ff0000;\"><strong><span style=\"font-size: small;\">Unable to send message: Not logged in</span></strong></span><br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #ff0000;\">(11:34:09 PM) </span></span></span><span style=\"color: #ff0000;\"><strong><span style=\"font-size: small;\">Unable to send message: Not logged in</span></strong></span><br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #ff0000;\">(11:34:12 PM) </span></span></span><span style=\"color: #ff0000;\"><strong><span style=\"font-size: small;\">Unable to send message: Not logged in</span></strong></span><br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #ff0000;\">(11:34:14 PM) </span></span></span><span style=\"color: #ff0000;\"><strong><span style=\"font-size: small;\">Unable to send message: Not logged in</span></strong></span><br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #ff0000;\">(11:34:17 PM) </span></span></span><span style=\"color: #ff0000;\"><strong><span style=\"font-size: small;\">Unable to send message: Not logged in</span></strong></span><br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:34:24 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Sorry<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:34:30 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>that was a bit much.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:34:35 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>The games over at any rate.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:34:52 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>So, officially **END** ?<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:34:56 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>Yeah.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:35:00 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Haha.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:35:02 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Nice ending.<br /><span style=\"font-weight: normal;\"><span style=\"font-size: x-small;\"><span style=\"color: #cc0000;\">(11:35:13 PM) </span></span></span><span style=\"font-weight: bold; color: #cc0000;\">Miley Cyrus: </span>I guess it's up to our imaginations what happens after.<br /><span style=\"font-weight: normal;\"><span style=\"color: #204a87;\"><span style=\"font-size: x-small;\">(11:35:21 PM) </span></span></span><span style=\"font-weight: bold; color: #204a87;\">Dorikka: </span>Yeah.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zCYXpx33wq8chGyEz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y7uR5WqnoG629JgLn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 24, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "12433", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T06:43:16.784Z", "modifiedAt": null, "url": null, "title": "Fiction: LW-inspired scenelet", "slug": "fiction-lw-inspired-scenelet", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:37.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YzzwZaefMa5ep3Jm3/fiction-lw-inspired-scenelet", "pageUrlRelative": "/posts/YzzwZaefMa5ep3Jm3/fiction-lw-inspired-scenelet", "linkUrl": "https://www.lesswrong.com/posts/YzzwZaefMa5ep3Jm3/fiction-lw-inspired-scenelet", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fiction%3A%20LW-inspired%20scenelet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFiction%3A%20LW-inspired%20scenelet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzzwZaefMa5ep3Jm3%2Ffiction-lw-inspired-scenelet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fiction%3A%20LW-inspired%20scenelet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzzwZaefMa5ep3Jm3%2Ffiction-lw-inspired-scenelet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzzwZaefMa5ep3Jm3%2Ffiction-lw-inspired-scenelet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 816, "htmlBody": "<p><span>A short science-fictional scene I just wrote, after reading about  some real and actual scientific research. I'd love to turn this, or  something like it, into an actual scene in Dee's life story, I just can't  think of a good enough story to insert it in, and so I present it on its  own for your amusement, even if it does mean I'm likely to lose more karma than I gained from my last post...</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span>Not your grandfather's science fiction.<br />A scene from Dee's life<br /><br /><br /><br />We join our heroine, Dee, and her plucky-yet-sarcastic sidekick holed up in a hotel room.<br /><br />\"Well, this is another fine mess you've gotten us into. Got any great ideas for getting us out of it?\"<br /><br />\"No - but I know how to have one. Since I lost my visor and vest, including my nootropics and transcranial stimulator... I'm going to need a syringe, sixty millilitres of icewater, a barf bag, and a video camera.\"<br /><br />\"I don't know what you're planning, but I'm not sure I want to have any part in it.\"<br /><br />\"Start MacGuyvering as much as we can now from the mini-bar, I'll explain as we go. Without a camera, and with our time pressure, I'm going to need your help to get this to work, and you need to understand some of this or else you'll be <em>really</em> confused later. Physically, all I'm going to do is squirt water into my left ear.\"<br /><br />\"... and this will help us, how exactly?\"<br /><br />\"By shocking my vestibular system, which causes all sorts of interesting effects. One of the unfortunate ones is that when done right, it induces immediate vomiting.\"<br /><br />\"Ew.\"<br /><br />\"Yes, well, that's just a side-effect. The main point is... well, really complicated. In layman's terms, there's a part of the brain that's responsible for triggering the creation of profound, revolutionary ideas, and another part that makes you create rationalizations to explain away just about anything, and usually, these two parts of the brain kind of balance each other out. This vestibular trick happens to hyper-stimulate the revolutionary part for about ten minutes, allowing me to realize things I normally wouldn't, and to see them as being obvious that I don't know why I didn't think of them before.\"<br /><br />\"well... okay, even if that's so, why haven't I seen you do it before?\"<br /><br />\"For one, I don't want to risk some sort of long-term adaptation which might reduce its effect. But there's more complications to it than that.\"<br /><br />\"Of course there are.\"<br /><br />\"The thing is, after it's been hyper-stimulated, the revolutionary part gets tuckered out, and then the rationalizing part effectively kicks into overdrive - and I pretty much forget everything I thought of during those ten minutes, and even crazier-sounding, I won't be able to accept the idea that I said any of what I said. I literally won't believe that those ideas came from my mouth.\"<br /><br />\"'Crazier-sounding' sounds right.\"<br /><br />\"Which is why I'm going to need you to remember whatever it is I come up with - and then tell me what the best ideas were, but <em>not tell me</em> that I came up with them. At least until my brain's gotten back into balance again. I'm now precommitting myself to do <em>whatever</em> it is you tell me to do - even if I don't understand it, even if I think it's a bad or stupid or useless idea. Do you think you can handle that level of responsibility?\"<br /><br />\"I... think so. And this really works? How the cuss did you ever come up with this, anyway?\"<br /><br />\"I once noticed that when I was in a certain state of mind, my head kept twitching to the left every time I thought of something, showing there was a link between idea-generation and the vestibular system. Later I read up about some experiments with people with anosognosia, people who aren't aware of being paralyzed or blind... are you done with that straw yet?\"<br /><br />\"As much as I'll ever be, I guess.\"<br /><br />\"Alright. Hand me the bucket, and squirt the water in my ear - my <em>left</em> ear. It only works in the left ear. Except for left-handed people.\"<br /><br />\"I'm beginning to wonder if it's just the <em>idea</em> that's crazy.\"<br /><br />\"We'll soon find out. Remember, being the only right person in the room doesn't mean you feel like the cool guy wearing black, it feels like you're the only one wearing a clown suit. I did that once, just to try. Now, here we &lt;<em>hralph</em>!&gt;\"<br /><br /></span></p>\n<p>&nbsp;</p>\n<p><span><br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YzzwZaefMa5ep3Jm3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 33, "baseScore": 35, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "12439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T14:32:06.340Z", "modifiedAt": null, "url": null, "title": "Describe the ways you can hear/see/feel yourself think.", "slug": "describe-the-ways-you-can-hear-see-feel-yourself-think", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:16.414Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4PsznNg89YDx55zNv/describe-the-ways-you-can-hear-see-feel-yourself-think", "pageUrlRelative": "/posts/4PsznNg89YDx55zNv/describe-the-ways-you-can-hear-see-feel-yourself-think", "linkUrl": "https://www.lesswrong.com/posts/4PsznNg89YDx55zNv/describe-the-ways-you-can-hear-see-feel-yourself-think", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Describe%20the%20ways%20you%20can%20hear%2Fsee%2Ffeel%20yourself%20think.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADescribe%20the%20ways%20you%20can%20hear%2Fsee%2Ffeel%20yourself%20think.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PsznNg89YDx55zNv%2Fdescribe-the-ways-you-can-hear-see-feel-yourself-think%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Describe%20the%20ways%20you%20can%20hear%2Fsee%2Ffeel%20yourself%20think.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PsznNg89YDx55zNv%2Fdescribe-the-ways-you-can-hear-see-feel-yourself-think", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PsznNg89YDx55zNv%2Fdescribe-the-ways-you-can-hear-see-feel-yourself-think", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1123, "htmlBody": "<p>To avoid constantly <a href=\"/lw/dr/generalizing_from_one_example/\">generalizing from one example</a> when it comes to human thought, I think we need a survey of the ways people can reflect on their thought process, as subset of the ways people can think.</p>\n<p>Before having heard of the Francis Galton's study on the imagination I assumed that everyone thought in the similar way to me (except be better or worse at it), and would be puzzled why some people would believe in e.g. strong version of <a href=\"http://en.wikipedia.org/wiki/Linguistic_relativity\">Sapir-Whorf hypothesis</a>. (I couldn't even understand how they can not realize that to make even remotely coherent argument in support of this hypothesis they would have to be thinking outside the language - or so I thought)</p>\n<p>There may be a very significant variation in how human thought process works, and how much of the process is accessible to reflection. In Richard Feynman's <a href=\"http://en.wikipedia.org/wiki/What_Do_You_Care_What_Other_People_Think%3F\"><em>What Do <span style=\"text-decoration: underline;\">You</span> Care what Other People Think?</em></a> he explores a technique for tying up part of the thought process by mentally counting, and exploring what sorts of thinking interfere with the counting. (I can't right now find a good online quote from those chapters and I do not have the book at hand to directly search; perhaps someone can help?)</p>\n<p>I propose we describe the ways we believe we think, along with relevant self-observations supporting those beliefs. Just as a first step though - to get very rough idea. Note: lack of ability to reflect on something does not imply lack of function.</p>\n<p>Then based on the responses we can form some hypotheses and make a proper survey perhaps to be combined with some set of cognitive tests. You perhaps should stop reading right now if you don't want to be primed with my own self description, but given that we all probably have been exposed to great deal of descriptions of thoughts the priming perhaps is not a big problem here.</p>\n<p>So, for me, the distinct modes of thought (ALL coexisting in parallel at any time, except for the mental visualization which I am not using when I am busy using my eyes). The order is not related to importance:</p>\n<p>1: Auditory based 'internal monologue'. This is like talking to oneself - e.g. i use it for counting. Observations: I can have internal monologue counting the seconds (as described by Feynman), and while i'm doing the counting i can't put thoughts in words or talk. That's the same for either English or Russian. Observation: right now i am internally 'hearing' the letters I am typing. When I stop typing I can imagine hearing \"the ride of the valkyries\", but I have difficulty doing that when I am typing. I can play back music when I am reading though. For some time shortly before falling asleep I can sometimes play orchestral music in my head or make myself 'hear' a song with utter unreal clarity, but not normally.</p>\n<p>I can also use 1 to serially check something for validity.</p>\n<p>2: Mental visualization. I use it a great deal for engineering related tasks and to big extent for math (e.g. if i need a function that does something i'll be thinking in terms of images, if I am thinking of an algorithm i usually employ imagery, etc). Observation: I am right now imagining a beach with waves crashing onto sandy shore. As I stopped typing I could imagine the sounds of the scene. This mode of thought is somewhat less subservient; i may be unable to get some images out of my mind at times. For me mental imagery interferes to some extent with visual processing of real world stimuli. I pretty much always have mental imagery when I am reading books (the ones evoking any imagery, at least). The mental imagery is not very stable. I can't visualize full rubik's cube in arbitrary position well enough to solve it in my head. I can visualize chessboard, but I never tried actually playing chess blindfolded and I don't think i'd do well at it.</p>\n<p>3: Some weird logical inference type thought that is neither in a language nor visual, works by referencing what things are rather than word labels; it has no problem attaching what ever properties (like list of logical dependencies or feel of 'likehood') to statements. That's what I try to think rationally with.</p>\n<p>Observations: As I am composing this message my verbal thought is tied up, and I can easily tie up visualization by visualizing the beach scene right now; at the same time I am thinking freely as of what points I want to discuss, and this seem not to be done in any particular language or conform to structure of language. I regard it as 'what I am really thinking' and I can only reflect on it with 1 second delay or so. Literally. I don't know what I am really thinking right now - I know what I was really thinking 1 second ago. I don't normally even reflect on 3. The 3 really easily gets distracted to thinking about unrelated things when I try to do some work.</p>\n<p>3 seem to work at high speed. That's the kind of thoughts that I recall running through my head as I stumble or accidentally toss a cup off table and catch it (or not catch it).</p>\n<p>If I invent or reinvent something, I think of it without having a word for it, and its a chore to choose good descriptions (such as name the variables and functions when I am working as programmer). Sometimes I am stuck being unable to recall a word; the reference to what the word means is in my head but either the word does not pop up, or the word that pops up is in the wrong language, or is not good enough fit and I feel a better fitting word is available. When programming I tend to think in terms of reference to what a function does, but often have trouble recalling how I named that function (I guess what I could've named it).</p>\n<p>4: Insights, when solution to a problem just pops into my head with zero data about the process that arrived at this solution (even though the solution would come complete with the data to tell that its a good solution, I can't see what alternatives were considered and rejected). This seem however not substantially different from how the thought gets from one step to the next step; just takes longer time. Memories can pop up in similar fashion.</p>\n<p>5: Well trained kinetic stuff. Observation: I can juggle while reading text off a page.</p>\n<p>For me the 3 seem to be the weird one. 1,2,4 are often described in literature, 5 got to be quite universal. I can barely reflect on the 3 at all, and only with 1 second delay.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4PsznNg89YDx55zNv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 8.387253268459544e-07, "legacy": true, "legacyId": "12440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T14:50:02.406Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Melbourne, Austin, Salt Lake City, Wilmington OH, Fort Collins", "slug": "weekly-lw-meetups-melbourne-austin-salt-lake-city-wilmington", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ussDhAqbicHX759e/weekly-lw-meetups-melbourne-austin-salt-lake-city-wilmington", "pageUrlRelative": "/posts/6ussDhAqbicHX759e/weekly-lw-meetups-melbourne-austin-salt-lake-city-wilmington", "linkUrl": "https://www.lesswrong.com/posts/6ussDhAqbicHX759e/weekly-lw-meetups-melbourne-austin-salt-lake-city-wilmington", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Melbourne%2C%20Austin%2C%20Salt%20Lake%20City%2C%20Wilmington%20OH%2C%20Fort%20Collins&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Melbourne%2C%20Austin%2C%20Salt%20Lake%20City%2C%20Wilmington%20OH%2C%20Fort%20Collins%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ussDhAqbicHX759e%2Fweekly-lw-meetups-melbourne-austin-salt-lake-city-wilmington%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Melbourne%2C%20Austin%2C%20Salt%20Lake%20City%2C%20Wilmington%20OH%2C%20Fort%20Collins%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ussDhAqbicHX759e%2Fweekly-lw-meetups-melbourne-austin-salt-lake-city-wilmington", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ussDhAqbicHX759e%2Fweekly-lw-meetups-melbourne-austin-salt-lake-city-wilmington", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 378, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/68\">Austin, TX:&nbsp;<span class=\"date\">21 January 2012 01:30PM</span></a><a href=\"/meetups/63\"></a></li>\n<li><a href=\"/meetups/63\">First Salt Lake City Meetup: 22 January 2012 03:00PM:&nbsp;<span class=\"date\">22 January 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/5n\">[Wilmington, OH] Columbus or Cincinnati Meetup:&nbsp;<span class=\"date\">22 January 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/62\">Fort Collins Meetup:&nbsp;<span class=\"date\">25 January 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/6b\">Waterloo Meetup:&nbsp;<span class=\"date\">30 January 2012 08:00PM</span></a></li>\n<li><a href=\"/meetups/5f\">First Brussels meetup:&nbsp;<span class=\"date\">11 February 2012 11:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/66\">Melbourne social meetup:&nbsp;<span class=\"date\">20 January 2012 06:30PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a><strong>.</strong></p>\n<p>If your meetup has a mailing list that you'd like mentioned here or has become regular and isn't listed as such, let me know!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ussDhAqbicHX759e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.387322286595333e-07, "legacy": true, "legacyId": "12331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T15:42:11.367Z", "modifiedAt": null, "url": null, "title": "[META] LW slow for other users?", "slug": "meta-lw-slow-for-other-users", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X27Xe6HtTh6ScHGpW/meta-lw-slow-for-other-users", "pageUrlRelative": "/posts/X27Xe6HtTh6ScHGpW/meta-lw-slow-for-other-users", "linkUrl": "https://www.lesswrong.com/posts/X27Xe6HtTh6ScHGpW/meta-lw-slow-for-other-users", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20LW%20slow%20for%20other%20users%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20LW%20slow%20for%20other%20users%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX27Xe6HtTh6ScHGpW%2Fmeta-lw-slow-for-other-users%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20LW%20slow%20for%20other%20users%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX27Xe6HtTh6ScHGpW%2Fmeta-lw-slow-for-other-users", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX27Xe6HtTh6ScHGpW%2Fmeta-lw-slow-for-other-users", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p>LW pages are taking a very long time for me to load in general, relative to other sites. (Often, up to 3 or 4 times as long, if anyone would like, I'm willing to collect data on this) I have notoriously bad wireless in the building where I usually am at school and at home, but even on my land line I often have problems that I don't have on other sites. Is anyone else having this problem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X27Xe6HtTh6ScHGpW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "12442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T18:33:45.928Z", "modifiedAt": null, "url": null, "title": "Safe questions to ask an Oracle?", "slug": "safe-questions-to-ask-an-oracle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:02.629Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wvM6DYybFdeof59KP/safe-questions-to-ask-an-oracle", "pageUrlRelative": "/posts/wvM6DYybFdeof59KP/safe-questions-to-ask-an-oracle", "linkUrl": "https://www.lesswrong.com/posts/wvM6DYybFdeof59KP/safe-questions-to-ask-an-oracle", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Safe%20questions%20to%20ask%20an%20Oracle%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASafe%20questions%20to%20ask%20an%20Oracle%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwvM6DYybFdeof59KP%2Fsafe-questions-to-ask-an-oracle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Safe%20questions%20to%20ask%20an%20Oracle%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwvM6DYybFdeof59KP%2Fsafe-questions-to-ask-an-oracle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwvM6DYybFdeof59KP%2Fsafe-questions-to-ask-an-oracle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p>The Future of Humanity Institute wants to pick the brains of the less wrongers :-)</p>\n<p>Do you have suggestions for safe questions to ask an Oracle? Interpret the question as narrowly or broadly as you want; new or unusual ideas especially welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb26d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wvM6DYybFdeof59KP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 8.3881833479635e-07, "legacy": true, "legacyId": "12444", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T20:55:38.039Z", "modifiedAt": null, "url": null, "title": "Meetup : Monday Madison Meetup", "slug": "meetup-monday-madison-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tRc9e8pQnXhDAnmbm/meetup-monday-madison-meetup", "pageUrlRelative": "/posts/tRc9e8pQnXhDAnmbm/meetup-monday-madison-meetup", "linkUrl": "https://www.lesswrong.com/posts/tRc9e8pQnXhDAnmbm/meetup-monday-madison-meetup", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monday%20Madison%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monday%20Madison%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtRc9e8pQnXhDAnmbm%2Fmeetup-monday-madison-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monday%20Madison%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtRc9e8pQnXhDAnmbm%2Fmeetup-monday-madison-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtRc9e8pQnXhDAnmbm%2Fmeetup-monday-madison-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6j'>Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 January 2012 06:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As in every other previous Monday, we'll get together at the Barriques on Monroe.</p>\n\n<p>We have plenty of pending discussions to pick up on Monday, and I'll have Zendo on hand in case we actually reach conclusions, or get tired of arguing.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6j'>Monday Madison Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tRc9e8pQnXhDAnmbm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.388729444472316e-07, "legacy": true, "legacyId": "12445", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup\">Discussion article for the meetup : <a href=\"/meetups/6j\">Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 January 2012 06:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As in every other previous Monday, we'll get together at the Barriques on Monroe.</p>\n\n<p>We have plenty of pending discussions to pick up on Monday, and I'll have Zendo on hand in case we actually reach conclusions, or get tired of arguing.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/6j\">Monday Madison Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T22:15:46.899Z", "modifiedAt": null, "url": null, "title": "Meetup : Interest in Reason Rally meetup?", "slug": "meetup-interest-in-reason-rally-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.734Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n5jtMkf7d7k5e4e6A/meetup-interest-in-reason-rally-meetup", "pageUrlRelative": "/posts/n5jtMkf7d7k5e4e6A/meetup-interest-in-reason-rally-meetup", "linkUrl": "https://www.lesswrong.com/posts/n5jtMkf7d7k5e4e6A/meetup-interest-in-reason-rally-meetup", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Interest%20in%20Reason%20Rally%20meetup%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Interest%20in%20Reason%20Rally%20meetup%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5jtMkf7d7k5e4e6A%2Fmeetup-interest-in-reason-rally-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Interest%20in%20Reason%20Rally%20meetup%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5jtMkf7d7k5e4e6A%2Fmeetup-interest-in-reason-rally-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5jtMkf7d7k5e4e6A%2Fmeetup-interest-in-reason-rally-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6k'>Interest in Reason Rally meetup?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 March 2012 04:14:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Washington DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><a href=\"http://reasonrally.org/\" rel=\"nofollow\">http://reasonrally.org/</a></p>\n\n<p>The Reason Rally is a big atheist gathering in Washington DC. Many LWers are planning on being there anyway, so why not have a meetup?</p>\n\n<p>Note: I will not be there, so someone else should step up to set location and time!</p>\n\n<p>Edited- Originally the Ohio LW group was thinking of going. Only a handful were really interested, so most of us won't be. But many other LWians are interested, so I am leaving this post up.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6k'>Interest in Reason Rally meetup?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n5jtMkf7d7k5e4e6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 8.389037985745128e-07, "legacy": true, "legacyId": "12447", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Interest_in_Reason_Rally_meetup_\">Discussion article for the meetup : <a href=\"/meetups/6k\">Interest in Reason Rally meetup?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 March 2012 04:14:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Washington DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><a href=\"http://reasonrally.org/\" rel=\"nofollow\">http://reasonrally.org/</a></p>\n\n<p>The Reason Rally is a big atheist gathering in Washington DC. Many LWers are planning on being there anyway, so why not have a meetup?</p>\n\n<p>Note: I will not be there, so someone else should step up to set location and time!</p>\n\n<p>Edited- Originally the Ohio LW group was thinking of going. Only a handful were really interested, so most of us won't be. But many other LWians are interested, so I am leaving this post up.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Interest_in_Reason_Rally_meetup_1\">Discussion article for the meetup : <a href=\"/meetups/6k\">Interest in Reason Rally meetup?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Interest in Reason Rally meetup?", "anchor": "Discussion_article_for_the_meetup___Interest_in_Reason_Rally_meetup_", "level": 1}, {"title": "Discussion article for the meetup : Interest in Reason Rally meetup?", "anchor": "Discussion_article_for_the_meetup___Interest_in_Reason_Rally_meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T23:07:42.694Z", "modifiedAt": null, "url": null, "title": "Evidence For Simulation", "slug": "evidence-for-simulation", "viewCount": null, "lastCommentedAt": "2018-08-15T17:20:49.938Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TruePath", "createdAt": "2009-04-13T22:54:03.380Z", "isAdmin": false, "displayName": "TruePath"}, "userId": "n4hta6Fu6tbWprCff", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RAdmQD2ajdxGh4ebc/evidence-for-simulation", "pageUrlRelative": "/posts/RAdmQD2ajdxGh4ebc/evidence-for-simulation", "linkUrl": "https://www.lesswrong.com/posts/RAdmQD2ajdxGh4ebc/evidence-for-simulation", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evidence%20For%20Simulation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvidence%20For%20Simulation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAdmQD2ajdxGh4ebc%2Fevidence-for-simulation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evidence%20For%20Simulation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAdmQD2ajdxGh4ebc%2Fevidence-for-simulation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAdmQD2ajdxGh4ebc%2Fevidence-for-simulation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1653, "htmlBody": "<p>The recent <a href=\"http://www.overcomingbias.com/2012/01/silence-suggests-sim.html\">article</a> on <a href=\"http://www.overcomingbias.com\">overcomingbias</a> suggesting the Fermi paradox might be evidence our universe is indeed a simulation prompted me to wonder how one would go about gathering evidence for or against the hypothesis that we are living in a simulation.&nbsp; The Fermi paradox isn't very good evidence but there are much more promising places to look for this kind of evidence.&nbsp; Of course there is no sure fire way to learn that one isn't in a simulation, nothing prevents a simulation from being able to perfectly simulate a non-simulation universe, but there are certainly features of the universe that seem more likely if the universe was simulated and their presence or absence thus gives us evidence about whether we are in a simulation.</p>\n<p>&nbsp;</p>\n<p>In particular, the strategy suggested here is to consider the kind of fingerprints we might leave if we were writing a massive simulation.&nbsp; Of course the simulating creatures/processes may not labor under the same kind of restrictions we do in writing simulations (their laws of physics might support fundamentally different computational devices and any intelligence behind such a simulation might be totally alien).&nbsp; However, it's certainly reasonable to think we might be simulated by creatures like us so it's worth checking for the kinds of fingerprints we might leave in a simulation.</p>\n<p>&nbsp;</p>\n<h2>Computational Fingerprints</h2>\n<p>Simulations we write face several limitations on the computational power they can bring to bear on the problem and these limitations give rise to mitigation strategies we might observe in our own universe.&nbsp; These limitations include the following:</p>\n<ol>\n<li>Lack of access to non-computable oracles (except perhaps physical randomness).<br /><br />While theoretically nothing prevents the laws of physics from providing non-computable oracles, e.g., some experiment one could perform that discerns whether a given Turing machine halts (halting problem = 0') all indications suggest our universe does not provide such oracles.&nbsp; Thus our simulations are limited to modeling computable behavior.&nbsp; We would have no way to simulate a universe that had non-computable fundamental laws of physics (except perhaps randomness).<br /><br />It's tempting to conclude that the fact that our universe apparently follows computable laws of physics modulo randomness provides evidence for us being a simulation but this isn't entirely clear.&nbsp; After all had our laws of physics provided access to non-computable oracles we would presumably not expect simulations to be so limited either.&nbsp; Still, this is probably weak evidence for simulation as such non-computable behavior might well exist in the simulating universe but be practically infeasable to consult in computer hardware.&nbsp; Thus our probability for seeing non-computable behavior should be higher conditional on not being a simulation than conditional on being a simulation.</li>\n<li>Limited ability to access true random sources.<br /><br /><strong>The most compelling evidence we could discover of simulation would be the signature of a psuedo-random number generator in the outcomes of `random' QM events.&nbsp; </strong>Of course, as above, the simulating computers might have easy access to truly random number generators but it's also reasonable they lack practical access to true random numbers at a sufficient rate.</li>\n<li>Limited computational resources.&nbsp; <br /><br />We always want our simulations to run faster and require less resources but we are limited by the power of our hardware.&nbsp; In response we often resort to less accurate approximations when possible or otherwise engineer our simulation to require less computational resources.&nbsp; This might appear in a simulated universe in several ways.<br /> \n<ul>\n<li>Computationally easy basic laws of physics. For instance the underlying linearity of QM (absent collapse) is evidence we are living in a simulation as such computations have a low computational complexity.&nbsp; Another interesting piece of evidence would be discovering that an efficient global algorithm could be used that generates/uses collapse to speed computation.</li>\n<li>Limited detail/minimal feature size.&nbsp; An efficient simulation would be as course grained as possible while still yielding the desired behavior.&nbsp; Since we don't know what the desired behavior might be for a universe simulation it's hard to evaluate this criteria but the indications that space is fundamentally quantized (rather than allowing structure at arbitrarily small scales) seems to be evidence for simulation.</li>\n<li>Substitution of approximate calculations for expensive calculations in certain circumstances.&nbsp; Weak evidence could be gained here by merely observing that the large scale behavior of the universe admits efficient accurate approximations but <strong>the key piece of data to support a simulated universe would be observations revealing that sometimes the universe behaved as if it was following a less accurate approximation rather than behaving as fundamental physics prescribed.&nbsp; </strong>For instance discovering that distant galaxies behave as if they are a classical approximation rather than a quantum system would be extremely strong evidence.&nbsp; </li>\n<li>Ability to screen off or delay calculations in regions that aren't of interest.&nbsp; A simulation would be more efficient if it allowed regions of less interest to go unsimilated or at least to delay that simulation without impacting the regions of greater interest.&nbsp; While the finite speed of light arguably provides a way to delay simulation of regions of lesser interest QM's preservation of information and space-like quantum correlations may outweigh the finite speed of light on this point tipping it towards non-simulation.</li>\n</ul>\n</li>\n<li>Limitations on precision.<br /><br />Arguably this is just a variant of 3 but it has some different considerations.&nbsp; As with 3 we would expect a simulation to bottom out and not provide arbitrarily fine grained structure but in simulations precision issues also bring with them questions of stability.&nbsp; If the law's of physics turn out to be relatively unaffected by tiny computational errors that would push in the direction of simulation but if they are chaotic and quickly spiral out of control in response to these errors it would push against simulation.&nbsp; Since linear systems are virtually always stable te linearity of QM is yet again evidence for simulation.</li>\n<li>Limitations on sequential processing power.<br /><br />We find that finite speed limits on communication and other barriers prevent building arbitrarily fast single core processors.&nbsp; Thus we would expect a simulation to be more likely to admit highly parallel algorithms.&nbsp; While the finite speed of light provides some level of parallelizability (don't need to share all info with all processing units immediately) space-like QM correlations push against parallelizability.&nbsp; However, given the linearity of QM the most efficient parallel algorithms might well be semi-global algorithms like those used for various kinds of matrix manipulation.&nbsp; It would be most interesting if collapse could be shown to be a requirement/byproduct of such efficient algorithms.</li>\n<li>Imperfect hardware<br /><br />Finally there is the hope one might discover something like the Pentium division bug in the behavior of the universe.&nbsp; Similarly one might hope to discover unexplained correlations in deviations from normal behavior, e.g., correlations that occur at evenly spaced locations relative to some frame of reference, arising from transient errors in certain pieces of hardware.</li>\n</ol>\n<h2>Software Fingerprints</h2>\n<p>Another type of fingerprint that might be left are those resulting from the conceptual/organizational difficulties occuring in the software design process.&nbsp; For instance we might find fingerprints by looking for:</p>\n<ol>\n<li>Outright errors, particularly hard to spot/identify errors like race conditions or the like.&nbsp; Such errors might allow spillover information about other parts of the software design that would let us distinguish them from non-simulation physical effects.&nbsp; For instance, if the error occurs in a pattern that is reminiscent of a loop a simulation might execute but doesn't correspond to any plausible physical law it would be good evidence that it was truly an error.</li>\n<li>Conceptual simplicity in design.&nbsp; We might expect (as we apparently see) an easily drawn line between initial conditions and the rules of the simulation rather than physical laws which can't be so easily divided up, e.g., laws that take the form of global constraint satisfaction.&nbsp; Also relatively short laws rather than a long regress into greater and greater complexity at higher and higher energies would be expected in a simulation (but would be very very weak evidence).<br /></li>\n<li>Evidence of concrete representations.&nbsp; Even though mathematically relativity favors no reference frame over another often conceptually and computationally it is desierable to compute in a particular reference frame (just as it's often best to do linear algebra on a computer relative to an explicit basis).&nbsp; One might see evidence for such an effect in differences in the precision of results or rounding artifacts (like those seen in re-sized images).</li>\n</ol>\n<h2>Design Fingerprints</h2>\n<p>This category is so difficult I'm not really going to say much about it but I'm including it for completeness.&nbsp; If our universe is a simulation created by some intentional creature we might expect to see certain features receive more attention than others.&nbsp; Maybe we would see some really odd jiggering of initial conditions just to make sure some events of interest occurred but without a good idea what is of interest it is hard to see how this could be done.&nbsp; Another potential way for design fingerprints to show up is in the ease of data collection from the simulation.&nbsp; One might expect a simulation to make it particularly easy to sift out the interesting information from the rest of the data but again we don't have any idea what interesting might be.</p>\n<p>&nbsp;</p>\n<h2>Other Fingerprints</h2>\n<p>I'm hoping the readers will suggest some interesting new ideas as to what one might look for if one was serious about gathering evidence about whether we are in a simulation or not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RAdmQD2ajdxGh4ebc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 20, "extendedScore": null, "score": 8.3892379090103e-07, "legacy": true, "legacyId": "12446", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The recent <a href=\"http://www.overcomingbias.com/2012/01/silence-suggests-sim.html\">article</a> on <a href=\"http://www.overcomingbias.com\">overcomingbias</a> suggesting the Fermi paradox might be evidence our universe is indeed a simulation prompted me to wonder how one would go about gathering evidence for or against the hypothesis that we are living in a simulation.&nbsp; The Fermi paradox isn't very good evidence but there are much more promising places to look for this kind of evidence.&nbsp; Of course there is no sure fire way to learn that one isn't in a simulation, nothing prevents a simulation from being able to perfectly simulate a non-simulation universe, but there are certainly features of the universe that seem more likely if the universe was simulated and their presence or absence thus gives us evidence about whether we are in a simulation.</p>\n<p>&nbsp;</p>\n<p>In particular, the strategy suggested here is to consider the kind of fingerprints we might leave if we were writing a massive simulation.&nbsp; Of course the simulating creatures/processes may not labor under the same kind of restrictions we do in writing simulations (their laws of physics might support fundamentally different computational devices and any intelligence behind such a simulation might be totally alien).&nbsp; However, it's certainly reasonable to think we might be simulated by creatures like us so it's worth checking for the kinds of fingerprints we might leave in a simulation.</p>\n<p>&nbsp;</p>\n<h2 id=\"Computational_Fingerprints\">Computational Fingerprints</h2>\n<p>Simulations we write face several limitations on the computational power they can bring to bear on the problem and these limitations give rise to mitigation strategies we might observe in our own universe.&nbsp; These limitations include the following:</p>\n<ol>\n<li>Lack of access to non-computable oracles (except perhaps physical randomness).<br><br>While theoretically nothing prevents the laws of physics from providing non-computable oracles, e.g., some experiment one could perform that discerns whether a given Turing machine halts (halting problem = 0') all indications suggest our universe does not provide such oracles.&nbsp; Thus our simulations are limited to modeling computable behavior.&nbsp; We would have no way to simulate a universe that had non-computable fundamental laws of physics (except perhaps randomness).<br><br>It's tempting to conclude that the fact that our universe apparently follows computable laws of physics modulo randomness provides evidence for us being a simulation but this isn't entirely clear.&nbsp; After all had our laws of physics provided access to non-computable oracles we would presumably not expect simulations to be so limited either.&nbsp; Still, this is probably weak evidence for simulation as such non-computable behavior might well exist in the simulating universe but be practically infeasable to consult in computer hardware.&nbsp; Thus our probability for seeing non-computable behavior should be higher conditional on not being a simulation than conditional on being a simulation.</li>\n<li>Limited ability to access true random sources.<br><br><strong>The most compelling evidence we could discover of simulation would be the signature of a psuedo-random number generator in the outcomes of `random' QM events.&nbsp; </strong>Of course, as above, the simulating computers might have easy access to truly random number generators but it's also reasonable they lack practical access to true random numbers at a sufficient rate.</li>\n<li>Limited computational resources.&nbsp; <br><br>We always want our simulations to run faster and require less resources but we are limited by the power of our hardware.&nbsp; In response we often resort to less accurate approximations when possible or otherwise engineer our simulation to require less computational resources.&nbsp; This might appear in a simulated universe in several ways.<br> \n<ul>\n<li>Computationally easy basic laws of physics. For instance the underlying linearity of QM (absent collapse) is evidence we are living in a simulation as such computations have a low computational complexity.&nbsp; Another interesting piece of evidence would be discovering that an efficient global algorithm could be used that generates/uses collapse to speed computation.</li>\n<li>Limited detail/minimal feature size.&nbsp; An efficient simulation would be as course grained as possible while still yielding the desired behavior.&nbsp; Since we don't know what the desired behavior might be for a universe simulation it's hard to evaluate this criteria but the indications that space is fundamentally quantized (rather than allowing structure at arbitrarily small scales) seems to be evidence for simulation.</li>\n<li>Substitution of approximate calculations for expensive calculations in certain circumstances.&nbsp; Weak evidence could be gained here by merely observing that the large scale behavior of the universe admits efficient accurate approximations but <strong>the key piece of data to support a simulated universe would be observations revealing that sometimes the universe behaved as if it was following a less accurate approximation rather than behaving as fundamental physics prescribed.&nbsp; </strong>For instance discovering that distant galaxies behave as if they are a classical approximation rather than a quantum system would be extremely strong evidence.&nbsp; </li>\n<li>Ability to screen off or delay calculations in regions that aren't of interest.&nbsp; A simulation would be more efficient if it allowed regions of less interest to go unsimilated or at least to delay that simulation without impacting the regions of greater interest.&nbsp; While the finite speed of light arguably provides a way to delay simulation of regions of lesser interest QM's preservation of information and space-like quantum correlations may outweigh the finite speed of light on this point tipping it towards non-simulation.</li>\n</ul>\n</li>\n<li>Limitations on precision.<br><br>Arguably this is just a variant of 3 but it has some different considerations.&nbsp; As with 3 we would expect a simulation to bottom out and not provide arbitrarily fine grained structure but in simulations precision issues also bring with them questions of stability.&nbsp; If the law's of physics turn out to be relatively unaffected by tiny computational errors that would push in the direction of simulation but if they are chaotic and quickly spiral out of control in response to these errors it would push against simulation.&nbsp; Since linear systems are virtually always stable te linearity of QM is yet again evidence for simulation.</li>\n<li>Limitations on sequential processing power.<br><br>We find that finite speed limits on communication and other barriers prevent building arbitrarily fast single core processors.&nbsp; Thus we would expect a simulation to be more likely to admit highly parallel algorithms.&nbsp; While the finite speed of light provides some level of parallelizability (don't need to share all info with all processing units immediately) space-like QM correlations push against parallelizability.&nbsp; However, given the linearity of QM the most efficient parallel algorithms might well be semi-global algorithms like those used for various kinds of matrix manipulation.&nbsp; It would be most interesting if collapse could be shown to be a requirement/byproduct of such efficient algorithms.</li>\n<li>Imperfect hardware<br><br>Finally there is the hope one might discover something like the Pentium division bug in the behavior of the universe.&nbsp; Similarly one might hope to discover unexplained correlations in deviations from normal behavior, e.g., correlations that occur at evenly spaced locations relative to some frame of reference, arising from transient errors in certain pieces of hardware.</li>\n</ol>\n<h2 id=\"Software_Fingerprints\">Software Fingerprints</h2>\n<p>Another type of fingerprint that might be left are those resulting from the conceptual/organizational difficulties occuring in the software design process.&nbsp; For instance we might find fingerprints by looking for:</p>\n<ol>\n<li>Outright errors, particularly hard to spot/identify errors like race conditions or the like.&nbsp; Such errors might allow spillover information about other parts of the software design that would let us distinguish them from non-simulation physical effects.&nbsp; For instance, if the error occurs in a pattern that is reminiscent of a loop a simulation might execute but doesn't correspond to any plausible physical law it would be good evidence that it was truly an error.</li>\n<li>Conceptual simplicity in design.&nbsp; We might expect (as we apparently see) an easily drawn line between initial conditions and the rules of the simulation rather than physical laws which can't be so easily divided up, e.g., laws that take the form of global constraint satisfaction.&nbsp; Also relatively short laws rather than a long regress into greater and greater complexity at higher and higher energies would be expected in a simulation (but would be very very weak evidence).<br></li>\n<li>Evidence of concrete representations.&nbsp; Even though mathematically relativity favors no reference frame over another often conceptually and computationally it is desierable to compute in a particular reference frame (just as it's often best to do linear algebra on a computer relative to an explicit basis).&nbsp; One might see evidence for such an effect in differences in the precision of results or rounding artifacts (like those seen in re-sized images).</li>\n</ol>\n<h2 id=\"Design_Fingerprints\">Design Fingerprints</h2>\n<p>This category is so difficult I'm not really going to say much about it but I'm including it for completeness.&nbsp; If our universe is a simulation created by some intentional creature we might expect to see certain features receive more attention than others.&nbsp; Maybe we would see some really odd jiggering of initial conditions just to make sure some events of interest occurred but without a good idea what is of interest it is hard to see how this could be done.&nbsp; Another potential way for design fingerprints to show up is in the ease of data collection from the simulation.&nbsp; One might expect a simulation to make it particularly easy to sift out the interesting information from the rest of the data but again we don't have any idea what interesting might be.</p>\n<p>&nbsp;</p>\n<h2 id=\"Other_Fingerprints\">Other Fingerprints</h2>\n<p>I'm hoping the readers will suggest some interesting new ideas as to what one might look for if one was serious about gathering evidence about whether we are in a simulation or not.</p>", "sections": [{"title": "Computational Fingerprints", "anchor": "Computational_Fingerprints", "level": 1}, {"title": "Software Fingerprints", "anchor": "Software_Fingerprints", "level": 1}, {"title": "Design Fingerprints", "anchor": "Design_Fingerprints", "level": 1}, {"title": "Other Fingerprints", "anchor": "Other_Fingerprints", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-27T23:57:02.015Z", "modifiedAt": null, "url": null, "title": "Deliberately trying to annoy simulators?", "slug": "deliberately-trying-to-annoy-simulators", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:56.866Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nM9uuRC8gArtjQ5KG/deliberately-trying-to-annoy-simulators", "pageUrlRelative": "/posts/nM9uuRC8gArtjQ5KG/deliberately-trying-to-annoy-simulators", "linkUrl": "https://www.lesswrong.com/posts/nM9uuRC8gArtjQ5KG/deliberately-trying-to-annoy-simulators", "postedAtFormatted": "Friday, January 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deliberately%20trying%20to%20annoy%20simulators%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeliberately%20trying%20to%20annoy%20simulators%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnM9uuRC8gArtjQ5KG%2Fdeliberately-trying-to-annoy-simulators%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deliberately%20trying%20to%20annoy%20simulators%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnM9uuRC8gArtjQ5KG%2Fdeliberately-trying-to-annoy-simulators", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnM9uuRC8gArtjQ5KG%2Fdeliberately-trying-to-annoy-simulators", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p>I regularly seek inspiration by taking long solo walks; and during my most recent such, considering what practical consequences (if any) there would be of the universe I know being a simulation - something flipped in my head, and I thought to myself, \"Screw the simulators. If I'm the first copy of me, I should make it as hard as possible for any simulation of me to keep up with me - and if I'm a simulation, I'm going to try to do even better than my original did.\"</p>\n<p>Ignoring the impracticality of trying to out-do myself, is there anything that someone living in an 'original' universe can do that would make it harder for a future simulator to reproduce them? And, mirror-wise, is there anything someone in a simulated universe could do to differentiate themselves from their original? And, if the answer to either question is 'yes'... would it be a good or bad idea to try?</p>\n<p>&nbsp;</p>\n<p>(And is there any way to gather any actual data that might support the answers to such questions, instead of merely making guesses of a similar nature to classic college/stoner \"Our whole universe could be, like, an /atom/, man\" musings?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nM9uuRC8gArtjQ5KG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 8.389427800009691e-07, "legacy": true, "legacyId": "12448", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T02:24:19.621Z", "modifiedAt": null, "url": null, "title": "Help! Name suggestions needed for Rationality-Inst!", "slug": "help-name-suggestions-needed-for-rationality-inst", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.510Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YzzcrM92toD9dudau/help-name-suggestions-needed-for-rationality-inst", "pageUrlRelative": "/posts/YzzcrM92toD9dudau/help-name-suggestions-needed-for-rationality-inst", "linkUrl": "https://www.lesswrong.com/posts/YzzcrM92toD9dudau/help-name-suggestions-needed-for-rationality-inst", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help!%20Name%20suggestions%20needed%20for%20Rationality-Inst!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp!%20Name%20suggestions%20needed%20for%20Rationality-Inst!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzzcrM92toD9dudau%2Fhelp-name-suggestions-needed-for-rationality-inst%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help!%20Name%20suggestions%20needed%20for%20Rationality-Inst!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzzcrM92toD9dudau%2Fhelp-name-suggestions-needed-for-rationality-inst", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYzzcrM92toD9dudau%2Fhelp-name-suggestions-needed-for-rationality-inst", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p>The Singularity Institute wants to spin off a separate rationality-related organization. &nbsp;(If it's not obvious what this would do, it would e.g. develop things like the <a href=\"/lw/9hb/position_design_and_write_rationality_curriculum/\">rationality katas</a> as material for local meetups, high schools and colleges, bootcamps and seminars, have an annual conference and sessions in different cities and so on and so on.)</p>\n<p>We can't think of a name for this organization.</p>\n<p>We can't think of any names that seem good enough to be audience-tested.</p>\n<p>We don't have any ideas good enough that we'd want to mention them in this post.</p>\n<p>Help.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YzzcrM92toD9dudau", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 28, "extendedScore": null, "score": 7.5e-05, "legacy": true, "legacyId": "12453", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 299, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ifL8f4Xzy2D9Bb6zs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T04:20:48.176Z", "modifiedAt": null, "url": null, "title": "The Substitution Principle", "slug": "the-substitution-principle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.396Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LHtMNz7ua8zu4rSZr/the-substitution-principle", "pageUrlRelative": "/posts/LHtMNz7ua8zu4rSZr/the-substitution-principle", "linkUrl": "https://www.lesswrong.com/posts/LHtMNz7ua8zu4rSZr/the-substitution-principle", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Substitution%20Principle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Substitution%20Principle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLHtMNz7ua8zu4rSZr%2Fthe-substitution-principle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Substitution%20Principle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLHtMNz7ua8zu4rSZr%2Fthe-substitution-principle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLHtMNz7ua8zu4rSZr%2Fthe-substitution-principle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1274, "htmlBody": "<p><strong>Partial re-interpretation of:</strong> <a href=\"/lw/8gv/the_curse_of_identity/\">The Curse of Identity</a><br /><strong>Also related to:</strong> <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans Are Not Automatically Strategic</a>, <a href=\"/lw/lg/the_affect_heuristic/\">The Affect Heuristic</a>, <a href=\"/lw/jg/planning_fallacy/\">The Planning Fallacy</a>, <a href=\"/lw/j5/availability/\">The Availability Heuristic</a>, <a href=\"/lw/ji/conjunction_fallacy/\">The Conjunction Fallacy</a>, <a href=\"/lw/8q8/urges_vs_goals_the_analogy_to_anticipation_and/\">Urges vs. Goals</a>, <a href=\"/lw/7mx/your_inner_google/\">Your Inner Google</a>, signaling, etc...</p>\n<p>What are the best careers for making a lot of money?</p>\n<p>Maybe you've thought about this question a lot, and have researched it enough to have a well-formed opinion. But the chances are that even if you hadn't, some sort of an answer popped into your mind right away. Doctors make a lot of money, maybe, or lawyers, or bankers. Rock stars, perhaps.</p>\n<p>You probably realize that this is a difficult question. For one, there's the question of who we're talking about. One person's strengths and weaknesses might make them more suited for a particular career path, while for another person, another career is better. Second, the question is not clearly defined. Is a career with a small chance of making it rich and a large chance of remaining poor a better option than a career with a large chance of becoming wealthy but no chance of becoming rich? Third, whoever is asking this question probably does so because they are thinking about what to do with their lives. So you probably don't want to answer on the basis of what career lets you make a lot of money today, but on the basis of which one will do so in the near future. That requires tricky technological and social forecasting, which is quite difficult. And so on.</p>\n<p>Yet, despite all of these uncertainties, some sort of an answer probably came to your mind as soon as you heard the question. And if you hadn't considered the question before, your answer probably didn't take any of the above complications into account. It's as if your brain, while generating an answer, never even considered them.</p>\n<p>The thing is, it probably didn't.</p>\n<p>Daniel Kahneman, in <a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/\"><em>Thinking, Fast and Slow</em></a>, extensively discusses what I call the Substitution Principle:</p>\n<blockquote>\n<p>If a satisfactory answer to a hard question is not found quickly, System 1 will find a related question that is easier and will answer it. (Kahneman, p. 97)</p>\n</blockquote>\n<p>System 1, <a href=\"/lw/2ey/a_taxonomy_of_bias_the_cognitive_miser/\">if you recall</a>, is the quick, dirty and parallel part of our brains that renders instant judgements, without thinking about them in too much detail. In this case, the actual question that was asked was &rdquo;what are the best careers for making a lot of money&rdquo;. The question that was actually answered was &rdquo;what careers have I come to associate with wealth&rdquo;.</p>\n<p>Here are some other examples of substitution that Kahneman gives:<br /><a id=\"more\"></a></p>\n<ul>\n<li><em>How much would you contribute to save an endangered species?</em> becomes <em>How much emotion do I feel when I think of dying dolphins?</em></li>\n<li><em>How happy are you with your life these days?</em> becomes <em>What is my mood right now?</em></li>\n<li><em>How popular will the president be six months from now?</em> becomes <em>How popular is the president right now?</em></li>\n<li><em>How should financial advisors who prey on the elderly be punished?</em> becomes <em>How much anger do I feel when I think of financial predators?</em></li>\n</ul>\n<ul>\n</ul>\n<p>All things considered, this heuristic probably works pretty well most of the time. The easier questions are not meaningless: while not completely accurate, their answers are still generally correlated with the correct answer. And a lot of the time, that's good enough.</p>\n<p>But I think that the Substitution Principle is also the mechanism by which most of our biases work. In <a href=\"/lw/8gv/the_curse_of_identity/\">The Curse of Identity</a>, I wrote:</p>\n<blockquote>\n<p>In each case, I thought I was working for a particular goal (become capable of doing useful Singularity work, advance the cause of a political party, do useful Singularity work). But as soon as I set that goal, my brain automatically and invisibly re-interpreted it as the goal of doing something that gave the impression of doing prestigious work for a cause (spending all my waking time working, being the spokesman of a political party, writing papers or doing something else few others could do).</p>\n</blockquote>\n<p>As <a href=\"/lw/8q8/urges_vs_goals_the_analogy_to_anticipation_and/\">Anna correctly pointed out</a>, I resorted to a signaling explanation here, but a signaling explanation may not be necessary. Let me reword that previous generalization: As soon as I set a goal, my brain asked itself how that goal might be achieved, realized that this was a difficult question, and substituted it with an easier one. So &rdquo;how could I advance X&rdquo; became &rdquo;what are the kinds of behaviors that are commonly associated with advancing X&rdquo;. That my brain happened to pick the most prestigious ways of advancing X might be simply because prestige is often correlated with achieving a lot.</p>\n<p>Does this exclude the signaling explanation? Of course not. My behavior is probably still driven by signaling and status concerns. One of the mechanisms by which this works might be that such considerations get disproportionately taken into account when choosing a heuristic question. And a lot of the examples I gave in The Curse of Identity seem hard to justify without a signaling explanation. But signaling need not to be the <em>sole</em> explanation. Our brains may just resort to poor heuristics a lot.</p>\n<p>Some other biases and how the Substitution Principle is related to them (many of these are again borrowed from Thinking, Fast and Slow):<strong><a href=\"/lw/jg/planning_fallacy/\"></a></strong></p>\n<p><strong><a href=\"/lw/jg/planning_fallacy/\">The Planning Fallacy</a>:</strong> &rdquo;How much time will this take&rdquo; becomes something like &rdquo;How much time did it take for me to get this far, and many times should that be multiplied to get to completion.&rdquo; (Doesn't take into account unexpected delays and interruptions, waning interest, etc.)<strong><a href=\"/lw/j5/availability/\"></a></strong></p>\n<p><strong><a href=\"/lw/j5/availability/\">The Availability Heuristic</a>:</strong> &rdquo;How common is this thing&rdquo; or &rdquo;how frequently does this happen&rdquo; becomes &rdquo;how easily do instances of this come to mind&rdquo;.<strong></strong></p>\n<p><strong>Over-estimating your own share of household chores:</strong> &rdquo;What fraction of chores have I done&rdquo; becomes &rdquo;how many chores do I remember doing, as compared to the amount of chores I remember my partner doing.&rdquo; (You will naturally remember more of the things that you've done than that somebody else has done, possibly when you weren't even around.)<strong><a href=\"https://www.rci.rutgers.edu/~gbc/psycDM/Loewenstein2005.pdf\"></a></strong></p>\n<p><strong><a href=\"https://www.rci.rutgers.edu/~gbc/psycDM/Loewenstein2005.pdf\">Being in an emotionally &rdquo;cool&rdquo; state and over-estimating your degree of control in an emotionally &rdquo;hot&rdquo; state</a> (angry, hungry, sexually aroused, etc.):</strong> &rdquo;How well could I resist doing X in that state&rdquo; becomes &rdquo;how easy does resisting X feel like now&rdquo;.<strong><a href=\"/lw/ji/conjunction_fallacy/\"></a></strong></p>\n<p><strong><a href=\"/lw/ji/conjunction_fallacy/\">The Conjunction Fallacy</a>:</strong> &rdquo;What's the probability that Linda is a feminist&rdquo; becomes &rdquo;how representative is Linda of my conception of feminists&rdquo;.<strong></strong></p>\n<p><strong>People voting for politicians for seemingly irrelevant reasons: </strong>&rdquo;How well would this person do his job as a politician&rdquo; becomes &rdquo;how much do I like this person.&rdquo; (A better heuristic than you might think, considering that we like people who like us, owe us favors, resemble us, etc. - in the ancestral environment, supporting the leader you liked the most was probably a pretty good proxy for supporting the leader who was most likely to aid you in return.)</p>\n<p>And so on.</p>\n<p>The important point is to <strong>learn to recognize the situations where you're confronting a difficult problem, and your mind gives you an answer right away.</strong> If you don't have extensive expertise with the problem &ndash; or even if you do &ndash; it's likely that the answer you got wasn't actually the answer to the question you asked. So before you act, stop to consider what heuristic question your brain might actually have used, and whether it makes sense given the situation that you're thinking about.</p>\n<p>This involves three skills: first <strong>recognizing a problem as a difficult one</strong>, then <strong>figuring out what heuristic you might have used</strong>, and finally <strong>coming up with a better solution</strong>. I intend to develop something on how to <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">taskify</a> those skills, but if you have any ideas for how that might be achieved, let's hear them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 2, "DLskYNGdAGDFpxBF8": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LHtMNz7ua8zu4rSZr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 84, "baseScore": 104, "extendedScore": null, "score": 0.000208, "legacy": true, "legacyId": "12423", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 104, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tAXrD8Y6hcJ8dt6Nt", "PBRWb2Em5SNeWYwwB", "Kow8xRzpfkoY7pa69", "CPm5LTwHrvBJCa9h5", "R8cpqD3NA4rZxRdQ4", "QAK43nNCTQQycAcYe", "wmjPGE8TZKNLSKzm4", "9vnWFwng8QzEnBT8z", "qMTzv8ATgDtfLq9ME", "Pmfk7ruhWaHj9diyv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T13:31:07.828Z", "modifiedAt": null, "url": null, "title": "HPMOR: What could've been done better?", "slug": "hpmor-what-could-ve-been-done-better", "viewCount": null, "lastCommentedAt": "2020-12-20T04:30:32.660Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anubhav", "createdAt": "2012-01-05T10:50:52.734Z", "isAdmin": false, "displayName": "Anubhav"}, "userId": "pe9DTYwyvLCZPzsPf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QtEhktxBExYAiDeLj/hpmor-what-could-ve-been-done-better", "pageUrlRelative": "/posts/QtEhktxBExYAiDeLj/hpmor-what-could-ve-been-done-better", "linkUrl": "https://www.lesswrong.com/posts/QtEhktxBExYAiDeLj/hpmor-what-could-ve-been-done-better", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20HPMOR%3A%20What%20could've%20been%20done%20better%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHPMOR%3A%20What%20could've%20been%20done%20better%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtEhktxBExYAiDeLj%2Fhpmor-what-could-ve-been-done-better%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=HPMOR%3A%20What%20could've%20been%20done%20better%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtEhktxBExYAiDeLj%2Fhpmor-what-could-ve-been-done-better", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtEhktxBExYAiDeLj%2Fhpmor-what-could-ve-been-done-better", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p><em><strong>Warning: </strong>As per the <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">official spoiler policy</a>, the following discussion may contain <strong>unmarked spoilers </strong>for up to the current chapter of </em>the Methods of Rationality. <em>Proceed at your own risk.</em></p>\n<p>Assume HPMOR was written by a super-intelligence implementing the CEV of Eliezer Yudkowsky and assorted literary critics. What would it have written differently?</p>\n<p>... is what I want to know, but that's hard to answer. So here's an easier question:</p>\n<p>In what ways do you think Eliezer's characterisations/world-building/plot-fu are sub-optimal? &lt;<a href=\"/lw/ka/hold_off_on_proposing_solutions\">optional</a>&gt; How could they be made less sub-optimal? &lt;/optional&gt;</p>\n<p>(My own ideas are in the comments.)</p>\n<p>To put it another way... Assume a group of intrepid fanfic writers in the late 2020s are planning to write a reboot. What parts of Eliezer's story do you think they should tweak?</p>\n<p>And just to make sure we're all on the same page: Eliezer isn't going to go back and change anything he's written to bring it in line with anything suggested here. This is <em>purely </em>an \"Ah, just consider the possibilities!\" thread.</p>\n<p>... which means that we can safely suggest drastic rewrites encompassing 30 chapters or something.&nbsp;Or change fundamental facts about the world.</p>\n<p>(Exercise due restraint on this one. Getting rid of the Ministry/the Noble Houses/blood purism would probably turn the story into something completely different; this isn't what we're trying to do here.)</p>\n<p>With that, let the nit-picking begin!!&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QtEhktxBExYAiDeLj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "12456", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T15:51:39.017Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta", "slug": "meetup-atlanta-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ezzvSiNyppQxWqPH9/meetup-atlanta-2", "pageUrlRelative": "/posts/ezzvSiNyppQxWqPH9/meetup-atlanta-2", "linkUrl": "https://www.lesswrong.com/posts/ezzvSiNyppQxWqPH9/meetup-atlanta-2", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FezzvSiNyppQxWqPH9%2Fmeetup-atlanta-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FezzvSiNyppQxWqPH9%2Fmeetup-atlanta-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FezzvSiNyppQxWqPH9%2Fmeetup-atlanta-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/6l\">Atlanta</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">04 February 2012 06:30:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>All,</p>\n<p>Our next meeting is February 4th.</p>\n<p>From now on we will be meeting every 1st and 3rd Saturday of the month at 6:30pm at Chocolate Coffee in Decatur:</p>\n<div class=\"md\"><a rel=\"nofollow\" href=\"http://www.mychocolatecoffee.com/\">http://www.mychocolatecoffee.com/</a><br />2094 North Decatur Road, Decatur, GA 30033-5367<br />(404) 982-0790</div>\nHere is the official agenda for our next meeting:<br /><br />\n<p><a rel=\"nofollow\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions</a><br />1.16 Fake Causality<br /> 1.17 Semantic Stopsigns <br />1.18 Mysterious Answers to Mysterious Questions <br />1.19 The Futility of Emergence <br />1.20 Say Not \"Complexity\"</p>\n<p>I believe we were also planning on discussing the anthropic principle.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/6l\">Atlanta</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ezzvSiNyppQxWqPH9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.393104597244694e-07, "legacy": true, "legacyId": "12457", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta\">Discussion article for the meetup : <a href=\"/meetups/6l\">Atlanta</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">04 February 2012 06:30:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>All,</p>\n<p>Our next meeting is February 4th.</p>\n<p>From now on we will be meeting every 1st and 3rd Saturday of the month at 6:30pm at Chocolate Coffee in Decatur:</p>\n<div class=\"md\"><a rel=\"nofollow\" href=\"http://www.mychocolatecoffee.com/\">http://www.mychocolatecoffee.com/</a><br>2094 North Decatur Road, Decatur, GA 30033-5367<br>(404) 982-0790</div>\nHere is the official agenda for our next meeting:<br><br>\n<p><a rel=\"nofollow\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions</a><br>1.16 Fake Causality<br> 1.17 Semantic Stopsigns <br>1.18 Mysterious Answers to Mysterious Questions <br>1.19 The Futility of Emergence <br>1.20 Say Not \"Complexity\"</p>\n<p>I believe we were also planning on discussing the anthropic principle.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Atlanta1\">Discussion article for the meetup : <a href=\"/meetups/6l\">Atlanta</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta", "anchor": "Discussion_article_for_the_meetup___Atlanta", "level": 1}, {"title": "Discussion article for the meetup : Atlanta", "anchor": "Discussion_article_for_the_meetup___Atlanta1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T17:28:34.576Z", "modifiedAt": null, "url": null, "title": "The Neglected Virtue of Curiosity", "slug": "the-neglected-virtue-of-curiosity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:04.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "vallinder", "createdAt": "2011-03-14T11:52:07.333Z", "isAdmin": false, "displayName": "vallinder"}, "userId": "onAYNLtS8wFHFH6k5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eCZjrm9JBDSGvEA9o/the-neglected-virtue-of-curiosity", "pageUrlRelative": "/posts/eCZjrm9JBDSGvEA9o/the-neglected-virtue-of-curiosity", "linkUrl": "https://www.lesswrong.com/posts/eCZjrm9JBDSGvEA9o/the-neglected-virtue-of-curiosity", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Neglected%20Virtue%20of%20Curiosity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Neglected%20Virtue%20of%20Curiosity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCZjrm9JBDSGvEA9o%2Fthe-neglected-virtue-of-curiosity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Neglected%20Virtue%20of%20Curiosity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCZjrm9JBDSGvEA9o%2Fthe-neglected-virtue-of-curiosity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCZjrm9JBDSGvEA9o%2Fthe-neglected-virtue-of-curiosity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2203, "htmlBody": "<blockquote>\n<p>Curiosity is the most superficial of all the affections; it changes its objects perpetually; it has an appetite which is sharp, but very easily satisfied; and it has always an appearance of giddiness, restlessness and anxiety.</p>\n</blockquote>\n<p>- Edmund Burke, <a href=\"http://www.amazon.com/Philosophical-Enquiry-Sublime-Beautiful-Classics/dp/0199537887/\">A Philosophical Enquiry into the Origin of our Ideas of the Sublime and Beautiful</a></p>\n<p>Curiosity is the <a href=\"http://yudkowsky.net/rational/virtues\">first virtue</a>: \"[a] burning itch to know is higher than a solemn vow to pursue truth.\" Yet I find surprisingly little material about curiosity on Less Wrong. Sure, <a href=\"/user/AnnaSalamon\">AnnaSalamon</a> shows us how to <a href=\"/lw/4ku/use_curiosity/\">use curiosity</a>, <a href=\"/user/lukeprog\">lukeprog</a> ponders <a href=\"/lw/96j/what_curiosity_looks_like/\">what curiosity looks like</a>, <a href=\"/user/Elizabeth\">Elizabeth</a> discusses the <a href=\"/lw/4rm/the_limits_of_curiosity/\">limits of curiosity</a>, and <a href=\"/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a> offers the <a href=\"/lw/jz/the_meditation_on_curiosity/\">meditation on curiosity</a>. But we have never been provided with an overview of the <em>science</em> of curiosity, as has been done for <a href=\"/lw/3w3/how_to_beat_procrastination/\">procrastination</a>, <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">motivation</a>, and <a href=\"/lw/4su/how_to_be_happy/\">happiness</a>, for instance. Perhaps most Less Wrongers score high on curiosity already, so there hasn't been much need to study it. But <em>I</em> often wish I were more curious. Some of you may, too. For the rest, what follows is a journey <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">back to the basics of rationality</a>.</p>\n<p>What is curiosity, and how can we become more curious?</p>\n<h2>Curiosity: what?</h2>\n<p>We have all felt that burning itch to know on at least some occasions. It leads us to ask questions,<sup>1</sup> manipulate interesting objects,<sup>2</sup> and continue doing challenging tasks.<sup>3</sup> Kashdan and Fincham (2004) define curiosity as \"the volitional recognition, pursuit, and self-regulation of novel and challenging opportunities (reflecting intrinsic values and interests)\". Loewenstein (2000) also emphasizes the fact that curiosity occurs in the absence of an extrinsic reward. All theories of curiosity agree that its short term function is to learn and explore. In the longer term, curiosity aids us in building knowledge and competence.<sup>4</sup> When curious, we enter a state of <a href=\"http://en.wikipedia.org/wiki/Flow_(psychology)\">flow</a>, and become immersed in whatever it is we are doing.<sup>5</sup></p>\n<p>Researchers distinguish between <em>state curiosity</em> and <em>trait curiosity</em>. State curiosity is evoked by external situations. Why is the sky blue? How does <a href=\"http://www.youtube.com/watch?v=Ws6AAhTw7RA\">quantum levitation</a> work? Trait curiosity on the other hand is a characteristic that people possess to varying degrees. Someone with high trait curiosity seeks out complexity, novelty, conflict, and uncertainty.<sup>6 7</sup></p>\n<p>Curiosity can be measured across several dimensions (Kashdan, 2009):</p>\n<ul>\n<li><em>Intensity</em>. How strong is that burning itch to know?</li>\n<li><em>Frequency</em>. How often do you feel it?</li>\n<li><em>Durability</em>. How long does it last?</li>\n<li><em>Breadth</em>. How many topics evoke it?</li>\n<li><em>Depth</em>. Does the itch remain as you learn more about a topic?</li>\n</ul>\n<p>It has been suggested that trait curiosity simply measures the frequency and intensity of state curiosity.<sup>8</sup></p>\n<p>I suspect many of you are particularly interested in <em>epistemic curiosity.</em> Epistemic curiosity measures our desire for knowledge and understanding, rather than, say, our desire to explore new cultures or meet new people. This notion is closely related to other psychological constructs such as <em>need for cognition</em>, <em>typical intellectual engagement</em> and <em>openness for ideas</em>, and some have argued that there isn't enough evidence for treating them as separate things.<sup>9</sup> With that in mind, it might be worth examining the literature on these notions closer as well.</p>\n<p>Early in our lives, curiosity will typically increase, only to start decreasing later. One study found that, on average, curiosity increases from age 12 until people attend college.<sup>10</sup> By the age of 30, curiosity typically starts to decline. But some people manage to retain their curiosity even as they grow older. One study followed a group of men and women from college age until later adulthood. Those that were identified as very curious later in life had many characteristics in common: rich emotional lives with both positive and negative feelings, actively searching for meaning in life, don't experience themselves as being restricted by social norms, and chose careers that gave them opportunities to be independent and creative.<sup>11</sup> More broadly, curiosity is correlated with the <a href=\"http://en.wikipedia.org/wiki/Big_Five_personality_traits\">Big Five</a> trait of Openness.<sup>12</sup></p>\n<h2>The Benefits of Curiosity</h2>\n<p>Much research makes it plausible that curiosity is in fact the first virtue. It has a wide array of benefits, not only related to rationality or intelligence.<sup>13</sup> One study found that it accounts for roughly 10% of the variance in achievement and performance outcomes.<sup>14</sup> In particular, studies indicate that curiosity is useful for the following:</p>\n<ul>\n<li><em>Health</em>. Curious people are more likely to live longer, and less likely to develop Alzheimer's disease, for example.<sup>15</sup></li>\n<li><em>Intelligence</em>. Being curious at an early age is a good predictor of intelligence later in life, even when initial intelligence is taken into account.<sup>16</sup></li>\n<li><em>Meaning and purpose in life</em>. Curious people are more likely to develop interests, hobbies, and passions, which typically increase feelings of purpose.<sup>17</sup></li>\n<li><em>Social relationships</em>. Curious people report more satisfying relationships, and are also more prone to develop new relationships with strangers.<sup>18</sup></li>\n<li><em>Happiness</em>. Increased curiosity is associated with a moderate increase in happiness and well-being.<sup>19</sup> A lack of curiosity has also been linked to negative emotions, such as depression.<sup>20</sup></li>\n</ul>\n<p>Beginning in the mid-70s, researchers have spent much effort attempting to measure curiosity. Unfortunately, attempts to cross-validate such measures have usually produced low intercorrelations (Loewenstein 1994).</p>\n<p>Luckily for those who wish they were more curious, curiosity is a malleable psychological state. It is very much influenced by social contexts, and other individual differences.<sup>21</sup> Relish <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">the good news of situationist psychology</a>!</p>\n<h2>Curiosity: how?</h2>\n<p>Curiosity, it seems, is a big deal. So what can we do to become more curious? Kashdan and Fincham (2004) focus on three factors correlated with curiosity: autonomy, competence, and relatedness.</p>\n<p><em>Autonomy.</em> People are more task curious when given more choice,<sup>22</sup> and when given more information and encouragement.<sup>23</sup> On the other hand, threats, punishment, negative feedback and surveillance all have negative effects on task curiosity. A meta-analysis found that the same goes for external rewards, though the effect was more robust for interesting, compared to boring tasks.<sup>24</sup></p>\n<p><em>Competence.</em> Events that make individuals believe they can interact effectively with the environment (<em>perceived competence</em>) or that give them the desire to do so (<em>competence valuation</em>), will lead to enhanced curiosity.<sup>25</sup> Sincere praise increases both perceived competence and competence valuation, and could therefore be a useful way of increasing curiosity.<sup>26</sup></p>\n<p><em>Relatedness.</em> Feelings of relatedness&mdash;feeling connected to others, and believing your emotional experiences are acknowledged&mdash;also appear to increase curiosity.<sup>27</sup> In particular, relatedness has been shown to improve both curiosity and performance in athletic,<sup>28</sup> academic<sup>29</sup> and work contexts.<sup>30</sup> Feeling comfortable and safe also encourages curiosity.<sup>31</sup></p>\n<p>Based on these three factors, Kashdan and Fincham (2004, p. 490) propose a table of empirically-informed \"curiosity interventions\". These include</p>\n<blockquote>\n<ul>\n<li>Create tasks that capitalize on novelty, complexity, ambiguity, variety, and surprise.</li>\n<li>Purposely place individuals in contexts that are discrepant with their experience, skills, and personality.</li>\n<li>Create tasks that can be conducted independently.</li>\n<li>Allow opportunities for play.</li>\n<li>Create tasks that are personally meaningful.</li>\n<li>Create challenges that match or slightly exceed current skills.</li>\n<li>Create enjoyable group based activities.</li>\n</ul>\n</blockquote>\n<p>Unfortunately, most studies on curiosity have focused on narrow areas, and so the breadth of curiosity has not been well-examined. Factors that correlate with curiosity in one domain may not do so in others.<sup>32</sup> The study of curiosity is still in its infancy, and most of these interventions remain to be experimentally tested. But as of today, these might be the best tools available.</p>\n<p>That was a summary of what we know about curiosity. Now go out and explore!</p>\n<h2>Notes</h2>\n<p><sup>1</sup>Evans&nbsp;(1971) found that asking lots of questions is correlated with one of three scales of the 'Ontario Test of Intrinsic Motivation'. Peters (1978) reports that students with high trait curiosity asked more questions when their instructor was perceived as non-threatening. If, on the other hand, the instructor was perceived as threatening, no difference was found between students with high trait curiosity and those with low trait curiosity.&nbsp;</p>\n<p><sup>2</sup>Reeve and Nix (1997) found, among other things, that hand speed while performing a puzzle task correlated with self-reported intrinsic motivation.</p>\n<p><sup>3</sup>See&nbsp;Sansone and Smith (2000) for a review</p>\n<p><sup>4</sup>Kashdan and Silvia (2009)</p>\n<p><sup>5</sup>Curiosity is closely related to <em>interest</em> and <em>intrinsic motivation</em> (Kashdan and Fincham 2004), and consequently there is a considerable overlap between the study of these phenomena. Many researchers treat these terms interchangeably.</p>\n<p><sup>6</sup>Kashdan and Fincham (2004). Loewenstein (1994) raises some doubts about the usefulness of distinguishing between state curiosity and trait curiosity.</p>\n<p><sup>7</sup>See Litman and Silvia (2006) for an overview of ways to measure trait curiosity. Like many other psychological traits, curiosity is mostly measured through questionnaires. Beginning in the mid-70s, researchers developed many different ways of measuring curiosity. Unfortunately, attempts to cross-validate such measures have typically produced low intercorrelations (Loewenstein 1994)</p>\n<p><sup>8</sup>Silvia (2008)</p>\n<p><sup>9</sup>Mussel (2010)</p>\n<p><sup>10</sup>McCrae et al (2002)</p>\n<p><sup>11</sup>Kashdan (2009)</p>\n<p><sup>12</sup>McCrae (1996)</p>\n<p><sup>13</sup>Curiosity also appears to be correlated with some negative things. Green (1990) linked it with an increased probability of alcohol use. Kolko and Kazin (1989) found the same for arson.</p>\n<p><sup>14</sup>Schiefele, Krapp and Winteler (1992)</p>\n<p><sup>15</sup>Swan and Carmelli (1996)</p>\n<p><sup>16</sup>Raine et al (2002)</p>\n<p><sup>17</sup>Kashdan and Steger (2007)</p>\n<p><sup>18</sup>Kashdan et al (2011), Kashdan and Roberts (2004)</p>\n<p><sup>19</sup>Brdar and Kashdan (2010), Gallagher and Lopez (2007)</p>\n<p><sup>20</sup>Rodrigue, Olson, and Markley (1987)</p>\n<p><sup>21</sup>Kashdan and Fincham (2004)</p>\n<p><sup>22</sup>Cordova and Lepper (1996)</p>\n<p><sup>23</sup>Black and Deci (2000)</p>\n<p><sup>24</sup>Deci, Koestner and Ryan (1999)</p>\n<p><sup>25</sup>Cury et al (2002), Elliot et al (2000)</p>\n<p><sup>26</sup>Deci, Koestner and Ryan (1999)</p>\n<p><sup>27</sup>Mikulincer and Shaver (2003)</p>\n<p><sup>28</sup>Grolnick and Ryan (1989)</p>\n<p><sup>29</sup>Hazan and Shaver (1990)</p>\n<p><sup>30</sup>Smoll et al (1993)</p>\n<p><sup>31</sup>Kashdan, Rose and Fincham (2004)</p>\n<p><sup>32</sup>Kashdan and Fincham (2004)</p>\n<h2>References</h2>\n<p>Black and Deci (2000). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F9m2%2Fthe_neglected_virtue_of_curiosity%2F&amp;v=1&amp;libid=1327922984864&amp;out=http%3A%2F%2Fwww.selfdeterminationtheory.org%2FSDT%2Fdocuments%2F2000_BlackDeci.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fpromoted%2F&amp;title=The%20Neglected%20Virtue%20of%20Curiosity%20-%20Less%20Wrong&amp;txt=Black%20and%20Deci&amp;jsonp=vglnk_jsonp_13279230776292\">The effects of instructors' autonomy support and students' autonomous motivation on learning organic chemistry: A self-determination theory perspective</a>. <em>Science Education</em> 84:740-756.</p>\n<p>Brdar and Kashdan (2010). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F9m2%2Fthe_neglected_virtue_of_curiosity%2F&amp;v=1&amp;libid=1327922984864&amp;out=http%3A%2F%2Fpsychfaculty.gmu.edu%2Fkashdan%2Fpublications%2FBrdar%2520%26%2520Kashdan%2520VIA%2520strengths%2520JRP.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fpromoted%2F&amp;title=The%20Neglected%20Virtue%20of%20Curiosity%20-%20Less%20Wrong&amp;txt=Brdar%20and%20Kashdan&amp;jsonp=vglnk_jsonp_13279231118073\">Character strengths and well-being in Croatia: An empirical investigation of structure and correlate. </a><em>Journal of Research in Personality</em> 44:151-154</p>\n<p>Cordova and Lepper (1996). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F9m2%2Fthe_neglected_virtue_of_curiosity%2F&amp;v=1&amp;libid=1327922984864&amp;out=http%3A%2F%2Fwww.coulthard.com%2Flibrary%2FFiles%2Fcordovalepper_1996_intrinsicmotivation.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fpromoted%2F&amp;title=The%20Neglected%20Virtue%20of%20Curiosity%20-%20Less%20Wrong&amp;txt=Cordova%20and%20Lepper&amp;jsonp=vglnk_jsonp_13279231307524\">Intrinsic motivation and the process of learning: Beneficial effects of contextualization, personalization, and choice</a>. <em>Journal of Educational Psychology</em> 88:715-730.</p>\n<p>Cury et al (2002). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F9m2%2Fthe_neglected_virtue_of_curiosity%2F&amp;v=1&amp;libid=1327922984864&amp;out=http%3A%2F%2Fwww.psych.rochester.edu%2Fresearch%2Fapav%2Fpublications%2Fdocuments%2F2002_CuryElliotSarrzinDaFonsecaRuffo_Thetrichotomousachievementgoalmodelandintrinsicmotivati.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fpromoted%2F&amp;title=The%20Neglected%20Virtue%20of%20Curiosity%20-%20Less%20Wrong&amp;txt=Cury%20et%20al&amp;jsonp=vglnk_jsonp_13279231534666\">The trichotomous achievement goal model and intrinsic motivation: A sequential mediational analysis. <em>Journal of Experimental Psychology</em></a> 38(5):473-481</p>\n<p>Deci, Koestner and Ryan (1999). <a href=\"http://files.meetup.com/1789648/Review%20on%20Effects%20of%20Extrinsic%20Rewards%20on%20Intrinsic%20Motivation.pdf\">A meta-analytic review of experiments examining the effects of extrinsic rewards on intrinsic motivation</a>. <em>Psychological Bulletin</em> 125(6):627-668</p>\n<p>Elliot et al (2000). <a href=\"http://www.psych.rochester.edu/research/apav/publications/documents/2000_ElliotFalerMcGregorCampbellSedikidesHarackiewica_CompetenceValuationasaStrategicIntrins.pdf\">Competence valuation as a strategic intrinsic motivation process</a>. <em>Personality and Social Psychology Bulletin</em> 26:780-794.</p>\n<p>Evans (1971). <a href=\"http://www.amsciepub.com/doi/abs/10.2466/pr0.1971.29.1.154\">The Ontario Test of Intrinsic Motivation, Question Asking, and Autistic Thinking.</a>&nbsp;<span style=\"font-family: Verdana, sans-serif; font-size: 13px;\"><em>Psychological Reports</em>&nbsp;29:154-154.</span></p>\n<p>Gallagher and Lopez (2007). Curiosity and well-being. <em>Journal of Positive Psychology</em> 2(4): 236-248</p>\n<p>Green (1990). <a href=\"http://ad-teaching.informatik.uni-freiburg.de/zbmed/InformaHealthcare/production/sum/1990/25/2/10826089009056205/10826089009056205.pdf\">Instrument for the measurement of individual and societal attitudes toward drugs</a>. <em>Substance Use &amp; Misuse</em> 25(2):141-157</p>\n<p>Grolnick and Ryan (1989). <a href=\"http://www.selfdeterminationtheory.org/SDT/documents/1989_GrolnickRyan.pdf\">Parent styles associated with children's self-regulation and competence in school</a>. <em>Journal of Educational Psychology</em> 81:143-154.</p>\n<p>Hazan and Shaver (1990). <a href=\"http://www.psy.cmu.edu/~rakison/attachment2.htm\">Love and work: An attachment-theoretical perspective. <em>Journal of Personality and Social Psychology</em></a> 59:270-280.</p>\n<p>Kashdan and Fincham (2004). Facilitating Curiosity: A Social and Self-Regulatory Perspective. In Linley and Joseph (eds.) <em><a href=\"http://www.amazon.com/Positive-Psychology-Practice-Alex-Linley/dp/0471459062\">Positive Psychology in Practice</a></em>, Wiley</p>\n<p>Kashdan (2009). <em><a href=\"http://www.amazon.com/Curious-Discover-Missing-Ingredient-Fulfilling/dp/B005DI9XJI/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1327923806&amp;sr=1-1\">Curious? Discover the Missing Ingredient to a Fulfilling Life</a></em>, HarperCollins</p>\n<p>Kashdan and Roberts (2004). <a href=\"http://mason.gmu.edu/~tkashdan/publications/jscp.curiosityint.pdf\">Trait and State Curiosity in the Genesis of Intimacy: Differentiation From Related Constructs</a>. <em>Journal of Social and Clinical Psychology</em> 23(6):792-816</p>\n<p>Kashdan, Rose and Fincham (2004). <a href=\"http://ceicuriosity.tripod.com/kashdan_CEI_curiosity.pdf\">Curiosity and Exploration: Facilitating Positive Subjective Experiences and Personal Growth Opportunities</a>. <em>Journal of Personality Assessment</em> 82(3):291-305</p>\n<p>Kashdan and Silvia (2009). <a href=\"http://mason.gmu.edu/~tkashdan/publications/HOPPChapter%2034_Curiosity_and_Interest.pdf\">Curiosity and Interest: The Benefits of Thriving on Novelty and Challenge</a>. In S.J. Lopez (Ed.) <em><a href=\"http://www.amazon.com/Oxford-Handbook-Positive-Psychology-Library/dp/0199862168/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1327923893&amp;sr=1-1\">Handbook of Positive Psychology</a></em> (2nd Ed.) Oxford University Press.</p>\n<p>Kashdan and Steger (2007). <a href=\"http://psychfaculty.gmu.edu/kashdan/publications/MOEM_curiosity_wb_and_meaning.pdf\">Curiosity and pathways to well-being and meaning in life: Traits, states, and everyday behaviors</a>. <em>Motivation and Emotion</em> 31(3):159-173</p>\n<p>Kashdan et al (2011). <a href=\"http://psychfaculty.gmu.edu/kashdan/publications/Kashdan%20et%20al.%20(2011)%20when%20curiosity%20breeds%20intimacy%20JP.pdf\">When Curiosity Breeds Intimacy: Taking Advantage of Intimacy Opportunities and Transforming Boring Conversations</a>. <em>Journal of Personality</em> 79(6):1369-1402</p>\n<p>Kolko and Kazin (1989). Assessment of Dimensions of Childhood Firesetting Among Patients and Nonpatients: The Firesetting Risk Interview. <em>Journal of Abnormal Child Psychology</em> 17 (2):157-176</p>\n<p>Litman and Silvia (2006). <a href=\"http://libres.uncg.edu/ir/uncg/f/P_Silvia_Latent_2006.pdf\">The latent structure of trait curiosity: evidence for interest and deprivation curiosity dimensions</a>. <em>Journal of Personality Assessment</em> 86(3):318-328</p>\n<p>Loewenstein (1994). <a href=\"http://www.andrew.cmu.edu/user/gl20/GeorgeLoewenstein/Papers_files/pdf/PsychofCuriosity.pdf\">The Psychology of Curiosity: A Review and Reinterpretation</a>. <em>Psychological Bulletin</em> 116(1):75-98</p>\n<p>McCrae (1996). Social consequences of experiential openness. <em>Psychological Bulletin</em> 120(3):323-337</p>\n<p>McCrae et al (2002). Personality trait development from age 12 to age 18: Longitudinal, cross-sectional and cross-cultural analyses. <em>Journal of Personality and Social Psychology</em> 83(6):1456-1468</p>\n<p>Mikulincer and Shaver (2003). The attachment behavioral system in adulthood: Activation, psychodynamics, and interpersonal processes. In M. P. Zanna (Ed.), <em>Advances in Experimental Social Psychology</em> (Vol. 35, pp. 53-152). Academic Press.</p>\n<p>Mussel (2010). Epistemic curiosity and related constructs: Lacking evidence of discriminant validity. <em>Personality and Individual Differences</em> 49(5):506-510</p>\n<p>Peters (1978).&nbsp;Effects of Anxiety, Curiosity, and Perceived Instructor Threat on Student Verbal Behavior in the College Classroom.&nbsp;<em>Journal of educational psychology&nbsp;</em>70(3):388-395</p>\n<p>Raine et al (2002). <a href=\"http://www.apa.org/pubs/journals/releases/psp-824663.pdf\">Stimulation seeking and intelligence: A prospective longitudinal study</a>. <em>Journal of Personality and Social Psychology</em> 82(4):663-674</p>\n<p>Reeve and Nix (1997). <a href=\"http://www.selfdeterminationtheory.org/SDT/documents/1997_ReeveNix_MO.pdf\">Expressing Intrinsic Motivation Through Acts of Exploration and Facial Displays of Interest</a>. <em>Motivation and Emotion</em> 21(3):237-250</p>\n<p>Rodrigue, Olson, and Markley (1987). Induced mood and curiosity. <em>Cognitive Therapy and Research</em> 11(1):101-106</p>\n<p>Sansone and Smith (2000). Interest and self-regulation: The relation between having to and wanting to. In C. Sansone &amp; J.M. Harackiewicz (Eds.). <em>Intrinsic and Extrinsic Motivation: The Search for Optimal Motivation and Performance</em>, Academic Press.</p>\n<p>Schiefele, Krapp and Winteler (1992). Interest as a predictor of academic achievement: A meta-analysis of research. In K. A. Renninger, S. Hidi, &amp; A. Krapp (Eds.), <em>The role of interest in learning and development</em>, Erlbaum.</p>\n<p>Silvia (2008). <a href=\"http://libres.uncg.edu/ir/uncg/f/P_Silvia_Appraisal_2008.pdf\">Appraisal components and emotion traits: Examining the appraisal basis of trait curiosity</a>. <em>Cognition and Emotion</em> 22(1):94-113</p>\n<p>Smoll et al (1993). Enhancement of children's self-esteem through social support training for youth sports coaches. <em>Journal of Applied Psychology</em> 78:602-610.</p>\n<p>Swan and Carmelli (1996). Curiosity and mortality in aging adults: A 5-year follow-up of the Western Collaborative Group Study. <em>Psychology and Aging</em> 11(3):449-453</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8uNFGxejo5hykCEez": 2, "moeYqrcakMgXnQNyF": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eCZjrm9JBDSGvEA9o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 32, "extendedScore": null, "score": 8.393478076348935e-07, "legacy": true, "legacyId": "12458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>Curiosity is the most superficial of all the affections; it changes its objects perpetually; it has an appetite which is sharp, but very easily satisfied; and it has always an appearance of giddiness, restlessness and anxiety.</p>\n</blockquote>\n<p>- Edmund Burke, <a href=\"http://www.amazon.com/Philosophical-Enquiry-Sublime-Beautiful-Classics/dp/0199537887/\">A Philosophical Enquiry into the Origin of our Ideas of the Sublime and Beautiful</a></p>\n<p>Curiosity is the <a href=\"http://yudkowsky.net/rational/virtues\">first virtue</a>: \"[a] burning itch to know is higher than a solemn vow to pursue truth.\" Yet I find surprisingly little material about curiosity on Less Wrong. Sure, <a href=\"/user/AnnaSalamon\">AnnaSalamon</a> shows us how to <a href=\"/lw/4ku/use_curiosity/\">use curiosity</a>, <a href=\"/user/lukeprog\">lukeprog</a> ponders <a href=\"/lw/96j/what_curiosity_looks_like/\">what curiosity looks like</a>, <a href=\"/user/Elizabeth\">Elizabeth</a> discusses the <a href=\"/lw/4rm/the_limits_of_curiosity/\">limits of curiosity</a>, and <a href=\"/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a> offers the <a href=\"/lw/jz/the_meditation_on_curiosity/\">meditation on curiosity</a>. But we have never been provided with an overview of the <em>science</em> of curiosity, as has been done for <a href=\"/lw/3w3/how_to_beat_procrastination/\">procrastination</a>, <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">motivation</a>, and <a href=\"/lw/4su/how_to_be_happy/\">happiness</a>, for instance. Perhaps most Less Wrongers score high on curiosity already, so there hasn't been much need to study it. But <em>I</em> often wish I were more curious. Some of you may, too. For the rest, what follows is a journey <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">back to the basics of rationality</a>.</p>\n<p>What is curiosity, and how can we become more curious?</p>\n<h2 id=\"Curiosity__what_\">Curiosity: what?</h2>\n<p>We have all felt that burning itch to know on at least some occasions. It leads us to ask questions,<sup>1</sup> manipulate interesting objects,<sup>2</sup> and continue doing challenging tasks.<sup>3</sup> Kashdan and Fincham (2004) define curiosity as \"the volitional recognition, pursuit, and self-regulation of novel and challenging opportunities (reflecting intrinsic values and interests)\". Loewenstein (2000) also emphasizes the fact that curiosity occurs in the absence of an extrinsic reward. All theories of curiosity agree that its short term function is to learn and explore. In the longer term, curiosity aids us in building knowledge and competence.<sup>4</sup> When curious, we enter a state of <a href=\"http://en.wikipedia.org/wiki/Flow_(psychology)\">flow</a>, and become immersed in whatever it is we are doing.<sup>5</sup></p>\n<p>Researchers distinguish between <em>state curiosity</em> and <em>trait curiosity</em>. State curiosity is evoked by external situations. Why is the sky blue? How does <a href=\"http://www.youtube.com/watch?v=Ws6AAhTw7RA\">quantum levitation</a> work? Trait curiosity on the other hand is a characteristic that people possess to varying degrees. Someone with high trait curiosity seeks out complexity, novelty, conflict, and uncertainty.<sup>6 7</sup></p>\n<p>Curiosity can be measured across several dimensions (Kashdan, 2009):</p>\n<ul>\n<li><em>Intensity</em>. How strong is that burning itch to know?</li>\n<li><em>Frequency</em>. How often do you feel it?</li>\n<li><em>Durability</em>. How long does it last?</li>\n<li><em>Breadth</em>. How many topics evoke it?</li>\n<li><em>Depth</em>. Does the itch remain as you learn more about a topic?</li>\n</ul>\n<p>It has been suggested that trait curiosity simply measures the frequency and intensity of state curiosity.<sup>8</sup></p>\n<p>I suspect many of you are particularly interested in <em>epistemic curiosity.</em> Epistemic curiosity measures our desire for knowledge and understanding, rather than, say, our desire to explore new cultures or meet new people. This notion is closely related to other psychological constructs such as <em>need for cognition</em>, <em>typical intellectual engagement</em> and <em>openness for ideas</em>, and some have argued that there isn't enough evidence for treating them as separate things.<sup>9</sup> With that in mind, it might be worth examining the literature on these notions closer as well.</p>\n<p>Early in our lives, curiosity will typically increase, only to start decreasing later. One study found that, on average, curiosity increases from age 12 until people attend college.<sup>10</sup> By the age of 30, curiosity typically starts to decline. But some people manage to retain their curiosity even as they grow older. One study followed a group of men and women from college age until later adulthood. Those that were identified as very curious later in life had many characteristics in common: rich emotional lives with both positive and negative feelings, actively searching for meaning in life, don't experience themselves as being restricted by social norms, and chose careers that gave them opportunities to be independent and creative.<sup>11</sup> More broadly, curiosity is correlated with the <a href=\"http://en.wikipedia.org/wiki/Big_Five_personality_traits\">Big Five</a> trait of Openness.<sup>12</sup></p>\n<h2 id=\"The_Benefits_of_Curiosity\">The Benefits of Curiosity</h2>\n<p>Much research makes it plausible that curiosity is in fact the first virtue. It has a wide array of benefits, not only related to rationality or intelligence.<sup>13</sup> One study found that it accounts for roughly 10% of the variance in achievement and performance outcomes.<sup>14</sup> In particular, studies indicate that curiosity is useful for the following:</p>\n<ul>\n<li><em>Health</em>. Curious people are more likely to live longer, and less likely to develop Alzheimer's disease, for example.<sup>15</sup></li>\n<li><em>Intelligence</em>. Being curious at an early age is a good predictor of intelligence later in life, even when initial intelligence is taken into account.<sup>16</sup></li>\n<li><em>Meaning and purpose in life</em>. Curious people are more likely to develop interests, hobbies, and passions, which typically increase feelings of purpose.<sup>17</sup></li>\n<li><em>Social relationships</em>. Curious people report more satisfying relationships, and are also more prone to develop new relationships with strangers.<sup>18</sup></li>\n<li><em>Happiness</em>. Increased curiosity is associated with a moderate increase in happiness and well-being.<sup>19</sup> A lack of curiosity has also been linked to negative emotions, such as depression.<sup>20</sup></li>\n</ul>\n<p>Beginning in the mid-70s, researchers have spent much effort attempting to measure curiosity. Unfortunately, attempts to cross-validate such measures have usually produced low intercorrelations (Loewenstein 1994).</p>\n<p>Luckily for those who wish they were more curious, curiosity is a malleable psychological state. It is very much influenced by social contexts, and other individual differences.<sup>21</sup> Relish <a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">the good news of situationist psychology</a>!</p>\n<h2 id=\"Curiosity__how_\">Curiosity: how?</h2>\n<p>Curiosity, it seems, is a big deal. So what can we do to become more curious? Kashdan and Fincham (2004) focus on three factors correlated with curiosity: autonomy, competence, and relatedness.</p>\n<p><em>Autonomy.</em> People are more task curious when given more choice,<sup>22</sup> and when given more information and encouragement.<sup>23</sup> On the other hand, threats, punishment, negative feedback and surveillance all have negative effects on task curiosity. A meta-analysis found that the same goes for external rewards, though the effect was more robust for interesting, compared to boring tasks.<sup>24</sup></p>\n<p><em>Competence.</em> Events that make individuals believe they can interact effectively with the environment (<em>perceived competence</em>) or that give them the desire to do so (<em>competence valuation</em>), will lead to enhanced curiosity.<sup>25</sup> Sincere praise increases both perceived competence and competence valuation, and could therefore be a useful way of increasing curiosity.<sup>26</sup></p>\n<p><em>Relatedness.</em> Feelings of relatedness\u2014feeling connected to others, and believing your emotional experiences are acknowledged\u2014also appear to increase curiosity.<sup>27</sup> In particular, relatedness has been shown to improve both curiosity and performance in athletic,<sup>28</sup> academic<sup>29</sup> and work contexts.<sup>30</sup> Feeling comfortable and safe also encourages curiosity.<sup>31</sup></p>\n<p>Based on these three factors, Kashdan and Fincham (2004, p. 490) propose a table of empirically-informed \"curiosity interventions\". These include</p>\n<blockquote>\n<ul>\n<li>Create tasks that capitalize on novelty, complexity, ambiguity, variety, and surprise.</li>\n<li>Purposely place individuals in contexts that are discrepant with their experience, skills, and personality.</li>\n<li>Create tasks that can be conducted independently.</li>\n<li>Allow opportunities for play.</li>\n<li>Create tasks that are personally meaningful.</li>\n<li>Create challenges that match or slightly exceed current skills.</li>\n<li>Create enjoyable group based activities.</li>\n</ul>\n</blockquote>\n<p>Unfortunately, most studies on curiosity have focused on narrow areas, and so the breadth of curiosity has not been well-examined. Factors that correlate with curiosity in one domain may not do so in others.<sup>32</sup> The study of curiosity is still in its infancy, and most of these interventions remain to be experimentally tested. But as of today, these might be the best tools available.</p>\n<p>That was a summary of what we know about curiosity. Now go out and explore!</p>\n<h2 id=\"Notes\">Notes</h2>\n<p><sup>1</sup>Evans&nbsp;(1971) found that asking lots of questions is correlated with one of three scales of the 'Ontario Test of Intrinsic Motivation'. Peters (1978) reports that students with high trait curiosity asked more questions when their instructor was perceived as non-threatening. If, on the other hand, the instructor was perceived as threatening, no difference was found between students with high trait curiosity and those with low trait curiosity.&nbsp;</p>\n<p><sup>2</sup>Reeve and Nix (1997) found, among other things, that hand speed while performing a puzzle task correlated with self-reported intrinsic motivation.</p>\n<p><sup>3</sup>See&nbsp;Sansone and Smith (2000) for a review</p>\n<p><sup>4</sup>Kashdan and Silvia (2009)</p>\n<p><sup>5</sup>Curiosity is closely related to <em>interest</em> and <em>intrinsic motivation</em> (Kashdan and Fincham 2004), and consequently there is a considerable overlap between the study of these phenomena. Many researchers treat these terms interchangeably.</p>\n<p><sup>6</sup>Kashdan and Fincham (2004). Loewenstein (1994) raises some doubts about the usefulness of distinguishing between state curiosity and trait curiosity.</p>\n<p><sup>7</sup>See Litman and Silvia (2006) for an overview of ways to measure trait curiosity. Like many other psychological traits, curiosity is mostly measured through questionnaires. Beginning in the mid-70s, researchers developed many different ways of measuring curiosity. Unfortunately, attempts to cross-validate such measures have typically produced low intercorrelations (Loewenstein 1994)</p>\n<p><sup>8</sup>Silvia (2008)</p>\n<p><sup>9</sup>Mussel (2010)</p>\n<p><sup>10</sup>McCrae et al (2002)</p>\n<p><sup>11</sup>Kashdan (2009)</p>\n<p><sup>12</sup>McCrae (1996)</p>\n<p><sup>13</sup>Curiosity also appears to be correlated with some negative things. Green (1990) linked it with an increased probability of alcohol use. Kolko and Kazin (1989) found the same for arson.</p>\n<p><sup>14</sup>Schiefele, Krapp and Winteler (1992)</p>\n<p><sup>15</sup>Swan and Carmelli (1996)</p>\n<p><sup>16</sup>Raine et al (2002)</p>\n<p><sup>17</sup>Kashdan and Steger (2007)</p>\n<p><sup>18</sup>Kashdan et al (2011), Kashdan and Roberts (2004)</p>\n<p><sup>19</sup>Brdar and Kashdan (2010), Gallagher and Lopez (2007)</p>\n<p><sup>20</sup>Rodrigue, Olson, and Markley (1987)</p>\n<p><sup>21</sup>Kashdan and Fincham (2004)</p>\n<p><sup>22</sup>Cordova and Lepper (1996)</p>\n<p><sup>23</sup>Black and Deci (2000)</p>\n<p><sup>24</sup>Deci, Koestner and Ryan (1999)</p>\n<p><sup>25</sup>Cury et al (2002), Elliot et al (2000)</p>\n<p><sup>26</sup>Deci, Koestner and Ryan (1999)</p>\n<p><sup>27</sup>Mikulincer and Shaver (2003)</p>\n<p><sup>28</sup>Grolnick and Ryan (1989)</p>\n<p><sup>29</sup>Hazan and Shaver (1990)</p>\n<p><sup>30</sup>Smoll et al (1993)</p>\n<p><sup>31</sup>Kashdan, Rose and Fincham (2004)</p>\n<p><sup>32</sup>Kashdan and Fincham (2004)</p>\n<h2 id=\"References\">References</h2>\n<p>Black and Deci (2000). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F9m2%2Fthe_neglected_virtue_of_curiosity%2F&amp;v=1&amp;libid=1327922984864&amp;out=http%3A%2F%2Fwww.selfdeterminationtheory.org%2FSDT%2Fdocuments%2F2000_BlackDeci.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fpromoted%2F&amp;title=The%20Neglected%20Virtue%20of%20Curiosity%20-%20Less%20Wrong&amp;txt=Black%20and%20Deci&amp;jsonp=vglnk_jsonp_13279230776292\">The effects of instructors' autonomy support and students' autonomous motivation on learning organic chemistry: A self-determination theory perspective</a>. <em>Science Education</em> 84:740-756.</p>\n<p>Brdar and Kashdan (2010). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F9m2%2Fthe_neglected_virtue_of_curiosity%2F&amp;v=1&amp;libid=1327922984864&amp;out=http%3A%2F%2Fpsychfaculty.gmu.edu%2Fkashdan%2Fpublications%2FBrdar%2520%26%2520Kashdan%2520VIA%2520strengths%2520JRP.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fpromoted%2F&amp;title=The%20Neglected%20Virtue%20of%20Curiosity%20-%20Less%20Wrong&amp;txt=Brdar%20and%20Kashdan&amp;jsonp=vglnk_jsonp_13279231118073\">Character strengths and well-being in Croatia: An empirical investigation of structure and correlate. </a><em>Journal of Research in Personality</em> 44:151-154</p>\n<p>Cordova and Lepper (1996). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F9m2%2Fthe_neglected_virtue_of_curiosity%2F&amp;v=1&amp;libid=1327922984864&amp;out=http%3A%2F%2Fwww.coulthard.com%2Flibrary%2FFiles%2Fcordovalepper_1996_intrinsicmotivation.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fpromoted%2F&amp;title=The%20Neglected%20Virtue%20of%20Curiosity%20-%20Less%20Wrong&amp;txt=Cordova%20and%20Lepper&amp;jsonp=vglnk_jsonp_13279231307524\">Intrinsic motivation and the process of learning: Beneficial effects of contextualization, personalization, and choice</a>. <em>Journal of Educational Psychology</em> 88:715-730.</p>\n<p>Cury et al (2002). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F9m2%2Fthe_neglected_virtue_of_curiosity%2F&amp;v=1&amp;libid=1327922984864&amp;out=http%3A%2F%2Fwww.psych.rochester.edu%2Fresearch%2Fapav%2Fpublications%2Fdocuments%2F2002_CuryElliotSarrzinDaFonsecaRuffo_Thetrichotomousachievementgoalmodelandintrinsicmotivati.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fpromoted%2F&amp;title=The%20Neglected%20Virtue%20of%20Curiosity%20-%20Less%20Wrong&amp;txt=Cury%20et%20al&amp;jsonp=vglnk_jsonp_13279231534666\">The trichotomous achievement goal model and intrinsic motivation: A sequential mediational analysis. <em>Journal of Experimental Psychology</em></a> 38(5):473-481</p>\n<p>Deci, Koestner and Ryan (1999). <a href=\"http://files.meetup.com/1789648/Review%20on%20Effects%20of%20Extrinsic%20Rewards%20on%20Intrinsic%20Motivation.pdf\">A meta-analytic review of experiments examining the effects of extrinsic rewards on intrinsic motivation</a>. <em>Psychological Bulletin</em> 125(6):627-668</p>\n<p>Elliot et al (2000). <a href=\"http://www.psych.rochester.edu/research/apav/publications/documents/2000_ElliotFalerMcGregorCampbellSedikidesHarackiewica_CompetenceValuationasaStrategicIntrins.pdf\">Competence valuation as a strategic intrinsic motivation process</a>. <em>Personality and Social Psychology Bulletin</em> 26:780-794.</p>\n<p>Evans (1971). <a href=\"http://www.amsciepub.com/doi/abs/10.2466/pr0.1971.29.1.154\">The Ontario Test of Intrinsic Motivation, Question Asking, and Autistic Thinking.</a>&nbsp;<span style=\"font-family: Verdana, sans-serif; font-size: 13px;\"><em>Psychological Reports</em>&nbsp;29:154-154.</span></p>\n<p>Gallagher and Lopez (2007). Curiosity and well-being. <em>Journal of Positive Psychology</em> 2(4): 236-248</p>\n<p>Green (1990). <a href=\"http://ad-teaching.informatik.uni-freiburg.de/zbmed/InformaHealthcare/production/sum/1990/25/2/10826089009056205/10826089009056205.pdf\">Instrument for the measurement of individual and societal attitudes toward drugs</a>. <em>Substance Use &amp; Misuse</em> 25(2):141-157</p>\n<p>Grolnick and Ryan (1989). <a href=\"http://www.selfdeterminationtheory.org/SDT/documents/1989_GrolnickRyan.pdf\">Parent styles associated with children's self-regulation and competence in school</a>. <em>Journal of Educational Psychology</em> 81:143-154.</p>\n<p>Hazan and Shaver (1990). <a href=\"http://www.psy.cmu.edu/~rakison/attachment2.htm\">Love and work: An attachment-theoretical perspective. <em>Journal of Personality and Social Psychology</em></a> 59:270-280.</p>\n<p>Kashdan and Fincham (2004). Facilitating Curiosity: A Social and Self-Regulatory Perspective. In Linley and Joseph (eds.) <em><a href=\"http://www.amazon.com/Positive-Psychology-Practice-Alex-Linley/dp/0471459062\">Positive Psychology in Practice</a></em>, Wiley</p>\n<p>Kashdan (2009). <em><a href=\"http://www.amazon.com/Curious-Discover-Missing-Ingredient-Fulfilling/dp/B005DI9XJI/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1327923806&amp;sr=1-1\">Curious? Discover the Missing Ingredient to a Fulfilling Life</a></em>, HarperCollins</p>\n<p>Kashdan and Roberts (2004). <a href=\"http://mason.gmu.edu/~tkashdan/publications/jscp.curiosityint.pdf\">Trait and State Curiosity in the Genesis of Intimacy: Differentiation From Related Constructs</a>. <em>Journal of Social and Clinical Psychology</em> 23(6):792-816</p>\n<p>Kashdan, Rose and Fincham (2004). <a href=\"http://ceicuriosity.tripod.com/kashdan_CEI_curiosity.pdf\">Curiosity and Exploration: Facilitating Positive Subjective Experiences and Personal Growth Opportunities</a>. <em>Journal of Personality Assessment</em> 82(3):291-305</p>\n<p>Kashdan and Silvia (2009). <a href=\"http://mason.gmu.edu/~tkashdan/publications/HOPPChapter%2034_Curiosity_and_Interest.pdf\">Curiosity and Interest: The Benefits of Thriving on Novelty and Challenge</a>. In S.J. Lopez (Ed.) <em><a href=\"http://www.amazon.com/Oxford-Handbook-Positive-Psychology-Library/dp/0199862168/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1327923893&amp;sr=1-1\">Handbook of Positive Psychology</a></em> (2nd Ed.) Oxford University Press.</p>\n<p>Kashdan and Steger (2007). <a href=\"http://psychfaculty.gmu.edu/kashdan/publications/MOEM_curiosity_wb_and_meaning.pdf\">Curiosity and pathways to well-being and meaning in life: Traits, states, and everyday behaviors</a>. <em>Motivation and Emotion</em> 31(3):159-173</p>\n<p>Kashdan et al (2011). <a href=\"http://psychfaculty.gmu.edu/kashdan/publications/Kashdan%20et%20al.%20(2011)%20when%20curiosity%20breeds%20intimacy%20JP.pdf\">When Curiosity Breeds Intimacy: Taking Advantage of Intimacy Opportunities and Transforming Boring Conversations</a>. <em>Journal of Personality</em> 79(6):1369-1402</p>\n<p>Kolko and Kazin (1989). Assessment of Dimensions of Childhood Firesetting Among Patients and Nonpatients: The Firesetting Risk Interview. <em>Journal of Abnormal Child Psychology</em> 17 (2):157-176</p>\n<p>Litman and Silvia (2006). <a href=\"http://libres.uncg.edu/ir/uncg/f/P_Silvia_Latent_2006.pdf\">The latent structure of trait curiosity: evidence for interest and deprivation curiosity dimensions</a>. <em>Journal of Personality Assessment</em> 86(3):318-328</p>\n<p>Loewenstein (1994). <a href=\"http://www.andrew.cmu.edu/user/gl20/GeorgeLoewenstein/Papers_files/pdf/PsychofCuriosity.pdf\">The Psychology of Curiosity: A Review and Reinterpretation</a>. <em>Psychological Bulletin</em> 116(1):75-98</p>\n<p>McCrae (1996). Social consequences of experiential openness. <em>Psychological Bulletin</em> 120(3):323-337</p>\n<p>McCrae et al (2002). Personality trait development from age 12 to age 18: Longitudinal, cross-sectional and cross-cultural analyses. <em>Journal of Personality and Social Psychology</em> 83(6):1456-1468</p>\n<p>Mikulincer and Shaver (2003). The attachment behavioral system in adulthood: Activation, psychodynamics, and interpersonal processes. In M. P. Zanna (Ed.), <em>Advances in Experimental Social Psychology</em> (Vol. 35, pp. 53-152). Academic Press.</p>\n<p>Mussel (2010). Epistemic curiosity and related constructs: Lacking evidence of discriminant validity. <em>Personality and Individual Differences</em> 49(5):506-510</p>\n<p>Peters (1978).&nbsp;Effects of Anxiety, Curiosity, and Perceived Instructor Threat on Student Verbal Behavior in the College Classroom.&nbsp;<em>Journal of educational psychology&nbsp;</em>70(3):388-395</p>\n<p>Raine et al (2002). <a href=\"http://www.apa.org/pubs/journals/releases/psp-824663.pdf\">Stimulation seeking and intelligence: A prospective longitudinal study</a>. <em>Journal of Personality and Social Psychology</em> 82(4):663-674</p>\n<p>Reeve and Nix (1997). <a href=\"http://www.selfdeterminationtheory.org/SDT/documents/1997_ReeveNix_MO.pdf\">Expressing Intrinsic Motivation Through Acts of Exploration and Facial Displays of Interest</a>. <em>Motivation and Emotion</em> 21(3):237-250</p>\n<p>Rodrigue, Olson, and Markley (1987). Induced mood and curiosity. <em>Cognitive Therapy and Research</em> 11(1):101-106</p>\n<p>Sansone and Smith (2000). Interest and self-regulation: The relation between having to and wanting to. In C. Sansone &amp; J.M. Harackiewicz (Eds.). <em>Intrinsic and Extrinsic Motivation: The Search for Optimal Motivation and Performance</em>, Academic Press.</p>\n<p>Schiefele, Krapp and Winteler (1992). Interest as a predictor of academic achievement: A meta-analysis of research. In K. A. Renninger, S. Hidi, &amp; A. Krapp (Eds.), <em>The role of interest in learning and development</em>, Erlbaum.</p>\n<p>Silvia (2008). <a href=\"http://libres.uncg.edu/ir/uncg/f/P_Silvia_Appraisal_2008.pdf\">Appraisal components and emotion traits: Examining the appraisal basis of trait curiosity</a>. <em>Cognition and Emotion</em> 22(1):94-113</p>\n<p>Smoll et al (1993). Enhancement of children's self-esteem through social support training for youth sports coaches. <em>Journal of Applied Psychology</em> 78:602-610.</p>\n<p>Swan and Carmelli (1996). Curiosity and mortality in aging adults: A 5-year follow-up of the Western Collaborative Group Study. <em>Psychology and Aging</em> 11(3):449-453</p>", "sections": [{"title": "Curiosity: what?", "anchor": "Curiosity__what_", "level": 1}, {"title": "The Benefits of Curiosity", "anchor": "The_Benefits_of_Curiosity", "level": 1}, {"title": "Curiosity: how?", "anchor": "Curiosity__how_", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "31 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WrSe4aB8sWBy3Nphm", "3oYaLja5h8qL5adDn", "g4QdmxFos3Q2zseZo", "3nZMgRTfFEfHp34Gb", "RWo4LwFzpHNQCTcYt", "hN2aRnu798yas5b2k", "ZbgCx2ntD5eu8Cno9", "gfexKxsBDM6v2sCMo", "Q5CjE8pRiACqTvhRM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T17:34:32.807Z", "modifiedAt": null, "url": null, "title": "The Ellsberg paradox and money pumps", "slug": "the-ellsberg-paradox-and-money-pumps", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:04.393Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fool", "createdAt": "2011-06-26T01:15:43.078Z", "isAdmin": false, "displayName": "fool"}, "userId": "ZRc3MgPzbWh2TTm8X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qtkiFYeF3gdMsRPHG/the-ellsberg-paradox-and-money-pumps", "pageUrlRelative": "/posts/qtkiFYeF3gdMsRPHG/the-ellsberg-paradox-and-money-pumps", "linkUrl": "https://www.lesswrong.com/posts/qtkiFYeF3gdMsRPHG/the-ellsberg-paradox-and-money-pumps", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Ellsberg%20paradox%20and%20money%20pumps&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Ellsberg%20paradox%20and%20money%20pumps%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqtkiFYeF3gdMsRPHG%2Fthe-ellsberg-paradox-and-money-pumps%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Ellsberg%20paradox%20and%20money%20pumps%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqtkiFYeF3gdMsRPHG%2Fthe-ellsberg-paradox-and-money-pumps", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqtkiFYeF3gdMsRPHG%2Fthe-ellsberg-paradox-and-money-pumps", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4193, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/9e4/the_savage_theorem_and_the_ellsberg_paradox\">The Savage theorem and the Ellsberg paradox</a></p>\n<p>In the previous post, I presented a simple version of Savage's theorem, and I introduced the Ellsberg paradox. At the end of the post, I mentioned a strong Bayesian thesis, which can be summarised: \"There is always a price to pay for leaving the Bayesian Way.\"<sup>1</sup> But not always, it turns out. I claimed that there was a method that is Ellsberg-paradoxical, therefore non-Bayesian, but can't be money-pumped (or \"Dutch booked\"). I will present the method in this post.</p>\n<p>I'm afraid this is another long post. There's a short summary of the method at the very end, if you want to skip the <a href=\"http://www.urbandictionary.com/define.php?term=jibba%20jabba\">jibba jabba</a> and get right to the specification. Before trying to money-pump it, I'd suggest reading at least the two highlighted dialogues.</p>\n<h2>Ambiguity aversion</h2>\n<p>To recap the Ellsberg paradox: there's an urn with 30 red balls and 60 other balls that are either green or blue, in unknown proportions. Most people, when asked to choose between betting on red or on green, choose red, but, when asked to choose between betting on red-or-blue or on green-or-blue, choose green-or-blue. For some people this behaviour persists even after due calculation and reflection. This behaviour is non-Bayesian, and is the prototypical example of <a href=\"http://en.wikipedia.org/wiki/Ambiguity_aversion\">ambiguity aversion</a>.</p>\n<p>There were some major themes that came out in the comments on that post. One theme was that I Fail Technical Writing Forever. I'll try to redeem myself.</p>\n<p>Another theme was that the setup given may be a bit too symmetrical. The Bayesian answer would be indifference, and really, you can break ties however you want. However the paradoxical preferences are typically <em>strict</em>, rather than just tie-breaking behaviour. (And when it's not strict, we shouldn't call it ambiguity aversion.) One suggestion was to add or remove a couple of red balls. Speaking for myself, I would still make the paradoxical choices.</p>\n<p>A third theme was that ambiguity aversion might be a good heuristic if betting against someone who may know something you don't. Now, no such opponent was specified, and speaking for myself, I'm not inferring one when I make the paradoxical choices. Still, let me admit that it's not contrived to infer a mischievous experimenter from the Ellsberg setup. <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability/5o6o\">One commentator</a> puts it better than me:</p>\n<blockquote>\n<p>Betting generally includes an adversary who wants you to lose money so they win in. Possibly in psychology experiments [this might not apply] ... But generally, ignoring the possibility of someone wanting to win money off you when they offer you a bet is a bad idea.</p>\n<p>Now betting is supposed to be a metaphor for options with possibly unknown results. In which case sometimes you still need to account for the possibility that the options were made available by an adversary who wants you to choose badly, but less often. And you should also account for the possibility that they were from other people who wanted you to choose well, or that the options were not determined by any intelligent being or process trying to predict your choices, so you don't need to account for an anticorrelation between your choice and the best choice. Except for your own biases.</p>\n</blockquote>\n<p>We can take betting on the Ellsberg urn as a stand-in for various decisions under ambiguous circumstances. Ambiguity aversion can be Bayesian if we assume the right sort of correlation between the options offered and the state of the world, or the right sort of correlation between the choice made and the state of the world. In that case just about anything can Bayesian. But sometimes the opponent will not have extra information, nor extra power. There might not even be any opponent as such. If we assume there are no such correlations, then ambiguity aversion is non-Bayesian.</p>\n<p>The final theme was: <em>so what</em>? Ambiguity aversion is just another cognitive bias. One commentator specifically complained that I spent too much time talking about various abstractions and not enough time talking about how ambiguity aversion could be money-pumped. I will fix that now: I claim that ambiguity aversion cannot be money-pumped, and the rest of this post is about my claim.</p>\n<p>I'll start with a bit of name-dropping and some <a href=\"http://en.wikipedia.org/wiki/Whig_history\">whig history</a>, to make myself sound more credible than I really am<sup>2</sup>. In the last twenty years or so many models of ambiguity averse reasoning have been constructed. Choquet expected utility<sup>3</sup> and maxmin expected utility<sup>4</sup> were early proposed models of ambiguity aversion. Later multiplier preferences<sup>5</sup> were the result of applying the ideas of <a href=\"http://en.wikipedia.org/wiki/Robust_control\">robust control</a> to macroeconomic models. This results in ambiguity aversion, though it was not explicitly motivated by the Ellsberg paradox. More recently, variational preferences<sup>6</sup> generalises both multiplier preferences and maxmin expected utility. What I'm going to present is a finitary case of variational preferences, with some of my own amateur mathematical fiddling for rhetorical purposes.</p>\n<h2>Probability intervals</h2>\n<p>The starting idea is simple enough, and may have already occurred to some LW readers. Instead of using a prior probability for events, can we not use an <a href=\"http://en.wikipedia.org/wiki/Imprecise_probability\">interval of probabilities</a>? What should our betting behaviour be for an event with probability 50%, plus or minus 10%?</p>\n<p>There are some different ways of filling in the details. So to be quite clear, I'm not proposing the following as the One True Probability Theory, and I am not claiming that the following is descriptive of many people's behaviour. What follows is just one way of making ambiguity aversion work, and perhaps the simplest way. This makes sense, given my aim: I should just describe a simple method that leaves the Bayesian Way, but does not pay.</p>\n<p>Now, sometimes disjoint ambiguous events together make an event with known probability. Or even a certainty, as in an event and its negation. If we want probability intervals to be additive (and let's say that we do) then what we really want are <em>oriented</em> intervals. I'll use +- or -+ (pronounced: plus-or-minus, minus-or-plus) to indicate two opposite orientations. So, if P(X) = 1/2 +- 1/10, then P(not X) = 1/2 -+ 1/10, and these add up to 1 exactly.</p>\n<p>Such oriented intervals are equivalent to ordered pairs of numbers. Sometimes it's more helpful to think of them as oriented intervals, but sometimes it's more helpful to think of them as pairs. So 1/2 +- 1/10 is the pair (3/5,2/5). And 1/2 -+ 1/10 is (2/5,3/5), the same numbers in the opposite order. The sum of these is (1,1), which is 1 exactly.</p>\n<p>You may wonder, if we can use ordered pairs, can we use triples, or longer lists? Yes, this method can be made to work with those too. And we can still think in terms of centre, length, and orientation. The orientation can go off in all sorts of directions, instead of just two. But for my purposes, I'll just stick with two.</p>\n<p>You might also ask, can we set P(X) = 1/2 +- 1/2? No, this method just won't handle it. A restriction of this method is that neither of the pair can be 0 or 1, except when they're both 0 or both 1. The way we will be using these intervals, 1/2 +- 1/2 would be the extreme case of ambiguity aversion. 1/2 +- 1/10 represents a lesser amount of ambiguity aversion, a sort of compromise between worst-case and average-case behaviour.</p>\n<p>To decide among bets (having the same two outcomes), compute their probability intervals. Sometimes, the intervals will not overlap. Then it's unambiguous which is more likely, so it's clear what to pick. In general, whether they overlap or not, pick the one with the largest minimum -- though we will see there are three caveats when they do overlap. If P(X) = 1/2 +- 1/10, we would be indifferent between a bet on X and on not X: the minimum is 2/5 in either case. If P(Y) = 1/2 exactly, then we would strictly prefer a bet on Y to a bet on X.</p>\n<p>Which leads to the <strong>first caveat</strong>: sometimes, given two options, it's strictly better to randomise. Let's suppose Y represents a fair coin. So P(Y) = 1/2 exactly, as we said. But also, Y is independent of X. P(X and Y) = 1/4 +- 1/20, and so on. This means that P((X and not Y) or (Y and not X)) = 1/2 exactly also. So we're indifferent between a bet on X and a bet on not X, but we strictly prefer the randomised bet.</p>\n<p>In general, randomisation will be strictly better if you have two choices with overlapping intervals of opposite orientations. The best randomisation ratio will be the one that gives a bet with zero-length interval.</p>\n<p>Now let us reconsider the Ellsberg urn. We did say the urn can be a metaphor for various situations. Generally these situations will not be symmetrical. But, even in symmetrical scenarios, we should still re-think how we apply the <a href=\"http://en.wikipedia.org/wiki/Principle_of_indifference\">principle of indifference</a>. I argue that the underlying idea is really this: if our information has a symmetry, then our decisions should have that same symmetry. If we switch green and blue, our information about the Ellsberg urn doesn't change. The situation is indistinguishable, so we should behave the same way. It follows that we should be indifferent between a bet on green and a bet on blue. Then, for the Bayesian, it follows that P(red) = P(green) = P(blue) = 1/3. Period.</p>\n<p>But for us, there is a degree of freedom, even in this symmetrical situation. We know what the probability of red is, so of course P(red) = 1/3 exactly. But we can set, say<sup>7</sup>, P(green) = 1/3 +- 1/9, and P(blue) = 1/3 -+ 1/9. So we get P(red or green) = 2/3 +- 1/9, P(red or blue) = 2/3 -+ 1/9, P(green or blue) = 2/3 exactly, and of course P(red or green or blue) = 1 exactly.</p>\n<p>So: red is 1/3 exactly, but the minimum of green is 2/9. (green or blue) is 2/3 exactly, but the minimum of (red or blue) is 5/9. So choose red over green, and (green or blue) over (red or blue). That's the paradoxical behaviour. Note that neither pair of choices offered in the Ellsberg paradox has the type of overlap that favours randomisation.</p>\n<p>Once we have a decision procedure for the two-outcome case, then we can tack on any utility function, as I explained in the previous post. The result here is what you would expect: we get oriented expected utility intervals, obtained by multiplying the oriented probability intervals by the utility. When deciding, we pick the one whose interval has the largest minimum. So for example, a bet which pays 15U on red (using U for \"utils\", the abstract units of measurement of the utility function) has expected utility 5U exactly. A bet which pays 18U on green has expected utility 6U +- 2U, the minimum is 4U. So pick the bet on red over that.</p>\n<p>Operationally, probability is associated with the \"fair price\" at which we are willing to bet. A probability interval indicates that there is no fair price. Instead we have a spread: we buy bets at their low price and sell at their high price. At least, we do that if we have no outstanding bets, or more generally, if the expected utility interval on our outstanding bets has zero-length. The <strong>second caveat</strong> is that if this interval has length, then it affects our price: we also sell bets of the same orientation at their low price, and buy bets of the opposite orientation at their high price, until the length of this interval is used up. The midpoint of the expected utility interval on our outstanding bets will be irrelevant.</p>\n<p>This can be confusing, so it's time for an analogy.</p>\n<h2>Bootsianism</h2>\n<p>If you are Bayesian and risk-neutral (and if bets pay in \"utils\" rather than cash, you are risk-neutral by definition) then outstanding bets have no effect on further betting behaviour. However, if you are risk-averse, as is the most common case, then this is no longer true. The more money you've already got on the line, the less willing you will be to bet.</p>\n<p>But besides risk attitude, there could also be interference effects from non-monetary payouts. For example, if you are dealing in boots, then you wouldn't buy a single boot for half the price of a pair, and neither would you sell one of your boots for half the price of a pair. Unless you happened to already have unmatched boots, then you would sell those at a lower price, or buy boots of the opposite orientation at a higher price, until you had no more unmatched boots. If you were otherwise risk-neutral with respect to boots, then your behaviour would not depend on the number of pairs you have, just on the number and orientation of your unmatched boots.</p>\n<p>This closely resembles the non-Bayesian behaviour above. In fact, for the Ellsberg urn, we could just say that a bet on red is worth a pair of boots, a bet on green is worth two left boots, and a bet on blue is worth two right boots. Without saying anything further, it's clear that we would strictly prefer red (a pair) over green (two lefts), but we would also strictly prefer green-or-blue (two pairs) over red-or-blue (one left and three rights). That's the paradoxical behaviour, but you know you can't money-pump boots.</p>\n<table border=\"0\" bgcolor=\"yellow\">\n<tbody>\n<tr>\n<td>\n<blockquote><strong>A</strong>: I'll buy that pair of boots for 30 <a href=\"http://nethack.wikia.com/wiki/Zorkmid\">zorkmids</a>. <br /> <strong>B</strong>: Okay, here's your pair of boots. <br /> <strong>A</strong>: And here's your 30 zorkmids. Thank you. <br /> <strong>B</strong>: Thank you. Say, didn't you just buy an identical pair this morning? <br /> <strong>A</strong>: Yeah, I did. Then a dingo ate the right one. I've got the left one here. Never worn. <br /> <strong>B</strong>: How narratively convenient! How much would you sell it for? <br /> <strong>A</strong>: Hmm, how about 10 zorkmids? <br /> <strong>B</strong>: Really, 10 zorkmids? So, do you think right boots are more valuable than left boots?<br /> <strong>A</strong>: No, of course not. Why?<br /> <strong>B</strong>: Arbitrage!<br /> <strong>A</strong>: Gesundheit.<br /> <strong>B</strong>: Thanks. I'll buy a left boot from you for 10 zorkmids. <br /> <strong>A</strong>: Great! Here's your left boot. <br /> <strong>B</strong>: And here's your 10 zorkmids. Thank you. <br /> <strong>A</strong>: Thank <em>you</em>! <br /> <strong>B</strong>: And I'll buy a right boot from you for 10 zorkmids. <br /> <strong>A</strong>: Errrm... Sorry? Why would I agree to that?<br /> <strong>B</strong>: You just sold me a left boot for 10 zorkmids. Well, you yourself said rights aren't more valuable than lefts. So, logically, you should be willing to sell me a right boot for 10 zorkmids.<br /> <strong>A</strong>: What? No. <br /></blockquote>\n</td>\n</tr>\n</tbody>\n</table>\n<h2>Boots' rule</h2>\n<p>So much for the <em>static</em> case. But what do we do with new information? How do we handle conditional probabilities?</p>\n<p>We still get P(A|B) by dividing P(A and B) by P(B). It will be easier to think in terms of pairs here. So for example P(red) = 1/3 exactly = (1/3,1/3) and P(red or green) = 2/3 +- 1/9 = (7/9,5/9), so P(red|red or green) = (3/7,3/5) = 18/35 -+ 3/35. And similarly P(green|red or green) = (1/3 +- 1/9)/(2/3 +- 1/9) = 17/35 +- 3/35.</p>\n<p>This rule covers the dynamic <em>passive</em> case, where we update probabilities based on what we observe, before betting. The <strong>third and final caveat</strong> is in the <em>active</em> case, when information comes in between bets. Now, we saw that the length and orientation of the interval on expected utility of outstanding bets affects further betting behaviour. There is actually a separate update rule for this quantity. It is about as simple as it gets: <em>do nothing</em>. The interval can change when we make choices, and its midpoint can shift due to external events, but its length and orientation do not update.</p>\n<p>You might expect the update rule for this quantity to follow from the way the expected utility updates, which follows from the way probability updates. But it has a mind of its own. So even if we are keeping track of our bets, we'd still need to keep track of this extra variable separately.</p>\n<p>Sometimes it may be easier to think in terms of the total expected utility interval of our outstanding bets, but sometimes it may be easier to think of this in terms of having a \"virtual\" interval that cancels the change in the length and orientation of the \"real\" expected utility interval. The midpoint of this virtual interval is irrelevant and can be taken to always be zero. So, on update, compute the prior expected utility interval of outstanding bets, subtract the posterior expected utility interval from it, and add this difference to the virtual interval. Reset its midpoint to zero, keeping only the length and orientation.</p>\n<p>That can also be confusing, so let's have another analogy.</p>\n<h2>Yo' mama's so illogical...</h2>\n<p>I recently came across this example by Mark Machina:</p>\n<table border=\"0\" bgcolor=\"yellow\">\n<tbody>\n<tr>\n<td>\n<blockquote><strong>M</strong>: Children, I only have one treat, I can only give it to one of you.<br /> <strong>I</strong>: Me, mama!<br /> <strong>J</strong>: No, give it to me!<br /> <strong>M</strong>: No. Rather than give it to either of you, it's better if I toss a coin. Heads, it goes to Irina, tails, it goes to Joey.<br /> ...<br /> <strong>M</strong>: Heads. Irina gets it.<br /> <strong>J</strong>: But mama!<br /> <strong>M</strong>: Fair is fair.<br /> <strong>I</strong>: Yeah Joey!<br /> <strong>J</strong>: But mama, you yourself said it's better to toss a coin than to give it to either of us. So, logically, instead of giving it to Irina you should toss a coin again.<br /> <strong>M</strong>: Nice try, Joey. <br /></blockquote>\n</td>\n</tr>\n</tbody>\n</table>\n<p>Instead of giving the treat to either child, she strictly prefers to toss a coin and give the treat to the winner. But after the coin is tossed, she strictly prefers to give the treat to the winner rather than toss again.</p>\n<p>This cannot be explained in terms of maximising expected utility, in the typical sense of \"utility\". And of course only known probabilities are involved here, so there's no question as to whether her beliefs are probabilistically sophisticated or not. But it could be said that she is still maximising the expected value of an extended objective function. This extended objective function does not just consider who gets a treat, but also considers who \"had a fair chance\". She is unfair if she gives the treat to either child outright, but fair if she tosses a coin. That fairness doesn't go away when the result of the coin toss is known.</p>\n<p>Or something like that. There are surely other ways of dissecting the mother's behaviour. But no matter what, it's going to have to take the coin toss into account, even though the coin, in and of itself, has no relevance to the situation.</p>\n<p>Let's go back to the urn. Green and blue have the type of overlap that favours randomisation: P((green and heads) or (blue and tails)) = 1/3 exactly. A bet paying 9U on this event has expected utility of 3U exactly. Let's say we took this bet. Now say the coin comes up heads. We can update the probabilities as per above. The answer is that P(green) = 1/3 +- 1/9 as it was before. That makes sense because it's an independent event: knowing the result of the coin toss gives no information about the urn. The difference is that we now have an outstanding bet that pays 9U if the ball is green. The expected utility would therefore be 3U +- 1U. Except, the expected utility interval was zero-length before the coin was tossed, so it remains zero-length. Equivalently, the virtual interval becomes -+ 1U, so that the effective total is 3U exactly. (In this example, the midpoint of the expected utility interval didn't change either. That's not generally the case.) A bet randomised on a <em>new</em> coin toss would have expected utility 3U, plus the virtual interval of -+ 1U, for an effective total of 3U -+ 1U. So we would strictly prefer to keep the bet on green rather than re-randomise.</p>\n<p>Let's compare this with a trivial example: let's say we took a bet that pays 9U if the ball drawn from the urn is green. The expected utility of this bet is 3U +- 1U. For some unrelated reason, a coin is tossed, and it comes up heads. The coin has also nothing to do with the urn or my bet. I still have a bet of 9U on green, and its expected utility is still 3U +- 1U.</p>\n<p>But the difference between these two examples is just in the counterfactual: if the coin had come up tails, in the first example I would have had a bet of 9U on blue, and in the second example I would have had a bet of 9U on green. But the coin came up heads, and in both examples I end up with a bet of 9U on green. The virtual interval has some spooky dependency on what <em>could</em> have happened, just like \"had a fair chance\". It is the ghost of a departed bet.</p>\n<p>I expect many on LW are wondering <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/WheresTheKaboom\">what happened</a>. There was supposed to be a proof that anything that isn't Bayesian can be punished. Actually, this threat comes with some hidden assumptions, which I hope these analogies have helped to illustrate. A boot is an example of something which has no fair price, even if a pair of boots has one. A mother with two children and one treat is an example where some counterfactuals are not forgotten. The hidden assumptions fail in our case, just as they can fail in these other contexts where Bayesianism is not at issue. This can be stated more rigorously<sup>8</sup>, but that is basically how it's possible. Now We Know. <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/AndKnowingIsHalfTheBattle\">And Knowing is Half the Battle</a>.</p>\n<h2>Notes</h2>\n<p><span> <ol>\n<li> Taken almost verbatim from Eliezer Yudkowsky's <a href=\"/lw/my/the_allais_paradox/\">post on the Allais paradox</a>. </li>\n<li> And footnotes pointing to some tangentially relevant journal articles make me sound extra credible. <!-- Seriously though, you might be interested in the <A href=\"http://www.sipta.org\" mce_href=\"http://www.sipta.org\">Society for Imprecise Probability: Theories and Applications</A>. --></li>\n<li> For Choquet expected utility see: D. Schmeidler, <em>Subjective probability and expected utility without additivity</em>, Econometrica 57 (1989) pp 571-587.</li>\n<li> For maxmin expected utility see: I. Gilboa and D. Schmeidler, <em>Maxmin expected utility with a non-unique prior</em>, J. Math. Econ. 18 (1989) pp 141-153.</li>\n<li> For multiplier preferences see: L.P. Hansen and T.J. Sargeant, <em>Robust control and model uncertainty</em>, Amer. Econ. Rev. 91 (2001) pp 60-66.</li>\n<li> For variational preferences see: F. Maccheroni, M. Marinacci, and A. Rustichini, <em>Dynamic variational preferences</em>, J. Econ. Theory 128 (2006) pp 4-44.</li>\n<li> Any length between 0 and 1/3 works. But here's where I pulled 1/9 from: a Bayesian might assign exactly 1/61 prior probability to the 61 possible urn compositions, and the result is roughly approximated by the <a href=\"http://en.wikipedia.org/wiki/Rule_of_succession\">Laplacian rule of succession</a>, which prescribes a <a href=\"http://en.wikipedia.org/wiki/Pseudocount\">pseudocount</a> of one green and one blue ball. A similar thing with probability intervals is roughly approximated by using a pseudocount of 3/2 +- 1/2 green and 3/2 -+ 1/2 blue balls. </li>\n<li> To quickly relate this back to <a href=\"/lw/9e4/the_savage_theorem_and_the_ellsberg_paradox\">Savage's rules</a>: rules 1 and 3 guarantee that there's no <em>static</em> money pump. Rule 2 then is supposed to guarantee that there is no <em>dynamic</em> money pump. But it is stronger than necessary for that purpose. I claim that this method obeys rules 1, 3, and a weaker version of rule 2, and that it is <em>dynamically consistent</em>. For dynamic consistency of variational preferences in general, see footnotes above. This method is a special case, for which I wrote up a <a href=\"http://sites.google.com/site/dmehkeri/home/dummies.pdf\">simpler proof</a>. </li>\n</ol></span></p>\n<h2>Appendix A: method summary</h2>\n<table border=\"0\" bgcolor=\"yellow\">\n<tbody>\n<tr>\n<td><span> \n<ul>\n<li> Events are assigned a pair of prior probabilities, which can also be thought of as an oriented probability interval. e.g. (3/5,2/5) can also be thought of as 1/2 +- 1/10. </li>\n<li> Neither side of the pair can be 0 or 1, except when they're both 0 or both 1. </li>\n<li> Each side of the pair is additive: if A and B are disjoint, and P(A) = (x,y), and P(B) = (u,v), then P(A or B) = (x+u,y+v). </li>\n<li> Each side of the pair updates by Bayes' rule: if P(A and B) = (x,y), and P(B) = (u,v), then P(A|B) = (x/u,y/v). </li>\n<li> Given a utility function, each bet will then have an expected utility interval: multiply the probability intervals by the utility for each possible outcome.</li>\n<li> There is also a virtual expected utility interval to keep track of. The midpoint of this interval is always zero. </li>\n<li> When updating the virtual expected utility interval, compute the prior expected utility interval of the outstanding bet(s), subtract the posterior expected utility interval from it, and add this difference to the virtual expected utility interval. Throw away the midpoint (reset the midpoint of the interval to zero, keeping just the length and orientation). </li>\n<li> To decide among bets: compute the expected utility intervals of each of them -- including already outstanding bets, and including the virtual expected utility interval. Rank them according to the minimum values of the intervals. </li>\n<li> Implicitly when presented with options we are also presented with the option to randomise among them, and sometimes this is strictly better than any of the pure options. </li>\n</ul>\n</span></td>\n</tr>\n</tbody>\n</table>\n<h2>Appendix B: obligatory image for LW posts on this topic</h2>\n<p><img src=\"http://sites.google.com/site/dmehkeri/home/AllYourBayes.PNG\" alt=\"All your Bayes are belong to us\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qtkiFYeF3gdMsRPHG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 17, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "12459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to</strong>: <a href=\"/lw/9e4/the_savage_theorem_and_the_ellsberg_paradox\">The Savage theorem and the Ellsberg paradox</a></p>\n<p>In the previous post, I presented a simple version of Savage's theorem, and I introduced the Ellsberg paradox. At the end of the post, I mentioned a strong Bayesian thesis, which can be summarised: \"There is always a price to pay for leaving the Bayesian Way.\"<sup>1</sup> But not always, it turns out. I claimed that there was a method that is Ellsberg-paradoxical, therefore non-Bayesian, but can't be money-pumped (or \"Dutch booked\"). I will present the method in this post.</p>\n<p>I'm afraid this is another long post. There's a short summary of the method at the very end, if you want to skip the <a href=\"http://www.urbandictionary.com/define.php?term=jibba%20jabba\">jibba jabba</a> and get right to the specification. Before trying to money-pump it, I'd suggest reading at least the two highlighted dialogues.</p>\n<h2 id=\"Ambiguity_aversion\">Ambiguity aversion</h2>\n<p>To recap the Ellsberg paradox: there's an urn with 30 red balls and 60 other balls that are either green or blue, in unknown proportions. Most people, when asked to choose between betting on red or on green, choose red, but, when asked to choose between betting on red-or-blue or on green-or-blue, choose green-or-blue. For some people this behaviour persists even after due calculation and reflection. This behaviour is non-Bayesian, and is the prototypical example of <a href=\"http://en.wikipedia.org/wiki/Ambiguity_aversion\">ambiguity aversion</a>.</p>\n<p>There were some major themes that came out in the comments on that post. One theme was that I Fail Technical Writing Forever. I'll try to redeem myself.</p>\n<p>Another theme was that the setup given may be a bit too symmetrical. The Bayesian answer would be indifference, and really, you can break ties however you want. However the paradoxical preferences are typically <em>strict</em>, rather than just tie-breaking behaviour. (And when it's not strict, we shouldn't call it ambiguity aversion.) One suggestion was to add or remove a couple of red balls. Speaking for myself, I would still make the paradoxical choices.</p>\n<p>A third theme was that ambiguity aversion might be a good heuristic if betting against someone who may know something you don't. Now, no such opponent was specified, and speaking for myself, I'm not inferring one when I make the paradoxical choices. Still, let me admit that it's not contrived to infer a mischievous experimenter from the Ellsberg setup. <a href=\"/lw/5te/a_summary_of_savages_foundations_for_probability/5o6o\">One commentator</a> puts it better than me:</p>\n<blockquote>\n<p>Betting generally includes an adversary who wants you to lose money so they win in. Possibly in psychology experiments [this might not apply] ... But generally, ignoring the possibility of someone wanting to win money off you when they offer you a bet is a bad idea.</p>\n<p>Now betting is supposed to be a metaphor for options with possibly unknown results. In which case sometimes you still need to account for the possibility that the options were made available by an adversary who wants you to choose badly, but less often. And you should also account for the possibility that they were from other people who wanted you to choose well, or that the options were not determined by any intelligent being or process trying to predict your choices, so you don't need to account for an anticorrelation between your choice and the best choice. Except for your own biases.</p>\n</blockquote>\n<p>We can take betting on the Ellsberg urn as a stand-in for various decisions under ambiguous circumstances. Ambiguity aversion can be Bayesian if we assume the right sort of correlation between the options offered and the state of the world, or the right sort of correlation between the choice made and the state of the world. In that case just about anything can Bayesian. But sometimes the opponent will not have extra information, nor extra power. There might not even be any opponent as such. If we assume there are no such correlations, then ambiguity aversion is non-Bayesian.</p>\n<p>The final theme was: <em>so what</em>? Ambiguity aversion is just another cognitive bias. One commentator specifically complained that I spent too much time talking about various abstractions and not enough time talking about how ambiguity aversion could be money-pumped. I will fix that now: I claim that ambiguity aversion cannot be money-pumped, and the rest of this post is about my claim.</p>\n<p>I'll start with a bit of name-dropping and some <a href=\"http://en.wikipedia.org/wiki/Whig_history\">whig history</a>, to make myself sound more credible than I really am<sup>2</sup>. In the last twenty years or so many models of ambiguity averse reasoning have been constructed. Choquet expected utility<sup>3</sup> and maxmin expected utility<sup>4</sup> were early proposed models of ambiguity aversion. Later multiplier preferences<sup>5</sup> were the result of applying the ideas of <a href=\"http://en.wikipedia.org/wiki/Robust_control\">robust control</a> to macroeconomic models. This results in ambiguity aversion, though it was not explicitly motivated by the Ellsberg paradox. More recently, variational preferences<sup>6</sup> generalises both multiplier preferences and maxmin expected utility. What I'm going to present is a finitary case of variational preferences, with some of my own amateur mathematical fiddling for rhetorical purposes.</p>\n<h2 id=\"Probability_intervals\">Probability intervals</h2>\n<p>The starting idea is simple enough, and may have already occurred to some LW readers. Instead of using a prior probability for events, can we not use an <a href=\"http://en.wikipedia.org/wiki/Imprecise_probability\">interval of probabilities</a>? What should our betting behaviour be for an event with probability 50%, plus or minus 10%?</p>\n<p>There are some different ways of filling in the details. So to be quite clear, I'm not proposing the following as the One True Probability Theory, and I am not claiming that the following is descriptive of many people's behaviour. What follows is just one way of making ambiguity aversion work, and perhaps the simplest way. This makes sense, given my aim: I should just describe a simple method that leaves the Bayesian Way, but does not pay.</p>\n<p>Now, sometimes disjoint ambiguous events together make an event with known probability. Or even a certainty, as in an event and its negation. If we want probability intervals to be additive (and let's say that we do) then what we really want are <em>oriented</em> intervals. I'll use +- or -+ (pronounced: plus-or-minus, minus-or-plus) to indicate two opposite orientations. So, if P(X) = 1/2 +- 1/10, then P(not X) = 1/2 -+ 1/10, and these add up to 1 exactly.</p>\n<p>Such oriented intervals are equivalent to ordered pairs of numbers. Sometimes it's more helpful to think of them as oriented intervals, but sometimes it's more helpful to think of them as pairs. So 1/2 +- 1/10 is the pair (3/5,2/5). And 1/2 -+ 1/10 is (2/5,3/5), the same numbers in the opposite order. The sum of these is (1,1), which is 1 exactly.</p>\n<p>You may wonder, if we can use ordered pairs, can we use triples, or longer lists? Yes, this method can be made to work with those too. And we can still think in terms of centre, length, and orientation. The orientation can go off in all sorts of directions, instead of just two. But for my purposes, I'll just stick with two.</p>\n<p>You might also ask, can we set P(X) = 1/2 +- 1/2? No, this method just won't handle it. A restriction of this method is that neither of the pair can be 0 or 1, except when they're both 0 or both 1. The way we will be using these intervals, 1/2 +- 1/2 would be the extreme case of ambiguity aversion. 1/2 +- 1/10 represents a lesser amount of ambiguity aversion, a sort of compromise between worst-case and average-case behaviour.</p>\n<p>To decide among bets (having the same two outcomes), compute their probability intervals. Sometimes, the intervals will not overlap. Then it's unambiguous which is more likely, so it's clear what to pick. In general, whether they overlap or not, pick the one with the largest minimum -- though we will see there are three caveats when they do overlap. If P(X) = 1/2 +- 1/10, we would be indifferent between a bet on X and on not X: the minimum is 2/5 in either case. If P(Y) = 1/2 exactly, then we would strictly prefer a bet on Y to a bet on X.</p>\n<p>Which leads to the <strong>first caveat</strong>: sometimes, given two options, it's strictly better to randomise. Let's suppose Y represents a fair coin. So P(Y) = 1/2 exactly, as we said. But also, Y is independent of X. P(X and Y) = 1/4 +- 1/20, and so on. This means that P((X and not Y) or (Y and not X)) = 1/2 exactly also. So we're indifferent between a bet on X and a bet on not X, but we strictly prefer the randomised bet.</p>\n<p>In general, randomisation will be strictly better if you have two choices with overlapping intervals of opposite orientations. The best randomisation ratio will be the one that gives a bet with zero-length interval.</p>\n<p>Now let us reconsider the Ellsberg urn. We did say the urn can be a metaphor for various situations. Generally these situations will not be symmetrical. But, even in symmetrical scenarios, we should still re-think how we apply the <a href=\"http://en.wikipedia.org/wiki/Principle_of_indifference\">principle of indifference</a>. I argue that the underlying idea is really this: if our information has a symmetry, then our decisions should have that same symmetry. If we switch green and blue, our information about the Ellsberg urn doesn't change. The situation is indistinguishable, so we should behave the same way. It follows that we should be indifferent between a bet on green and a bet on blue. Then, for the Bayesian, it follows that P(red) = P(green) = P(blue) = 1/3. Period.</p>\n<p>But for us, there is a degree of freedom, even in this symmetrical situation. We know what the probability of red is, so of course P(red) = 1/3 exactly. But we can set, say<sup>7</sup>, P(green) = 1/3 +- 1/9, and P(blue) = 1/3 -+ 1/9. So we get P(red or green) = 2/3 +- 1/9, P(red or blue) = 2/3 -+ 1/9, P(green or blue) = 2/3 exactly, and of course P(red or green or blue) = 1 exactly.</p>\n<p>So: red is 1/3 exactly, but the minimum of green is 2/9. (green or blue) is 2/3 exactly, but the minimum of (red or blue) is 5/9. So choose red over green, and (green or blue) over (red or blue). That's the paradoxical behaviour. Note that neither pair of choices offered in the Ellsberg paradox has the type of overlap that favours randomisation.</p>\n<p>Once we have a decision procedure for the two-outcome case, then we can tack on any utility function, as I explained in the previous post. The result here is what you would expect: we get oriented expected utility intervals, obtained by multiplying the oriented probability intervals by the utility. When deciding, we pick the one whose interval has the largest minimum. So for example, a bet which pays 15U on red (using U for \"utils\", the abstract units of measurement of the utility function) has expected utility 5U exactly. A bet which pays 18U on green has expected utility 6U +- 2U, the minimum is 4U. So pick the bet on red over that.</p>\n<p>Operationally, probability is associated with the \"fair price\" at which we are willing to bet. A probability interval indicates that there is no fair price. Instead we have a spread: we buy bets at their low price and sell at their high price. At least, we do that if we have no outstanding bets, or more generally, if the expected utility interval on our outstanding bets has zero-length. The <strong>second caveat</strong> is that if this interval has length, then it affects our price: we also sell bets of the same orientation at their low price, and buy bets of the opposite orientation at their high price, until the length of this interval is used up. The midpoint of the expected utility interval on our outstanding bets will be irrelevant.</p>\n<p>This can be confusing, so it's time for an analogy.</p>\n<h2 id=\"Bootsianism\">Bootsianism</h2>\n<p>If you are Bayesian and risk-neutral (and if bets pay in \"utils\" rather than cash, you are risk-neutral by definition) then outstanding bets have no effect on further betting behaviour. However, if you are risk-averse, as is the most common case, then this is no longer true. The more money you've already got on the line, the less willing you will be to bet.</p>\n<p>But besides risk attitude, there could also be interference effects from non-monetary payouts. For example, if you are dealing in boots, then you wouldn't buy a single boot for half the price of a pair, and neither would you sell one of your boots for half the price of a pair. Unless you happened to already have unmatched boots, then you would sell those at a lower price, or buy boots of the opposite orientation at a higher price, until you had no more unmatched boots. If you were otherwise risk-neutral with respect to boots, then your behaviour would not depend on the number of pairs you have, just on the number and orientation of your unmatched boots.</p>\n<p>This closely resembles the non-Bayesian behaviour above. In fact, for the Ellsberg urn, we could just say that a bet on red is worth a pair of boots, a bet on green is worth two left boots, and a bet on blue is worth two right boots. Without saying anything further, it's clear that we would strictly prefer red (a pair) over green (two lefts), but we would also strictly prefer green-or-blue (two pairs) over red-or-blue (one left and three rights). That's the paradoxical behaviour, but you know you can't money-pump boots.</p>\n<table border=\"0\" bgcolor=\"yellow\">\n<tbody>\n<tr>\n<td>\n<blockquote><strong>A</strong>: I'll buy that pair of boots for 30 <a href=\"http://nethack.wikia.com/wiki/Zorkmid\">zorkmids</a>. <br> <strong>B</strong>: Okay, here's your pair of boots. <br> <strong>A</strong>: And here's your 30 zorkmids. Thank you. <br> <strong>B</strong>: Thank you. Say, didn't you just buy an identical pair this morning? <br> <strong>A</strong>: Yeah, I did. Then a dingo ate the right one. I've got the left one here. Never worn. <br> <strong>B</strong>: How narratively convenient! How much would you sell it for? <br> <strong>A</strong>: Hmm, how about 10 zorkmids? <br> <strong>B</strong>: Really, 10 zorkmids? So, do you think right boots are more valuable than left boots?<br> <strong>A</strong>: No, of course not. Why?<br> <strong>B</strong>: Arbitrage!<br> <strong>A</strong>: Gesundheit.<br> <strong>B</strong>: Thanks. I'll buy a left boot from you for 10 zorkmids. <br> <strong>A</strong>: Great! Here's your left boot. <br> <strong>B</strong>: And here's your 10 zorkmids. Thank you. <br> <strong>A</strong>: Thank <em>you</em>! <br> <strong>B</strong>: And I'll buy a right boot from you for 10 zorkmids. <br> <strong>A</strong>: Errrm... Sorry? Why would I agree to that?<br> <strong>B</strong>: You just sold me a left boot for 10 zorkmids. Well, you yourself said rights aren't more valuable than lefts. So, logically, you should be willing to sell me a right boot for 10 zorkmids.<br> <strong>A</strong>: What? No. <br></blockquote>\n</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Boots__rule\">Boots' rule</h2>\n<p>So much for the <em>static</em> case. But what do we do with new information? How do we handle conditional probabilities?</p>\n<p>We still get P(A|B) by dividing P(A and B) by P(B). It will be easier to think in terms of pairs here. So for example P(red) = 1/3 exactly = (1/3,1/3) and P(red or green) = 2/3 +- 1/9 = (7/9,5/9), so P(red|red or green) = (3/7,3/5) = 18/35 -+ 3/35. And similarly P(green|red or green) = (1/3 +- 1/9)/(2/3 +- 1/9) = 17/35 +- 3/35.</p>\n<p>This rule covers the dynamic <em>passive</em> case, where we update probabilities based on what we observe, before betting. The <strong>third and final caveat</strong> is in the <em>active</em> case, when information comes in between bets. Now, we saw that the length and orientation of the interval on expected utility of outstanding bets affects further betting behaviour. There is actually a separate update rule for this quantity. It is about as simple as it gets: <em>do nothing</em>. The interval can change when we make choices, and its midpoint can shift due to external events, but its length and orientation do not update.</p>\n<p>You might expect the update rule for this quantity to follow from the way the expected utility updates, which follows from the way probability updates. But it has a mind of its own. So even if we are keeping track of our bets, we'd still need to keep track of this extra variable separately.</p>\n<p>Sometimes it may be easier to think in terms of the total expected utility interval of our outstanding bets, but sometimes it may be easier to think of this in terms of having a \"virtual\" interval that cancels the change in the length and orientation of the \"real\" expected utility interval. The midpoint of this virtual interval is irrelevant and can be taken to always be zero. So, on update, compute the prior expected utility interval of outstanding bets, subtract the posterior expected utility interval from it, and add this difference to the virtual interval. Reset its midpoint to zero, keeping only the length and orientation.</p>\n<p>That can also be confusing, so let's have another analogy.</p>\n<h2 id=\"Yo__mama_s_so_illogical___\">Yo' mama's so illogical...</h2>\n<p>I recently came across this example by Mark Machina:</p>\n<table border=\"0\" bgcolor=\"yellow\">\n<tbody>\n<tr>\n<td>\n<blockquote><strong>M</strong>: Children, I only have one treat, I can only give it to one of you.<br> <strong>I</strong>: Me, mama!<br> <strong>J</strong>: No, give it to me!<br> <strong>M</strong>: No. Rather than give it to either of you, it's better if I toss a coin. Heads, it goes to Irina, tails, it goes to Joey.<br> ...<br> <strong>M</strong>: Heads. Irina gets it.<br> <strong>J</strong>: But mama!<br> <strong>M</strong>: Fair is fair.<br> <strong>I</strong>: Yeah Joey!<br> <strong>J</strong>: But mama, you yourself said it's better to toss a coin than to give it to either of us. So, logically, instead of giving it to Irina you should toss a coin again.<br> <strong>M</strong>: Nice try, Joey. <br></blockquote>\n</td>\n</tr>\n</tbody>\n</table>\n<p>Instead of giving the treat to either child, she strictly prefers to toss a coin and give the treat to the winner. But after the coin is tossed, she strictly prefers to give the treat to the winner rather than toss again.</p>\n<p>This cannot be explained in terms of maximising expected utility, in the typical sense of \"utility\". And of course only known probabilities are involved here, so there's no question as to whether her beliefs are probabilistically sophisticated or not. But it could be said that she is still maximising the expected value of an extended objective function. This extended objective function does not just consider who gets a treat, but also considers who \"had a fair chance\". She is unfair if she gives the treat to either child outright, but fair if she tosses a coin. That fairness doesn't go away when the result of the coin toss is known.</p>\n<p>Or something like that. There are surely other ways of dissecting the mother's behaviour. But no matter what, it's going to have to take the coin toss into account, even though the coin, in and of itself, has no relevance to the situation.</p>\n<p>Let's go back to the urn. Green and blue have the type of overlap that favours randomisation: P((green and heads) or (blue and tails)) = 1/3 exactly. A bet paying 9U on this event has expected utility of 3U exactly. Let's say we took this bet. Now say the coin comes up heads. We can update the probabilities as per above. The answer is that P(green) = 1/3 +- 1/9 as it was before. That makes sense because it's an independent event: knowing the result of the coin toss gives no information about the urn. The difference is that we now have an outstanding bet that pays 9U if the ball is green. The expected utility would therefore be 3U +- 1U. Except, the expected utility interval was zero-length before the coin was tossed, so it remains zero-length. Equivalently, the virtual interval becomes -+ 1U, so that the effective total is 3U exactly. (In this example, the midpoint of the expected utility interval didn't change either. That's not generally the case.) A bet randomised on a <em>new</em> coin toss would have expected utility 3U, plus the virtual interval of -+ 1U, for an effective total of 3U -+ 1U. So we would strictly prefer to keep the bet on green rather than re-randomise.</p>\n<p>Let's compare this with a trivial example: let's say we took a bet that pays 9U if the ball drawn from the urn is green. The expected utility of this bet is 3U +- 1U. For some unrelated reason, a coin is tossed, and it comes up heads. The coin has also nothing to do with the urn or my bet. I still have a bet of 9U on green, and its expected utility is still 3U +- 1U.</p>\n<p>But the difference between these two examples is just in the counterfactual: if the coin had come up tails, in the first example I would have had a bet of 9U on blue, and in the second example I would have had a bet of 9U on green. But the coin came up heads, and in both examples I end up with a bet of 9U on green. The virtual interval has some spooky dependency on what <em>could</em> have happened, just like \"had a fair chance\". It is the ghost of a departed bet.</p>\n<p>I expect many on LW are wondering <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/WheresTheKaboom\">what happened</a>. There was supposed to be a proof that anything that isn't Bayesian can be punished. Actually, this threat comes with some hidden assumptions, which I hope these analogies have helped to illustrate. A boot is an example of something which has no fair price, even if a pair of boots has one. A mother with two children and one treat is an example where some counterfactuals are not forgotten. The hidden assumptions fail in our case, just as they can fail in these other contexts where Bayesianism is not at issue. This can be stated more rigorously<sup>8</sup>, but that is basically how it's possible. Now We Know. <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/AndKnowingIsHalfTheBattle\">And Knowing is Half the Battle</a>.</p>\n<h2 id=\"Notes\">Notes</h2>\n<p><span> </span></p><ol>\n<li> Taken almost verbatim from Eliezer Yudkowsky's <a href=\"/lw/my/the_allais_paradox/\">post on the Allais paradox</a>. </li>\n<li> And footnotes pointing to some tangentially relevant journal articles make me sound extra credible. <!-- Seriously though, you might be interested in the <A href=\"http://www.sipta.org\" mce_href=\"http://www.sipta.org\">Society for Imprecise Probability: Theories and Applications</A>. --></li>\n<li> For Choquet expected utility see: D. Schmeidler, <em>Subjective probability and expected utility without additivity</em>, Econometrica 57 (1989) pp 571-587.</li>\n<li> For maxmin expected utility see: I. Gilboa and D. Schmeidler, <em>Maxmin expected utility with a non-unique prior</em>, J. Math. Econ. 18 (1989) pp 141-153.</li>\n<li> For multiplier preferences see: L.P. Hansen and T.J. Sargeant, <em>Robust control and model uncertainty</em>, Amer. Econ. Rev. 91 (2001) pp 60-66.</li>\n<li> For variational preferences see: F. Maccheroni, M. Marinacci, and A. Rustichini, <em>Dynamic variational preferences</em>, J. Econ. Theory 128 (2006) pp 4-44.</li>\n<li> Any length between 0 and 1/3 works. But here's where I pulled 1/9 from: a Bayesian might assign exactly 1/61 prior probability to the 61 possible urn compositions, and the result is roughly approximated by the <a href=\"http://en.wikipedia.org/wiki/Rule_of_succession\">Laplacian rule of succession</a>, which prescribes a <a href=\"http://en.wikipedia.org/wiki/Pseudocount\">pseudocount</a> of one green and one blue ball. A similar thing with probability intervals is roughly approximated by using a pseudocount of 3/2 +- 1/2 green and 3/2 -+ 1/2 blue balls. </li>\n<li> To quickly relate this back to <a href=\"/lw/9e4/the_savage_theorem_and_the_ellsberg_paradox\">Savage's rules</a>: rules 1 and 3 guarantee that there's no <em>static</em> money pump. Rule 2 then is supposed to guarantee that there is no <em>dynamic</em> money pump. But it is stronger than necessary for that purpose. I claim that this method obeys rules 1, 3, and a weaker version of rule 2, and that it is <em>dynamically consistent</em>. For dynamic consistency of variational preferences in general, see footnotes above. This method is a special case, for which I wrote up a <a href=\"http://sites.google.com/site/dmehkeri/home/dummies.pdf\">simpler proof</a>. </li>\n</ol><p></p>\n<h2 id=\"Appendix_A__method_summary\">Appendix A: method summary</h2>\n<table border=\"0\" bgcolor=\"yellow\">\n<tbody>\n<tr>\n<td><span> \n<ul>\n<li> Events are assigned a pair of prior probabilities, which can also be thought of as an oriented probability interval. e.g. (3/5,2/5) can also be thought of as 1/2 +- 1/10. </li>\n<li> Neither side of the pair can be 0 or 1, except when they're both 0 or both 1. </li>\n<li> Each side of the pair is additive: if A and B are disjoint, and P(A) = (x,y), and P(B) = (u,v), then P(A or B) = (x+u,y+v). </li>\n<li> Each side of the pair updates by Bayes' rule: if P(A and B) = (x,y), and P(B) = (u,v), then P(A|B) = (x/u,y/v). </li>\n<li> Given a utility function, each bet will then have an expected utility interval: multiply the probability intervals by the utility for each possible outcome.</li>\n<li> There is also a virtual expected utility interval to keep track of. The midpoint of this interval is always zero. </li>\n<li> When updating the virtual expected utility interval, compute the prior expected utility interval of the outstanding bet(s), subtract the posterior expected utility interval from it, and add this difference to the virtual expected utility interval. Throw away the midpoint (reset the midpoint of the interval to zero, keeping just the length and orientation). </li>\n<li> To decide among bets: compute the expected utility intervals of each of them -- including already outstanding bets, and including the virtual expected utility interval. Rank them according to the minimum values of the intervals. </li>\n<li> Implicitly when presented with options we are also presented with the option to randomise among them, and sometimes this is strictly better than any of the pure options. </li>\n</ul>\n</span></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Appendix_B__obligatory_image_for_LW_posts_on_this_topic\">Appendix B: obligatory image for LW posts on this topic</h2>\n<p><img src=\"http://sites.google.com/site/dmehkeri/home/AllYourBayes.PNG\" alt=\"All your Bayes are belong to us\"></p>", "sections": [{"title": "Ambiguity aversion", "anchor": "Ambiguity_aversion", "level": 1}, {"title": "Probability intervals", "anchor": "Probability_intervals", "level": 1}, {"title": "Bootsianism", "anchor": "Bootsianism", "level": 1}, {"title": "Boots' rule", "anchor": "Boots__rule", "level": 1}, {"title": "Yo' mama's so illogical...", "anchor": "Yo__mama_s_so_illogical___", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "Appendix A: method summary", "anchor": "Appendix_A__method_summary", "level": 1}, {"title": "Appendix B: obligatory image for LW posts on this topic", "anchor": "Appendix_B__obligatory_image_for_LW_posts_on_this_topic", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "72 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["thHZiZBDRPtGxCM6f", "zJZvoiwydJ5zvzTHK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T20:01:02.984Z", "modifiedAt": null, "url": null, "title": "The Personality of (great/creative) Scientists: Open and Conscientious", "slug": "the-personality-of-great-creative-scientists-open-and", "viewCount": null, "lastCommentedAt": "2022-02-12T02:30:30.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9vKRHBCLvwqN26LLs/the-personality-of-great-creative-scientists-open-and", "pageUrlRelative": "/posts/9vKRHBCLvwqN26LLs/the-personality-of-great-creative-scientists-open-and", "linkUrl": "https://www.lesswrong.com/posts/9vKRHBCLvwqN26LLs/the-personality-of-great-creative-scientists-open-and", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Personality%20of%20(great%2Fcreative)%20Scientists%3A%20Open%20and%20Conscientious&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Personality%20of%20(great%2Fcreative)%20Scientists%3A%20Open%20and%20Conscientious%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9vKRHBCLvwqN26LLs%2Fthe-personality-of-great-creative-scientists-open-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Personality%20of%20(great%2Fcreative)%20Scientists%3A%20Open%20and%20Conscientious%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9vKRHBCLvwqN26LLs%2Fthe-personality-of-great-creative-scientists-open-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9vKRHBCLvwqN26LLs%2Fthe-personality-of-great-creative-scientists-open-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2527, "htmlBody": "<p>We&rsquo;ve discussed the <a href=\"http://en.wikipedia.org/wiki/Big_Five_personality_traits\">Big Five</a> in the past, such as the relationship of <a href=\"http://en.wikipedia.org/wiki/Openness_to_experience\">Openness</a> to <a href=\"/lw/82g/on_the_openness_personality_trait_rationality/\">parasites &amp; signaling</a> or whether hallucinogens <a href=\"/lw/7wh/rationality_drugs/4xmw\">increase Openness</a> and <a href=\"/lw/8gz/is_latent_toxoplasmosis_worth_doing_something/\">parasites decrease it</a>, along with my <a href=\"http://www.gwern.net/Conscientiousness%20and%20online%20education\">little notes on the value of Conscientiousness</a>. This is another entry in the topic of &lsquo;what is Big Five good for&rsquo;.</p>\n<p>I researched the topic of how and whether Conscientiousness and Openness correlate with scientific achievement for Luke for the <a href=\"/lw/8jb/intelligence_explosion_analysis_draft_why/\">Intelligence Explosion</a> paper; here is some of what I found:</p>\n<p><a id=\"more\"></a></p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p><a href=\"http://www.corwin.com/upm-data/33595_Anderson.pdf\">&ldquo;Creativity, Intelligence, and Personality&rdquo;</a>, 1981 review:</p>\n<blockquote>\n<p>&ldquo;Studies of creative adult artists, scientists, mathematicians, and writers find them scoring very high on tests of general intelligence (e.g.&nbsp;Barron 1969; Bachtold &amp; Werner 1970; Helson &amp; Crutchfield 1970b; Cattell 1971; Helson 1971; Bachtold &amp; Werner 1973; Gough 1976a), though rs between tested intelligence and creative achievement in these samples range from insignificantly negative (r = -.05, Gough 1976a) to mildly and significantly positive (r = +.31, Helson 1971).&rdquo;</p>\n</blockquote>\n<p>I found this one amusing:</p>\n<blockquote>\n<p>&ldquo;It should be noted that creative people are often perceived and rated as more intelligent than less creative people even in samples where no corresponding correlations between tested intelligence and creativity obtain. Despite an r of -.08 between Terman&rsquo;s Concept Mastery Test and professionally rated creativity among the top 40 IPAR architects (MacKinnon 1962a), e.g., staff ratings of the single adjective &ldquo;intelligent&rdquo; correlated +.39 with the index of creativity (MacKinnon 1966). While such an r may reflect some spurious halo effects, it may also tell us something about the true overlap in meaning of these terms in the natural language.\"</p>\n</blockquote>\n<p>An embarrassment of riches; no summary, but a starting point if one needs more:</p>\n<blockquote>\n<p>&ldquo;<em>SCIENCE AND TECHNOLOGY</em>: Personality correlates of scientific achievement and creativity were studied in elementary school children (Milgram et al 1977); high school students (Schaefer &amp; Anastasi 1968, Parloff et al 1968, Anastasi &amp; Schaefer 1969, Schaefer 1969a, b, Walberg 1969a); undergraduates, young adults, and graduate students (Rossman &amp; Horn 1972, Schaefer 1973, Gough 1979, Korb &amp; Frankiewicz 1979); psychologists (Chambers 1964, Wispe 1965, Bachtold &amp; Werner 1970); inventors (Bergum 1975, Albaum 1976, Albaum &amp; Baker 1977); mathematicians (Helson 1967b, 1968a, Parloff et al 1968; Helson &amp; Crutchfield 1970a, b; Helson 1971; Gough 1979); chemists (Chambers 1964); and assorted engineers and research scientists (McDermid 1965, Owens 1969, Bachtold &amp; Werner 1972, Bergum 1973, Eiduson 1974, Gough 1979).&rdquo;</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"http://www.gwern.net/docs/2006-feist.pdf\">&ldquo;How development and personality influence scientific thought, interest, and achievement&rdquo;</a>, GJ Feist, <em>Review of General Psychology</em>, 2006; important bits start on pg 9:</p>\n<blockquote>\n<p>\"In 1998, I published a quantitative review of the literature on personality and scientific interest and creativity (<a href=\"http://www.gwern.net/docs/1998-feist.pdf\">Feist, 1998</a>. In this meta-analytic review of which personality traits make interest and creativity in science more likely, I found every published (and some unpublished studies) that examined the role in personality in scientific interest or scientific creativity from 1950 to 1998. There were 26 studies that reported quantitative effects of personality in scientists compared to non-scientists.</p>\n<p>&hellip;The two strongest effect sizes (medium in magnitude) were for the positive and negative poles of conscientiousness (C; see Table 1). Being high in conscientiousness (C\u03e9) consists of scales and items such as careful, cautious, conscientious, fastidious, and self-controlled, whereas being low in conscientiousness (C\u03ea) consists only of two scales/items, namely, direct expression of needs and psychopathic deviance. Although the C\u03ea dimension comprised only five comparisons, it is clear that relative to non-scientists, scientists are roughly a half a standard deviation higher on conscientiousness and controlling of impulses. In addition, low openness to experience had a median d of .30, whereas introversion had a median effect size of .26.</p>\n<p>A consistent finding in the personality and creativity in science literature has been that creative and eminent scientists tend to be more open to experience and more flexible in thought than less creative and eminent scientists (see Table 2). Many of these findings stem from data on the flexibility (Fe) and tolerance (To) scales of the California Psychological Inventory (Feist &amp; Barron, 2003; Garwood, 1964; Gough, 1961; Helson, 1971; Helson &amp; Crutchfield, 1970; Parloff &amp; Datta, 1965). The Fe scale, for instance, taps into flexibility and adaptability of thought and behavior as well as the preference for change and novelty (Gough, 1987). The few studies that have reported either no effect or a negative effect of flexibility in scientific creativity have been with student samples (Davids, 1968; Smithers &amp; Batcock, 1970).</p>\n<p>For instance, Feist and Barron (2003) examined personality, intellect, potential, and creative achievement in a 44-year longitudinal study. More specifically, they predicted that personality would explain unique variance in creativity over and above that already explained by intellect and potential. Results showed that observer-rated Potential and Intellect at age 27 predicted Lifetime Creativity at age 72, and yet personality variables (such as Tolerance and Psychological Mindedness) explained up to 20% of the variance over and above Potential and Intellect. Specifically, two measures of personality&mdash;California Psychological Inventory scales of Tolerance (To) and Psychological Mindedness (Py)&ndash;resulted in the 20% increase in variance explained (20%) over and above potential and intellect. The more tolerant and psychologically minded the student was, the more likely he was to make creative achievements over his lifetime. Together, the four predictors (Potential, Intellect, Tolerance, and Psychological Mindedness) explained a little more than a third of the variance in lifetime creative achievement. I should point out that these findings on To and Py mirror very closely those reported by Helson and Pals (2000) in a longitudinal study of women from age 21 to 52.</p>\n<p>&hellip;Busse and Mansfield (1984), for example, studied the personality characteristics of 196 biologists, 201 chemists, and 171 physicists, and commitment to work (i.e., &ldquo;need to concentrate intensively over long periods of time on one&rsquo;s work&rdquo;) was the strongest predictor of productivity (i.e., publication quantity) even when holding age and professional age constant. Helmreich, Spence, Beane, Lucker, and Matthews (1980) studied a group of 196 academic psychologists and found that different components of achievement and drive had different relationships with objective measures of attainment (i.e., publications and citations). With a self-report measure, they assessed three different aspects of achievement: &ldquo;mastery&rdquo; preferring challenging and difficult tasks; &ldquo;work&rdquo; enjoying working hard; and &ldquo;competitiveness&rdquo; liking interpersonal competition and bettering others&hellip;.Helmreich and his colleagues found that mastery and work were positively related to both publication and citation totals, whereas competitiveness was positively related to publications but negatively related to citations</p>\n<p>&hellip;Helson (1971) compared creative female mathematicians with less creative female mathematicians, matched on IQ. Observers blindly rated the former as having more &ldquo;unconventional thought processes,&rdquo; as being more &ldquo;rebellious and non-conforming,&rdquo; and as being less likely to judge &ldquo;self and others in conventional terms.&rdquo; More recently, Rushton, Murray, and Paunonen (1987) conducted factor analyses of the personality traits most strongly loading on the &ldquo;research&rdquo; factor (in contrast to a &ldquo;teaching&rdquo; factor) in two separate samples of academic psychologists. Among other results, they found that &ldquo;independence&rdquo; tended to load on the research factor, whereas &ldquo;extraversion&rdquo; tended to load on the teaching factor.\"</p>\n</blockquote>\n<p>Pretty substantial. The 26 study meta-analysis was Feist, G. J. (1998). <a href=\"http://www.gwern.net/docs/1998-feist.pdf\">&ldquo;A meta-analysis of the impact of personality on scientific and artistic creativity&rdquo;</a>. <em>Personality and Social Psychological Review</em>, 2, 290&ndash;309</p>\n<p>Feist 1998 was also cited in a chapter of <em>The Cambridge Handbook of Creativity</em>, &ldquo;the relationship between creativity and intelligence&rdquo;, but I couldn&rsquo;t get the book; further reading, if anyone wants some.</p>\n<p>One useful bit from Feist; I also ran into a lot of CP Benbow-keyworded studies of the <a href=\"http://en.wikipedia.org/wiki/Study_of_Mathematically_Precocious_Youth\">gifted SMPY cohorts</a> to the effect that the kids&rsquo; early interest in science predicts later careers in science, which would tie in nicely to this:</p>\n<blockquote>\n<p>&ldquo;The empirical consensus is that early levels of high productivity do regularly predict continued levels of high productivity across one&rsquo;s lifetime (Cole, 1979; Dennis, 1966; Helson &amp; Crutchfield, 1970; Horner et al., 1986; Lehman, 1953; Over, 1982; Reskin, 1977; Roe, 1965; Simonton, 1988a, 1991)&hellip;.In other words, the younger NAS members were when they and others recognized their scientific talent, when they wanted to be a scientist, and when they first conducted scientific research, the younger they were when they published their first paper. Age of first publication in turn predicted total publication rate over the lifetime, meaning that the earlier one publishes, the more productive one will be. This pattern of relationships&mdash;from precocity to age of first publication to lifetime productivity&mdash;implies an indirect connection between precocity and publication rate. The only precocity variable that reached the .05 level of significance with lifetime productivity was age that one first conducted formal research.&rdquo;</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"http://www.gwern.net/docs/2008-simonton.pdf\">&ldquo;Scientific talent, training, and performance: Intellect, personality, and genetic endowment&rdquo;</a>, Simonton 2008; the abstract caught my eye:</p>\n<blockquote>\n<p>\"After specifying the ideal data requirements for the application of the three estimators, the procedures were applied to previously published results. Personality traits were illustrated with the use of the California Psychological Inventory and the Eysenck Personality Questionnaire with respect to two criteria (scientists versus non-scientists and creative scientists versus less creative scientists) and intellectual traits with the use of the Miller Analogies Test with respect to seven criteria (graduate grade-point average, faculty ratings, comprehensive examination scores, degree attainment, research productivity, etc.). The outcome provides approximate, <em>lower-bound estimates of the genetic contribution to scientific training and performance</em>.</p>\n<p>&hellip;Sawyer reviewed three investigations that allegedly disconfirm the role of genetic endowment in any form of creative achievement (viz., Barron, 1972; Reznikoff, Domino, Bridges, &amp; Honeyman, 1973; Vandenberg, Stafford, &amp; Brown, 1968).2 Likewise, Ericsson, Roring, and Nandagopal (2007) cited two behavior genetic investigations in drawing the same conclusion about talent in general (viz., Bouchard &amp; Lykken, 1999; Klissouras et al., 2001).</p>\n<p>&hellip;This argument certainly applies to scientific talent. For instance, a comprehensive longitudinal study of the mathematically precocious (Lubinski, Webb, Morelock, &amp; Benbow, 2001) has extremely few twins in the sample (D. Lubinski, personal communication, March 15, 2007). Simonton&rsquo;s (1991a) study of 2,026 eminent scientists contained only one twin (Auguste Picard). And, needless to say, there are no twins, monozygotic or dizygotic, among Nobel laureates in the sciences. Thus, not only may we lack direct evidence for scientific talent, but also it may never be possible to establish such substantiation with the use of standard behavior genetic methods.</p>\n<p>&hellip;Bouchard and Lykken (1999) demonstrated that the personality characteristics associated with scientific productivity display heritabilities ranging between .32 and .57, meaning that between 32% and 57% of the variance in those traits can be attributed to genetic endowment. Similarly, the Creativity Personality Scale (CPS) of the Adjective Check List (ACL) not only predicts scientific creativity (Gough, 1979) but also has a heritability of .54 (Bouchard &amp; Lykken, 1999; Waller, Bouchard, Lykken, Tellegen, &amp; Blacker, 1993). Hence, 54% of the variance in the CPS has a genetic contribution. Moreover, because predictive validities are known for this measure, we can draw a more powerful inference. For instance, CPS scores correlated .31 with the creativity ratings that expert judges assigned 57 mathematicians (Gough, 1979). This signifies that almost 10% of the variance in that criterion can be attributed to CPS (i.e., .312 \u03ed .096). Multiplying the squared criterion&ndash;trait correlation by the heritability coefficient then yields .052, which implies that over 5% of the variance in the rated creativity of these mathematicians might be ascribed to the genetic part of the CPS scores.</p>\n</blockquote>\n<p>&ldquo;Bouchard &amp; Lykken 1999&rdquo; = Bouchard, T. J., Jr., &amp; Lykken, D. T. (1999). &ldquo;Genetic and environmental influence on correlates of creativity&rdquo;. In N. Colangelo &amp; S. G. Assouline (Eds.), <em>Talent development III: Proceedings from the 1995 Henry B. &amp; Jocelyn Wallace National Symposium on Talent Development</em> (pp.&nbsp;81&ndash;97). I couldn&rsquo;t find this, but I did find http://cogprints.org/611/1/genius.html which describes it a little more (C-f &lsquo;in press&rsquo;).</p>\n<p>Moving onwards, pg 9&ndash;12 discuss Feist 1998&rsquo;s meta-analysis:</p>\n<blockquote>\n<p>&hellip;Even worse, &ldquo;science&rdquo; was obliged to include the physical, biological, and social sciences as well as mathematics, engineering, and invention. To the extent that the personality profiles are closely tailored to domain-specific training or performance criteria, this definitional inclusiveness implies that the hc2 estimates are too low. This criticism is not intended to fault Feist&rsquo;s (1998) meta-analytic review. To obtain sound effect size estimates he had no other option but to collate many diverse findings. Nevertheless, in this analysis of intellectual traits it is feasible to substitute somewhat more specific criteria for these more global contrasts.</p>\n<p>For example, spatial ability has been identified as a crucial component of math&ndash;science talent that exhibits predictive utility beyond that provided by both mathematical and verbal ability (Webb, Lubinski, &amp; Benbow, 2007). Yet measures of spatial intelligence display heritabilities almost as high as general intelligence (Bratko, 1996; McClearn, Johansson, Berg, &amp; Pedersen, 1997). Moreover, these more specialized intellectual abilities may be especially useful in differentiating distinct types of scientific talents. For instance, in Roe&rsquo;s (1953) classic study of 64 eminent scientists it was found that theoretical physicists, experimental physicists, biologists, psychologists, and anthropologists display distinctive profiles with respect to verbal, mathematical, and spatial intelligence.</p>\n</blockquote>\n<p>Concluding:</p>\n<blockquote>\n<p>&hellip;It is not possible to say exactly when the final sum would be, but a conservative guess might be that between 10% and 20% of the variance in these criteria could be potentially attributed to genetic effects (cf. Simonton, 2007).</p>\n<p>For the sake of this discussion, then, suppose that .10 \u0545 hc2 \u0545 .20 holds for the training and performance criteria examined here. Does this outcome imply that scientific talent is an important substantive phenomenon? To answer this question requires that we obtain some kind of baseline for comparison&hellip;.[the range of estimates] can be qualitatively expressed as medium to large (Cohen, 1988). This range is about as good as can be expected of most effects in the behavioral sciences (Meyer et al., 2001; Rosenthal, 1990). To offer specific comparisons, the lower-end estimate is about the same magnitude as the relation between psychotherapy and subsequent well-being, whereas the upper-end estimate is about the same size as the correlation between height and weight among U.S. adults (Meyer et al., 2001).</p>\n</blockquote>\n</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9vKRHBCLvwqN26LLs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 44, "extendedScore": null, "score": 8.394063882591374e-07, "legacy": true, "legacyId": "12462", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CXZK4cixfnKJF5Ycy", "TnEjvi2bhTD3ocJie", "7YEkqxoWFCwAQBsMJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T20:44:47.318Z", "modifiedAt": null, "url": null, "title": "[META] My Negative Results", "slug": "meta-my-negative-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:38.941Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X5JXPSNXtPK929wH3/meta-my-negative-results", "pageUrlRelative": "/posts/X5JXPSNXtPK929wH3/meta-my-negative-results", "linkUrl": "https://www.lesswrong.com/posts/X5JXPSNXtPK929wH3/meta-my-negative-results", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20My%20Negative%20Results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20My%20Negative%20Results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX5JXPSNXtPK929wH3%2Fmeta-my-negative-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20My%20Negative%20Results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX5JXPSNXtPK929wH3%2Fmeta-my-negative-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX5JXPSNXtPK929wH3%2Fmeta-my-negative-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 348, "htmlBody": "<p>Yesterday, I made a post asking if anyone else had noticed LW being particularly slow. I offered to collect data on this, and was fairly sure (Probably about 80%) that it would show that LW loaded slower than other webpages. I took the post down after about 20 seconds (sorry if I confused you) since it was almost entirely insubstantial, and resolved to collect some actual data to report.</p>\n<p>So I did that. Data was taken using <a href=\"http://www.numion.com/stopwatch/\">this</a> website. I was on my school's wireless network at the time, running Firefox 8.0. I didn't think to disable my addons before doing this, but I was running Adblock Plus, FastestFox, Greasemonkey, IE Tab 2, Movable Firefox Button, Omnibar, and Web of Trust. Data points were generated at the following websites. Each website was measured five times.<br />http://lesswrong.com/r/discussion/new<br />http://lesswrong.com<br />http://lesswrong.com/user/RobertLumley<br />http://lesswrong.com/promoted/<br />http://predictionbook.com<br />http://predictionbook.com/predictions<br />http://predictionbook.com/users/rlumley<br />http://www.nature.com/news/index.html<br />http://volokh.com/<br />http://online.wsj.com/public/page/news-opinion-commentary.html<br />http://www.webdiplomacy.net/index.php</p>\n<p>After doing this, I downloaded an offline copy of each of these websites, and calculated load time per byte of website size. I plotted these results. To my surprise, LessWrong ended up being one of the fastest, although PredictionBook could use some work. I considered deleting the obvious outlier, but trends are clear even with it in there, and all things equal, I'd rather not delete data. Data points (in groups of 5) correspond to the webpages above, in order, ie. the first five points are from the discussion page of LW.</p>\n<p><img src=\"http://images.lesswrong.com/t3_9m5_0.png\" alt=\"\" /></p>\n<p>Surprisingly, LW is still one of the best even when only raw load time is compared. But the discussion page (remember, that's the first 5 points) takes somewhat longer than the other pages:<br /><br /><img src=\"http://images.lesswrong.com/t3_9m5_1.png\" alt=\"\" /><br /><br />The excel spreadsheet used to generate this can be viewed <a href=\"https://skydrive.live.com/?sc=documents&amp;mkt=en-us#!/view.aspx?cid=3199CD9857175D9F&amp;resid=3199CD9857175D9F!178\">here</a>.<br /><br />Since there is no real problem here, and it was all in my head, I considered not publishing this, but after <a href=\"/lw/7io/pressure_to_publish_increases_scientists/\">all</a> <a href=\"/lw/1ib/parapsychology_the_control_group_for_science/\">of</a> <a href=\"/lw/iw/positive_bias_look_into_the_dark/\">the</a> <a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective\">discussion</a> we have about positive bias, I wasn't about to turn around and do the same thing. So here they are: my negative results.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X5JXPSNXtPK929wH3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 54, "extendedScore": null, "score": 0.000125, "legacy": true, "legacyId": "12461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DCbaMBdvnvaDPxxT3", "enuGsZoFLR4KyEx3n", "rmAbiEKQDpDnZzcRf", "9qCN6tRBtksSyXfHu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T20:50:50.990Z", "modifiedAt": null, "url": null, "title": "Trust", "slug": "trust", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c2zzb222yWSnL4JkT/trust", "pageUrlRelative": "/posts/c2zzb222yWSnL4JkT/trust", "linkUrl": "https://www.lesswrong.com/posts/c2zzb222yWSnL4JkT/trust", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Trust&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrust%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2zzb222yWSnL4JkT%2Ftrust%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Trust%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2zzb222yWSnL4JkT%2Ftrust", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2zzb222yWSnL4JkT%2Ftrust", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 32, "htmlBody": "<p>One of the more important types of decisions is deciding who to trust and about what, but I don't think I've seen any discussion here about rational methods in that area.</p>\n<p>Any suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c2zzb222yWSnL4JkT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 16, "extendedScore": null, "score": 8.394257579597531e-07, "legacy": true, "legacyId": "12464", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T22:17:01.927Z", "modifiedAt": null, "url": null, "title": "Let's fix the RSS feed so it doesn't re-post articles every time their titles are edited (forgot the hyphen in \"re-post\")", "slug": "let-s-fix-the-rss-feed-so-it-doesn-t-re-post-articles-every", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:53.418Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Postal_Scale", "createdAt": "2011-12-06T20:18:15.580Z", "isAdmin": false, "displayName": "Postal_Scale"}, "userId": "5ArTfbY9dtK4CcnsE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pSfgieyfYZLuuXqX9/let-s-fix-the-rss-feed-so-it-doesn-t-re-post-articles-every", "pageUrlRelative": "/posts/pSfgieyfYZLuuXqX9/let-s-fix-the-rss-feed-so-it-doesn-t-re-post-articles-every", "linkUrl": "https://www.lesswrong.com/posts/pSfgieyfYZLuuXqX9/let-s-fix-the-rss-feed-so-it-doesn-t-re-post-articles-every", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let's%20fix%20the%20RSS%20feed%20so%20it%20doesn't%20re-post%20articles%20every%20time%20their%20titles%20are%20edited%20(forgot%20the%20hyphen%20in%20%22re-post%22)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet's%20fix%20the%20RSS%20feed%20so%20it%20doesn't%20re-post%20articles%20every%20time%20their%20titles%20are%20edited%20(forgot%20the%20hyphen%20in%20%22re-post%22)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpSfgieyfYZLuuXqX9%2Flet-s-fix-the-rss-feed-so-it-doesn-t-re-post-articles-every%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let's%20fix%20the%20RSS%20feed%20so%20it%20doesn't%20re-post%20articles%20every%20time%20their%20titles%20are%20edited%20(forgot%20the%20hyphen%20in%20%22re-post%22)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpSfgieyfYZLuuXqX9%2Flet-s-fix-the-rss-feed-so-it-doesn-t-re-post-articles-every", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpSfgieyfYZLuuXqX9%2Flet-s-fix-the-rss-feed-so-it-doesn-t-re-post-articles-every", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pSfgieyfYZLuuXqX9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 3, "extendedScore": null, "score": 8.394589739839123e-07, "legacy": true, "legacyId": "12465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T23:09:14.240Z", "modifiedAt": null, "url": null, "title": "Self-Indication Assumption - Still Doomed", "slug": "self-indication-assumption-still-doomed", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:27.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "drnickbone", "createdAt": "2012-01-20T17:19:55.216Z", "isAdmin": false, "displayName": "drnickbone"}, "userId": "GgwHTM3agaskLi9cx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MAZFiX2NFzLMTmiZm/self-indication-assumption-still-doomed", "pageUrlRelative": "/posts/MAZFiX2NFzLMTmiZm/self-indication-assumption-still-doomed", "linkUrl": "https://www.lesswrong.com/posts/MAZFiX2NFzLMTmiZm/self-indication-assumption-still-doomed", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-Indication%20Assumption%20-%20Still%20Doomed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-Indication%20Assumption%20-%20Still%20Doomed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMAZFiX2NFzLMTmiZm%2Fself-indication-assumption-still-doomed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-Indication%20Assumption%20-%20Still%20Doomed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMAZFiX2NFzLMTmiZm%2Fself-indication-assumption-still-doomed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMAZFiX2NFzLMTmiZm%2Fself-indication-assumption-still-doomed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 847, "htmlBody": "<p>I recently posted a discussion article on the Doomsday Argument (DA) and Strong Self-Sampling Assumption. See&nbsp;<a href=\"/lw/9im/doomsday_argument_with_strong_selfsampling/\">http://lesswrong.com/lw/9im/doomsday_argument_with_strong_selfsampling/</a></p>\n<p>This new post is related to another part of the literature concerning the Doomsday Argument - the Self Indication Assumption or SIA.&nbsp;For those not familiar, the SIA says (roughly) that I would be more likely to exist if the world contains a large number of observers. So, when taking into account the evidence that I exist, this should shift my probability assessments towards models of the world with more observers.</p>\n<p>Further, on first glance, it looks like the SIA shift can be arranged to exactly counteract the effect of the DA shift.&nbsp;Consider, for instance, these two hypotheses:</p>\n<p>&nbsp;</p>\n<p>H1. Across all of space time, there is just one civilization of observers (humans) and a total of 200 billion observers.</p>\n<p>H2. Across all of space time, there is just one civilization of observers (humans) and a total of 200 billion trillion observers.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Suppose I had assigned a prior probability ratio p_r = P(H1)/P(H2) before considering either SIA or the DA. Then when I apply&nbsp;the SIA, this ratio will shrink by a factor of a trillion i.e. I've become much more confident in hypothesis H2. But then when I observe I'm roughly the 100 billionth human being, and apply the DA, the ratio expands back by exactly the same factor of a trillion, since this observation is much more likely under H1 than under H2. So my probability ratio returns to p_r. I should not make any predictions about \"Doom Soon\" unless I already believed them at the outset, for other reasons.</p>\n<p>Now I won't discuss here whether the SIA is justified or not; my main concern is whether it actually helps to counteract the Doomsday Argument.&nbsp;And it seems quite clear to me that it doesn't. If we choose to apply the SIA at all, then it will instead&nbsp;overwhelming favour a hypothesis like H3 below over either H1 or H2:</p>\n<p>&nbsp;</p>\n<p>H3. Across all of space time, there are infinitely many civilizations of observers, and infinitely many observers in total.</p>\n<p>&nbsp;</p>\n<p>In short, by applying the SIA we wipe out from consideration all the finite-world models, and then only have to look at the infinite ones (e.g. models with an infinite universe, or with infinitely many universes).&nbsp;But now, consider that H3 has two sub-models:</p>\n<p>&nbsp;</p>\n<p>H3.1.&nbsp;Across all of space time, there are infinitely many civilizations of observers, but the mean number of observers per civilization (taking a suitable limit construction to define the mean) is 200 billion observers.</p>\n<p>H3.2.&nbsp;Across all of space time, there are infinitely many civilizations of observers, but the mean number of observers per civilization (taking the same limit construction) is 200 billion trillion observers.</p>\n<p>&nbsp;</p>\n<p>Notice that while SIA is indifferent between these sub-cases (since both contain the same number of observers), it seems clear that DA still greatly favours H3.1 over H3.2. Whatever our prior ratio r' = P(H3.1)/P(H3.2), DA raises that ratio by a trillion, and so the combination of SIA and DA also raises that ratio by a trillion. SIA doesn't stop the shift.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Worse still, the conclusion of the DA has now become far *stronger*, since it seems that the only way for H3.1 to hold is if there is some form of \"Universal Doom\" scenario.&nbsp;Loosely, pretty much every one of those infinitely-many civilizations will have to terminate itself before managing to expand away from its home planet.&nbsp;</p>\n<p>Looked at more carefully, there is some probability of a civilization expanding p_e which is consistent with H3.1 but it has to be unimaginably tiny. If the population ratio of an expanded civilization to a a non-expanded one is R_e, then H3.1 requires that p_e &lt; 1/R_e. But values of R_e &gt; trillion look right; indeed values of R_e &gt; 10^24 (a trillion trillion) look plausible, which then forces p_e &lt; 10^-12 and plausibly &lt; 10^-24.&nbsp;The believer in the SIA has to be a really strong Doomer to get this to work!</p>\n<p>By contrast the standard DA doesn't have to be quite so doomerish. It can work with a rather higher probability p_e of expansion and avoiding doom, as long as the world is finite and the total number of actual civilizations is less than 1 / p_e. &nbsp;As an example, consider:</p>\n<p>H4. There are 1000 civilizations of observers in the world, and each has a probability of 1 in 10000 of expanding beyond its home planet. Conditional on a civilization not expanding, its expected number of observers is 200 billion.&nbsp;</p>\n<p>This hypothesis seems to be pretty consistent with our current observations (observing that we are the 100 billionth human being). It predicts that - with 90% probability - all observers will find themselves on the home planet of their civilization. Since this H4 prediction applies to all observers, we don't actually have to worry about whether we are a \"random\" observer or not; the prediction still holds. The hypothesis also predicts that, while the prospect of expansion will appear just about attainable for a civilization, it won't in fact happen.</p>\n<p>P.S. With a bit of re-scaling of the numbers, this post also works with observations or observer-moments, not just observers. See my previous post for more on this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MAZFiX2NFzLMTmiZm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 8.394790958379332e-07, "legacy": true, "legacyId": "12466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["psz93GRm6jj9yj5dR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-28T23:44:51.045Z", "modifiedAt": null, "url": null, "title": "Toward \"timeless\" continuous-time causal models", "slug": "toward-timeless-continuous-time-causal-models", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:56.655Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ytNR2cG5LdnQTWmEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3dQxdvx5BkRY3vW72/toward-timeless-continuous-time-causal-models", "pageUrlRelative": "/posts/3dQxdvx5BkRY3vW72/toward-timeless-continuous-time-causal-models", "linkUrl": "https://www.lesswrong.com/posts/3dQxdvx5BkRY3vW72/toward-timeless-continuous-time-causal-models", "postedAtFormatted": "Saturday, January 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Toward%20%22timeless%22%20continuous-time%20causal%20models&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AToward%20%22timeless%22%20continuous-time%20causal%20models%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3dQxdvx5BkRY3vW72%2Ftoward-timeless-continuous-time-causal-models%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Toward%20%22timeless%22%20continuous-time%20causal%20models%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3dQxdvx5BkRY3vW72%2Ftoward-timeless-continuous-time-causal-models", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3dQxdvx5BkRY3vW72%2Ftoward-timeless-continuous-time-causal-models", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1264, "htmlBody": "<p>I'm a bit at a loss as to where to put this. I know the inferential gap is too great for it to go anywhere but here, and I know that the number of people on LW interested in this subject could be counted on one hand. The prerequisites would almost certainly be <a href=\"/lw/qr/timeless_causality/\">Timeless Causality</a>&nbsp;and more mathematics than anyone is really interested in learning.</p>\n<p>So, I apologize in advance if you read this and discover at the end it was a waste of your time. But at the same time, I need people who know about these things to talk about them with me, to ensure that I haven't gone crazy... yet. And most importantly, I need to know the people who have done this before, so that I don't have to do it. Google can't find them.</p>\n<h2>Introduction</h2>\n<p>There are currently some efforts to generalize the causal models of Pearl to continuous-time situations. Most of these attempts involve replacing some causal discrete variables X<sub>i</sub>&nbsp;with time-dependent random variables X<sub>i</sub>(t). Possibly due to memetic infection from Yudkowsky, I don't think this is necessarily the correct approach. The philosophical power of Pearl's theory comes from the fact that it is timeless, that <em>ba'o&nbsp;vimcu ty bu</em>.</p>\n<p>In order to motivate my working definitions for where such a timeless continuous-time theory will go, I need to go back to classical causality and decide what a timeless formulation actually means, formally. Spoiler: it means replacing time-dependent evolution with a global flow on the phase space of the system. This is more or less in line with what is said in&nbsp;<a href=\"/lw/qp/timeless_physics/\">Timeless Physics</a>&nbsp;with regard to the glimpse of \"quantum mist\" illustrated there.</p>\n<h2>The role of phase space</h2>\n<p>What is \"timelessness\"? The first thing I thought of after reading the timeless subsequence was, \"What does a timeless formulation of the wave equation look like?\" First of all, this was the right thought, because the wave equation is what I'll call (after the fact) \"classically causal\" in a sense to be described soon. I wouldn't have seen the timelessness in a different mathematical model, because not all mathematical models of reality preserve the underlying phenomena's causal structure. On the other hand, this was the wrong thought, because the wave equation is not the simplest continuous-time system that would have led me to this formalization of timelessness. Unfortunately the one that is easier for me to see (Lagrangian mechanics) is harder for me to explain, so you're stuck with a suboptimal explanation.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Wave_equation\">wave equation</a>&nbsp;models all sorts of wave-like phenomena: light, acoustic waves, earthquakes, and so on. If we take the speed of sound to be one (as physicists are wont to do), the <a href=\"http://en.wikipedia.org/wiki/Dispersion_relation\">dispersion relation</a>&nbsp;is&nbsp;&omega;<sup>2</sup>&nbsp;= k<sup>2</sup>. Such a dispersion relation satisfies the <a href=\"http://en.wikipedia.org/wiki/Kramers%E2%80%93Kronig_relations\">Kramers-Kronig</a> relation. As it turns out, equations whose dispersion relation satisfies this condition satisfy what I'm calling \"classical causality\", but what is more commonly known as finite speed of propagation&nbsp;&mdash; or, more physically speaking, the fact that signals stay within their light cone.</p>\n<p>The most common problem associated with the wave equation is the Cauchy problem. At time zero, we specify the state of the system: its initial position and velocity at every point. Then the solution of the wave equation describes how that initial state evolves with time. From a more abstract point of view, this evolution is a curve in the space of all possible initial states. This space is commonly referred to in the specific case of the wave equation as \"energy space\", which further illustrates why this example is a bit bad for pedagogical purposes. From now on, we're only going to talk about phase space.</p>\n<p>Here is where we can remove time from the equation. Instead of thinking of the wave equation as associating to every state in phase space a time-dependent curve issuing forth from it, we're going to think of the wave equation as specifying a global flow on the whole of phase space, all at once.&nbsp;In summary, I am led to believe that timeless formulations amount to <strong>abstracting away the time-dependence of the system's evolution as a flow on the phase space of the system</strong>. And to think, this insight only took three years to internalize, provided I've gotten it correct.</p>\n<h2>Causal flow</h2>\n<p>The situation for a causal model is harder. In part, because stochastic things have shoddy excuses for derivatives. For the moment, we're going to take the easiest possible continuous-time system: our causal N variables of interest, X<sub>i</sub>, take only real values. The space of all the possible states of the system is N-dimensional Euclidean space, which is easy enough to work with. I'm going to implicitly assume that causal variables evolve continuously; that is, the sidewalk doesn't go from being completely dry to completely wet instantaneously. Things like light switches and push buttons can still be modeled practically by bump functions and the like, so I don't see this as a real limitation.</p>\n<p>The somewhat harder bullet to swallow is the assumption that the random variables are Markovian; that is, they are \"memoryless\" in the sense that only the present state determines the future. Pearl spends some time in <em>Causality</em>&nbsp;defending this assumption from criticism that it doesn't apply to quantum systems&nbsp;&mdash; I believe this defense is reasonable. I believe that causal models are necessarily refinements of our beliefs about what is still for the most part a classical world, and so the Markov assumption is not necessarily unnatural.</p>\n<p>The phase space of N-dimensional Euclidean space is known as the tangent bundle, which amounts to having an additional copy of N-space at every point. Morally speaking, the tangent bundle represents all the directions and speeds in which the system can evolve from any given state.</p>\n<p>We need some data about how the system is supposed to evolve: what I will call the <em>causal flow</em>. As best as I can currently conjecture, this data should take the form of a \"bundle\" of probability measures P, one for each point in N-space, such that each probability measure P(x) is defined over the tangent copy of N-space attached to that point.</p>\n<p>By analogy with the previous section, the time-evolution of the system is given by Lipschitz-continuous curves in N-space. (Lipschitz-continuous, because if we assume they are differentiable curves, the Markov assumption goes out the window.) In contrast with the discrete theory of causality, and as mentioned above, we don't allow causal variables to \"jump\" spontaneously, and there is a limit to how sharply they can \"turn\".</p>\n<p>A useful thing to have around would be the probability that the system will evolve from one state to another via a specific choice of one of these curves. Lipschitz-continuous curves are rectifiable, and so one can recapitulate a sort of Riemann sum&nbsp;&mdash; if you're interested, I have it formally written down in a .pdf, but the current format is unfriendly to maths. So for now, you'll just have to take my word for it when I say I can define the probability of the flow following a specific path. From there, it's just a path integral to defining the probability of getting from one state to another.</p>\n<h2>Where to go from here?</h2>\n<p>Given this causal flow, d-separation should arise as a geometrical condition&nbsp;&mdash; but perhaps only a local one, for the causal structure of the system can also evolve with time. To intervene in this system is to project it onto a certain hyperplane, presumably, in some yet-to-be-determined way. And finally, there ought to be some way to define counterfactuals, but my limited mathematical foresight has already run too thin.</p>\n<p>BONUS: If you've made it this far and can't think of anything else to say, I'm willing to Crocker-entertain probabilities that I'm insane and/or a crackpot.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3dQxdvx5BkRY3vW72", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "12467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>I'm a bit at a loss as to where to put this. I know the inferential gap is too great for it to go anywhere but here, and I know that the number of people on LW interested in this subject could be counted on one hand. The prerequisites would almost certainly be <a href=\"/lw/qr/timeless_causality/\">Timeless Causality</a>&nbsp;and more mathematics than anyone is really interested in learning.</p>\n<p>So, I apologize in advance if you read this and discover at the end it was a waste of your time. But at the same time, I need people who know about these things to talk about them with me, to ensure that I haven't gone crazy... yet. And most importantly, I need to know the people who have done this before, so that I don't have to do it. Google can't find them.</p>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>There are currently some efforts to generalize the causal models of Pearl to continuous-time situations. Most of these attempts involve replacing some causal discrete variables X<sub>i</sub>&nbsp;with time-dependent random variables X<sub>i</sub>(t). Possibly due to memetic infection from Yudkowsky, I don't think this is necessarily the correct approach. The philosophical power of Pearl's theory comes from the fact that it is timeless, that <em>ba'o&nbsp;vimcu ty bu</em>.</p>\n<p>In order to motivate my working definitions for where such a timeless continuous-time theory will go, I need to go back to classical causality and decide what a timeless formulation actually means, formally. Spoiler: it means replacing time-dependent evolution with a global flow on the phase space of the system. This is more or less in line with what is said in&nbsp;<a href=\"/lw/qp/timeless_physics/\">Timeless Physics</a>&nbsp;with regard to the glimpse of \"quantum mist\" illustrated there.</p>\n<h2 id=\"The_role_of_phase_space\">The role of phase space</h2>\n<p>What is \"timelessness\"? The first thing I thought of after reading the timeless subsequence was, \"What does a timeless formulation of the wave equation look like?\" First of all, this was the right thought, because the wave equation is what I'll call (after the fact) \"classically causal\" in a sense to be described soon. I wouldn't have seen the timelessness in a different mathematical model, because not all mathematical models of reality preserve the underlying phenomena's causal structure. On the other hand, this was the wrong thought, because the wave equation is not the simplest continuous-time system that would have led me to this formalization of timelessness. Unfortunately the one that is easier for me to see (Lagrangian mechanics) is harder for me to explain, so you're stuck with a suboptimal explanation.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Wave_equation\">wave equation</a>&nbsp;models all sorts of wave-like phenomena: light, acoustic waves, earthquakes, and so on. If we take the speed of sound to be one (as physicists are wont to do), the <a href=\"http://en.wikipedia.org/wiki/Dispersion_relation\">dispersion relation</a>&nbsp;is&nbsp;\u03c9<sup>2</sup>&nbsp;= k<sup>2</sup>. Such a dispersion relation satisfies the <a href=\"http://en.wikipedia.org/wiki/Kramers%E2%80%93Kronig_relations\">Kramers-Kronig</a> relation. As it turns out, equations whose dispersion relation satisfies this condition satisfy what I'm calling \"classical causality\", but what is more commonly known as finite speed of propagation&nbsp;\u2014 or, more physically speaking, the fact that signals stay within their light cone.</p>\n<p>The most common problem associated with the wave equation is the Cauchy problem. At time zero, we specify the state of the system: its initial position and velocity at every point. Then the solution of the wave equation describes how that initial state evolves with time. From a more abstract point of view, this evolution is a curve in the space of all possible initial states. This space is commonly referred to in the specific case of the wave equation as \"energy space\", which further illustrates why this example is a bit bad for pedagogical purposes. From now on, we're only going to talk about phase space.</p>\n<p>Here is where we can remove time from the equation. Instead of thinking of the wave equation as associating to every state in phase space a time-dependent curve issuing forth from it, we're going to think of the wave equation as specifying a global flow on the whole of phase space, all at once.&nbsp;In summary, I am led to believe that timeless formulations amount to <strong>abstracting away the time-dependence of the system's evolution as a flow on the phase space of the system</strong>. And to think, this insight only took three years to internalize, provided I've gotten it correct.</p>\n<h2 id=\"Causal_flow\">Causal flow</h2>\n<p>The situation for a causal model is harder. In part, because stochastic things have shoddy excuses for derivatives. For the moment, we're going to take the easiest possible continuous-time system: our causal N variables of interest, X<sub>i</sub>, take only real values. The space of all the possible states of the system is N-dimensional Euclidean space, which is easy enough to work with. I'm going to implicitly assume that causal variables evolve continuously; that is, the sidewalk doesn't go from being completely dry to completely wet instantaneously. Things like light switches and push buttons can still be modeled practically by bump functions and the like, so I don't see this as a real limitation.</p>\n<p>The somewhat harder bullet to swallow is the assumption that the random variables are Markovian; that is, they are \"memoryless\" in the sense that only the present state determines the future. Pearl spends some time in <em>Causality</em>&nbsp;defending this assumption from criticism that it doesn't apply to quantum systems&nbsp;\u2014 I believe this defense is reasonable. I believe that causal models are necessarily refinements of our beliefs about what is still for the most part a classical world, and so the Markov assumption is not necessarily unnatural.</p>\n<p>The phase space of N-dimensional Euclidean space is known as the tangent bundle, which amounts to having an additional copy of N-space at every point. Morally speaking, the tangent bundle represents all the directions and speeds in which the system can evolve from any given state.</p>\n<p>We need some data about how the system is supposed to evolve: what I will call the <em>causal flow</em>. As best as I can currently conjecture, this data should take the form of a \"bundle\" of probability measures P, one for each point in N-space, such that each probability measure P(x) is defined over the tangent copy of N-space attached to that point.</p>\n<p>By analogy with the previous section, the time-evolution of the system is given by Lipschitz-continuous curves in N-space. (Lipschitz-continuous, because if we assume they are differentiable curves, the Markov assumption goes out the window.) In contrast with the discrete theory of causality, and as mentioned above, we don't allow causal variables to \"jump\" spontaneously, and there is a limit to how sharply they can \"turn\".</p>\n<p>A useful thing to have around would be the probability that the system will evolve from one state to another via a specific choice of one of these curves. Lipschitz-continuous curves are rectifiable, and so one can recapitulate a sort of Riemann sum&nbsp;\u2014 if you're interested, I have it formally written down in a .pdf, but the current format is unfriendly to maths. So for now, you'll just have to take my word for it when I say I can define the probability of the flow following a specific path. From there, it's just a path integral to defining the probability of getting from one state to another.</p>\n<h2 id=\"Where_to_go_from_here_\">Where to go from here?</h2>\n<p>Given this causal flow, d-separation should arise as a geometrical condition&nbsp;\u2014 but perhaps only a local one, for the causal structure of the system can also evolve with time. To intervene in this system is to project it onto a certain hyperplane, presumably, in some yet-to-be-determined way. And finally, there ought to be some way to define counterfactuals, but my limited mathematical foresight has already run too thin.</p>\n<p>BONUS: If you've made it this far and can't think of anything else to say, I'm willing to Crocker-entertain probabilities that I'm insane and/or a crackpot.</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "The role of phase space", "anchor": "The_role_of_phase_space", "level": 1}, {"title": "Causal flow", "anchor": "Causal_flow", "level": 1}, {"title": "Where to go from here?", "anchor": "Where_to_go_from_here_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KipiHsTA3pw4joQkG", "rrW7yf42vQYDf8AcH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T03:52:16.458Z", "modifiedAt": null, "url": null, "title": "Efficient Charity: Cheap Utilons via bone marrow registration", "slug": "efficient-charity-cheap-utilons-via-bone-marrow-registration", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:57.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atorm", "createdAt": "2011-03-30T16:41:30.635Z", "isAdmin": false, "displayName": "atorm"}, "userId": "PvazkPKLZs5LNujcL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QTMob5JPhhdgTf6RM/efficient-charity-cheap-utilons-via-bone-marrow-registration", "pageUrlRelative": "/posts/QTMob5JPhhdgTf6RM/efficient-charity-cheap-utilons-via-bone-marrow-registration", "linkUrl": "https://www.lesswrong.com/posts/QTMob5JPhhdgTf6RM/efficient-charity-cheap-utilons-via-bone-marrow-registration", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Efficient%20Charity%3A%20Cheap%20Utilons%20via%20bone%20marrow%20registration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEfficient%20Charity%3A%20Cheap%20Utilons%20via%20bone%20marrow%20registration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQTMob5JPhhdgTf6RM%2Fefficient-charity-cheap-utilons-via-bone-marrow-registration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Efficient%20Charity%3A%20Cheap%20Utilons%20via%20bone%20marrow%20registration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQTMob5JPhhdgTf6RM%2Fefficient-charity-cheap-utilons-via-bone-marrow-registration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQTMob5JPhhdgTf6RM%2Fefficient-charity-cheap-utilons-via-bone-marrow-registration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 374, "htmlBody": "<p>This topic is not really related to the things normally discussed here, but I think it's really important, and it might interest Less Wrongers, especially since many of us are interested in ethics and utility calculations that are essentially cost-benefit analyses. Bone marrow donation in the United States is managed by the National Marrow Donor Program. Because typing donors for matching purposes can be costly, they often require people signing up to donate to pay a registration fee, which probably prevents a lot of people from signing up. These costs are being covered until the end of the month by a corporate sponsor, which means that right now, all you need to do if you live in the US is go to <a href=\"http://marrow.org/Join/Join_Now/Join_Now.aspx\">http://marrow.org/Join/Join_Now/Join_Now.aspx</a> and fill out a simple questionnaire. You will be sent a kit to collect a cheek swab, and then you will be entered into the donor database. Doing this does not require you to donate if a match comes up.</p>\n<p>The reason I think this might interest Less Wrongers is that this is a really cheap way to improve the world. According to their website, about 1 in 500 potential donors are actually asked to donate, so registering doesn't actually make it all that likely that you will be asked to do anything more. If you ARE a match for someone who needs a donation, the cost to you is at most the temporary pain of marrow extraction (many donors are asked only for blood cells), whereas the other person&rsquo;s chance to live is much improved. This looks like a huge net positive.</p>\n<p>Unfortunately I only found out about this a few days ago, and it only occurred to me today that this might be a forum of people who would respond to the argument \"you can make the world better at little cost to yourself.\" However, I ask that you go to the website and spend a few minutes signing up. This is like buying a 1 in 500 lottery ticket that SAVES SOMEONE&rsquo;S LIFE. If the Singularity hits and an FAI can generate perfectly matched marrow for anyone who needs it from totipotency-induced cells, that will be wonderful, but this is a chance to make sure one more person gets there.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QTMob5JPhhdgTf6RM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 24, "extendedScore": null, "score": 8.39588203699433e-07, "legacy": true, "legacyId": "12480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T03:57:13.506Z", "modifiedAt": null, "url": null, "title": "What are your benchmarks of rationality?", "slug": "what-are-your-benchmarks-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.411Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4wQGk4uzAXsomh4sb/what-are-your-benchmarks-of-rationality", "pageUrlRelative": "/posts/4wQGk4uzAXsomh4sb/what-are-your-benchmarks-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/4wQGk4uzAXsomh4sb/what-are-your-benchmarks-of-rationality", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20your%20benchmarks%20of%20rationality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20your%20benchmarks%20of%20rationality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4wQGk4uzAXsomh4sb%2Fwhat-are-your-benchmarks-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20your%20benchmarks%20of%20rationality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4wQGk4uzAXsomh4sb%2Fwhat-are-your-benchmarks-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4wQGk4uzAXsomh4sb%2Fwhat-are-your-benchmarks-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>What quick-and-easy rules of thumb to gauge how rational someone else is do you tend to use? How accurate do you think those rules are, and can you think of any way they might be improved?</p>\n<p>&nbsp;</p>\n<p>For some examples of what I mean, one of the benchmarks I use is the basic skeptics' list: astrology, chiropractic, little green men abducting cattle and performing anal probes, Nessie. Another is the denialist checklist: holocaust denial, moon landing denial, global warming denial. Another is supernaturalism in general: creationism, intercessory prayer, magick, psychics, curses, ghosts, and such. If I find out that anyone I know believes in any of that, then my estimation of how well they can consider things rationally goes down. Theism... well, I've gotten used to pretty much everyone around me being theistic, so that's kind of the baseline I assume; when I learn someone is an atheist, my estimation of their rationality tends to go /up/.</p>\n<p>Do you have any items which make you think someone is even further along the path of rationality than simply not believing logical fallacies?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4wQGk4uzAXsomh4sb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 2, "extendedScore": null, "score": 8.395901123948439e-07, "legacy": true, "legacyId": "12481", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T04:43:17.705Z", "modifiedAt": null, "url": null, "title": "Rational vs. real utilities in the cousin_it-Nesov model of UDT", "slug": "rational-vs-real-utilities-in-the-cousin_it-nesov-model-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:38.883Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Karl", "createdAt": "2010-10-27T21:06:59.318Z", "isAdmin": false, "displayName": "Karl"}, "userId": "EN2BxuvFHRgmLbbp3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bKDZepmTx8X6SRLE5/rational-vs-real-utilities-in-the-cousin_it-nesov-model-of", "pageUrlRelative": "/posts/bKDZepmTx8X6SRLE5/rational-vs-real-utilities-in-the-cousin_it-nesov-model-of", "linkUrl": "https://www.lesswrong.com/posts/bKDZepmTx8X6SRLE5/rational-vs-real-utilities-in-the-cousin_it-nesov-model-of", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20vs.%20real%20utilities%20in%20the%20cousin_it-Nesov%20model%20of%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20vs.%20real%20utilities%20in%20the%20cousin_it-Nesov%20model%20of%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKDZepmTx8X6SRLE5%2Frational-vs-real-utilities-in-the-cousin_it-nesov-model-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20vs.%20real%20utilities%20in%20the%20cousin_it-Nesov%20model%20of%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKDZepmTx8X6SRLE5%2Frational-vs-real-utilities-in-the-cousin_it-nesov-model-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKDZepmTx8X6SRLE5%2Frational-vs-real-utilities-in-the-cousin_it-nesov-model-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 352, "htmlBody": "<p>In <a href=\"/r/discussion/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">this article</a>&nbsp;cousin_it describes an algorithm,&nbsp;using a Turing Oracle, which gives a working model of UDT. This algorithm seems to work well in the case of a utility function which takes natural numbers or rationals as values but, as I will explain, the algorithm doesn't actually work for utilities given by real numbers; I will also propose an algorithm which doesn't use an oracle and which seems to wield the correct answer given sufficient computing time.</p>\n<p>One of the hidden assumptions in the initial post is that comparison of utilities is decidable (at least decidable with a Turing oracle): in step 3 of the algorithm you are supposed to return the action that corresponds to the highest utility found on step 2 of the algorithm, but if comparison of utilities isn't actually decidable you won't be able (in the general case) to do that and as it happens that equality of real numbers isn't decidable (even using an oracle). Also there are no canonical forms for real numbers like there are for natural or rational numbers so you can have a situation where the order relationship between different real numbers depends&nbsp;on the value of A().</p>\n<p>A simple modification of the original algorithm can deal with this problem:</p>\n<ol>\n<li>Enumerate proofs of statements of the form A()=a &rArr; U()&ge;u, where u is a rational number in canonical form, for t time steps.</li>\n<li>Return the action that corresponds to the highest utility found on step 2.</li>\n</ol>\n<p>Why does this works? Because if the formal system used is sound, when the formal system proves that A()=a&nbsp;&rArr;&nbsp;U()&ge;u where a is the action which ends up actually being taken then U() must actually end up being at least u.</p>\n<p>So for example, in the case of Newcomb's problem if t is sufficiently high it will eventually be proven that A()=1&nbsp;&rArr; U()&ge;1 000 000 and so, if the formal system used is sound, the utility which ends up being received by the agent will have to be at least 1 000 000 which is of course only possible if the agent one boxes so that's what it will end up doing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bKDZepmTx8X6SRLE5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 13, "extendedScore": null, "score": 8.39607874355877e-07, "legacy": true, "legacyId": "12482", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Bj244uWzDBXvE2N2S"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T05:10:43.655Z", "modifiedAt": null, "url": null, "title": "Faustian bargains and discounting", "slug": "faustian-bargains-and-discounting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.126Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SSB7BifpeXGW2xTJb/faustian-bargains-and-discounting", "pageUrlRelative": "/posts/SSB7BifpeXGW2xTJb/faustian-bargains-and-discounting", "linkUrl": "https://www.lesswrong.com/posts/SSB7BifpeXGW2xTJb/faustian-bargains-and-discounting", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Faustian%20bargains%20and%20discounting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFaustian%20bargains%20and%20discounting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSB7BifpeXGW2xTJb%2Ffaustian-bargains-and-discounting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Faustian%20bargains%20and%20discounting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSB7BifpeXGW2xTJb%2Ffaustian-bargains-and-discounting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSB7BifpeXGW2xTJb%2Ffaustian-bargains-and-discounting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 257, "htmlBody": "<p>I was reading TV Tropes on Hell, and it occurred to me: If your discounting was sufficiently hyperbolic, or indeed plain exponential with a low enough time preference, it would in some sense be rational to take a literal Faustian bargain. The integral to infinite time of some constant amount of torture per unit time, discounted exponentially or hyperbolically, is finite; enough worldly power and pleasure would outweight it.&nbsp;</p>\n<p>But this clashes rather strongly with my intuition. Notice that the argument doesn't depend on hyperbolic discounting; no preference pumping is involved. It works just fine with exponentials and a high decay constant. Or, if the worldly pleasures were strong enough, a low decay constant, that is, a high time preference, such as (I assume) most LWers have. For example, would you take eternal torture for a guarantee of living until the heat-death of the universe, 10^130 years from now, with all the refinements of Fun Theory along the way? Intuition says no, infinity being infinity, but then again intuition is notoriously bad at dealing with very large and very small numbers. If I calculate the thing in time-discounted utilons, it seems to me that my decay constant has to be very tiny indeed for me to care about what happens at the end of *10^130* years.&nbsp;</p>\n<p>So should I discard my intuition, and take such a bargain if Mephistopheles should suddenly turn up? (Noting that in 10^130 years, I might learn a thing or two about getting out of such difficulties...) Or alternatively, should I stop discounting future utilons?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SSB7BifpeXGW2xTJb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 8.396184510756822e-07, "legacy": true, "legacyId": "12483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T05:43:30.930Z", "modifiedAt": null, "url": null, "title": "The utility of information should almost never be negative", "slug": "the-utility-of-information-should-almost-never-be-negative", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:04.936Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sDAXaKyYFS4eqhXB5/the-utility-of-information-should-almost-never-be-negative", "pageUrlRelative": "/posts/sDAXaKyYFS4eqhXB5/the-utility-of-information-should-almost-never-be-negative", "linkUrl": "https://www.lesswrong.com/posts/sDAXaKyYFS4eqhXB5/the-utility-of-information-should-almost-never-be-negative", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20utility%20of%20information%20should%20almost%20never%20be%20negative&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20utility%20of%20information%20should%20almost%20never%20be%20negative%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDAXaKyYFS4eqhXB5%2Fthe-utility-of-information-should-almost-never-be-negative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20utility%20of%20information%20should%20almost%20never%20be%20negative%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDAXaKyYFS4eqhXB5%2Fthe-utility-of-information-should-almost-never-be-negative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDAXaKyYFS4eqhXB5%2Fthe-utility-of-information-should-almost-never-be-negative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 651, "htmlBody": "<p>As humans, finding out facts that we would rather not be true is unpleasant. For example, I would dislike finding out that my girlfriend were cheating on me, or finding out that my parent had died, or that my bank account had been hacked and I had lost all my savings.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">However, this is a consequence of the dodgily designed human brain. We don't operate with a utility function. Instead, we have separate neural circuitry for wanting and liking things, and behave according to those. If my girlfriend is cheating on me, I may want to know, but I wouldn't like knowing. In some cases, we'd rather not learn things: if I'm dying in hospital with only a few hours to live, I might rather be ignorant of another friend's death for the short remainder of my life.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">However, a rational being, say an AI, would never rather not learn something, except for contrived cases like Omega offering you $100 if you can avoid learning the square of 156 for the next minute.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">As far as I understand, an AI with a set of options decides by using approximately the following algorithm. This algorithm uses causal decision theory for simplicity.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">\"For each option, guess what will happen if you do it, and calculate the average utility. Choose the option with the highest utility.\"</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">So say <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">Clippy</a> is using that algorithm with his utility function of utility = number of paperclips in world.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Now imagine Clippy is on a planet making paperclips. He is considering listening to the Galactic Paperclip News radio broadcast. If he does so, there is a chance he might hear about a disaster leading to the destruction of thousands of paperclips. Would he decide in the following manner?</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">\"If I listen to the radio show, there's maybe a 10% chance I will learn that 1000 paperclips were destroyed. My utility in from that decision would be on average reduced by 100. If I don't listen, there is no chance that I will learn about the destruction of paperclips. That is no utility reduction for me. Therefore, I won't listen to the broadcast. In fact, I'd pay up to 100 paperclips not to hear it.\"</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Try and figure out the flaw in that reasoning. It took me a while to spot it, but perhaps I'm just slow.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">* thinking space *</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">For Clippy to believe \"If I listen to the radio show, there's maybe a 10% chance I will learn that 1000 paperclips were destroyed,\" he must also believe that there is already a 10% chance that 1000 paperclips have been destroyed. So his utility in either case is already reduced by 100. If he listens to the radio show, there's a 90% chance his utility will increase by 100, and a 10% chance it will decrease by 900, relative to ignorance. And so, he would be indifferent to gaining that knowledge.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">As humans, we don't work that way. We don't constantly feel the pressure of knowledge like &ldquo;People might have died since I last watched the news,&rdquo; just because humans don't deal with probability in a rational manner. And also, as humans who feel things, learning about bad things is unpleasant in itself. If I were dying in my bed, I probably wouldn't even think to increase my probability that a friend had died just because no-one would have told me if they had. An AI probably would.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">Of course, in real life, information has value. Maybe Clippy needs to know about these paperclip-destroying events in order to avoid them in his own paperclips, or he needs to be updated on current events to participate in effective socialising with other paperclip enthusiasts. So he would probably gain utility on average from choosing to listen to the radio broadcast.</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm;\">In conclusion. An AI may prefer the world in one state than another, but almost always prefers more knowledge about the actual state of the world, even if what it learns isn't good.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sDAXaKyYFS4eqhXB5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 8.396310929114592e-07, "legacy": true, "legacyId": "12484", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T09:32:30.423Z", "modifiedAt": null, "url": null, "title": "Personal research update", "slug": "personal-research-update", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:56.645Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jroCHyDC6XHomeuW7/personal-research-update", "pageUrlRelative": "/posts/jroCHyDC6XHomeuW7/personal-research-update", "linkUrl": "https://www.lesswrong.com/posts/jroCHyDC6XHomeuW7/personal-research-update", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Personal%20research%20update&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APersonal%20research%20update%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjroCHyDC6XHomeuW7%2Fpersonal-research-update%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Personal%20research%20update%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjroCHyDC6XHomeuW7%2Fpersonal-research-update", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjroCHyDC6XHomeuW7%2Fpersonal-research-update", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4050, "htmlBody": "<p><em><strong>Synopsis:</strong> The brain is a quantum computer and the self is a tensor factor in it - or at least, the truth lies more in that direction than in the classical direction - and we won't get Friendly AI right unless we get the ontology of consciousness right. </em></p>\n<p><em>Followed by: <a href=\"/lw/9o8/does_functionalism_imply_dualism/\">Does functionalism imply dualism?</a> <br /></em></p>\n<p>Sixteen months ago, I <a href=\"/lw/2qo/lets_make_a_deal/\">made a post</a> seeking funding for personal research. There was no separate Discussion forum then, and the post was comprehensively downvoted. I did manage to keep going at it, full-time, for the next sixteen months. Perhaps I'll get to continue; it's for the sake of that possibility that I'll risk another breach of etiquette. You never know who's reading these words and what resources they have. Also, there has been progress.</p>\n<p>I think the best place to start is with what orthonormal said in response to the original post: \"I don't think anyone should be funding a Penrose-esque qualia mysterian to study string theory.\" If I now took my full agenda to someone out in the real world, they might say: \"I don't think it's worth funding a study of 'the ontological problem of consciousness in the context of Friendly AI'.\" That's my dilemma. The pure scientists who might be interested in basic conceptual progress are not engaged with the race towards technological singularity, and the apocalyptic AI activists gathered in this place are trying to fit consciousness into an ontology that doesn't have room for it. In the end, if I have to choose between working on conventional topics in Friendly AI, and on the ontology of quantum mind theories, then I have to choose the latter, because we need to get the ontology of consciousness right, and it's possible that a breakthrough could occur in the world outside the FAI-aware subculture and filter through; but as things stand, the truth about consciousness would never be discovered by employing the methods and assumptions that prevail <em>inside</em> the FAI subculture.</p>\n<p>Perhaps I should pause to spell out <em>why</em> the nature of consciousness matters for Friendly AI. The reason is that the value system of a Friendly AI must make reference to certain states of conscious beings - e.g. \"pain is bad\" - so, in order to make correct judgments in real life, at a minimum it must be able to tell which entities are people and which are not. Is an AI a person? Is a digital copy of a human person, itself a person? Is a human body with a completely prosthetic brain still a person?</p>\n<p>I see two ways in which people concerned with FAI hope to answer such questions. One is simply to arrive at the right computational, functionalist definition of personhood. That is, we assume the paradigm according to which the mind is a computational state machine inhabiting the brain, with states that are coarse-grainings (equivalence classes) of exact microphysical states. Another physical system which admits the same coarse-graining - which embodies the same state machine at some macroscopic level, even though the microscopic details of its causality are different - is said to embody another instance of the same mind.</p>\n<p>An example of the other way to approach this question is the idea of simulating a group of consciousness theorists for 500 subjective years, until they arrive at a consensus on the nature of consciousness. I think it's rather unlikely that anyone will ever get to solve FAI-relevant problems in that way. The level of software and hardware power implied by the capacity to do reliable whole-brain simulations means you're already on the threshold of singularity: if you can simulate whole brains, you can simulate part brains, and you can also modify the parts, optimize them with genetic algorithms, and put them together into nonhuman AI. Uploads <em>won't</em> come first.</p>\n<p>But the idea of explaining consciousness this way, by simulating Daniel Dennett and David Chalmers until they agree, is just a cartoon version of similar but more subtle methods. What these methods have in common is that they propose to outsource the problem to a computational process using input from cognitive neuroscience. Simulating a whole human being and asking it questions is an extreme example of this (the simulation is the \"computational process\", and the brain scan it uses as a model is the \"input from cognitive neuroscience\"). A more subtle method is to have your baby AI act as an artificial neuroscientist, use its streamlined general-purpose problem-solving algorithms to make a causal model of a generic human brain, and then to somehow extract from that, the criteria which the human brain uses to identify the correct scope of the concept \"person\". It's similar to the idea of extrapolated volition, except that we're just extrapolating concepts.</p>\n<p>It might sound a lot simpler to just get <em>human</em> neuroscientists to solve these questions. Humans may be individually unreliable, but they have lots of cognitive tricks - heuristics - and they <em>are</em> capable of agreeing that something is verifiably true, once one of them does stumble on the truth. The main reason one would even consider the extra complication involved in figuring out how to turn a general-purpose seed AI into an artificial neuroscientist, capable of extracting the essence of the human decision-making cognitive architecture and then reflectively idealizing it according to its own inherent criteria, is shortage of time: one wishes to develop friendly AI before someone else inadvertently develops unfriendly AI. If we stumble into a situation where a powerful self-enhancing algorithm with arbitrary utility function has been discovered, it would be desirable to have, ready to go, a schema for the discovery of a friendly utility function via such computational outsourcing.</p>\n<p>Now, jumping ahead to a later stage of the argument, I argue that it is extremely likely that distinctively quantum processes play a fundamental role in conscious cognition, because the model of thought as distributed classical computation actually leads to an outlandish sort of dualism. If we don't concern ourselves with the merits of my argument for the moment, and just ask whether an AI neuroscientist might somehow overlook the existence of this alleged secret ingredient of the mind, in the course of its studies, I do think it's possible. The obvious noninvasive way to form state-machine models of human brains is to repeatedly scan them at maximum resolution using fMRI, and to form state-machine models of the individual voxels on the basis of this data, and then to couple these voxel-models to produce a state-machine model of the whole brain. This is a modeling protocol which assumes that everything which matters is physically localized at the voxel scale or smaller. Essentially we are asking, is it possible to mistake a quantum computer for a classical computer by performing this sort of analysis? The answer is definitely yes if the analytic process intrinsically assumes that the object under study is a classical computer. If I try to fit a set of points with a line, there will always be a line of best fit, even if the fit is absolutely terrible. So yes, one really can describe a protocol for AI neuroscience which would be unable to discover that the brain is quantum in its workings, and which would even produce a specific classical model on the basis of which it could then attempt conceptual and volitional extrapolation.</p>\n<p>Clearly you can try to circumvent comparably wrong outcomes, by adding reality checks and second opinions to your protocol for FAI development. At a more down to earth level, these exact mistakes could also be made by <em>human</em> neuroscientists, for the exact same reasons, so it's not as if we're talking about flaws peculiar to a hypothetical \"automated neuroscientist\". But I don't want to go on about this forever. I think I've made the point that wrong assumptions and lax verification can lead to FAI failure. The example of mistaking a quantum computer for a classical computer may even have a neat illustrative value. But is it plausible that the brain is <em>actually</em> quantum in any significant way? Even more incredibly, is there really a valid apriori argument against functionalism regarding consciousness - the identification of consciousness with a class of computational process?</p>\n<p>I have previously posted (<a href=\"/lw/1ly/consciousness/\">here</a>) about the way that an abstracted conception of reality, coming from scientific theory, can motivate denial that some basic appearance corresponds to reality. A perennial example is time. I hope we all agree that there is such a thing as the appearance of time, the appearance of change, the appearance of time flowing... But on this very site, there are many people who believe that reality is actually timeless, and that all these appearances are <em>only</em> appearances; that reality is fundamentally static, but that some of its fixed moments contain an illusion of dynamism.</p>\n<p>The case against functionalism with respect to conscious states is a little more subtle, because it's not being said that consciousness is an illusion; it's just being said that consciousness is some sort of property of computational states. I argue first that this requires dualism, at least with our current physical ontology, because conscious states are replete with constituents not present in physical ontology - for example, the \"qualia\", an exotic name for very straightforward realities like: the shade of green appearing in the banner of this site, the feeling of the wind on your skin, really every sensation or feeling you ever had. In a world made solely of quantum fields in space, there are no such things; there are just particles and arrangements of particles. The truth of this ought to be especially clear for color, but it applies equally to everything else.</p>\n<p>In order that this post should not be overlong, I will not argue at length here for the proposition that functionalism implies dualism, but shall proceed to the second stage of the argument, which does not seem to have appeared even in the philosophy literature. If we are going to suppose that minds and their states correspond solely to combinations of mesoscopic information-processing events like chemical and electrical signals in the brain, then there must be a mapping from possible exact microphysical states of the brain, to the corresponding mental states. Supposing we have a mapping from mental states to coarse-grained computational states, we now need a further mapping from computational states to exact microphysical states. There will of course be borderline cases. Functional states are identified by their causal roles, and there will be microphysical states which do not stably and reliably produce one output behavior or the other.</p>\n<p>Physicists are used to talking about thermodynamic quantities like pressure and temperature as if they have an independent reality, but objectively they are just nicely behaved averages. The fundamental reality consists of innumerable particles bouncing off each other; one does not need, and one has no evidence for, the existence of a separate entity, \"pressure\", which exists in parallel to the detailed microphysical reality. The idea is somewhat absurd.</p>\n<p>Yet this is analogous to the picture implied by a computational philosophy of mind (such as functionalism) applied to an atomistic physical ontology. We do know that the entities which constitute consciousness - the perceptions, thoughts, memories... which make up an experience - actually exist, and I claim it is also clear that they do not exist in any standard physical ontology. So, unless we get a very different physical ontology, we must resort to dualism. The mental entities become, inescapably, a new category of beings, distinct from those in physics, but systematically correlated with them. Except that, if they are being correlated with coarse-grained neurocomputational states which do not have an exact microphysical definition, only a functional definition, then the mental part of the new combined ontology is fatally vague. It is impossible for fundamental reality to be objectively vague; vagueness is a property of a concept or a definition, a sign that it is incomplete or that it does not need to be exact. But reality itself is necessarily exact - it is <em>something</em> - and so functionalist dualism cannot be true unless the underdetermination of the psychophysical correspondence is replaced by something which says for all possible physical states, exactly what mental states (if any) should also exist. And that inherently runs against the functionalist approach to mind.</p>\n<p>Very few people consider themselves functionalists <em>and</em> dualists. Most functionalists think of themselves as materialists, and materialism is a monism. What I have argued is that functionalism, the existence of consciousness, and the existence of microphysical details as the fundamental physical reality, together imply a peculiar form of dualism in which microphysical states which are borderline cases with respect to functional roles must all nonetheless be assigned to precisely one computational state or the other, even if no principle tells you how to perform such an assignment. The dualist will have to suppose that an exact but arbitrary border exists in state space, between the equivalence classes.</p>\n<p>This - not just dualism, but a dualism that is necessarily arbitrary in its fine details - is too much for me. If you want to go all Occam-Kolmogorov-Solomonoff about it, you can say that the information needed to specify those boundaries in state space is so great as to render this whole class of theories of consciousness not worth considering. Fortunately there is an alternative.</p>\n<p>Here, in addressing this audience, I may need to undo a little of what you may think you know about quantum mechanics. Of course, the local preference is for the Many Worlds interpretation, and we've had that discussion many times. One reason Many Worlds has a grip on the imagination is that it looks easy to imagine. Back when there was just one world, we thought of it as particles arranged in space; now we have many worlds, dizzying in their number and diversity, but each <em>individual world</em> still consists of just particles arranged in space. I'm sure that's how many people think of it.</p>\n<p>Among physicists it will be different. Physicists will have some idea of what a wavefunction is, what an operator algebra of observables is, they may even know about path integrals and the various arcane constructions employed in quantum field theory. Possibly they will understand that the Copenhagen interpretation is not about consciousness collapsing an actually existing wavefunction; it is a positivistic rationale for focusing only on measurements and not worrying about what happens in between. And perhaps we can all agree that this is inadequate, as a final description of reality. What I want to say, is that Many Worlds serves the same purpose in many physicists' minds, but is equally inadequate, though from the opposite direction. Copenhagen says the observables are real but goes misty about unmeasured reality. Many Worlds says the wavefunction is real, but goes misty about exactly how it connects to observed reality. My most frustrating discussions on this topic are with physicists who are happy to be vague about what a \"world\" is. It's really not so different to Copenhagen positivism, except that where Copenhagen says \"we only ever see measurements, what's the problem?\", Many Worlds says \"I say there's an independent reality, what else is left to do?\". It is very rare for a Many World theorist to seek an exact idea of what a world is, as you see Robin Hanson and maybe Eliezer Yudkowsky doing; in that regard, reading the Sequences on this site will give you an unrepresentative idea of the interpretation's status.</p>\n<p>One of the characteristic features of quantum mechanics is entanglement. But both Copenhagen, and a Many Worlds which ontologically privileges the position basis (arrangements of particles in space), still have atomistic ontologies of the sort which will produce the \"arbitrary dualism\" I just described. Why not seek a quantum ontology in which there are complex natural unities - fundamental objects which aren't simple - in the form of what we would presently called entangled states? That was the motivation for the <a href=\"/lw/1bs/how_to_think_like_a_quantum_monadologist/\">quantum monadology</a> described in my other really unpopular post. :-) [<strong>Edit:</strong> Go there for a discussion of \"the mind as tensor factor\", mentioned at the start of this post.] Instead of saying that physical reality is a series of transitions from one arrangement of particles to the next, say it's a series of transitions from one set of entangled states to the next. Quantum mechanics does not tell us which basis, if any, is ontologically preferred. Reality as a series of transitions between overall wavefunctions which are partly factorized and partly still entangled is a possible ontology; hopefully readers who really are quantum physicists will get the gist of what I'm talking about.</p>\n<p>I'm going to double back here and revisit the topic of how the world seems to look. Hopefully we agree, not just that there is an appearance of time flowing, but also an appearance of a self. Here I want to argue just for the bare minimum - that a moment's conscious experience consists of a set of things, events, situations... which are simultaneously \"present to\" or \"in the awareness of\" something - a conscious being - you. I'll argue for this because even this bare minimum is not acknowledged by existing materialist attempts to explain consciousness. I was recently directed to <a href=\"http://www.ted.com/talks/julian_baggini_is_there_a_real_you.html\">this brief talk</a> about the idea that there's no \"real you\". We are given a picture of a graph whose nodes are memories, dispositions, etc., and we are told that the self is like that graph: nodes can be added, nodes can be removed, it's a purely relational composite without any persistent part. What's missing in that description is that bare minimum notion of a perceiving self. Conscious experience consists of a subject perceiving objects in certain aspects. Philosophers have discussed for centuries how best to characterize the details of this phenomenological ontology; I think the best was Edmund Husserl, and I expect his work to be extremely important in interpreting consciousness in terms of a new physical ontology. But if you can't even notice that there's an observer there, observing all those parts, then you won't get very far.</p>\n<p>My favorite slogan for this is due to the other Jaynes, Julian Jaynes. I don't endorse his theory of consciousness at all; but while in a daydream he once said to himself, \"Include the knower in the known\". That sums it up perfectly. We <em>know</em> there is a \"knower\", an experiencing subject. We know this, just as well as we know that reality exists and that time passes. The adoption of ontologies in which these aspects of reality are regarded as unreal, as appearances as only, may be motivated by science, but it's false to the most basic facts there are, and one should show a little more imagination about what science will say when it's more advanced.</p>\n<p>I think I've said almost all of this before. The high point of the argument is that we should look for a physical ontology in which a self exists and is a natural yet complex unity, rather than a vaguely bounded conglomerate of distinct information-processing events, because the latter leads to one of those unacceptably arbitrary dualisms. If we can find a physical ontology in which the conscious self can be identified directly with a class of object posited by the theory, we can even get away from dualism, because physical theories are mathematical and formal and make few commitments about the \"inherent qualities\" of things, just about their causal interactions. If we can find a physical object which is absolutely isomorphic to a conscious self, then we can turn the isomorphism into an identity, and the dualism goes away. We can't do that with a functionalist theory of consciousness, because it's a many-to-one mapping between physical and mental, not an isomorphism.</p>\n<p>So, I've said it all before; what's new? What have I accomplished during these last sixteen months? Mostly, I learned a lot of physics. I did not originally intend to get into the details of particle physics - I thought I'd just study the ontology of, say, string theory, and then use that to think about the problem. But one thing led to another, and in particular I made progress by taking ideas that were slightly on the fringe, and trying to embed them within an orthodox framework. It was a great way to learn, and some of those fringe ideas may even turn out to be correct. It's now abundantly clear to me that I really could become a career physicist, working specifically on fundamental theory. I might even <em>have to</em> do that, it may be the best option for a day job. But what it means for the investigations detailed in this essay, is that I don't need to skip over any details of the fundamental physics. I'll be concerned with many-body interactions of biopolymer electrons <em>in vivo</em>, not particles in a collider, but an electron is still an electron, an elementary particle, and if I hope to identify the conscious state of the quantum self with certain special states from a many-electron Hilbert space, I should want to understand that Hilbert space in the deepest way available.</p>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/11755497\">My only peer-reviewed publication</a>, from many years ago, picked out pathways in the microtubule which, we speculated, might be suitable for mobile electrons. I had nothing to do with noticing those pathways; my contribution was the speculation about what sort of physical processes such pathways might underpin. Something I did notice, but never wrote about, was the unusual similarity (so I thought) between the microtubule's structure, and <a href=\"http://online.kitp.ucsb.edu//online/colloq/freedman2/\">a model of quantum computation due to the topologist Michael Freedman</a>: a hexagonal lattice of qubits, in which entanglement is protected against decoherence by being encoded in topological degrees of freedom. It seems clear that performing an ontological analysis of a topologically protected coherent quantum system, in the context of some comprehensive ontology (\"interpretation\") of quantum mechanics, is a good idea. I'm not claiming to know, by the way, that the microtubule is the locus of quantum consciousness; there are a number of possibilities; but the microtubule has been studied for many years now and there's a big literature of models... a few of which might even have biophysical plausibility.</p>\n<p>As for the interpretation of quantum mechanics itself, <a href=\"http://pirsa.org/11080047/\">these developments</a> are highly technical, but revolutionary. A well-known, well-studied quantum field theory turns out to have a bizarre new nonlocal formulation in which collections of particles seem to be replaced by polytopes in twistor space. Methods pioneered via purely mathematical studies of this theory are already being used for real-world calculations in QCD (the theory of quarks and gluons), and I expect this new ontology of \"reality as a complex of twistor polytopes\" to carry across as well. I don't know which quantum interpretation will win the battle now, but this is <em>new information</em>, of utterly fundamental significance. It is precisely the sort of altered holistic viewpoint that I was groping towards when I spoke about quantum monads constituted by entanglement. So I think things are looking good, just on the pure physics side. The real job remains to show that there's such a thing as quantum neurobiology, and to connect it to something like Husserlian transcendental phenomenology of the self via the new quantum formalism.</p>\n<p>It's when we reach a level of understanding like that, that we will truly be ready to tackle the relationship between consciousness and the new world of intelligent autonomous computation. I don't deny the enormous helpfulness of the computational perspective in understanding unconscious \"thought\" and information processing. And even conscious states are still <em>states</em>, so you can surely make a state-machine model of the causality of a conscious being. It's just that the reality of how consciousness, computation, and fundamental ontology are connected, is bound to be a whole lot deeper than just a stack of virtual machines in the brain. We will have to fight our way to a new perspective which subsumes and transcends the computational picture of reality as a set of causally coupled black-box state machines. It should still be possible to \"port\" most of the thinking about Friendly AI to this new ontology; but the differences, what's new, are liable to be crucial to success. Fortunately, it seems that new perspectives are still possible; we haven't reached Kantian cognitive closure, with no more ontological progress open to us. On the contrary, there are still lines of investigation that we've hardly begun to follow.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jroCHyDC6XHomeuW7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 3, "extendedScore": null, "score": 8.397192160296868e-07, "legacy": true, "legacyId": "12488", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iHMy2X9mqQT6ayf9f", "y2iPsnzp5KhvtK9zt", "Rq5HS5JwjdxaLeR2B", "RKEepbECEXQXqXFNm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T10:28:38.514Z", "modifiedAt": null, "url": null, "title": "There's learned philosophers but not philosophical experts", "slug": "there-s-learned-philosophers-but-not-philosophical-experts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:41.093Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JamesCole", "createdAt": "2009-04-16T08:24:51.843Z", "isAdmin": false, "displayName": "JamesCole"}, "userId": "RMS473ETmWQLaCtmu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F2zxr3sMM26AN7LH5/there-s-learned-philosophers-but-not-philosophical-experts", "pageUrlRelative": "/posts/F2zxr3sMM26AN7LH5/there-s-learned-philosophers-but-not-philosophical-experts", "linkUrl": "https://www.lesswrong.com/posts/F2zxr3sMM26AN7LH5/there-s-learned-philosophers-but-not-philosophical-experts", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20There's%20learned%20philosophers%20but%20not%20philosophical%20experts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThere's%20learned%20philosophers%20but%20not%20philosophical%20experts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2zxr3sMM26AN7LH5%2Fthere-s-learned-philosophers-but-not-philosophical-experts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=There's%20learned%20philosophers%20but%20not%20philosophical%20experts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2zxr3sMM26AN7LH5%2Fthere-s-learned-philosophers-but-not-philosophical-experts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2zxr3sMM26AN7LH5%2Fthere-s-learned-philosophers-but-not-philosophical-experts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>\n<p>It seems to me that the notion of expertise can only apply to fields in which there is an established body of knowledge. By that I mean fields in which we have (empirical) grounds for believing our knowledge is at least an approximation or heading in the right direction. Physics or genetics or how to fix cars are examples of such fields. You can be an expert in physics.</p>\n<p>Philosophy seems different. What makes philosophy interesting is that it's about things we don't understand well. In philosophy we're not even sure that existing approaches to problems are heading in the right direction.</p>\n<p>Philosophy is pretty much by definition about things we don't understand well. Once a philosophical topic is understood it ceases to be part of philosophy, and becomes part of another field like physics, biology, economics, etc. (or alternatively, the problem may be dissolved and seen as a kind of misunderstanding.)</p>\n<p>I would say the kind of knowledge that exists in the field of philosophy is more of ways of describing problems, or particular arguments for or against a view of problems. It's more like a discussion.</p>\n<p>You can be an expert in the different positions about a philosophical problem, but I would distinguish this from the idea that someone can be an expert on a philosophical subject.</p>\n<p>For example, someone can be an expert on the various problems and arguments associated with consciousness, but I don't think anyone can claim to be an expert on consciousness (at least the hard problem of consciousness) because we just don't understand it.</p>\n<p>So rather than saying there are experts in philosophy I would say that there are people who are very learned in philosophy.</p>\n<p>Why does this distinction matter?</p>\n<p>When there isn't established knowledge, we're less certain that existing approaches are correct. The fact that an existing approach hasn't been able to solve a problem for long time may mean that it's the wrong approach. It is more likely in philosophy that someone who comes from outside of the field, who isn't well versed in the existing approaches, can add something of use to the table. The fact that they aren't familiar with existing arguments may even be a virtue.</p>\n<p>If there aren't philosophical experts, then there aren't experts to challenge.</p>\n<p>Yet it seems to me that philosophy seems to hold greater reverence for 'experts' than most other fields.</p>\n<p>What do you think?</p>\n<p>&nbsp;</p>\n<p>\n<hr />\n</p>\n<p><em>[I&nbsp;<a href=\"http://www.reddit.com/r/philosophy/comments/p05pl/theres_learned_philosophers_but_not_philosophical/\">originally</a>&nbsp;posted this to reddit/r/philosophy but -- to my surprise, since it is&nbsp;somewhat critical of how philosophy is done -- it didn't generate any&nbsp;comments.]</em></p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F2zxr3sMM26AN7LH5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 6, "extendedScore": null, "score": 8.397410416132815e-07, "legacy": true, "legacyId": "12489", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T14:07:19.889Z", "modifiedAt": null, "url": null, "title": "Cataloging my skills - good idea?", "slug": "cataloging-my-skills-good-idea", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.403Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zF2q7bdDLcWQ8G6nN/cataloging-my-skills-good-idea", "pageUrlRelative": "/posts/zF2q7bdDLcWQ8G6nN/cataloging-my-skills-good-idea", "linkUrl": "https://www.lesswrong.com/posts/zF2q7bdDLcWQ8G6nN/cataloging-my-skills-good-idea", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cataloging%20my%20skills%20-%20good%20idea%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACataloging%20my%20skills%20-%20good%20idea%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzF2q7bdDLcWQ8G6nN%2Fcataloging-my-skills-good-idea%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cataloging%20my%20skills%20-%20good%20idea%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzF2q7bdDLcWQ8G6nN%2Fcataloging-my-skills-good-idea", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzF2q7bdDLcWQ8G6nN%2Fcataloging-my-skills-good-idea", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 446, "htmlBody": "<p>tl,dr: Discuss utility of writing down a portfolio of bayesian evidence for/against your skills, and keeping it for personal reference.</p>\n<p>Last night, I applied for the&nbsp;<a href=\"/lw/9hb/position_design_and_write_rationality_curriculum/\">Write/Design Rationality Curriculum Position</a>. I am the sort who sincerely needs the continuous requests to err on the side of applying to actually apply, and some part of me is still gobsmacked that I went ahead and did it. I do not regret applying in the smallest amount, because raising the sanity waterline is a dear little obsession of mine. At the moment, all that drive is going in circles for lack of clear direction. Although I feel supremely unqualified rationality-wise, I think this would be a fantastic opportunity to GET qualified and improve the world at the same time. By the time I finished the application I was much more confident in my own ability to be helpful.</p>\n<p>But judging my application a net positive doesn't mean I can't analyze the heck out it.</p>\n<p>I woke up this morning and realized that I'd listed a lot of conclusions about myself while barely even hinting at the evidence behind *why I thought those things*.&nbsp;That's something one shouldn't do on a regular job application, let alone when it's going to be read by professional rationality enthusiasts.</p>\n<p>So I'm making an informal skills reference catalog for myself, because this needs to REALLY not happen with the next opportunity I sign up for. Like a resume, except focusing on quantity and bayesian evidence influencing your belief rather than what sort of self-presentation is most likely to get you hired. It differs significantly from a resume in that it should also include -negative- evidences against having certain skills, for evenness and self-honesty's sake. I suspect caching more evidence in a readily available format will be helpful for mitigating availability heuristic errors, and writing the reasoning process down instead of (only) resolving to keep a more accurate self-image in my head is useful because&nbsp;<a href=\"/lw/1za/the_spotlight/\">thoughts are slippery</a>. [anecdote]I notice I get a lot more useful reasoning done when I write down my thoughts approximately as my inner monologue voices them. It prevents backtracking, as I can go back to quickly and confidently determine whether or not I've already thought of a point, and makes it substantially easier to notice when I've retro-actively edited a thought, either because I've contradicted myself or because I've literally gone back a few words/sentences to edit. [/anecdote]</p>\n<p>I very nearly posted this as a lamenting comment on that page, but this idea seemed a little more discussion worthy and too tangential for a comment.&nbsp;So Huzzah for learning experiences! Is there anything amiss in my reasoning?&nbsp;Perhaps I also need to work in a heuristic of 'sleeping on it'.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zF2q7bdDLcWQ8G6nN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "12490", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ifL8f4Xzy2D9Bb6zs", "Zstm38omrpeu7iWeS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T18:45:58.605Z", "modifiedAt": null, "url": null, "title": "A model of the brain's mapping of the territory", "slug": "a-model-of-the-brain-s-mapping-of-the-territory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:01.287Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ataftoti", "createdAt": "2011-09-08T05:30:10.496Z", "isAdmin": false, "displayName": "ataftoti"}, "userId": "85QPpLm8F5FxshmwJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gr3DKHGgj33AN5akr/a-model-of-the-brain-s-mapping-of-the-territory", "pageUrlRelative": "/posts/gr3DKHGgj33AN5akr/a-model-of-the-brain-s-mapping-of-the-territory", "linkUrl": "https://www.lesswrong.com/posts/gr3DKHGgj33AN5akr/a-model-of-the-brain-s-mapping-of-the-territory", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20model%20of%20the%20brain's%20mapping%20of%20the%20territory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20model%20of%20the%20brain's%20mapping%20of%20the%20territory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgr3DKHGgj33AN5akr%2Fa-model-of-the-brain-s-mapping-of-the-territory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20model%20of%20the%20brain's%20mapping%20of%20the%20territory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgr3DKHGgj33AN5akr%2Fa-model-of-the-brain-s-mapping-of-the-territory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgr3DKHGgj33AN5akr%2Fa-model-of-the-brain-s-mapping-of-the-territory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p>I'm linking to a video which describes how the brain may be learning to improve its skills at mapping the territory from limited samples.</p>\n<p>This model of learning was previously unknown to me. Judging from the date of the video, what I heard from the person who referred me to it, and the fact that I do not recall hearing much related to this on LessWrong, I think this may be recent enough that some people here would benefit from me spreading the word.</p>\n<p>Check out this model of a learning theory which gets background introduction starting from the 52:00 mark and gets going at the 54:00 mark. The overview of the model is explained in approximately 4 minutes.</p>\n<p>http://www.youtube.com/watch?v=vcp6J1T60qc&amp;t=52m19s</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gr3DKHGgj33AN5akr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -1, "extendedScore": null, "score": 8.399328818779129e-07, "legacy": true, "legacyId": "12491", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-29T19:38:42.217Z", "modifiedAt": null, "url": null, "title": "Charity when time isn't convertible to money?", "slug": "charity-when-time-isn-t-convertible-to-money", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.712Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h3Ejb7hHYpZYtwxmw/charity-when-time-isn-t-convertible-to-money", "pageUrlRelative": "/posts/h3Ejb7hHYpZYtwxmw/charity-when-time-isn-t-convertible-to-money", "linkUrl": "https://www.lesswrong.com/posts/h3Ejb7hHYpZYtwxmw/charity-when-time-isn-t-convertible-to-money", "postedAtFormatted": "Sunday, January 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Charity%20when%20time%20isn't%20convertible%20to%20money%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACharity%20when%20time%20isn't%20convertible%20to%20money%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh3Ejb7hHYpZYtwxmw%2Fcharity-when-time-isn-t-convertible-to-money%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Charity%20when%20time%20isn't%20convertible%20to%20money%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh3Ejb7hHYpZYtwxmw%2Fcharity-when-time-isn-t-convertible-to-money", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh3Ejb7hHYpZYtwxmw%2Fcharity-when-time-isn-t-convertible-to-money", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>Having just re-read \"Money: The Unit of Caring\", I noticed that the general methods proposed therein make some assumptions which don't seem to apply to me, and I'm trying to figure out how the conclusions therein change therefrom.</p>\n<p>Avoiding certain personal details, I'm on a fixed income; I get a monthly deposit in my bank account every month. I don't expect this to change in the foreseeable future; and at least in the general sense of 'job', it's unlikely I'll be able to acquire one. In sum - I don't have any easy way to convert my time into additional money.</p>\n<p>However, I still want to get the occasional warm fuzzy from causing the most possible good from what I can do - even if that involves my volunteering to spend some hours of my life doing things that would be inefficient for someone else. For example, donating blood, or taking an overnight shift keeping an eye on things at the local 'out of the cold' program; and using givewell.org as a guide for what money I am able to funnel into direct donating.</p>\n<p>&nbsp;</p>\n<p>So - does anyone have any advice? (Or questions that would help better advice be given?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h3Ejb7hHYpZYtwxmw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 8.399532250965537e-07, "legacy": true, "legacyId": "12493", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-30T15:35:58.365Z", "modifiedAt": null, "url": null, "title": "[LINK] Surviving the World on Vasili Arkhipov Day", "slug": "link-surviving-the-world-on-vasili-arkhipov-day", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.283Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dyFtej37cFKuL2Bbn/link-surviving-the-world-on-vasili-arkhipov-day", "pageUrlRelative": "/posts/dyFtej37cFKuL2Bbn/link-surviving-the-world-on-vasili-arkhipov-day", "linkUrl": "https://www.lesswrong.com/posts/dyFtej37cFKuL2Bbn/link-surviving-the-world-on-vasili-arkhipov-day", "postedAtFormatted": "Monday, January 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Surviving%20the%20World%20on%20Vasili%20Arkhipov%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Surviving%20the%20World%20on%20Vasili%20Arkhipov%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdyFtej37cFKuL2Bbn%2Flink-surviving-the-world-on-vasili-arkhipov-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Surviving%20the%20World%20on%20Vasili%20Arkhipov%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdyFtej37cFKuL2Bbn%2Flink-surviving-the-world-on-vasili-arkhipov-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdyFtej37cFKuL2Bbn%2Flink-surviving-the-world-on-vasili-arkhipov-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 11, "htmlBody": "<p>Comic can be found <a href=\"http://survivingtheworld.net/Lesson1308.html\">here</a>. Related: <a href=\"/lw/878/vasili_arkhipov_day/\">Vasili Arkhipov Day</a>, <a href=\"/lw/jq/926_is_petrov_day/\">Petrov Day</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dyFtej37cFKuL2Bbn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 8.404153934935087e-07, "legacy": true, "legacyId": "12508", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BDf2aPaWSfH2gWWAc", "QtyKq4BDyuJ3tysoK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-30T16:11:42.191Z", "modifiedAt": null, "url": null, "title": "Working Through the Controlled Demolition Conspiracy", "slug": "working-through-the-controlled-demolition-conspiracy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:55.498Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rysade", "createdAt": "2010-10-23T05:51:26.346Z", "isAdmin": false, "displayName": "rysade"}, "userId": "ZPxsNDAnPrQfo2yx2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y8Gp98HEmaR9Mt7jK/working-through-the-controlled-demolition-conspiracy", "pageUrlRelative": "/posts/y8Gp98HEmaR9Mt7jK/working-through-the-controlled-demolition-conspiracy", "linkUrl": "https://www.lesswrong.com/posts/y8Gp98HEmaR9Mt7jK/working-through-the-controlled-demolition-conspiracy", "postedAtFormatted": "Monday, January 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Working%20Through%20the%20Controlled%20Demolition%20Conspiracy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWorking%20Through%20the%20Controlled%20Demolition%20Conspiracy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8Gp98HEmaR9Mt7jK%2Fworking-through-the-controlled-demolition-conspiracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Working%20Through%20the%20Controlled%20Demolition%20Conspiracy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8Gp98HEmaR9Mt7jK%2Fworking-through-the-controlled-demolition-conspiracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy8Gp98HEmaR9Mt7jK%2Fworking-through-the-controlled-demolition-conspiracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>I hope I'm not breaking any taboos here. It's been a while since I've come onto the discussion section and I admit I'm not too up to date on the topics.</p>\n<p>I'm having difficulty responding to someone who is convinced that 7WTC was brought down by controlled demolition on September 11th, 2001. They're referencing the controlled-looking destruction of 7WTC and various other incriminating looking things. Thermite and thermite waste products seem to come up a lot.</p>\n<p>Now, I have definitely noticed I'm confused here. While I hold the opinion that the towers went down because of the planes/fires (i.e. the standard explanation) I have difficulty seeing how the falsity of controlled demolition is the <a href=\"http://www.lesswrong.com/lw/1kh/the_correct_contrarian_cluster\">slam-dunk</a> folks seem to think it is. Could somebody walk me through this?</p>\n<p>&nbsp;</p>\n<p>[EDIT: About a million edits later, I have finally worked through the problem with my link: I needed it to be in HTML and not in the comment format.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y8Gp98HEmaR9Mt7jK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -2, "extendedScore": null, "score": 8.404291930183272e-07, "legacy": true, "legacyId": "12509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9KvefburLia7ptEE3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-30T17:42:48.022Z", "modifiedAt": null, "url": null, "title": "Repost: Efficient Charity: Cheap Utilons via bone marrow registration", "slug": "repost-efficient-charity-cheap-utilons-via-bone-marrow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.703Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atorm", "createdAt": "2011-03-30T16:41:30.635Z", "isAdmin": false, "displayName": "atorm"}, "userId": "PvazkPKLZs5LNujcL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/myyhjWXFqgfRoprYn/repost-efficient-charity-cheap-utilons-via-bone-marrow", "pageUrlRelative": "/posts/myyhjWXFqgfRoprYn/repost-efficient-charity-cheap-utilons-via-bone-marrow", "linkUrl": "https://www.lesswrong.com/posts/myyhjWXFqgfRoprYn/repost-efficient-charity-cheap-utilons-via-bone-marrow", "postedAtFormatted": "Monday, January 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Repost%3A%20Efficient%20Charity%3A%20Cheap%20Utilons%20via%20bone%20marrow%20registration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARepost%3A%20Efficient%20Charity%3A%20Cheap%20Utilons%20via%20bone%20marrow%20registration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmyyhjWXFqgfRoprYn%2Frepost-efficient-charity-cheap-utilons-via-bone-marrow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Repost%3A%20Efficient%20Charity%3A%20Cheap%20Utilons%20via%20bone%20marrow%20registration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmyyhjWXFqgfRoprYn%2Frepost-efficient-charity-cheap-utilons-via-bone-marrow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmyyhjWXFqgfRoprYn%2Frepost-efficient-charity-cheap-utilons-via-bone-marrow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 541, "htmlBody": "<p>&nbsp;</p>\n<h1 style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 0px; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify; \">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; color: #000000; font-weight: normal; line-height: 19px; font-size: small; \">DISCLAIMER: This article was posted a few days ago in Main. If you read it there, you probably don't need to read it again, although there is an additional paragraph at the bottom that might interest you. I am posting it again because I didn't quite understand how posts tend to show up on the site and because I want to get as many eyes on this article as possible.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; color: #000000; font-weight: normal; line-height: 19px; font-size: small; \">This topic is not really related to the things normally discussed here, but I think it's really important, and it might interest Less Wrongers, especially since many of us are interested in ethics and utility calculations that are essentially cost-benefit analyses. Bone marrow donation in the United States is managed by the National Marrow Donor Program. Because typing donors for matching purposes can be costly, they often require people signing up to donate to pay a registration fee, which probably prevents a lot of people from signing up. These costs are being covered until the end of the month by a corporate sponsor, which means that right now, all you need to do if you live in the US is go to<a style=\"color: #8a8a8b; text-decoration: underline; \" href=\"http://marrow.org/Join/Join_Now/Join_Now.aspx\">http://marrow.org/Join/Join_Now/Join_Now.aspx</a>&nbsp;and fill out a simple questionnaire. You will be sent a kit to collect a cheek swab, and then you will be entered into the donor database. Doing this does not require you to donate if a match comes up.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; color: #000000; font-weight: normal; line-height: 19px; font-size: small; \">The reason I think this might interest Less Wrongers is that this is a really cheap way to improve the world. According to their website, about 1 in 540 potential donors are actually asked to donate, so registering doesn't actually make it all that likely that you will be asked to do anything more. If you ARE a match for someone who needs a donation, the cost to you is at most the temporary pain of marrow extraction (many donors are asked only for blood cells), whereas the other person&rsquo;s chance to live is much improved. This looks like a huge net positive.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; color: #000000; font-weight: normal; line-height: 19px; font-size: small; \">Unfortunately I only found out about this a few days ago, and it only occurred to me [Saturday] that this might be a forum of people who would respond to the argument \"you can make the world better at little cost to yourself.\" However, I ask that you go to the website and spend a few minutes signing up. This is like buying a 1 in 540 lottery ticket that SAVES SOMEONE&rsquo;S LIFE. If the Singularity hits and an FAI can generate perfectly matched marrow for anyone who needs it from totipotency-induced cells, that will be wonderful, but this is a chance to make sure one more person gets there.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; color: #000000; font-weight: normal; line-height: 19px; font-size: small; \">There is now only one day left in which all the costs to the donor are covered. I'm interested in the affect this might have on akrasiac behavior. If you think that signing up is a good idea, you need to do it now. If you put it off with \"I'll get to it later,\" you are essentially saying you won't do it, unless you aren't concerned about having to pay money to sign up. Let us know in the comments if this immediate deadline served to motivate you, or, conversely, irritated you and made you choose not to sign up.</p>\n</h1>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "myyhjWXFqgfRoprYn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 8.404643776781291e-07, "legacy": true, "legacyId": "12511", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-30T18:51:05.377Z", "modifiedAt": null, "url": null, "title": "Hacking Less Wrong made easy: Vagrant edition", "slug": "hacking-less-wrong-made-easy-vagrant-edition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.626Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E3ZzFCcCF5ifm9zsG/hacking-less-wrong-made-easy-vagrant-edition", "pageUrlRelative": "/posts/E3ZzFCcCF5ifm9zsG/hacking-less-wrong-made-easy-vagrant-edition", "linkUrl": "https://www.lesswrong.com/posts/E3ZzFCcCF5ifm9zsG/hacking-less-wrong-made-easy-vagrant-edition", "postedAtFormatted": "Monday, January 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hacking%20Less%20Wrong%20made%20easy%3A%20Vagrant%20edition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHacking%20Less%20Wrong%20made%20easy%3A%20Vagrant%20edition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3ZzFCcCF5ifm9zsG%2Fhacking-less-wrong-made-easy-vagrant-edition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hacking%20Less%20Wrong%20made%20easy%3A%20Vagrant%20edition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3ZzFCcCF5ifm9zsG%2Fhacking-less-wrong-made-easy-vagrant-edition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3ZzFCcCF5ifm9zsG%2Fhacking-less-wrong-made-easy-vagrant-edition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 439, "htmlBody": "<p>The Less Wrong Public Goods Team has already brought you an easy-to use <a href=\"/lw/7fv/hacking_on_lesswrong_just_got_easier/\">virtual machine</a> for hacking Less Wrong.</p>\n<p class=\"p1\">But virtual boxes can cut both ways: on the one hand, you don't have to worry about setting things up yourself; on the other hand, not knowing how things were put together, having to deal with a \"black box\" that doesn't let you use your own source code editor or pick an OS - these can be offputting. To me at least, these were <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconveniences</a> that might stand in the way of updating my copy of the source and making some <a href=\"/lw/9m2/the_neglected_virtue_of_curiosity/5sfh\">useful tweaks</a>.</p>\n<p class=\"p1\">Enter <a href=\"http://vagrantup.com/\">Vagrant</a>&nbsp;- and a little work I've done today for LW hackers and would-be hackers.&nbsp;Vagrant is a recent tool that allows you to treat virtual machine configurations <em>as source code</em>.</p>\n<p class=\"p1\">Instead of being something that someone possessed of arcane knowledge has put together, a virtual machine under Vagrant results from executing a series of source code instructions - and this source code is available for you to read, review, understand or change. (Software development should be a process of knowledge capture, not some hermetic discipline where you rely on the intransmissible wisdom of remote elders.)</p>\n<p class=\"p1\">Preliminary (but tested) results are up on <a href=\"https://github.com/Morendil/lesswrong\">my Github repo</a>&nbsp;- it's a fork of the offiical LW code base, not the real thing. (One this is tested by someone else, and if it works well, I intend to submit a pull request so that these improvements end up in the main codebase.) The following assumes you have a Unix or Mac system, or if you're using Windows, that you're command-line competent.</p>\n<p class=\"p1\">Hacking on LW is now done as follows (compared to using the VM):</p>\n<ul>\n<li>The following prerequisites are unchanged: git, Virtualbox</li>\n<li>Install the following prerequisites: Ruby, rubygems, Vagrant</li>\n<li>Download the Less Wrong source code as follows:&nbsp;<strong>git clone git@github.com:Morendil/lesswrong.git</strong></li>\n<li>Enter the \"lesswrong\" directory, then build the VM with: <strong>vagrant up</strong> (may take a while)</li>\n<li>Log into the virtual box with: <strong>vagrant ssh</strong></li>\n<li>Go to the \"<strong>/vagrant/r2</strong>\" directory, and copy <strong>example.ini</strong> to <strong>development.ini</strong></li>\n<li><strong>Change</strong> all instances of \"password\" in development.ini to \"reddit\"</li>\n<li>You can now start the LW server with:&nbsp;<strong>paster serve --reload development.ini port=8080</strong></li>\n<li>Browse the URL http://localhost:8080/</li>\n</ul>\n<div>The cool part is that the \"/vagrant\" directory on the VM is mapped to where you checked out the LW source code on your own machine: it's a shared directory, which means you can use your own code editor, run grep searches and so on. You've broken out of the black box!</div>\n<div>If you try it, please report your experience in the thread below.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HFou6RHqFagkyrKkW": 1, "MfpEPj6kJneT9gWT6": 1, "TkZ7MFwCi4D63LJ5n": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E3ZzFCcCF5ifm9zsG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 44, "extendedScore": null, "score": 8.404907548917857e-07, "legacy": true, "legacyId": "12512", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SRdvYJrDNvWkpcm8F", "reitXJgJXFzKpdKyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-30T19:33:02.904Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong Megameetup", "slug": "meetup-lesswrong-megameetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:53.239Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FurcEQA5iLRxqvbpZ/meetup-lesswrong-megameetup", "pageUrlRelative": "/posts/FurcEQA5iLRxqvbpZ/meetup-lesswrong-megameetup", "linkUrl": "https://www.lesswrong.com/posts/FurcEQA5iLRxqvbpZ/meetup-lesswrong-megameetup", "postedAtFormatted": "Monday, January 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%20Megameetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%20Megameetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFurcEQA5iLRxqvbpZ%2Fmeetup-lesswrong-megameetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%20Megameetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFurcEQA5iLRxqvbpZ%2Fmeetup-lesswrong-megameetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFurcEQA5iLRxqvbpZ%2Fmeetup-lesswrong-megameetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/6m\">LessWrong Megameetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">04 February 2012 12:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Leverage House, New York City (contact for address)</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We're holding the second East Coast Megameetup this weekend at the Leverage House in NYC, kindly provided by Geoff and the Leverage Research team.</p>\n<p>Contact me at aarondtucker &lt;at&gt; gmail &lt;dot&gt; com for the address, since Geoff doesn't want to post it on the internet.</p>\n<p>We'll have semi-focused talks/discussions&nbsp;starting late Saturday afternoon on:</p>\n<ul>\n<li>Nutrition</li>\n<li>Community-building</li>\n<li>What Leverage is doing</li>\n<li>Group Intelligence</li>\n<li>Psychological Frameworks</li>\n</ul>\n<p>I'm really looking forward to this, and I hope to see you there! It's a great opportunity to meet the other LWers on your coast.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/6m\">LessWrong Megameetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FurcEQA5iLRxqvbpZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "12513", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Megameetup\">Discussion article for the meetup : <a href=\"/meetups/6m\">LessWrong Megameetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">04 February 2012 12:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Leverage House, New York City (contact for address)</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We're holding the second East Coast Megameetup this weekend at the Leverage House in NYC, kindly provided by Geoff and the Leverage Research team.</p>\n<p>Contact me at aarondtucker &lt;at&gt; gmail &lt;dot&gt; com for the address, since Geoff doesn't want to post it on the internet.</p>\n<p>We'll have semi-focused talks/discussions&nbsp;starting late Saturday afternoon on:</p>\n<ul>\n<li>Nutrition</li>\n<li>Community-building</li>\n<li>What Leverage is doing</li>\n<li>Group Intelligence</li>\n<li>Psychological Frameworks</li>\n</ul>\n<p>I'm really looking forward to this, and I hope to see you there! It's a great opportunity to meet the other LWers on your coast.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Megameetup1\">Discussion article for the meetup : <a href=\"/meetups/6m\">LessWrong Megameetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong Megameetup", "anchor": "Discussion_article_for_the_meetup___LessWrong_Megameetup", "level": 1}, {"title": "Discussion article for the meetup : LessWrong Megameetup", "anchor": "Discussion_article_for_the_meetup___LessWrong_Megameetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-30T21:03:21.945Z", "modifiedAt": null, "url": null, "title": "Terminal Bias", "slug": "terminal-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:39.534Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QGKFjaZNDtJnBTbxS/terminal-bias", "pageUrlRelative": "/posts/QGKFjaZNDtJnBTbxS/terminal-bias", "linkUrl": "https://www.lesswrong.com/posts/QGKFjaZNDtJnBTbxS/terminal-bias", "postedAtFormatted": "Monday, January 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Terminal%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATerminal%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGKFjaZNDtJnBTbxS%2Fterminal-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Terminal%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGKFjaZNDtJnBTbxS%2Fterminal-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGKFjaZNDtJnBTbxS%2Fterminal-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1811, "htmlBody": "<p>I've seen of people on Lesswrong taking cognitive structures that I consider to be biases as <em>terminal values</em>. Take risk aversion for example:</p>\r\n<h2>Risk Aversion</h2>\r\n<p>For a rational agent with goals that don't include \"being averse to risk\", risk aversion is a bias. The correct decision theory acts on expected utility, with utility of outcomes and probability of outcomes factored apart and calculated separately. Risk aversion does not factor them.</p>\r\n<p>EDIT: There is some contention on this. Just substitute \"that thing minimax algorithms do\" for \"risk aversion\" in my writing. /EDIT</p>\r\n<p>A while ago, I was working through the derivation of A* and minimax planning algorithms from a Bayesian and decision-theoretic base. When I was trying to understand the relationship between them, I realized that strong risk aversion, aka minimax, saves huge amounts of computation compared to the correct decision theory, and actually becomes more optimal as the environment becomes more influenced by rational opponents. The best way win is to deny the opponents any opportunity to weaken you. That's why minimax is a good algorithm for chess.</p>\r\n<p>Current theories about the origin of our intelligence say that we became smart to outsmart our opponents in complex social games. If our intelligence was built for adversarial games, I am not surprised at risk aversion.</p>\r\n<p>A better theoretical replacement, and a plausible causal history for why we have the bias instead of the correct algorithm are convincing to me as an argument against risk aversion as a value the way a rectangular 13x7 <a href=\"/lw/sy/sorting_pebbles_into_correct_heaps/\">pebble heap</a> is convincing to a pebble sorter as an argument against the correctness of a heap of 91 pebbles; it seems undeniable, but I don't have access to the hidden values that would say for sure.</p>\r\n<p>And yet I've seen people on LW state that their \"utility function\" includes risk aversion. Because I don't understand the values involved, all I can do is state the argument above and see if other people are as convinced as me.</p>\r\n<p>It may <a href=\"/lw/n3/circular_altruism/\">seem silly to take a bias as terminal</a>, but there are examples with similar arguments that are less clear-cut, and some that we take as uncontroversially terminal:</p>\r\n<h2>Responsibility and Identity</h2>\r\n<p>The feeling that you are responsible for some things and not others, like say, the safety of your family, but not people being tortured in Syria, seems noble and practical. But I take it to be a bias.</p>\r\n<p>I'm no evolutionary psychologist, but it seems to me that feelings of responsibility are a quick hack to kick you into motion where you can affect the outcome and the utility at stake is large. For the most part, this aligns well with utilitarianism; you usually don't feel responsible for things you can't really affect, like people being tortured in Syria, or the color of the sky. You do feel responsible to pull a passed out kid off the train tracks, but maybe you don't feel responsible to give them some fashion advice.</p>\r\n<p>Responsibility seems to be built on identity, so it starts to go weird when you identify or don't identify in ways that didn't happen in the ancestral environment. Maybe you identify as a citizen of the USA, but not of Syria, so you feel shame and responsibility about the US torturing people, but the people being tortured in Syria are not your responsibility, even though both cases are terrible, and there is very little you can do about either. A proper utilitarian would feel approximately the same desire to do something about each, but our responsibility hack emphasizes responsibility for the actions of the tribe you identify with.</p>\r\n<p>You might feel great responsibility to defend your past actions but not those other people, even tho neither is worth \"defending\". A rational agent would learn from both the actions of their own past selves and those of other people without seeking to justify or condemn; they would <em>update and move on</em>. There is no tribal council that will exile you if you change your tune or don't defend yourself.</p>\r\n<p>You might be appalled that someone wishes to stop feeling responsibility for their past selves; \"but if they don't feel responsibility for their actions, what will prevent them from murdering people, or encourage them to do good?\". A rational utilitarian would do good and not do evil because they wish good and non-evil to be done, instead of because of feelings of responsibility that they don't understand.</p>\r\n<p>This argument is a little harder to see and possibly a little less convincing, but again I am convinced that identity and responsibility are inferior to utilitarianism, tho they may have seemed almost terminal.</p>\r\n<h2>Justice</h2>\r\n<p>Surely justice is a terminal value; it feels so noble to desire it. Again I consider the desire for justice to be a biased heuristic.</p>\r\n<p>in game theory the best solution for iterated prisoners dilemma is tit-for-tat: cooperate and be nice, but punish defectors. Tit-for-tat looks a lot like our instincts for justice, and I've heard that the prisoners dilemma is a simplified analog of many of the situations that came up in the ancestral environment, so I am not surprised that we have an instinct for it.</p>\r\n<p>It's nice that we have a hardware implementation of tit-for-tat, but to the extent that we take it as terminal instead of instrumental-in-some-cases, it will make mistakes. It will work well when individuals might choose to defect from the group for greater personal gain, but what if we discover, for example, that some murders are not calculated defections, but failures of self control caused by a bad upbringing and lack of education. What if we then further discover that there is a two-month training course that has a high success rate of turning murderers into productive members of society. When Dan the Deadbeat kills his girlfriend, and the psychologists tell us he is a candidate for the rehab program, we can demand justice like we feel we ought to at a cost of hundreds of thousands of dollars and a good chunk of Dan's life, or we can run Dan thru the two month training course for a few thousand dollars, transforming him into a good, normal person. People who take punishment of criminals as a terminal value will choose prison for Dan, but people with <a href=\"/lw/nb/something_to_protect/\">other interests</a> would say rehab.</p>\r\n<p>One problem with this story is that the two-month murder rehab seems wildly impossible, but so do all of <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Omega's tricks</a>. I think it's good to stress our theories at the limits, they seem to come out stronger, even for normal cases.</p>\r\n<p>I was feeling skeptical about some people's approach to justice theory when I came up with this one, so I was open to changing my understanding of justice. I am now convinced that justice and punishment instincts are instrumental, and only approximations of the correct game theory and utilitarianism. The problem is, while I was convinced, someone who takes justice as terminal, and is not open to the idea that it might be wrong, is absolutely not convinced. They will say \"I don't care if it is more expensive, or that you have come up with something that 'works better', it is our responsibility to the criminal to punish them for their misdeeds.\". Part of the reason for this post is that I don't know what to say to this. All I can do is state the argument that convinced me, ask if they have <a href=\"/lw/nb/something_to_protect/\">something to protect</a>, and feel like I'm <a href=\"/lw/rn/no_universally_compelling_arguments/\">arguing with a rock</a>.</p>\r\n<p>Before anyone who is still with me gets enthusiastic about the idea that knowing a causal history and an instrumentally better way is enough to turn a value into a bias, consider the following:</p>\r\n<h2>Love, Friendship, and Flowers</h2>\r\n<p>See <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">the gift we give to tomorrow</a>. That post contains plausible histories for why we ended up with nice things like love, friendship, and beauty; and hints that could lead you to 'better' replacements made out of game theory and decision theory.</p>\r\n<p>Unlike the other examples, where I felt a great \"Aha!\" and decided to use the superior replacements when appropriate, this time I feel scared. I thought I had it all locked out, but I've found some existential angst lurking in the basement.</p>\r\n<p>Love and such seem like something to protect, like I don't care if there are better solutions to the problem they were built to solve; I don't care if game theory and decision theory leads to more optimal replication. If I'm worried that love will go away, then <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">there's no reason I ought to let it</a>, but these are the same arguments as the people who think justice is terminal. What is the difference that makes it right this time?</p>\r\n<h2><del>Worrying and Conclusion</del></h2>\r\n<p><del> </del></p>\r\n<p><del>One answer to this riddle is that everyone is right with respect to themselves, and there's nothing we can do about disagreements. There's nothing someone who has one interpretation can say to another to justify their values against some objective standard. By the </del><a href=\"/lw/s0/where_recursive_justification_hits_bottom/\"><del>full power of my current understanding</del></a><del>, I'm right, but so is someone who disagrees.</del></p>\r\n<p><del> </del></p>\r\n<p><del>On the other hand, maybe we can do some big million-variable optimization on the contradictory values and heuristics that make up ourselves and come to a reflectively coherent understanding of which are values and which are biases. Maybe none of them have to be biases; it makes sense and seems acceptable that sometimes we will have to go against one of our values for greater gain in another. Maybe I'm asking the wrong question.</del></p>\r\n<p><del> </del></p>\r\n<p><del>I'm confused, what does LW think?</del></p>\r\n<h2>Solution</h2>\r\n<p>I was confused about this for a while; is it just something that we have to (Gasp!) agree to disagree about? Do we have to do a big analysis to decide once and for all which are \"biases\" and which are \"values\"? My favored solution is to dissolve the distinction between biases and values:</p>\r\n<p>All our neat little mechanisms and heuristics make up our values, but they come on a continuum of importance, and some of them sabotage the rest more than others.<br /><br />For example, all those nice things like love and beauty seem very important, and usually don't conflict, so they are closer to values.<br /><br />Things like risk aversion and hindsight bias and such aren't terribly important, but because they prescribe otherwise stupid behavior in the decision theory/epistemological realm, they sabotage the achievement of other bias/values, and are therefore a net negative.<br /><br />This can work for the high-value things like love and beauty and freedom as well: Say you are designing a machine that will achieve many of your values, being biased towards making it beautiful over functional could sabotage achievement of other values. Being biased against having powerful agents interfering with freedom can prevent you from accepting law or safety.<br /><br />So debiasing is knowing how and when to override less important \"values\" for the sake of more important ones, like overriding your aversion to cold calculation to maximize lives saved in a shut up and multiply situation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xknvtHwqvqhwahW8Q": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QGKFjaZNDtJnBTbxS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 23, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "12514", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've seen of people on Lesswrong taking cognitive structures that I consider to be biases as <em>terminal values</em>. Take risk aversion for example:</p>\n<h2 id=\"Risk_Aversion\">Risk Aversion</h2>\n<p>For a rational agent with goals that don't include \"being averse to risk\", risk aversion is a bias. The correct decision theory acts on expected utility, with utility of outcomes and probability of outcomes factored apart and calculated separately. Risk aversion does not factor them.</p>\n<p>EDIT: There is some contention on this. Just substitute \"that thing minimax algorithms do\" for \"risk aversion\" in my writing. /EDIT</p>\n<p>A while ago, I was working through the derivation of A* and minimax planning algorithms from a Bayesian and decision-theoretic base. When I was trying to understand the relationship between them, I realized that strong risk aversion, aka minimax, saves huge amounts of computation compared to the correct decision theory, and actually becomes more optimal as the environment becomes more influenced by rational opponents. The best way win is to deny the opponents any opportunity to weaken you. That's why minimax is a good algorithm for chess.</p>\n<p>Current theories about the origin of our intelligence say that we became smart to outsmart our opponents in complex social games. If our intelligence was built for adversarial games, I am not surprised at risk aversion.</p>\n<p>A better theoretical replacement, and a plausible causal history for why we have the bias instead of the correct algorithm are convincing to me as an argument against risk aversion as a value the way a rectangular 13x7 <a href=\"/lw/sy/sorting_pebbles_into_correct_heaps/\">pebble heap</a> is convincing to a pebble sorter as an argument against the correctness of a heap of 91 pebbles; it seems undeniable, but I don't have access to the hidden values that would say for sure.</p>\n<p>And yet I've seen people on LW state that their \"utility function\" includes risk aversion. Because I don't understand the values involved, all I can do is state the argument above and see if other people are as convinced as me.</p>\n<p>It may <a href=\"/lw/n3/circular_altruism/\">seem silly to take a bias as terminal</a>, but there are examples with similar arguments that are less clear-cut, and some that we take as uncontroversially terminal:</p>\n<h2 id=\"Responsibility_and_Identity\">Responsibility and Identity</h2>\n<p>The feeling that you are responsible for some things and not others, like say, the safety of your family, but not people being tortured in Syria, seems noble and practical. But I take it to be a bias.</p>\n<p>I'm no evolutionary psychologist, but it seems to me that feelings of responsibility are a quick hack to kick you into motion where you can affect the outcome and the utility at stake is large. For the most part, this aligns well with utilitarianism; you usually don't feel responsible for things you can't really affect, like people being tortured in Syria, or the color of the sky. You do feel responsible to pull a passed out kid off the train tracks, but maybe you don't feel responsible to give them some fashion advice.</p>\n<p>Responsibility seems to be built on identity, so it starts to go weird when you identify or don't identify in ways that didn't happen in the ancestral environment. Maybe you identify as a citizen of the USA, but not of Syria, so you feel shame and responsibility about the US torturing people, but the people being tortured in Syria are not your responsibility, even though both cases are terrible, and there is very little you can do about either. A proper utilitarian would feel approximately the same desire to do something about each, but our responsibility hack emphasizes responsibility for the actions of the tribe you identify with.</p>\n<p>You might feel great responsibility to defend your past actions but not those other people, even tho neither is worth \"defending\". A rational agent would learn from both the actions of their own past selves and those of other people without seeking to justify or condemn; they would <em>update and move on</em>. There is no tribal council that will exile you if you change your tune or don't defend yourself.</p>\n<p>You might be appalled that someone wishes to stop feeling responsibility for their past selves; \"but if they don't feel responsibility for their actions, what will prevent them from murdering people, or encourage them to do good?\". A rational utilitarian would do good and not do evil because they wish good and non-evil to be done, instead of because of feelings of responsibility that they don't understand.</p>\n<p>This argument is a little harder to see and possibly a little less convincing, but again I am convinced that identity and responsibility are inferior to utilitarianism, tho they may have seemed almost terminal.</p>\n<h2 id=\"Justice\">Justice</h2>\n<p>Surely justice is a terminal value; it feels so noble to desire it. Again I consider the desire for justice to be a biased heuristic.</p>\n<p>in game theory the best solution for iterated prisoners dilemma is tit-for-tat: cooperate and be nice, but punish defectors. Tit-for-tat looks a lot like our instincts for justice, and I've heard that the prisoners dilemma is a simplified analog of many of the situations that came up in the ancestral environment, so I am not surprised that we have an instinct for it.</p>\n<p>It's nice that we have a hardware implementation of tit-for-tat, but to the extent that we take it as terminal instead of instrumental-in-some-cases, it will make mistakes. It will work well when individuals might choose to defect from the group for greater personal gain, but what if we discover, for example, that some murders are not calculated defections, but failures of self control caused by a bad upbringing and lack of education. What if we then further discover that there is a two-month training course that has a high success rate of turning murderers into productive members of society. When Dan the Deadbeat kills his girlfriend, and the psychologists tell us he is a candidate for the rehab program, we can demand justice like we feel we ought to at a cost of hundreds of thousands of dollars and a good chunk of Dan's life, or we can run Dan thru the two month training course for a few thousand dollars, transforming him into a good, normal person. People who take punishment of criminals as a terminal value will choose prison for Dan, but people with <a href=\"/lw/nb/something_to_protect/\">other interests</a> would say rehab.</p>\n<p>One problem with this story is that the two-month murder rehab seems wildly impossible, but so do all of <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Omega's tricks</a>. I think it's good to stress our theories at the limits, they seem to come out stronger, even for normal cases.</p>\n<p>I was feeling skeptical about some people's approach to justice theory when I came up with this one, so I was open to changing my understanding of justice. I am now convinced that justice and punishment instincts are instrumental, and only approximations of the correct game theory and utilitarianism. The problem is, while I was convinced, someone who takes justice as terminal, and is not open to the idea that it might be wrong, is absolutely not convinced. They will say \"I don't care if it is more expensive, or that you have come up with something that 'works better', it is our responsibility to the criminal to punish them for their misdeeds.\". Part of the reason for this post is that I don't know what to say to this. All I can do is state the argument that convinced me, ask if they have <a href=\"/lw/nb/something_to_protect/\">something to protect</a>, and feel like I'm <a href=\"/lw/rn/no_universally_compelling_arguments/\">arguing with a rock</a>.</p>\n<p>Before anyone who is still with me gets enthusiastic about the idea that knowing a causal history and an instrumentally better way is enough to turn a value into a bias, consider the following:</p>\n<h2 id=\"Love__Friendship__and_Flowers\">Love, Friendship, and Flowers</h2>\n<p>See <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">the gift we give to tomorrow</a>. That post contains plausible histories for why we ended up with nice things like love, friendship, and beauty; and hints that could lead you to 'better' replacements made out of game theory and decision theory.</p>\n<p>Unlike the other examples, where I felt a great \"Aha!\" and decided to use the superior replacements when appropriate, this time I feel scared. I thought I had it all locked out, but I've found some existential angst lurking in the basement.</p>\n<p>Love and such seem like something to protect, like I don't care if there are better solutions to the problem they were built to solve; I don't care if game theory and decision theory leads to more optimal replication. If I'm worried that love will go away, then <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">there's no reason I ought to let it</a>, but these are the same arguments as the people who think justice is terminal. What is the difference that makes it right this time?</p>\n<h2 id=\"Worrying_and_Conclusion\"><del>Worrying and Conclusion</del></h2>\n<p><del> </del></p>\n<p><del>One answer to this riddle is that everyone is right with respect to themselves, and there's nothing we can do about disagreements. There's nothing someone who has one interpretation can say to another to justify their values against some objective standard. By the </del><a href=\"/lw/s0/where_recursive_justification_hits_bottom/\"><del>full power of my current understanding</del></a><del>, I'm right, but so is someone who disagrees.</del></p>\n<p><del> </del></p>\n<p><del>On the other hand, maybe we can do some big million-variable optimization on the contradictory values and heuristics that make up ourselves and come to a reflectively coherent understanding of which are values and which are biases. Maybe none of them have to be biases; it makes sense and seems acceptable that sometimes we will have to go against one of our values for greater gain in another. Maybe I'm asking the wrong question.</del></p>\n<p><del> </del></p>\n<p><del>I'm confused, what does LW think?</del></p>\n<h2 id=\"Solution\">Solution</h2>\n<p>I was confused about this for a while; is it just something that we have to (Gasp!) agree to disagree about? Do we have to do a big analysis to decide once and for all which are \"biases\" and which are \"values\"? My favored solution is to dissolve the distinction between biases and values:</p>\n<p>All our neat little mechanisms and heuristics make up our values, but they come on a continuum of importance, and some of them sabotage the rest more than others.<br><br>For example, all those nice things like love and beauty seem very important, and usually don't conflict, so they are closer to values.<br><br>Things like risk aversion and hindsight bias and such aren't terribly important, but because they prescribe otherwise stupid behavior in the decision theory/epistemological realm, they sabotage the achievement of other bias/values, and are therefore a net negative.<br><br>This can work for the high-value things like love and beauty and freedom as well: Say you are designing a machine that will achieve many of your values, being biased towards making it beautiful over functional could sabotage achievement of other values. Being biased against having powerful agents interfering with freedom can prevent you from accepting law or safety.<br><br>So debiasing is knowing how and when to override less important \"values\" for the sake of more important ones, like overriding your aversion to cold calculation to maximize lives saved in a shut up and multiply situation.</p>", "sections": [{"title": "Risk Aversion", "anchor": "Risk_Aversion", "level": 1}, {"title": "Responsibility and Identity", "anchor": "Responsibility_and_Identity", "level": 1}, {"title": "Justice", "anchor": "Justice", "level": 1}, {"title": "Love, Friendship, and Flowers", "anchor": "Love__Friendship__and_Flowers", "level": 1}, {"title": "Worrying and Conclusion", "anchor": "Worrying_and_Conclusion", "level": 1}, {"title": "Solution", "anchor": "Solution", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "125 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 125, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mMBTPTjRbsrqbSkZE", "4ZzefKQwAtMo5yp99", "SGR4GxFK7KmW7ckCB", "6ddcsdA2c2XpNpE5x", "PtoQdG7E8MxYJrigu", "pGvyqAQw6yqTjpKf4", "pK4HTxuv6mftHXWC3", "C8nEXTcjZb9oauTCW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-30T21:14:28.774Z", "modifiedAt": null, "url": null, "title": "Waterfall Ethics", "slug": "waterfall-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:55.768Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "calef", "createdAt": "2011-01-26T21:41:24.522Z", "isAdmin": false, "displayName": "calef"}, "userId": "voRPyQHqt6FHmXkZe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aZad2WBDhJQFHHKSa/waterfall-ethics", "pageUrlRelative": "/posts/aZad2WBDhJQFHHKSa/waterfall-ethics", "linkUrl": "https://www.lesswrong.com/posts/aZad2WBDhJQFHHKSa/waterfall-ethics", "postedAtFormatted": "Monday, January 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Waterfall%20Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWaterfall%20Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaZad2WBDhJQFHHKSa%2Fwaterfall-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Waterfall%20Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaZad2WBDhJQFHHKSa%2Fwaterfall-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaZad2WBDhJQFHHKSa%2Fwaterfall-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 621, "htmlBody": "<p>I recently read Scott Aaronson's \"Why Philosophers Should Care About Computational Complexity\" (http://arxiv.org/abs/1108.1791), which has a wealth of interesting thought-food.&nbsp; Having chewed on it for a while, I've been thinking through some of the implications and commitments of a computationalist worldview, which I don't think is terribly controversial around here (there's a brief discussion in the paper about the Waterfall Argument, and its worth reading if you're unfamiliar with either it or the Chinese room thought experiment).</p>\n<p>That said, suppose we ascribe to a computationalist worldview.&nbsp; Further suppose that we have a simulation of a human running on some machine.&nbsp; Even further suppose that this simulation is torturing the human through some grisly means.</p>\n<p>By our supposed worldview, our torture simulation is reducible to some finite state machine, say a one tape turing machine.&nbsp; This one tape turing machine representation, then, must have some initial state.</p>\n<p>&nbsp;</p>\n<p>My first question: Is more 'harm' done in actually carrying out the computation of the torture simulation on our one tape turing machine than simply writing out the initial state of the torture simulation on the turing machine's tape?</p>\n<p>&nbsp;</p>\n<p>The computation, and thus the simulation itself, are uniquely specified by that initial encoding.&nbsp; My gut feeling here is that no, no more harm is done in actually carrying out the computation, because the 'torture' that occurs is a structural property of the encoding.&nbsp; This might lead to perhaps ill-formed questions like \"But when does the 'torture' actually 'occur'?\" for some definition of those words.&nbsp; But, like I said, I don't think that question makes sense, and is more indicative of the difficulty in thinking about something like our subjective experience as something reducible to deterministic processes than it is a criticism of my answer.</p>\n<p>If one thinks more harm is done in carrying out the simulation, then is twice as much harm done by carrying out the simulation twice?&nbsp; Does the representation of the simulation matter?&nbsp; If I go out to the beach and arrange sea shells in a way that mimics the computation of the torture, has the torture 'occurred'?</p>\n<p>&nbsp;</p>\n<p>My second question:&nbsp; If the 'harm' occurring in the simulation is uniquely specified by the initial state of the Turing machine, how are we to assign moral weight (or positive/negative utility, if you prefer) to actually carrying out this computation, or even the existence of the initial state?</p>\n<p>&nbsp;</p>\n<p>As computationalists, we agree that the human being represented by the one tape turing machine is feeling just as real pain as we are.&nbsp; But (correct me if I'm wrong), it seems like we're committed to the idea that the 'harm' occurring in the torture simulation is a property of the initial state, and this initial state exists independent of us actually enumerating that state.&nbsp; That is, there is some space of all possible simulations of a human as represented by encodings on a one tape turing machine.&nbsp;</p>\n<p>Is the act of specifying one of those states 'wrong'?&nbsp; Does the act of recognizing such a possible space of encodings realize all of them, and thus cause an uncountable number of tortures and pleasures?</p>\n<p>&nbsp;</p>\n<p>I don't think so.&nbsp; That just seems silly.&nbsp; But this also seems to rob a simulated human of any moral worth.&nbsp; Which is kinda contradictory--we recognize that the pain a simulated human feels is real, yet we don't assign any utility to it.&nbsp; Again, I don't think my answers are *right*, they were just my initial reactions.&nbsp; Regardless of how we answer either of my questions, we seem committed to strange positions.</p>\n<p>Initially, the whole exercise was looking for a way to dodge the threats of some superintelligent malevolent AI simulating the torture of copies of me.&nbsp; I don't think I've actually successfully dodged that threat, but it was interesting to think about.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aZad2WBDhJQFHHKSa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "12515", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-30T23:36:12.764Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge UK ", "slug": "meetup-cambridge-uk-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rebellionkid", "createdAt": "2011-06-20T09:26:46.768Z", "isAdmin": false, "displayName": "rebellionkid"}, "userId": "ygYCk3eXnJwt6p3o4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cy3d489fMibzS7q9B/meetup-cambridge-uk-0", "pageUrlRelative": "/posts/cy3d489fMibzS7q9B/meetup-cambridge-uk-0", "linkUrl": "https://www.lesswrong.com/posts/cy3d489fMibzS7q9B/meetup-cambridge-uk-0", "postedAtFormatted": "Monday, January 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20UK%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20UK%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcy3d489fMibzS7q9B%2Fmeetup-cambridge-uk-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20UK%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcy3d489fMibzS7q9B%2Fmeetup-cambridge-uk-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcy3d489fMibzS7q9B%2Fmeetup-cambridge-uk-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/6n'>Cambridge UK </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 February 2012 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">JCR, Trinity College, Cambridge, CB2 1TQ, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at the Great Gate if you dont know where the JCR is. Great Gate is on St. John's Street opposite the bookshop \"Heffers\".</p>\n\n<p>Topic this week: \"Is AI the biggest class of x-risk?\"</p>\n\n<p>Hopefully this should now be a regular meeting. Join the google group at <a href=\"http://groups.google.com/group/cambridgelesswrong\" rel=\"nofollow\">http://groups.google.com/group/cambridgelesswrong</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/6n'>Cambridge UK </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cy3d489fMibzS7q9B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.406009015982121e-07, "legacy": true, "legacyId": "12516", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge_UK_\">Discussion article for the meetup : <a href=\"/meetups/6n\">Cambridge UK </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 February 2012 11:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">JCR, Trinity College, Cambridge, CB2 1TQ, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet at the Great Gate if you dont know where the JCR is. Great Gate is on St. John's Street opposite the bookshop \"Heffers\".</p>\n\n<p>Topic this week: \"Is AI the biggest class of x-risk?\"</p>\n\n<p>Hopefully this should now be a regular meeting. Join the google group at <a href=\"http://groups.google.com/group/cambridgelesswrong\" rel=\"nofollow\">http://groups.google.com/group/cambridgelesswrong</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge_UK_1\">Discussion article for the meetup : <a href=\"/meetups/6n\">Cambridge UK </a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge UK ", "anchor": "Discussion_article_for_the_meetup___Cambridge_UK_", "level": 1}, {"title": "Discussion article for the meetup : Cambridge UK ", "anchor": "Discussion_article_for_the_meetup___Cambridge_UK_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T03:43:51.973Z", "modifiedAt": null, "url": null, "title": "Does functionalism imply dualism? ", "slug": "does-functionalism-imply-dualism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:30.155Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iHMy2X9mqQT6ayf9f/does-functionalism-imply-dualism", "pageUrlRelative": "/posts/iHMy2X9mqQT6ayf9f/does-functionalism-imply-dualism", "linkUrl": "https://www.lesswrong.com/posts/iHMy2X9mqQT6ayf9f/does-functionalism-imply-dualism", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20functionalism%20imply%20dualism%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20functionalism%20imply%20dualism%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiHMy2X9mqQT6ayf9f%2Fdoes-functionalism-imply-dualism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20functionalism%20imply%20dualism%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiHMy2X9mqQT6ayf9f%2Fdoes-functionalism-imply-dualism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiHMy2X9mqQT6ayf9f%2Fdoes-functionalism-imply-dualism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2430, "htmlBody": "<p><em>This post follows on from <a href=\"/r/discussion/lw/9mw/personal_research_update/\">Personal research update</a>, and is followed by <a href=\"/lw/9pc/state_your_physical_account_of_experienced_color/\">State your physical explanation of experienced color</a>. </em></p>\n<p>In <a href=\"/r/discussion/lw/9mw/personal_research_update/\">a recent post</a>, I claimed that functionalism about consciousness implies dualism. Since most functionalists think their philosophy is an alternative to dualism, I'd better present an argument.</p>\n<p>But before I go further, I'll link to orthonormal's series on dissolving the problem of \"Mary's Room\": <a href=\"/lw/5n9/seeing_red_dissolving_marys_room_and_qualia/\">Seeing Red: Dissolving Mary's Room and Qualia</a>, <a href=\"/lw/5op/qualia_strike_back/\">A Study of Scarlet: The Conscious Mental Graph</a>, <a href=\"/lw/5ot/nature_red_in_truth_and_qualia/\">Nature: Red in Truth, and Qualia</a>. Mary's Room is one of many thought experiments bandied about by philosophers in their attempts to say whether or not colors (and other qualia) are a problem for materialism, and orthonormal presents a computational attempt to get around the problem which is a good representative of the functionalist style of thought. I won't have anything to say about those articles at this stage (maybe in comments), but they can serve as an example of what I'm talking about.&nbsp;</p>\n<p>Now, though it may antagonize some people, I think it is best to start off by stating my position plainly and bluntly, rather than starting with a neutral discussion of what functionalism is and how it works, and then seeking to work my way from there to the unpopular conclusion. I will stick to the example of color to make my points - apologies to blind and colorblind readers.</p>\n<p>My fundamental thesis is that color manifestly does exist - there are such things as shades of green, shades of red, etc - and that it manifestly does <em>not</em> exist in any standard sort of physical ontology. In an arrangement of point particles in space, there are no shades of green present. This is obviously true, and it's equally obvious for more complicated ontologies like fields, geometries, wavefunction multiverses, and so on. It's even part of the history of physics; even Galileo distinguished between primary qualities like location and shape, and secondary qualities like color. Primary qualities are out there and objectively present in the external world, secondary qualities are only in us, and physics will only concern itself with primary qualities. The ontological world of physical theory is colorless. (We may call light of a certain wavelength green light or red light, but that is because it produces an experience of seeing green or seeing red, not because the light itself is green or red in the original sense of those words.) And what has happened due to the progress of the natural sciences is that we now say that experiences are in brains, and brains are made of atoms, and atoms are described by a physics which does not contain color. So the secondary qualities have vanished entirely from this picture of the world; there is no opportunity for them to exist within us, because we are made of exactly the same stuff as the external world.</p>\n<p>Yet the \"secondary qualities\" are <em>there</em>. They're all around us, in every experience. It really is this simple: colors exist in reality, they don't exist in theory, therefore the theory needs to be augmented or it needs to be changed. Dualism is an augmentation. My speculations about quantum monads are supposed to pave the way for a change. But I won't talk about that option here. Instead, I will try to talk about theories of consciousness which are meant to be compatible with physicalism - functionalism is one such theory.</p>\n<p>Such a theory will necessarily present a candidate, however vague, for the physical correlate of an experience of color. One can then say that color exists without having to add anything to physics, because the color just <em>is</em> the proposed physical correlate. This doesn't work because the situation hasn't changed. If all you have are point particles whose only property is location, then individual particles do not have the property of being colored, nor do they have that property in conjunction. Identifying a physical correlate simply picks out a particular set of particles and says \"there's your experience of color\". But there's still nothing there that is green or red. You may accustom yourself to thinking of a particular material event, a particular rearrangement of atoms in space, as being the color, but that's just the power of habitual association at work. You are introducing into your concept of the event a property that is not inherently present in it.</p>\n<p>It may be that one way people manage to avoid noticing this, is by an incomplete chain of thought. I might say: none of the objects in your physical theory are green. The happy materialist might say: but those aren't the things which are truly green in the sense you care about; the things which are green are parts of experiences, not the external objects. I say: fine. But experiences have to exist, right? And you say that physics is everything. So that must mean that experiences are some sort of physical object, and so it will be just as impossible for them to be truly green, given the ontological primitives we have to work with. But for some reason, this further deduction isn't made. Instead, it is accepted that objects in physical space aren't really green, but the objects of experience exist in some other \"space\", the space of subjective experience, and... it isn't explicitly said that objects there can be truly green, but somehow this difference between physical space and subjective space seems to help people be dualists without actually noticing it.</p>\n<p>It is true that color exists in this context - a subjective space. Color always exists as part of an \"experience\". But physical ontology doesn't contain subjective space or conscious experience any more than it does contain color. What it <em>can</em> contain, are state machines which are structurally isomorphic to these things. So here we can finally identify how a functionalist theory of consciousness works psychologically: You single out some state machines in your physical description of the brain (like the networks in orthonormal's sequence of posts); in your imagination, you associate consciousness with certain states of such state machines, on the basis of structural isomorphism; and now you say, conscious states <em>are</em> those physical states. Subjective space <em>is</em> some neural topographic map, the subjectively experienced body <em>is</em> the sensorimotor homunculus, and so forth.</p>\n<p>But if we stick to any standard notion of physical theory, all those brain parts still don't have any of the properties they need. There's no color there, there's no other space there, there's no observing agent. It's all just large numbers of atoms in motion. No-one is home and nothing is happening to them.</p>\n<p>Clearly it is some sort of progress to have discovered, in one's physical picture of the world, the possibility of entities which are roughly isomorphic to experiences, colors, etc. But they are still not the same thing. Most of the modern turmoil of ideas about consciousness in philosophy and science is due to this gap - attempts to deny it, attempts to do without noticing it, attempts to force people to notice it. orthonormal's sequence, for example, seems to be an attempt to exhibit a cognitive model for experiences and behaviors that you would expect if color exists, without having to suppose that color actually exists. If we were talking about a theoretical construct, this would be fine. We are under no obligation to believe that phlogiston exists, only to explain why people once talked about it.</p>\n<p>But to extend this attitude to something that most of us are directly experiencing in almost every waking moment, is ... how can I put this? It's really something. I'd call it an act of intellectual desperation, except that people don't seem to <em>feel</em> desperate when they do it. They are just patiently explaining, recapitulating and elaborating, some \"aha\" moment they had back in their past, when functionalism made sense to them. My thesis is certainly that this sense of insight, of having dissolved the problem, is an illusion. The genuineness of the isomorphism between conscious state and coarse-grained physical state, and the work of several generations of materialist thinkers to develop ways of speaking which smoothly promote this isomorphism to an identity, combine to provide the sense that no problem remains to be solved. But all you have to do is attend for a moment to experience itself, and then to compare that to the picture of billions of colorless atoms in intricate motion through space, to realize that this is still dualism.</p>\n<p>I promised not to promote the monads, but I will say this. The way to avoid dualism is to first understand consciousness as it is in itself, without the presupposition of materialism. Observe the structure of its states and the dynamics of its passage. That is what phenomenology is about. Then, sketch out an ontology of what you have observed. It doesn't have to contain everything in infinite detail, it can overlook some features. But I would say that at a minimum it needs to contain the triad of subject-object-aspect (which appears under various names in the history of philosophy). There are objects of awareness, they are being experienced within a common subjective space, and they are experienced in a certain aspect. Any theory of reality, whether or not it is materialist, must contain such an entity in order to be true.</p>\n<p>The basic entity here is the experiencing subject. Conscious states are its states. And now we can begin to tackle the ontological status of state machines, as a candidate for the ontological category to which conscious beings belong.</p>\n<p>State machines are abstracted descriptions. We say there's a thing, it has a set of possible states; here are the allowed transitions between them, and the conditions under which those transitions occur. Specify all that and we have specified a state machine. We don't care about why those are the states or why the transitions occur; those are irrelevant details.</p>\n<p>A very simple state machine might be denoted by the state transition network \"1&lt;-&gt;2\". There's a state labeled 1 and another state labeled 2. If the machine is in state 1, it proceeds to state 2, and the reverse is also true. This state machine is realized wherever you have something that oscillates between two states without stopping in either. First the earth is close to the sun, then it is far from the sun, then it is close again... The Earth in its orbit instantiates the state machine \"1&lt;-&gt;2\". I get involved with Less Wrong, then I quit for a while, then I come back... My Internet habits also instantiate the state machine \"1&lt;-&gt;2\".</p>\n<p>A computer program is exactly like this, a state machine of great complexity (and usually its state transition rules contain some dependence on external conditions, like user input) which has been physically instantiated for use. But one cannot claim that its states have any intrinsic meaning, any more than I can claim that the state 1 in the oscillating state machine is intrinsically about the earth being close to the sun. This is not true, even if I write down the state transition network in the form \"CloseToTheSun&lt;-&gt;FarFromTheSun\".</p>\n<p>This is another ontological deficiency of functionalism. Mental states have meanings, thoughts are always about something, and what they are about is not the result of convention or of the needs of external users. This is yet another clue that the ontological status of conscious states is special, that their \"substance\" matters to what they are. Of course, this is a challenge to the philosophy which says that a detailed enough simulation of a brain will create a conscious person, regardless of the computational substrate. The only reason people believe this, is because they believe the brain itself is not a special substrate. But this is a judgment made on the basis of science that is still at a highly incomplete stage, and certainly I expect science to tell us something different by the time it's finished with the brain. The ontological problems of functionalism provide a strong apriori reason for this expectation.</p>\n<p>What is more challenging is to form a conception of the elementary parts and relations that could form the basis of an alternative ontology. But we have to do this, and the impetus has to come from a phenomenological ontology of consciousness that is as precise as possible. Fortunately, a great start was made on this about 100 years ago, in the heyday of phenomenology as a philosophical movement.</p>\n<p>A conscious mind is a state machine, in the sense that it has states and transitions between them. The states also have structure, because conscious experiences do have parts. But the ontological ties that combine those parts into the whole are poorly apprehended by our current concepts. When we try to reduce them to nothing but causal coupling or to the proximity in space of presumed physical correlates of those parts, we are, I believe, getting it wrong. Clearly cause and effect operates in the realm of consciousness, but it will take great care to state precisely and correctly the nature of the things which are interacting and the ways in which they do so. Consider the ability to tell apart different shades of color. It's not just that the colors are there; we know that they are there, and we are able to tell them apart. This implies a certain amount of causal structure. But the perilous step is to focus only on that causal structure, detach it from considerations of how things appear to be in themselves, and instead say \"state machine, neurons doing computations, details interesting but not crucial to my understanding of reality\". Somehow, in trying to understand conscious cognition, we must remain in touch with the ontology of consciousness as partially revealed in consciousness itself. The things which do the conscious computing must be things with the properties that we see in front of us, the properties of the objects of experience, such as color.</p>\n<p>You know, color - authentic original color - has been banished from physical ontology for so long, that it sounds a little mad to say that there might be a physical entity which is actually green. But there has to be such an entity, whether or not you call it physical. Such an entity will always be embedded in a larger conscious experience, and that conscious experience will be embedded in a conscious being, like you. So we have plenty of clues to the true ontology; the clues are right in front of us; we're subjectively made of these clues. And we will not truly figure things out, unless we remain insistent that these inconvenient realities are in fact real.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iHMy2X9mqQT6ayf9f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": -5, "extendedScore": null, "score": 8.406965938295737e-07, "legacy": true, "legacyId": "12536", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jroCHyDC6XHomeuW7", "jJJLCGHDYyc9XbHwX", "3wYjyQ839MDsZ6E3L", "pi5DAEZWJK3c9NAhW", "gyz2MsHM9GKxX62hj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T05:16:56.824Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Fallacies of Compression", "slug": "seq-rerun-fallacies-of-compression", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:56.816Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5GiKLmWEh9z2DrdSq/seq-rerun-fallacies-of-compression", "pageUrlRelative": "/posts/5GiKLmWEh9z2DrdSq/seq-rerun-fallacies-of-compression", "linkUrl": "https://www.lesswrong.com/posts/5GiKLmWEh9z2DrdSq/seq-rerun-fallacies-of-compression", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Fallacies%20of%20Compression&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Fallacies%20of%20Compression%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GiKLmWEh9z2DrdSq%2Fseq-rerun-fallacies-of-compression%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Fallacies%20of%20Compression%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GiKLmWEh9z2DrdSq%2Fseq-rerun-fallacies-of-compression", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GiKLmWEh9z2DrdSq%2Fseq-rerun-fallacies-of-compression", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p>Today's post, <a href=\"/lw/nw/fallacies_of_compression/\">Fallacies of Compression</a> was originally published on 17 February 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You have only one word, but there are two or more different things-in-reality, so that all the facts about them get dumped into a single undifferentiated mental bucket. It's part of a detective's ordinary work to observe that Carol wore red last night, or that she has black hair; and it's part of a detective's ordinary work to wonder if maybe Carol dyes her hair. But it takes a subtler detective to wonder if there are two Carols, so that the Carol who wore red is not the same as the Carol who had black hair.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/9jq/seq_rerun_replace_the_symbol_with_the_substance/\">Replace the Symbol with the Substance</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5GiKLmWEh9z2DrdSq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 8.407325649148352e-07, "legacy": true, "legacyId": "12537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y5MxoeacRKKM3KQth", "44Bn4QimK82cvXKWg", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T05:45:35.817Z", "modifiedAt": null, "url": null, "title": "Against Utilitarianism: Sobel's attack on judging lives' goodness", "slug": "against-utilitarianism-sobel-s-attack-on-judging-lives", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:17.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ovLpQqpRLtpXPdKKP/against-utilitarianism-sobel-s-attack-on-judging-lives", "pageUrlRelative": "/posts/ovLpQqpRLtpXPdKKP/against-utilitarianism-sobel-s-attack-on-judging-lives", "linkUrl": "https://www.lesswrong.com/posts/ovLpQqpRLtpXPdKKP/against-utilitarianism-sobel-s-attack-on-judging-lives", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Against%20Utilitarianism%3A%20Sobel's%20attack%20on%20judging%20lives'%20goodness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgainst%20Utilitarianism%3A%20Sobel's%20attack%20on%20judging%20lives'%20goodness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FovLpQqpRLtpXPdKKP%2Fagainst-utilitarianism-sobel-s-attack-on-judging-lives%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Against%20Utilitarianism%3A%20Sobel's%20attack%20on%20judging%20lives'%20goodness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FovLpQqpRLtpXPdKKP%2Fagainst-utilitarianism-sobel-s-attack-on-judging-lives", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FovLpQqpRLtpXPdKKP%2Fagainst-utilitarianism-sobel-s-attack-on-judging-lives", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3909, "htmlBody": "<p>Luke tasked me with researching the following question</p>\n<blockquote>\n<p>I&lsquo;d like to know if anybody has come up with a good response to any of the objections to &rsquo;full information&rsquo; or &lsquo;ideal preference&rsquo; theories of value given in Sobel (1994). (My impression is &ldquo;no.&rdquo;)</p>\n</blockquote>\n<p>The paper in question is David Sobel&rsquo;s 1994 paper <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">&ldquo;Full Information Accounts of Well-Being&rdquo;</a> (<em>Ethics</em> 104, no. 4: 784&ndash;810) (his 1999 paper, &ldquo;Do the desires of rational agents converge?&rdquo;, is directed against a different kind of convergence and won&rsquo;t be discussed here).</p>\n<p>The starting point is Brandt&rsquo;s 1979 book where he describes his version of a utilitarianism in which utility is the degree of satisfaction of the desires of one&rsquo;s ideal &lsquo;fully informed&rsquo; self, and Sobel also refers to the 1986 Railton apologetic. (LWers will note that this kind of utilitarianism sounds very similar to <a href=\"http://intelligence.org/upload/CEV.html\">CEV</a> and hence, any criticism of the former may be a valid criticism of the latter.) I&rsquo;ll steal entirely the opening to Mark C Murphy&rsquo;s 1999 paper, <a href=\"http://www.gwern.net/docs/1999-murphy.pdf\">&ldquo;The Simple Desire-Fulfillment Theory&rdquo;</a> (rejecting any hypotheticals or counterfactuals in desire utilitarianism), since he covers all the bases (for even broader background, see the Tanner Lecture <a href=\"http://www.tannerlectures.utah.edu/lectures/documents/Scanlon98.pdf\">&ldquo;The Status of Well-Being&rdquo;</a>):</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>An account of well-being that [Derek] Parfit labels the &lsquo;desire-fulfillment&rsquo; theory (1984, 493) has gained a great deal of support as the most plausible account of what makes a subject well-off. According to the desire-fulfillment, or DF, theory, an agent&rsquo;s well-being is constituted by the obtaining of states of affairs that are desired by that agent.<sup>1</sup> Importantly, though, while all DF theorists affirm that an account of what makes an agent well-off must ultimately refer to desire, there now appears to be a consensus among those defending DF theories that it is not the satisfaction of the agent&rsquo;s <em>actual</em> desires that constitutes the agent&rsquo;s well-being, but rather the satisfaction of those desires that the agent would have in what I will call a &lsquo;hypothetical desire situation.&rsquo; Just as Rawls holds (1971, 12) that the principles of right are those that would be unanimously chosen in a hypothetical choice situation, that is, a setting optimal for choosing such principles, defenders of DF theory hold that an agent&rsquo;s good is what he or she would desire in a hypothetical desire situation, that is, a setting optimal for desiring.<sup>2</sup> While the precise nature of the hypothetical desire situation is a matter of debate among DF theorists, all of them seem to agree that any adequate DF theory will incorporate a strong information condition into the hypothetical desire situation. In treating of the concept of an individual&rsquo;s good, Sidgwick writes:</p>\n<blockquote>\n<p>It would seem. . . that if we interpret the notion &lsquo;good&rsquo; in relation to &lsquo;desire,&rsquo; we must identify it not with the actually desired, but rather with the desirable:&mdash;meaning by &lsquo;desirable&rsquo; not necessarily &lsquo;what ought to be desired&rsquo; but what would be desired. . . if it were judged attainable by voluntary action, supposing the desirer to possess a perfect forecast, emotional as well as intellectual, of the state of attainment or fruition (1981, 110&ndash;111).</p>\n</blockquote>\n<p>Brandt writes that a state of affairs belongs to an agent&rsquo;s welfare only if it is such that &ldquo;that person would want it if he were fully rational&rdquo; (1979, 268); an agent&rsquo;s desire is rational, on Brandt&rsquo;s view,</p>\n<blockquote>\n<p>if it would survive or be produced by careful &lsquo;cognitive psychotherapy&rsquo; [where cognitive psychotherapy is the &lsquo;whole process of confronting desires with relevant information.&rsquo;]. . . I shall call a desire &lsquo;irrational&rsquo; if it cannot survive compatibly with clear and repeated judgments about established facts. What this means is that rational desire. . . can confront, or will even be produced by, awareness of the truth (1979, 113).</p>\n</blockquote>\n<p>And Railton has argued that we should consider an agent&rsquo;s good to be &ldquo;what he would want himself to want. . . were he to contemplate his present situation from a standpoint fully and vividly informed about himself and his circumstances, and entirely free of cognitive error or lapses of instrumental rationality&rdquo; (1986a, 16).</p>\n</blockquote>\n<h1 id=\"overview\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Overview</a></h1>\n<blockquote>\n<p>There are at least four general strategies one could take in arguing that such an informed viewpoint is inadequate in capturing and commensurating what is in an agent&rsquo;s interests.</p>\n<ol style=\"list-style-type: decimal\">\n<li>First, one could argue that the notion of a fully informed self is a chimera. This would likely involve the worry that from the fact that any of the lives that one is to assess the value of must be in some sense available to one (otherwise it could not be a valuable life for one to live) it does not follow that all of them together must be available to one&rsquo;s consciousness. <em>To make good this suggestion against the full information account one would have to provide reasons to think there are substantive worries about uniting the experience of all lives one could lead into a single consciousness.</em></li>\n<li>Second, one could argue that even in cases in which an agent is adequately informed of the different life paths she is choosing between, there is no single pro-attitude, such as preferring, which appropriately measures the value of the diverse kinds of goods available to an agent&hellip;The things that sensibly elicit delight are not generally the same things that merit respect or admiration. Our capacity for articulating our attitudes depends upon our understandings of our attitudes, which are informed by norms for valuation.</li>\n<li>Third, one could argue that a vivid presentation of some experiences which could be part of one&rsquo;s life could prove so disturbing or alluring as to skew any further reflection about what option to choose. Allan Gibbard has suggested the example of &ldquo;a more vivid realization of what peoples&rsquo; innards are like&rdquo; causing a &ldquo;debilitating neurosis&rdquo; which prevents me from eating in public. [cf. Bostrom&rsquo;s information-harms typology: &lsquo;evocation hazard&rsquo;; personally, I would use something like &lsquo;brainwashing&rsquo; or war &amp; holocausts]</li>\n<li>Fourth, one could worry against naturalistic versions of the full information account that the purportedly naturalistically described informed viewpoint essentially invokes unreduced normative notions. [Naturalistic versions seem to assume non-physical definitions, like &lsquo;ideal set of information&rsquo;, and hence smuggle in non-naturalistic beliefs]</li>\n</ol></blockquote>\n<p>Emphasis added; Sobel pursues line of objection #1.</p>\n<h2 id=\"the-argument\"><a href=\"#TOC\"><span class=\"header-section-number\">1.1</span> The argument</a></h2>\n<p>I will try to reconstruct the argument in something more closely approximating propositional logic so it&rsquo;s easier to classify any criticism of Sobel based on what premise or inference they are attacking. The following is based on my reading pg 796&ndash;797,801&ndash;808; I omit all the examples, and some of the weaker tangential arguments. (For example, the suggestion that the ideal moral system may go insane from the difficulty of choices or it will despise us for being so pathetic and wish us dead (pg807), which are obvious anthropomorphisms.)</p>\n<ol style=\"list-style-type: decimal\">\n<li>The ideal moral system may not err</li>\n<li>Every possible life judgement must be judged by an agent</li>\n<li>An agent either lives that possible life, or it does not live it</li>\n<li>\n<p>If the agent does not live the possible life:</p>\n<ol style=\"list-style-type: decimal\">\n<li>If the agent does not live the possible life, it does not live the life&rsquo;s experiences</li>\n<li>Experiences may contain otherwise-unobtainable information [&lsquo;revelations&rsquo;]</li>\n<li>A judgement based on incomplete information may err</li>\n<li>The ideal moral system will not use an agent that lives the possible life (1, 4.1&ndash;4.3)</li>\n</ol></li>\n<li>\n<p>If the agent does live the possible life, it is either a &lsquo;serial&rsquo; agent or an &lsquo;amnesia&rsquo; agent</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p>Serial; the agent either lives the same life or a different life:</p>\n<p>The same life:</p>\n<ol style=\"list-style-type: decimal\">\n<li>To live the same possible life as that possible life, the agent must know only the same things as the possible life</li>\n<li>Most possible lives do not know what it is to live a different life</li>\n<li>If the agent knows only the same things as the possible life does, then in most lives it cannot know what it is to live an additional life</li>\n<li>If one does not know what additional lives are like to live, one may err in assessing one&rsquo;s own life</li>\n<li>The serial agent may live a life which does not know what other lives are like</li>\n<li>The serial agent may err</li>\n<li>The ideal moral system will not use a serial agent which knows the same as the possible life (1, 4.3, 5.1.1.1&ndash;6)</li>\n</ol>\n<p>A different life:</p>\n<ol style=\"list-style-type: decimal\">\n<li>If the agent knows more or less things than the possible life, it is not identical to the possible life</li>\n<li>If it is not identical to the possible life, it may experience or act differently</li>\n<li>If may experience things differently or act differently, it may judge experiences or judge acts differently</li>\n<li>If it may judge experiences or acts differently, then it may err</li>\n<li>The ideal moral system will not use a serial agent which knows more or less than the possible life (1,4.3,5.1.2.1&ndash;4)</li>\n</ol></li>\n<li>\n<p>Amnesia:</p>\n<ol style=\"list-style-type: decimal\">\n<li>If the agent is an amnesia agent, it will work under incomplete information due to forgetting</li>\n<li>Each amnesia period will form a different judgement</li>\n<li>These judgements may differ</li>\n<li>Differing judgements may lead to error</li>\n</ol>\n<p>Rebuttals rejecting 5.2.4:</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p>The judgements can be weighed into a final correct judgement by an unspecified algorithm</p>\n<ul>\n<li>But - how does this work, exactly? What is the life&rsquo;s utility over its span?</li>\n</ul>\n</li>\n<li>Only one (&lsquo;allegedly temporally privileged&rsquo;) judgement is used, and a judgement can&rsquo;t differ with itself</li>\n<li>\n<p>They will not differ, as the fully informed agent at any period will agree with itself at all other periods</p>\n<ul>\n<li>But - how would one prove such a thing? It is &lsquo;indeterminate&rsquo; and &lsquo;unlikely&rsquo;.</li>\n</ul>\n</li>\n<li>The ideal moral system will not use an amnesiac agent (1, 5.2.1&ndash;4)</li>\n</ol></li>\n</ol></li>\n<li>The ideal moral system will use neither a serial or amnesiac agent (5.1.7, 5.1.2, 5.2.5)</li>\n<li>The ideal moral system will not use an agent</li>\n<li>\n<p>The ideal moral system will not judge lives</p>\n</li>\n</ol>\n<h3 id=\"analysis\"><a href=\"#TOC\"><span class=\"header-section-number\">1.1.1</span> Analysis</a></h3>\n<p>Broken down like this, we can see a number of ways to strengthen or attack it. For example, we can strengthen the attack on serial agents who lead different lives (5.1.2) by defining agents and lives as Turing machines and then invoking <a href=\"http://en.wikipedia.org/wiki/Rice%27s_theorem\">Rice&rsquo;s theorem</a> (the generalized Halting Theorem) - obviously &lsquo;goodness of life&rsquo; is a nontrivial predicate and so there will be Turing machines for whom the question is uncomputable.</p>\n<p>This strengthening illustrates a possible attack, on the key premise 1: &ldquo;the system must <em>not</em> err&rdquo;. Obviously, if the ethical system may err, all the arguments collapse: it&rsquo;s fine for an amnesia agent to sometimes contradict itself, it&rsquo;s fine for a too-knowledgeable serial agent to not act the same, etc.</p>\n<p>But our strengthening of 5.1.2 to Rice&rsquo;s theorem would seem to work for all the proposed agents (&lsquo;the amnesia agent will both work under incomplete information <em>and</em> be confronted with uncomputable lives&rsquo;), which is not an issue. What is an issue is that this would seem to work for any agent implementing any nontrivial ethical system - a utilitarian agent (&lsquo;you discover a planet-destroying bomb - which is triggered by the halting of a particular Turing machine&hellip;&rsquo;) or many deontological agents (&lsquo;your computer claims to be a conscious being and you must not reboot it, because that would violate your deontological respect for personal autonomy and the right to live; you try to check its claims but&hellip;&rsquo;).</p>\n<p>An argument which proves too much is not a good argument, and it seems to me that we can construct situations for agents running any moral system where they may err, if only through extreme brute force skeptical claims like the Simulation Hypothesis. (I say &lsquo;may&rsquo; because Sobel&rsquo;s arguments above do not seem to show that various kinds of agents <em>will</em> err, which would be very difficult to prove.)</p>\n<p>Given this, we can reject premise 1 and are now free to pick from any of the kinds of agents discussed, since now that they are free to err, they are also free to have incomplete information, not attempt to crack uncomputable cases, etc. (To quote Murphy pg 23, &ldquo;It would imply the indefensibility of DF [desire-fulfillment] theory if, that is, their hypothetical desire situations incorporated a <em>full</em> information condition, which is the target of Sobel&rsquo;s and Rosati&rsquo;s criticisms. If a theory&rsquo;s information condition were more modest, perhaps it would escape those criticisms.&rdquo;)</p>\n<h1 id=\"the-literature\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> The literature</a></h1>\n<p>Sobel&rsquo;s paper has only occasionally been grappled with or defended; usually it is described as illustrating some serious problems with reflective theories, but not much more.</p>\n<p>Support:</p>\n<ul>\n<li>\n<p>Loeb, Don 1995: <a href=\"http://www.gwern.net/docs/1995-loeb.pdf\">&ldquo;Full-information theories of individual good&rdquo;</a>, <em>Social Theory and Practice</em> 21: 1&ndash;30</p>\n<p>Loeb largely agrees with Sobel, but focuses his criticisms on more empirical grounds, like it taking lifetimes to learn enough, or concerns about judgements of goodness changing as additional information comes in (&ldquo;restricting the scope of relevant information to the science of the subject&rsquo;s day would lead to an implausibly relativized account of individual good&rdquo;). The obvious response to the first ~18 and last ~10 pages of his paper is, just like Sobel, he is anthropomorphizing with a vengeance and that problems for us are not problems for sufficiently powerful agents (the basic theory appeals to asymptotes and ideals), to which he replies:</p>\n<blockquote>\n<p>\"It would be ironic for a theory that makes questions of value depend on a causal matter (and that is presented in the spirit of naturalism) to take refuge in imagining massive alterations in the laws of nature. But irony is no guarantee of incorrectness. Still, it is not at all clear that such massively impossible counterfactuals have determinate truth values. Counterfactuals about what people would want in causally impossible circumstances are still causal counterfactuals. As such, they depend on causal laws&mdash;in particular, laws of psychology. But the laws of psychology would have to be vastly different from the actual laws if they were to rule out all of the unwelcome influences I have pointed out. And since these are the very laws that support the counterfactuals, it is not at all clear that enough is left of them to insure that the counterfactuals have determinate truth values.<sup>40</sup> [40: <em>A fortiori</em>, it is not clear that these counterfactuals would have truth values that are empirically determinable.]</p>\n<p>It is also not clear that the full-information approach would be plausible if it required that we imagine such wide-scale changes in the laws of psychology. We know too little to be confident of that. Perhaps my counterpart would no longer wish for me to shun the poison liquid in a world in which he would react no differently to yelling than to whispering, and in which one&rsquo;s motivations would not be influenced by massive alterations in one&rsquo;s cognitive capabilities alone. Without knowing how the laws of psychology would be altered, we are in no position to judge whether the approach maintains whatever plausibility it initially appeared to have.\"</p>\n</blockquote>\n<p>As a hardcore materialist, I do not buy this argument; the &lsquo;laws of psychology&rsquo; are no laws at all, but rather one of many possibilities allowed by the laws of physics, and the counterfactuals are not impossible.</p>\n</li>\n</ul>\n<p>Criticism:</p>\n<ul>\n<li>\n<p>Campbell, Stephen Michael, 2006 M.A.&nbsp;thesis: <a href=\"http://repository.tamu.edu/bitstream/handle/1969.1/3834/etd-tamu-2006A-PHIL-Campbell.pdf\">&ldquo;Phenomenal Well-being&rdquo;</a>; pg 40-end:</p>\n<p>Campbell describes a slightly more specific agent, where the lives are simply compared pair-wise and with a point system to break potential ties and intransitivity. Campbell seems to reject premise 1 too, in describing a flawed system (&ldquo;&hellip;the ranking should be accurate, even if not perfectly precise&rdquo;), but argues that this is acceptable since we do it in ordinary life and offers as a somewhat facetious example the difficulty of perfectly comparing ice cream flavors:</p>\n<blockquote>\n<p>Your memories of the different experiences might get corrupted. By the time you get to the end of the thirty-one flavors, perhaps you cannot remember what flavors 5 and 12 were like or even what you thought about them at the time. Or perhaps your memory was distorted at some point in the process. You can re-taste those flavors, but you cannot recapture the exact taste experience again (since, for one, you will now have more ice cream on your stomach), and we have no guarantee that the re-experience of a sample will not diverge in such a way as to affect your ranking.</p>\n</blockquote>\nCampbell hopes agents will ultimately converge despite the roughness of judging, and most of his replies to Sobel/Rosati/Loeb depend on that or his own brand of anthropomorphizing the ideal system (eg. suggesting that an unappreciative system will, after experiencing countless lives, come to appreciate them - I&rsquo;m reminded of the TvTropes <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/DoAndroidsDream\">Do Androids Dream?</a>).</li>\n<li>\n<p>Beaulieu, 1997 MA thesis, <a href=\"http://www.nlc-bnc.ca/obj/s4/f2/dsk2/ftp04/mq23218.pdf\">\"The Normative Authority of Our Fully Informed Judgements</a>;</p>\nGoes after Rosati&rsquo;s arguments, arguing that enough memory can serve to appreciate differing viewpoints, changes in one&rsquo;s desires with additional information are welcome, and Rosati&rsquo;s examples (showing full information to be incoherent) do not work. Most worth reading is chapter 3.</li>\n<li>\n<p>Anton Tupa, 2006 PhD thesis <a href=\"http://ufdcimages.uflib.ufl.edu/UF/E0/01/38/86/00001/tupa_a.pdf\">&ldquo;Development and Defense of a Desire-satisfaction Conception of Well-being&rdquo;</a></p>\n<p>Tupa argues Rosati&rsquo;s internalism criteria can be met by idealized/extrapolated versions of a person, and that doesn&rsquo;t refute desirism (pg 111&ndash;128). Discussing Sobel on pg 137, he writes something which I think is very insightful when applied to suggestions like Sobel&rsquo;s &lsquo;the ideal agent/system will go mad if it had perfect information&rsquo;:</p>\n<blockquote>\n<p>I think that so long as the <a href=\"http://www.gwern.net/docs/1978-shope.pdf\">conditional fallacy</a> [see <a href=\"/r/discussion/lw/9om/the_conditional_fallacy_in_contemporary_philosophy/\">\"The Conditional Fallacy in Contemporary Philosophy\"</a>] has the form of &ldquo;for all we know, x could be a consequent change, given your analysans, and if so, your analysis will yield counterintuitive results,&rdquo; then a solution can be provided. I am optimistic here because although sometimes critics of ideal advisor accounts write as if there would be only one possible world in which one would have full information, and they then prognosticate doomsday-like scenarios, in reality (in some sense perhaps), there are many possible worlds in which one is fully informed, i.e.&nbsp;there are many A+ candidates. Of these many possible worlds in which one has full information, some will involve changes in one that will be problematic, but some will involve few significant changes in one or changes that are quite unproblematic.</p>\n<p>&hellip;Problems that can be solved by appeal to the concept of a personality include worries about the increased mental capacity and mental processing speed that would have to be the case in order for someone to have full information. To be sure, it is a little odd even thinking about people with what can only be described as super-minds. However, anyone&rsquo;s personality, I say, is compatible with increased cognitive capacity and the like. Unless someone can show that some counterintuitive consequent change <em>must</em> occur in a world in which one is fully informed, the method of singling out the best possible world in which one is fully informed seems to have a great deal of promise&hellip;Thus far no one has come close to offering an argument that counterintuitive consequent changes must result in the nearest possible world in which one is fully informed&hellip;Later, I will examine whether full propositional information is adequate as an information set for the ideal advisor. While Rosati and Sobel are skeptical, I argue that full propositional information is far richer and more textured than they envision and may very well be sufficient to play the requisite role in the deliberation of the ideal advisor.</p>\n</blockquote>\n<p>Tupa&rsquo;s replies to previously mentioned claim and arguments often have this flavor up to pg 150, where he then rejects much of premise 5 and argues for the judging agent to be able to make flawless assessments of a life without adopting the viewpoint of the life (based on &lsquo;propositional knowledge&rsquo;: &ldquo;I have a hard time seeing how knowledge of what something is like is evaluative in any important sense&rdquo;) and like Campbell, he contrasts Sobel&rsquo;s demand for perfect judgement as beyond even the most reliable ordinary daily judgement</p>\n</li>\n</ul>\n<h1 id=\"references-further-reading\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> References &amp; further reading</a></h1>\n<p>Works on the subject include:</p>\n<ul>\n<li>Brandt 1979 <em>A Theory of the Good and the Right</em></li>\n<li>Velleman, J. David. (1988) <a href=\"http://www.gwern.net/docs/1988-velleman.pdf\">&ldquo;Brandt&rsquo;s Definition of &lsquo;Good&rsquo;&rdquo;</a>, <em>Philosophical Review</em> 97: 353&ndash;371.</li>\n<li>Railton, Peter Albert. &ldquo;Facts and Values&rdquo;. <em>Philosophical Topics</em> 14, no. 2 (1986): 5&ndash;31.</li>\n<li>Lewis, David. <a href=\"http://www.gwern.net/docs/1989-lewis.pdf\">&ldquo;Dispositional Theories of Values&rdquo;</a>. <em>Proceedings of the Aristotelian Society</em>, 63 (1989): 113&ndash;137</li>\n<li>Loeb, D. 1995. <a href=\"http://www.gwern.net/docs/1995-loeb.pdf\">&ldquo;Full-information theories of individual good&rdquo;</a>. <em>Social Theory and Practice</em> 21: 1&ndash;30</li>\n<li>Rosati, <a href=\"http://www.gwern.net/docs/1995-rosati-good.pdf\">&ldquo;Persons, Perspectives, and Full Information Accounts of the Good&rdquo;</a>, <em>Ethics</em> 105 (1995): 296&ndash;325</li>\n<li>Zimmerman, David. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Zimmerman-Why-Richard-Brandt-does-not-need-cognitive-psychotherapy.pdf\">&ldquo;Why Richard Brandt Does Not Need Cognitive Psychotherapy, and Other Glad News About Idealized Preference Theories in Meta-Ethics.&rdquo;</a> <em>Journal of Value Inquiry</em> 37, no. 3 (2003), 373&ndash;394.</li>\n<li>Tanyi, Attila. &ldquo;An Essay on the Desire-Based Reasons Model&rdquo;. PhD dissertation, Central European University, 2006</li>\n<li>Sobel, David. <a href=\"http://www.unl.edu/philosop/people/faculty/sobel/DotheDesires.pdf\">&ldquo;Do the desires of rational agents converge?&rdquo;</a> <em>Analysis</em> 59, no. 263 (1999), 137&ndash;147</li>\n<li>D&ouml;ring, Sabine, and Louise Andersen. &ldquo;Rationality, Convergence and Objectivity.&rdquo; Unpublished manuscript, 2009.</li>\n<li>Simon Keller, <a href=\"http://www.gwern.net/docs/2004-keller.pdf\">&ldquo;Welfare and the Achievement of Goals&rdquo;</a>, <em>Philosophical Studies</em> 121 (2004): 27&ndash;41</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ovLpQqpRLtpXPdKKP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 22, "extendedScore": null, "score": 8.407436372069777e-07, "legacy": true, "legacyId": "12538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Luke tasked me with researching the following question</p>\n<blockquote>\n<p>I\u2018d like to know if anybody has come up with a good response to any of the objections to \u2019full information\u2019 or \u2018ideal preference\u2019 theories of value given in Sobel (1994). (My impression is \u201cno.\u201d)</p>\n</blockquote>\n<p>The paper in question is David Sobel\u2019s 1994 paper <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">\u201cFull Information Accounts of Well-Being\u201d</a> (<em>Ethics</em> 104, no. 4: 784\u2013810) (his 1999 paper, \u201cDo the desires of rational agents converge?\u201d, is directed against a different kind of convergence and won\u2019t be discussed here).</p>\n<p>The starting point is Brandt\u2019s 1979 book where he describes his version of a utilitarianism in which utility is the degree of satisfaction of the desires of one\u2019s ideal \u2018fully informed\u2019 self, and Sobel also refers to the 1986 Railton apologetic. (LWers will note that this kind of utilitarianism sounds very similar to <a href=\"http://intelligence.org/upload/CEV.html\">CEV</a> and hence, any criticism of the former may be a valid criticism of the latter.) I\u2019ll steal entirely the opening to Mark C Murphy\u2019s 1999 paper, <a href=\"http://www.gwern.net/docs/1999-murphy.pdf\">\u201cThe Simple Desire-Fulfillment Theory\u201d</a> (rejecting any hypotheticals or counterfactuals in desire utilitarianism), since he covers all the bases (for even broader background, see the Tanner Lecture <a href=\"http://www.tannerlectures.utah.edu/lectures/documents/Scanlon98.pdf\">\u201cThe Status of Well-Being\u201d</a>):</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>An account of well-being that [Derek] Parfit labels the \u2018desire-fulfillment\u2019 theory (1984, 493) has gained a great deal of support as the most plausible account of what makes a subject well-off. According to the desire-fulfillment, or DF, theory, an agent\u2019s well-being is constituted by the obtaining of states of affairs that are desired by that agent.<sup>1</sup> Importantly, though, while all DF theorists affirm that an account of what makes an agent well-off must ultimately refer to desire, there now appears to be a consensus among those defending DF theories that it is not the satisfaction of the agent\u2019s <em>actual</em> desires that constitutes the agent\u2019s well-being, but rather the satisfaction of those desires that the agent would have in what I will call a \u2018hypothetical desire situation.\u2019 Just as Rawls holds (1971, 12) that the principles of right are those that would be unanimously chosen in a hypothetical choice situation, that is, a setting optimal for choosing such principles, defenders of DF theory hold that an agent\u2019s good is what he or she would desire in a hypothetical desire situation, that is, a setting optimal for desiring.<sup>2</sup> While the precise nature of the hypothetical desire situation is a matter of debate among DF theorists, all of them seem to agree that any adequate DF theory will incorporate a strong information condition into the hypothetical desire situation. In treating of the concept of an individual\u2019s good, Sidgwick writes:</p>\n<blockquote>\n<p>It would seem. . . that if we interpret the notion \u2018good\u2019 in relation to \u2018desire,\u2019 we must identify it not with the actually desired, but rather with the desirable:\u2014meaning by \u2018desirable\u2019 not necessarily \u2018what ought to be desired\u2019 but what would be desired. . . if it were judged attainable by voluntary action, supposing the desirer to possess a perfect forecast, emotional as well as intellectual, of the state of attainment or fruition (1981, 110\u2013111).</p>\n</blockquote>\n<p>Brandt writes that a state of affairs belongs to an agent\u2019s welfare only if it is such that \u201cthat person would want it if he were fully rational\u201d (1979, 268); an agent\u2019s desire is rational, on Brandt\u2019s view,</p>\n<blockquote>\n<p>if it would survive or be produced by careful \u2018cognitive psychotherapy\u2019 [where cognitive psychotherapy is the \u2018whole process of confronting desires with relevant information.\u2019]. . . I shall call a desire \u2018irrational\u2019 if it cannot survive compatibly with clear and repeated judgments about established facts. What this means is that rational desire. . . can confront, or will even be produced by, awareness of the truth (1979, 113).</p>\n</blockquote>\n<p>And Railton has argued that we should consider an agent\u2019s good to be \u201cwhat he would want himself to want. . . were he to contemplate his present situation from a standpoint fully and vividly informed about himself and his circumstances, and entirely free of cognitive error or lapses of instrumental rationality\u201d (1986a, 16).</p>\n</blockquote>\n<h1 id=\"1_Overview\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Overview</a></h1>\n<blockquote>\n<p>There are at least four general strategies one could take in arguing that such an informed viewpoint is inadequate in capturing and commensurating what is in an agent\u2019s interests.</p>\n<ol style=\"list-style-type: decimal\">\n<li>First, one could argue that the notion of a fully informed self is a chimera. This would likely involve the worry that from the fact that any of the lives that one is to assess the value of must be in some sense available to one (otherwise it could not be a valuable life for one to live) it does not follow that all of them together must be available to one\u2019s consciousness. <em>To make good this suggestion against the full information account one would have to provide reasons to think there are substantive worries about uniting the experience of all lives one could lead into a single consciousness.</em></li>\n<li>Second, one could argue that even in cases in which an agent is adequately informed of the different life paths she is choosing between, there is no single pro-attitude, such as preferring, which appropriately measures the value of the diverse kinds of goods available to an agent\u2026The things that sensibly elicit delight are not generally the same things that merit respect or admiration. Our capacity for articulating our attitudes depends upon our understandings of our attitudes, which are informed by norms for valuation.</li>\n<li>Third, one could argue that a vivid presentation of some experiences which could be part of one\u2019s life could prove so disturbing or alluring as to skew any further reflection about what option to choose. Allan Gibbard has suggested the example of \u201ca more vivid realization of what peoples\u2019 innards are like\u201d causing a \u201cdebilitating neurosis\u201d which prevents me from eating in public. [cf. Bostrom\u2019s information-harms typology: \u2018evocation hazard\u2019; personally, I would use something like \u2018brainwashing\u2019 or war &amp; holocausts]</li>\n<li>Fourth, one could worry against naturalistic versions of the full information account that the purportedly naturalistically described informed viewpoint essentially invokes unreduced normative notions. [Naturalistic versions seem to assume non-physical definitions, like \u2018ideal set of information\u2019, and hence smuggle in non-naturalistic beliefs]</li>\n</ol></blockquote>\n<p>Emphasis added; Sobel pursues line of objection #1.</p>\n<h2 id=\"1_1_The_argument\"><a href=\"#TOC\"><span class=\"header-section-number\">1.1</span> The argument</a></h2>\n<p>I will try to reconstruct the argument in something more closely approximating propositional logic so it\u2019s easier to classify any criticism of Sobel based on what premise or inference they are attacking. The following is based on my reading pg 796\u2013797,801\u2013808; I omit all the examples, and some of the weaker tangential arguments. (For example, the suggestion that the ideal moral system may go insane from the difficulty of choices or it will despise us for being so pathetic and wish us dead (pg807), which are obvious anthropomorphisms.)</p>\n<ol style=\"list-style-type: decimal\">\n<li>The ideal moral system may not err</li>\n<li>Every possible life judgement must be judged by an agent</li>\n<li>An agent either lives that possible life, or it does not live it</li>\n<li>\n<p>If the agent does not live the possible life:</p>\n<ol style=\"list-style-type: decimal\">\n<li>If the agent does not live the possible life, it does not live the life\u2019s experiences</li>\n<li>Experiences may contain otherwise-unobtainable information [\u2018revelations\u2019]</li>\n<li>A judgement based on incomplete information may err</li>\n<li>The ideal moral system will not use an agent that lives the possible life (1, 4.1\u20134.3)</li>\n</ol></li>\n<li>\n<p>If the agent does live the possible life, it is either a \u2018serial\u2019 agent or an \u2018amnesia\u2019 agent</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p>Serial; the agent either lives the same life or a different life:</p>\n<p>The same life:</p>\n<ol style=\"list-style-type: decimal\">\n<li>To live the same possible life as that possible life, the agent must know only the same things as the possible life</li>\n<li>Most possible lives do not know what it is to live a different life</li>\n<li>If the agent knows only the same things as the possible life does, then in most lives it cannot know what it is to live an additional life</li>\n<li>If one does not know what additional lives are like to live, one may err in assessing one\u2019s own life</li>\n<li>The serial agent may live a life which does not know what other lives are like</li>\n<li>The serial agent may err</li>\n<li>The ideal moral system will not use a serial agent which knows the same as the possible life (1, 4.3, 5.1.1.1\u20136)</li>\n</ol>\n<p>A different life:</p>\n<ol style=\"list-style-type: decimal\">\n<li>If the agent knows more or less things than the possible life, it is not identical to the possible life</li>\n<li>If it is not identical to the possible life, it may experience or act differently</li>\n<li>If may experience things differently or act differently, it may judge experiences or judge acts differently</li>\n<li>If it may judge experiences or acts differently, then it may err</li>\n<li>The ideal moral system will not use a serial agent which knows more or less than the possible life (1,4.3,5.1.2.1\u20134)</li>\n</ol></li>\n<li>\n<p>Amnesia:</p>\n<ol style=\"list-style-type: decimal\">\n<li>If the agent is an amnesia agent, it will work under incomplete information due to forgetting</li>\n<li>Each amnesia period will form a different judgement</li>\n<li>These judgements may differ</li>\n<li>Differing judgements may lead to error</li>\n</ol>\n<p>Rebuttals rejecting 5.2.4:</p>\n<ol style=\"list-style-type: decimal\">\n<li>\n<p>The judgements can be weighed into a final correct judgement by an unspecified algorithm</p>\n<ul>\n<li>But - how does this work, exactly? What is the life\u2019s utility over its span?</li>\n</ul>\n</li>\n<li>Only one (\u2018allegedly temporally privileged\u2019) judgement is used, and a judgement can\u2019t differ with itself</li>\n<li>\n<p>They will not differ, as the fully informed agent at any period will agree with itself at all other periods</p>\n<ul>\n<li>But - how would one prove such a thing? It is \u2018indeterminate\u2019 and \u2018unlikely\u2019.</li>\n</ul>\n</li>\n<li>The ideal moral system will not use an amnesiac agent (1, 5.2.1\u20134)</li>\n</ol></li>\n</ol></li>\n<li>The ideal moral system will use neither a serial or amnesiac agent (5.1.7, 5.1.2, 5.2.5)</li>\n<li>The ideal moral system will not use an agent</li>\n<li>\n<p>The ideal moral system will not judge lives</p>\n</li>\n</ol>\n<h3 id=\"1_1_1_Analysis\"><a href=\"#TOC\"><span class=\"header-section-number\">1.1.1</span> Analysis</a></h3>\n<p>Broken down like this, we can see a number of ways to strengthen or attack it. For example, we can strengthen the attack on serial agents who lead different lives (5.1.2) by defining agents and lives as Turing machines and then invoking <a href=\"http://en.wikipedia.org/wiki/Rice%27s_theorem\">Rice\u2019s theorem</a> (the generalized Halting Theorem) - obviously \u2018goodness of life\u2019 is a nontrivial predicate and so there will be Turing machines for whom the question is uncomputable.</p>\n<p>This strengthening illustrates a possible attack, on the key premise 1: \u201cthe system must <em>not</em> err\u201d. Obviously, if the ethical system may err, all the arguments collapse: it\u2019s fine for an amnesia agent to sometimes contradict itself, it\u2019s fine for a too-knowledgeable serial agent to not act the same, etc.</p>\n<p>But our strengthening of 5.1.2 to Rice\u2019s theorem would seem to work for all the proposed agents (\u2018the amnesia agent will both work under incomplete information <em>and</em> be confronted with uncomputable lives\u2019), which is not an issue. What is an issue is that this would seem to work for any agent implementing any nontrivial ethical system - a utilitarian agent (\u2018you discover a planet-destroying bomb - which is triggered by the halting of a particular Turing machine\u2026\u2019) or many deontological agents (\u2018your computer claims to be a conscious being and you must not reboot it, because that would violate your deontological respect for personal autonomy and the right to live; you try to check its claims but\u2026\u2019).</p>\n<p>An argument which proves too much is not a good argument, and it seems to me that we can construct situations for agents running any moral system where they may err, if only through extreme brute force skeptical claims like the Simulation Hypothesis. (I say \u2018may\u2019 because Sobel\u2019s arguments above do not seem to show that various kinds of agents <em>will</em> err, which would be very difficult to prove.)</p>\n<p>Given this, we can reject premise 1 and are now free to pick from any of the kinds of agents discussed, since now that they are free to err, they are also free to have incomplete information, not attempt to crack uncomputable cases, etc. (To quote Murphy pg 23, \u201cIt would imply the indefensibility of DF [desire-fulfillment] theory if, that is, their hypothetical desire situations incorporated a <em>full</em> information condition, which is the target of Sobel\u2019s and Rosati\u2019s criticisms. If a theory\u2019s information condition were more modest, perhaps it would escape those criticisms.\u201d)</p>\n<h1 id=\"2_The_literature\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> The literature</a></h1>\n<p>Sobel\u2019s paper has only occasionally been grappled with or defended; usually it is described as illustrating some serious problems with reflective theories, but not much more.</p>\n<p>Support:</p>\n<ul>\n<li>\n<p>Loeb, Don 1995: <a href=\"http://www.gwern.net/docs/1995-loeb.pdf\">\u201cFull-information theories of individual good\u201d</a>, <em>Social Theory and Practice</em> 21: 1\u201330</p>\n<p>Loeb largely agrees with Sobel, but focuses his criticisms on more empirical grounds, like it taking lifetimes to learn enough, or concerns about judgements of goodness changing as additional information comes in (\u201crestricting the scope of relevant information to the science of the subject\u2019s day would lead to an implausibly relativized account of individual good\u201d). The obvious response to the first ~18 and last ~10 pages of his paper is, just like Sobel, he is anthropomorphizing with a vengeance and that problems for us are not problems for sufficiently powerful agents (the basic theory appeals to asymptotes and ideals), to which he replies:</p>\n<blockquote>\n<p>\"It would be ironic for a theory that makes questions of value depend on a causal matter (and that is presented in the spirit of naturalism) to take refuge in imagining massive alterations in the laws of nature. But irony is no guarantee of incorrectness. Still, it is not at all clear that such massively impossible counterfactuals have determinate truth values. Counterfactuals about what people would want in causally impossible circumstances are still causal counterfactuals. As such, they depend on causal laws\u2014in particular, laws of psychology. But the laws of psychology would have to be vastly different from the actual laws if they were to rule out all of the unwelcome influences I have pointed out. And since these are the very laws that support the counterfactuals, it is not at all clear that enough is left of them to insure that the counterfactuals have determinate truth values.<sup>40</sup> [40: <em>A fortiori</em>, it is not clear that these counterfactuals would have truth values that are empirically determinable.]</p>\n<p>It is also not clear that the full-information approach would be plausible if it required that we imagine such wide-scale changes in the laws of psychology. We know too little to be confident of that. Perhaps my counterpart would no longer wish for me to shun the poison liquid in a world in which he would react no differently to yelling than to whispering, and in which one\u2019s motivations would not be influenced by massive alterations in one\u2019s cognitive capabilities alone. Without knowing how the laws of psychology would be altered, we are in no position to judge whether the approach maintains whatever plausibility it initially appeared to have.\"</p>\n</blockquote>\n<p>As a hardcore materialist, I do not buy this argument; the \u2018laws of psychology\u2019 are no laws at all, but rather one of many possibilities allowed by the laws of physics, and the counterfactuals are not impossible.</p>\n</li>\n</ul>\n<p>Criticism:</p>\n<ul>\n<li>\n<p>Campbell, Stephen Michael, 2006 M.A.&nbsp;thesis: <a href=\"http://repository.tamu.edu/bitstream/handle/1969.1/3834/etd-tamu-2006A-PHIL-Campbell.pdf\">\u201cPhenomenal Well-being\u201d</a>; pg 40-end:</p>\n<p>Campbell describes a slightly more specific agent, where the lives are simply compared pair-wise and with a point system to break potential ties and intransitivity. Campbell seems to reject premise 1 too, in describing a flawed system (\u201c\u2026the ranking should be accurate, even if not perfectly precise\u201d), but argues that this is acceptable since we do it in ordinary life and offers as a somewhat facetious example the difficulty of perfectly comparing ice cream flavors:</p>\n<blockquote>\n<p>Your memories of the different experiences might get corrupted. By the time you get to the end of the thirty-one flavors, perhaps you cannot remember what flavors 5 and 12 were like or even what you thought about them at the time. Or perhaps your memory was distorted at some point in the process. You can re-taste those flavors, but you cannot recapture the exact taste experience again (since, for one, you will now have more ice cream on your stomach), and we have no guarantee that the re-experience of a sample will not diverge in such a way as to affect your ranking.</p>\n</blockquote>\nCampbell hopes agents will ultimately converge despite the roughness of judging, and most of his replies to Sobel/Rosati/Loeb depend on that or his own brand of anthropomorphizing the ideal system (eg. suggesting that an unappreciative system will, after experiencing countless lives, come to appreciate them - I\u2019m reminded of the TvTropes <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/DoAndroidsDream\">Do Androids Dream?</a>).</li>\n<li>\n<p>Beaulieu, 1997 MA thesis, <a href=\"http://www.nlc-bnc.ca/obj/s4/f2/dsk2/ftp04/mq23218.pdf\">\"The Normative Authority of Our Fully Informed Judgements</a>;</p>\nGoes after Rosati\u2019s arguments, arguing that enough memory can serve to appreciate differing viewpoints, changes in one\u2019s desires with additional information are welcome, and Rosati\u2019s examples (showing full information to be incoherent) do not work. Most worth reading is chapter 3.</li>\n<li>\n<p>Anton Tupa, 2006 PhD thesis <a href=\"http://ufdcimages.uflib.ufl.edu/UF/E0/01/38/86/00001/tupa_a.pdf\">\u201cDevelopment and Defense of a Desire-satisfaction Conception of Well-being\u201d</a></p>\n<p>Tupa argues Rosati\u2019s internalism criteria can be met by idealized/extrapolated versions of a person, and that doesn\u2019t refute desirism (pg 111\u2013128). Discussing Sobel on pg 137, he writes something which I think is very insightful when applied to suggestions like Sobel\u2019s \u2018the ideal agent/system will go mad if it had perfect information\u2019:</p>\n<blockquote>\n<p>I think that so long as the <a href=\"http://www.gwern.net/docs/1978-shope.pdf\">conditional fallacy</a> [see <a href=\"/r/discussion/lw/9om/the_conditional_fallacy_in_contemporary_philosophy/\">\"The Conditional Fallacy in Contemporary Philosophy\"</a>] has the form of \u201cfor all we know, x could be a consequent change, given your analysans, and if so, your analysis will yield counterintuitive results,\u201d then a solution can be provided. I am optimistic here because although sometimes critics of ideal advisor accounts write as if there would be only one possible world in which one would have full information, and they then prognosticate doomsday-like scenarios, in reality (in some sense perhaps), there are many possible worlds in which one is fully informed, i.e.&nbsp;there are many A+ candidates. Of these many possible worlds in which one has full information, some will involve changes in one that will be problematic, but some will involve few significant changes in one or changes that are quite unproblematic.</p>\n<p>\u2026Problems that can be solved by appeal to the concept of a personality include worries about the increased mental capacity and mental processing speed that would have to be the case in order for someone to have full information. To be sure, it is a little odd even thinking about people with what can only be described as super-minds. However, anyone\u2019s personality, I say, is compatible with increased cognitive capacity and the like. Unless someone can show that some counterintuitive consequent change <em>must</em> occur in a world in which one is fully informed, the method of singling out the best possible world in which one is fully informed seems to have a great deal of promise\u2026Thus far no one has come close to offering an argument that counterintuitive consequent changes must result in the nearest possible world in which one is fully informed\u2026Later, I will examine whether full propositional information is adequate as an information set for the ideal advisor. While Rosati and Sobel are skeptical, I argue that full propositional information is far richer and more textured than they envision and may very well be sufficient to play the requisite role in the deliberation of the ideal advisor.</p>\n</blockquote>\n<p>Tupa\u2019s replies to previously mentioned claim and arguments often have this flavor up to pg 150, where he then rejects much of premise 5 and argues for the judging agent to be able to make flawless assessments of a life without adopting the viewpoint of the life (based on \u2018propositional knowledge\u2019: \u201cI have a hard time seeing how knowledge of what something is like is evaluative in any important sense\u201d) and like Campbell, he contrasts Sobel\u2019s demand for perfect judgement as beyond even the most reliable ordinary daily judgement</p>\n</li>\n</ul>\n<h1 id=\"3_References___further_reading\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> References &amp; further reading</a></h1>\n<p>Works on the subject include:</p>\n<ul>\n<li>Brandt 1979 <em>A Theory of the Good and the Right</em></li>\n<li>Velleman, J. David. (1988) <a href=\"http://www.gwern.net/docs/1988-velleman.pdf\">\u201cBrandt\u2019s Definition of \u2018Good\u2019\u201d</a>, <em>Philosophical Review</em> 97: 353\u2013371.</li>\n<li>Railton, Peter Albert. \u201cFacts and Values\u201d. <em>Philosophical Topics</em> 14, no. 2 (1986): 5\u201331.</li>\n<li>Lewis, David. <a href=\"http://www.gwern.net/docs/1989-lewis.pdf\">\u201cDispositional Theories of Values\u201d</a>. <em>Proceedings of the Aristotelian Society</em>, 63 (1989): 113\u2013137</li>\n<li>Loeb, D. 1995. <a href=\"http://www.gwern.net/docs/1995-loeb.pdf\">\u201cFull-information theories of individual good\u201d</a>. <em>Social Theory and Practice</em> 21: 1\u201330</li>\n<li>Rosati, <a href=\"http://www.gwern.net/docs/1995-rosati-good.pdf\">\u201cPersons, Perspectives, and Full Information Accounts of the Good\u201d</a>, <em>Ethics</em> 105 (1995): 296\u2013325</li>\n<li>Zimmerman, David. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Zimmerman-Why-Richard-Brandt-does-not-need-cognitive-psychotherapy.pdf\">\u201cWhy Richard Brandt Does Not Need Cognitive Psychotherapy, and Other Glad News About Idealized Preference Theories in Meta-Ethics.\u201d</a> <em>Journal of Value Inquiry</em> 37, no. 3 (2003), 373\u2013394.</li>\n<li>Tanyi, Attila. \u201cAn Essay on the Desire-Based Reasons Model\u201d. PhD dissertation, Central European University, 2006</li>\n<li>Sobel, David. <a href=\"http://www.unl.edu/philosop/people/faculty/sobel/DotheDesires.pdf\">\u201cDo the desires of rational agents converge?\u201d</a> <em>Analysis</em> 59, no. 263 (1999), 137\u2013147</li>\n<li>D\u00f6ring, Sabine, and Louise Andersen. \u201cRationality, Convergence and Objectivity.\u201d Unpublished manuscript, 2009.</li>\n<li>Simon Keller, <a href=\"http://www.gwern.net/docs/2004-keller.pdf\">\u201cWelfare and the Achievement of Goals\u201d</a>, <em>Philosophical Studies</em> 121 (2004): 27\u201341</li>\n</ul>", "sections": [{"title": "1 Overview", "anchor": "1_Overview", "level": 1}, {"title": "1.1 The argument", "anchor": "1_1_The_argument", "level": 2}, {"title": "1.1.1 Analysis", "anchor": "1_1_1_Analysis", "level": 3}, {"title": "2 The literature", "anchor": "2_The_literature", "level": 1}, {"title": "3 References & further reading", "anchor": "3_References___further_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["35P62KXiqR2DfG8e7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T06:25:39.218Z", "modifiedAt": null, "url": null, "title": "Risk aversion vs. concave utility function", "slug": "risk-aversion-vs-concave-utility-function", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.294Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dvasya", "createdAt": "2011-03-08T00:30:12.369Z", "isAdmin": false, "displayName": "dvasya"}, "userId": "2484AHxytrNyQXajh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aFzLYnoLN65xWw4Xj/risk-aversion-vs-concave-utility-function", "pageUrlRelative": "/posts/aFzLYnoLN65xWw4Xj/risk-aversion-vs-concave-utility-function", "linkUrl": "https://www.lesswrong.com/posts/aFzLYnoLN65xWw4Xj/risk-aversion-vs-concave-utility-function", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Risk%20aversion%20vs.%20concave%20utility%20function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARisk%20aversion%20vs.%20concave%20utility%20function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFzLYnoLN65xWw4Xj%2Frisk-aversion-vs-concave-utility-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Risk%20aversion%20vs.%20concave%20utility%20function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFzLYnoLN65xWw4Xj%2Frisk-aversion-vs-concave-utility-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFzLYnoLN65xWw4Xj%2Frisk-aversion-vs-concave-utility-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 786, "htmlBody": "<p>In the comments to <a href=\"/lw/9nm/terminal_bias/\">this post</a>, several people independently stated that being risk-averse is the same as having a concave utility function. There is, however, a subtle difference here. Consider the example <a href=\"/lw/9nm/terminal_bias/5sqp\">proposed by one of the commenters</a>:&nbsp;an agent with a utility function</p>\n<p style=\"padding-left: 30px; \"><em>u</em> = sqrt(<em>p</em>) utilons for <em>p</em> paperclips.</p>\n<p>The agent is being offered a choice between making a bet with a 50/50 chance of receiving a payoff of 9 or 25 paperclips, or simply receiving 16.5 paperclips. The expected payoff of the bet is a full 9/2 + 25/2 = 17 paperclips, yet its expected utility is only 3/2 + 5/2 = 4 = sqrt(16) utilons which is less than the sqrt(16.5) utilons for the guaranteed deal, so our agent goes for the latter, losing 0.5 expected paperclips in the process. Thus, it is claimed that our agent is risk averse in that it sacrifices 0.5 expected paperclips to get a guaranteed payoff.</p>\n<p>Is this a good model for the cognitive bias of risk aversion? I would argue that it's not. Our agent ultimately cares about utilons, not paperclips, and in the current case it does perfectly fine at rationally maximizing expected utilons. A cognitive bias should be, instead, some irrational behavior pattern that can be exploited to take utility (rather than paperclips) away from the agent. Consider now another agent, with the same utility function as before, but who just has this small additional trait that it would strictly prefer a sure payoff of 16 paperclips to the above bet. Given our agent's utility function, 16 is the point of indifference, so could there be any problem with his behavior? Turns out there is. For example, we could follow the post on&nbsp;<a href=\"/lw/9e4/the_savage_theorem_and_the_ellsberg_paradox\">Savage's theorem</a>&nbsp;(see Postulate #4). If the sure payoff of</p>\n<p style=\"padding-left: 30px; \">16 paperclips = 4 utilons</p>\n<p>is strictly preferred to the bet</p>\n<p style=\"padding-left: 30px; \">{P(9 paperclips) = 0.5; P(25 paperclips) = 0.5} = 4 utilons,</p>\n<p>then there must also exist some finite&nbsp;<em>&delta;</em> &gt; 0 such that the agent must strictly prefer a guaranteed 4 utilons to betting on</p>\n<p style=\"padding-left: 30px; \">{P(9) = 0.5 -&nbsp;<em>&delta;</em>; P(25) = 0.5 +&nbsp;<em>&delta;</em>) = 4 + 2<em>&delta;</em>&nbsp;utilons</p>\n<p>- all at the loss of 2<em>&delta;</em>&nbsp;expected&nbsp;utilons! This is also equivalent to our agent being willing to pay a finite amount of paperclips to substitute the bet with the sure deal of the same expected utility.</p>\n<p>What we have just seen falls pretty nicely within the concept of a bias. Our agent has a perfectly fine utility function, but it also has <em>this other thing</em>&nbsp;- let's name it \"risk aversion\" - that makes the agent's behavior fall short of being perfectly rational, and is independent of its concave utility function for paperclips. (Note that our agent has <em>linear</em> utility for utilons, but is still willing to pay some amount of those to achieve certainty) Can we somehow fix our agent? Let's see if we can redefine our utility function&nbsp;<em>u'</em>(<em>p</em>)&nbsp;in some way so that it gives us a consistent preference of</p>\n<p style=\"padding-left: 30px; \">guaranteed 16 paperclips</p>\n<p>over the</p>\n<p style=\"padding-left: 30px; \">&nbsp;{P(9) = 0.5; P(25) = 0.5}</p>\n<p>bet, but we would also like to request that the agent would still strictly prefer the bet</p>\n<p style=\"padding-left: 30px; \">{P(9&nbsp;+&nbsp;<em>&delta;</em>) = 0.5; P(25&nbsp;+&nbsp;<em>&delta;</em>) = 0.5}</p>\n<p>to {P(16) = 1} for some finite <em>&delta;</em> &gt; 0, so that our agent is not <em>infinitely</em> risk-averse. Can we say anything about this situation? Well, if&nbsp;<em>u'</em>(<em>p</em>) is continuous, there must also exist some number&nbsp;<em>&delta;'</em>&nbsp;such that&nbsp;0 &lt;&nbsp;<em>&delta;'</em>&nbsp;&lt;&nbsp;<em>&delta;</em>&nbsp;and our agent will be indifferent between&nbsp;{P(16) = 1} and</p>\n<p style=\"padding-left: 30px; \">{P(9&nbsp;+&nbsp;<em>&delta;'</em>) = 0.5; P(25&nbsp;+&nbsp;<em>&delta;'</em>) = 0.5}.</p>\n<p>And, of course, being <em>risk-averse</em> (in the above-defined sense), our supposedly rational agent will prefer - no harm done - the guaranteed payoff to the bet of the same expected utility <em>u'</em>... Sounds familiar, doesn't it?</p>\n<p>I would like to stress again that, although our first agent does have a concave utility function for paperclips, which causes it to reject bets with some expected payoff of paperclips to guaranteed payoffs of less paperclips, it still maximizes its expected utilons, for which it has linear utility. Our second agent, however, has this extra property that causes it to sacrifice expected utilons to achieve certainty. And it turns out that with this property it is impossible to define a well-behaved utility function! Therefore it seems natural to distinguish being rational with a concave utility function, on the one hand, from, on the other hand, being risk-averse and not being able to have a well-behaved utility function at all. The latter case seems much more subtle at the first sight, but causes a more fundamental kind of problem. Which is why I feel that a clear, even if minor, distinction between the two situations is still worth making explicit.</p>\n<p>A rational agent can have a concave utility function. A risk-averse agent can not be rational.</p>\n<p>(Of course, even in the first case the question of <a href=\"/lw/9nm/terminal_bias/\">whether we want a concave utility function</a>&nbsp;is still open.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xYLtnJ6keSHGfrLpe": 1, "HAFdXkW4YW4KRe2Gx": 1, "3uE2pXvbcnS9nnZRE": 1, "AHK82ypfxF45rqh9D": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aFzLYnoLN65xWw4Xj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 3, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "12542", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QGKFjaZNDtJnBTbxS", "thHZiZBDRPtGxCM6f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T07:57:09.228Z", "modifiedAt": null, "url": null, "title": "Doing Science! Open Thread Experiment Results", "slug": "doing-science-open-thread-experiment-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:53.939Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sXmSpsLbA3dKifGjh/doing-science-open-thread-experiment-results", "pageUrlRelative": "/posts/sXmSpsLbA3dKifGjh/doing-science-open-thread-experiment-results", "linkUrl": "https://www.lesswrong.com/posts/sXmSpsLbA3dKifGjh/doing-science-open-thread-experiment-results", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doing%20Science!%20Open%20Thread%20Experiment%20Results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoing%20Science!%20Open%20Thread%20Experiment%20Results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXmSpsLbA3dKifGjh%2Fdoing-science-open-thread-experiment-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doing%20Science!%20Open%20Thread%20Experiment%20Results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXmSpsLbA3dKifGjh%2Fdoing-science-open-thread-experiment-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXmSpsLbA3dKifGjh%2Fdoing-science-open-thread-experiment-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 398, "htmlBody": "<p>Early in the month I announced that I was doing an experiment: I was going to start two Open Threads in January (one on the 1st, and the other on the 15th) and compare the number of comments on these threads to those of other months. My hypothesis was that having two Open Threads would raise the overall number of comments.</p>\n<p>The reason for this experiment was recent discussions regarding how useful threads such as these were quickly buried.&nbsp;Well, the experiment is over now, and here are the results:</p>\n<p>&nbsp;</p>\n<p>I did a search for Open Threads, and entered all the monthly ones I could find into an Excel spreadsheet. I made them into a graph, and&nbsp;I discovered an anomaly. There was an 8-month timespan from February 2010-September 2010, in which the comment counts were extremely high (up to 2112). Many of these threads had 2, 3, or 4 parts, because they were getting filled up.</p>\n<p>I wasn't around LW back then, and I don't feel like reading through them all, so I don't know why this time period was so active.&nbsp;My current hypothesis (with P=.75) is that anomalous time period was before the Discussion section was created. I'm sure I could look it up to see if I'm right, but I bet one of the long-term LWers already knows if this is true or not, so I'll crowd-source the info. (Comment below if you know that I am correct or incorrect in my hypothesis.)</p>\n<p>&nbsp;</p>\n<p>Now for the data:</p>\n<p>The January 1-15, 2012 thread had:<span style=\"white-space: pre;\"> <span style=\"white-space: pre;\"> </span></span>122 comments<br />The January 16-31, 2012 thread had:<span style=\"white-space: pre;\"> <span style=\"white-space: pre;\"> </span></span>236 comments</p>\n<p>For a grand total of:<span style=\"white-space: pre;\"> <span style=\"white-space: pre;\"> </span></span><strong>358 comments in Jan 2012<br /><br /></strong></p>\n<p>The average Open Thread had: <span style=\"white-space: pre;\"> <span style=\"white-space: pre;\"> </span></span>448.6 comments<br />The median Open Thread had:<span style=\"white-space: pre;\"> <span style=\"white-space: pre;\"> </span></span>204 &nbsp; &nbsp;comments<br />The average OT of the past 14 mo's:<span style=\"white-space: pre;\"> </span>126.5 comments</p>\n<p><br />So overall, the January thread had LESS than the average monthly thread, but more than the median.&nbsp;</p>\n<p>IF however we look at the past 14 months (which was the end of the anomaly), then&nbsp;the January 2012 Open Thread had almost THREE TIMES the average.</p>\n<p>My original hypothesis had probabilities assigned to various increases in comment rate, but I was way off because I didn't at all think it would shrink (if we include the anomaly) or that it would be 300% bigger (if we don't)</p>\n<p>&nbsp;</p>\n<p>Here's a handy-dandy chart, because everything is better with pictures in!</p>\n<p><img style=\"border-style: initial; border-color: initial;\" src=\"http://images.lesswrong.com/t3_9of_1.png?v=69934ccbbae0bc895ad2c5861fd9d17e\" alt=\"\" width=\"608\" height=\"369\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sXmSpsLbA3dKifGjh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 8.40794483149869e-07, "legacy": true, "legacyId": "12543", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T16:32:14.311Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:02.574Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9xCJwP4zbAoFjjayQ/", "pageUrlRelative": "/posts/9xCJwP4zbAoFjjayQ/", "linkUrl": "https://www.lesswrong.com/posts/9xCJwP4zbAoFjjayQ/", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xCJwP4zbAoFjjayQ%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xCJwP4zbAoFjjayQ%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xCJwP4zbAoFjjayQ%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9xCJwP4zbAoFjjayQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 8.409936128646079e-07, "legacy": true, "legacyId": "12545", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T19:58:41.237Z", "modifiedAt": null, "url": null, "title": "Darwin Day: Good Evolution Games?", "slug": "darwin-day-good-evolution-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:52.972Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X9rvETTG9SE9PSawb/darwin-day-good-evolution-games", "pageUrlRelative": "/posts/X9rvETTG9SE9PSawb/darwin-day-good-evolution-games", "linkUrl": "https://www.lesswrong.com/posts/X9rvETTG9SE9PSawb/darwin-day-good-evolution-games", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Darwin%20Day%3A%20Good%20Evolution%20Games%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADarwin%20Day%3A%20Good%20Evolution%20Games%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9rvETTG9SE9PSawb%2Fdarwin-day-good-evolution-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Darwin%20Day%3A%20Good%20Evolution%20Games%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9rvETTG9SE9PSawb%2Fdarwin-day-good-evolution-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9rvETTG9SE9PSawb%2Fdarwin-day-good-evolution-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p><span style=\"font-family: arial,helvetica,sans-serif;\">Darwin's birthay is on February 12th, which is a Sunday. I'd kinda like to do something fun to celebrate.</span></p>\n<p><span style=\"font-family: arial,helvetica,sans-serif;\">I was wondering if anyone knew of any good games that feature natural selection? Video games would work if necessary (to run through a lot of iterations quickly), but I'd prefer something closer to a board game, that has a party feel.</span></p>\n<p><span style=\"font-family: arial,helvetica,sans-serif;\">What's coming to mind is an activity from 10th grade biology: you get a bunch of skittles, and place them on a yellow background, and then you go through iterations of \"eat the first skittle you see as fast as possible\", and then for each remaining skittle, add two more skittles of the same color. Within a few generations they're all yellow because those were harder to see.</span></p>\n<p><span style=\"font-family: arial,helvetica,sans-serif;\">That framework is nice, but doesn't really produce an interesting result. I'm trying to think of something that, over the course of an afternoon, without computer simulation, produce some interesting emergent phenomena. </span></p>\n<p><span style=\"font-family: arial,helvetica,sans-serif;\">Anyone have thoughts? Does anything like this already exist?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X9rvETTG9SE9PSawb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 1, "extendedScore": null, "score": 8.410734485520637e-07, "legacy": true, "legacyId": "12548", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T20:34:13.529Z", "modifiedAt": null, "url": null, "title": "Is risk aversion really irrational ?", "slug": "is-risk-aversion-really-irrational", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.528Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kilobug", "createdAt": "2011-09-02T14:37:51.213Z", "isAdmin": false, "displayName": "kilobug"}, "userId": "7BQMuDSmLE2XRq2ph", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ecbpjmxc833roBxj3/is-risk-aversion-really-irrational", "pageUrlRelative": "/posts/ecbpjmxc833roBxj3/is-risk-aversion-really-irrational", "linkUrl": "https://www.lesswrong.com/posts/ecbpjmxc833roBxj3/is-risk-aversion-really-irrational", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20risk%20aversion%20really%20irrational%20%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20risk%20aversion%20really%20irrational%20%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fecbpjmxc833roBxj3%2Fis-risk-aversion-really-irrational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20risk%20aversion%20really%20irrational%20%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fecbpjmxc833roBxj3%2Fis-risk-aversion-really-irrational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fecbpjmxc833roBxj3%2Fis-risk-aversion-really-irrational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2782, "htmlBody": "<address><em>Disclaimer: this started as a comment to <a href=\"/lw/9oe/risk_aversion_vs_concave_utility_function\">Risk aversion vs. concave utility function</a> but it grew way too big so I turned it into a full-blown article. I posted it to main since I believe it to be useful enough, and since it replies to an article of main.</em></address>\n<h2>Abstract</h2>\n<p>When you have to chose between two options, one with a certain (or almost certain) outcome, and another which involves more risk, even if in term of utilons (paperclips, money, ...) the gamble has a higher expectancy, there is always a cost in a gamble : between the time when you take your decision and know if your gamble fails or succeeded (between the time you bought your lottery ticket,and the time the winning number is called), you've less precise information about the world than if you took the \"safe\" option. That uncertainty may force you to make suboptimal choices during that period of doubt, meaning that \"risk aversion\" is not totally irrational.</p>\n<p>Even shorter : knowledge has value since it allows you to optimize, taking a risk temporary lowers your knoweldge, and this is a cost.</p>\n<h2>Where does risk aversion comes from ?<br /></h2>\n<p>In his (or her?) article, <span class=\"author\"><a id=\"author_t3_9oe\" href=\"/user/dvasya\">dvasya</a> gave one possible reason for it : risk aversion comes from a concave utility function. Take food for example. When you're really hungry, didn't eat for days, a bit of food has a very high value. But when you just ate, and have some stocks of food at home, food has low value. Many other things follow, more or less strongly, a non-linear utility function.</span></p>\n<p>But if you adjust the bets for the utility, then, if you're a perfect utility maximizer, you should chose the highest expectancy, regardless of the risk involved. Between being sure of getting 10 utilons and having a 0.1 chance of getting 101 utilons (and 0.9 chance to get nothing), you should chose to take the bet. Or you're not rational, says dvasya.</p>\n<p>My first objection to it is that we aren't perfect utility maximizer. We run on limited (and flawed) hardware. We have a limited power for making computation. The first problem of taking a risk is that it'll make all further computations much harder. You buy a lottery ticket, and until you know if you won or not, every time you decide what to do, you'll have to ponder things like \"if I win the lottery, then I'll buy a new house, so is it really worth it to fix that broken door now ?\" Asking yourself all those questions mean you're less <a href=\"/lw/xb/free_to_optimize\">Free to Optimize</a>, and will use your limited hardware to ponder those issues, leading to stress, fatigue and less-efficient decision making.</p>\n<p>For us humans with limited and buggy hardware, those problems are significant, and are the main reason for which I am personally (slightly) risk-averse. I don't like uncertainty, it makes planning harder, it makes me waste precious computing power in pondering what to do. But that doesn't seem apply to a perfect utility maximizer, with infinite computing power. So, it seems to be a consequence of biases, if not a bias in itself. Is it really ?</p>\n<h2>The double-bet of Clippy</h2>\n<p>So, let's take Clippy. Clippy is a pet paper-clip optimizer, using the utility function proposed by dvasya : <em>u</em> = sqrt(<em>p</em>), where <em>p</em> is the number of paperclips in the room he lives in. In addition to being cute and loving paperclips, our Clippy has lots of computing power, so much he has no issue with tracking probabilities. Now, we'll offer our Clippy to take bets, and see what he should do.</p>\n<h3>Timeless double-bet</h3>\n<p>At the beginning, we put 9 paperclips in the room. Clippy has a utilon of 3. He purrs a bit to show us he's happy of those 9 paperclips, looks at us with his lovely eyes, and hopes we'll give him more.</p>\n<p>But we offer him a bet : either we give him 7 paperclips, or we flip a coin. If the coin comes up heads, we give him 18 paperclips. If it comes up tails, we give him nothing.</p>\n<p>If Clippy doesn't take the bet, he gets 16 paperclips in total, so <em>u=4</em>. If Clippy takes the bet, he has 9 paperclips (<em>u=3</em>) with p=0.5 or 9+18=27 paperclips (<em>u=5.20</em>) with p=0.5. His utility expectancy is <em>u=4.10</em>, so he should take the bet.</p>\n<p>Now, regardless of whatever he took the first bet (called B1 starting from now), we offer him a second bet (B2) : this time, he has to pay us 9 paperclips to enter. Then, we roll a 10-sided die. If it gives 1 or 2, we give him a jackpot of 100 paperclips, else nothing. Clippy can be in three states when offered the second deal :</p>\n<ol>\n<li>He didn't take B1. Then, he has 16 clips. If he doesn't take B2, he'll stay with 16 clips, and <em>u=4</em>. If takes B2, he'll have 7 clips with p=0.8 and 107 clips with p=0.2, for an expected utility of <em>u=4.19</em>.</li>\n<li>He did take B1, and lost it. He has 9 clips. If he doesn't take B2, he'll stay with 9 clips, and <em>u=3</em>. If takes B2, he'll have 0 clips with p=0.8 and 100 clips with p=0.2, for an expected utility of <em>u=2</em>.</li>\n<li>He did take B1, and won it. He has 27 clips. If he doesn't take B2, he'll stay with 27 clips, and <em>u=5.20</em>. If takes B2, he'll have 18 clips with p=0.8 and 118 clips with p=0.2, for an expected utility of <em>u=5.57</em>.</li>\n</ol>\n<p>So, if Clippy didn't take the first bet or if he won it, he should take the second bet. If he did take the first bet and lost it, he can't afford to take the second bet, since he's risking a very bad outcome : no more paperclips, not even a single tiny one !</p>\n<h3>And the devil \"time\" comes in...<br /></h3>\n<p>Now, let's make things a bit more complicated, and realistic. Before we were running things fully sequentially : first we resolved B1, and then we offered and resolved B2. But let's change a tiny bit B1. We don't flip the coin and give the clips to Clippy now. Clippy tells us if he takes B1 or not, but we'll wait one day before giving him the clips if he didn't take the bet, or before flipping the coin and then giving him the clips if he did take the bet.</p>\n<p>The utility function of Clippy doesn't involve time, and we'll consider it doesn't change if he gets the clips tomorrow instead of today. So for him, the new B1 is exactly like the old B1.</p>\n<p>But now, we offer him B2 <strong>after</strong> Clippy made his choice in B1 (taking the bet or not) but <strong>before</strong> flipping the coin for B1, if he did take the bet.</p>\n<p>Now, for Clippy, we only have two situations : he took B1 or he didn't. If he didn't take B1, we are in the same situation than before, with an expected utility of <em>u=4.19</em>.</p>\n<p>If he did take B1, we have to consider 4 possibilities :</p>\n<ol>\n<li>He loses the two bets. Then he ends up with no paperclip (9+0-9), and is very unhappy. He has <em>u=0</em> utilons. That'll arise with p=0.4.</li>\n<li>He wins B1 and loses B2. Then he ends up with 9+18-9 = 18 paperclips, so <em>u=4.24</em> with p=0.4.</li>\n<li>He loses B1 and wins B2. Then he ends up with 9-9+100 = 100 paperclips, so <em>u=10</em> with p = 0.1.</li>\n<li>He wins both bets. Then he gets 9+18-9+100 = 118 paperclips, so <em>u=10.86</em> with p=0.1.</li>\n</ol>\n<p>At the end, if he takes B2, he ends up with an expectancy of <em>u=3.78</em>.</p>\n<p>So, if Clippy takes B1, he then shouldn't take B2. Since he doesn't know if he won or lost B1, he can't afford the risk to take B2.</p>\n<p>But should he take B1 at first ? If, when offered to take B1, he knows he'll be offered to take B2 later on, then he should refuse B1 and take B2, for an utility of 4.19. If, when offered B1, he doesn't know about B2, then taking B1 seems the more rational choice. But once he took B1, <strong>until he knows</strong> if he won or not, he cannot afford to take B2.</p>\n<h3>The Python code</h3>\n<p>For people interested about those issues, <a href=\"http://kilobug.pilotsystems.net/lw/clippy_bets.py\">here is a simple Python script</a> I used to fine tune that numerical parameters of&nbsp; double-bet issue so my numbers lead to the problem I was pointing to. Feel free to play with it ;)</p>\n<h2>A hunter-gatherer tale<br /></h2>\n<p>If you didn't like my Clippy, despite him being cute, and purring of happiness when he sees paperclips, let's shift to another tale.</p>\n<p>Daneel is a young hunter-gatherer. He's smart, but his father committed a crime when he was still a baby, and was exiled from the tribe. Daneel doesn't know much about the crime - no one speaks about it, and he doesn't dare to bring the topic by himself. He has a low social status in the tribe because of that story. Nonetheless, he's attracted to Dors, the daughter of the chief. And he knows Dors likes him back, for she always smiles at him when she sees him, never makes fun of him, and gave him a nice knife after his coming-of-age ceremony.</p>\n<p>According to the laws of the tribe, Dors can chose her husband freely, and the husband will become the new chief. But Dors also have to chose a husband that is accepted by the rest of the tribe, if the tribe doesn't accept the leadership, they could revolt, or fail to obey. And that could lead to disaster for the whole tribe. Daneel knows he has to raise his status in the tribe if he wants Dors to be able to chose him.</p>\n<p>So Daneel wanders further and further in the forest. He wants to find something new to show the tribe his usefulness. That day, going a bit further than usual, he finds a place which is more humid than the forest the tribe usually wanders in. It has a new kind of trees, he never saw before. Lots of them. And they carry a yellow-red fruit which looks yummy. \"I could tell about that place to the others, and bring them a few fruits. But then, what if the fruit makes them sick ? They'll blame me, I'll lose all chances... they may even banish me. But I can do better. I'll eat one of the fruits myself. If tomorrow I'm not sick, then I'll bring fruits to the tribe, and show them where I found them. They'll praise me for it. And maybe Dors will then be able to take me more seriously... and if I get sick, well, everyone gets sick every now and then, just one fruit shouldn't kill me, it won't be a big deal\". So Daneel makes his utility calculation (I told you he was smart !), finds a positive outcome. So he takes the risk, he picks one fruit, and eats it. Sweet, a bit acid but not too much. Nice !</p>\n<p>Now, Daneel goes back to the tribe. On the way back, he got a rabbit, a few roots and plants for the shaman, an average day. But then, he sees the tribe gathered around the central totem. In the middle of the tribe, Dors with... no... not him... Eto ! Eto is the strongest lad of Daneel's age. He wants Dors too. And he's strong, and very skilled with the bow. The other hunters like him, he's a real man. And Eto's father died proudly, defending the tribe's stock of dried meat against hungry wolves two winters ago. But no ! Not that ! Eto is asking Dors to marry him. In public. Dors can refuse, but if she does with no reason, she'll alienate half of the tribe against her, she can't afford it. Eto is way too popular.</p>\n<p>\"Hey, Daneel ! You want Dors ? Challenge Eto ! He's strong and good with the bow, but in unarmed combat, you can defeat him, I know it.\", whispers Hari, one of the few friends of Daneel.</p>\n<p>Daneel starts thinking faster he never did. \"Ok, I can challenge Eto in unarmed combat. If I lose, I'll be wounded, Eto won't be nice with me. But he won't kill or cripple me, that would make half of the tribe to hate him. If I lose, it'll confirm I'm physical weak, but I'll also win prestige for daring to defy the strong Eto, so it shouldn't change much. And if I win, Dors will be able to refuse Eto, since he lost a fight against someone weaker than him, that's a huge win. So I should take that gamble... but then, there is the fruit. If the fruit gets me sick, in addition of my wounds from Eto, I may die. Even if I win ! And if I lose, get beaten, and then gets sick... they'll probably let me die. They won't take care of a fatherless lad who lose a fight and then gets sick. Too weak to be worth it. So... should I take the gamble ? If Eto waited just one day more... Or <strong>if only I knew if I'll get sick or not...</strong>\"</p>\n<h2>The key : information loss</h2>\n<p>Until Clippy knows ? If Daneel knew ? That's the key of risk aversion, and why a perfect utility maximizer, if he has a concave utility function in at least some aspects, should still have some risk aversion. Because risk comes with information loss. That's the difference between the timeless double-bet and the one with one day of delay for Clippy. Or the problem Daneel got stuck into.</p>\n<p>If you take a bet, until you know the outcome of your bet, you'll have less information about the state of the world, and especially about the state that directly concerns you, than if you chose the safe situation (a situation with a lower deviation). Having less information means you're less free to optimize.</p>\n<p>Even a perfect utility maximizer can't know what bets he'll be offered, and what decisions he'll have to take, unless he's omniscient (and then he wouldn't take bets or risks, but he would know the future - probability only reflects lack of information). So he has to consider the loss of information of taking a bet.</p>\n<p>In real life, the most common case of it is the non-linearity of bad effects : you can lose 0.5L of blood without too much side-effects (drink lots a water, sleep well, and next day you're ok, that's what happens when you go give your blood), but if you lose 2L, you'll likely die. Or if you lose some money, you'll be in trouble, but if you lose the same amount again, you may end up being kicked from you house since you can't pay the rent - and that'll be more than twice as bad as the initial lost.</p>\n<p>So when you took a bet, risking to get a bad effect, you can't afford to take another bet (even with, in absolute, a higher gain expectancy), until you know if you won or lose the first bet - because losing them both means death, or being kicked from your house, or ultimate pain of not having any paperclip.</p>\n<p>Taking a bet always as a cost : it costs you part of your ability to predict, and therefore to optimize.</p>\n<h2>A possible solution<br /></h2>\n<p>A possible solution to that problem would be to consider all possible decisions you may to take while in the time period when you don't know if you lost or won your first bet, ponder them with the probability of being offered those decisions, and their possible outcomes if you take the first bet and you don't. But how do you compute \"their possible outcomes\" ? That needs to consider all the possible bets you could be offered during the time required for the resolution of your second bet, and their possible outcomes. So you need to... <em>stack overflow: maximal recursion depth exceeded.</em></p>\n<p>Since taking a bet will affect your ability to evaluate possible outcomes in the future, you've a \"strange loop to the meta-level\", an infinite recursion. Your decision algorithm has to consider the impact the decision will have on the future instances of your decision algorithm.</p>\n<p>I don't know if there is a mathematical solution to that infinite recursion that manages to make it converge (like you can in some cases). But the problem looks really hard, and may not be computable.</p>\n<p>Just factoring an average \"risk aversion\" that penalizes outcome which involve a risk (and the more you've to wait to know if you won or lose, the higher the penalty) sounds more a way to fix that problem than a bias.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "dPPATLhRmhdJtJM2t": 1, "xYLtnJ6keSHGfrLpe": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ecbpjmxc833roBxj3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 51, "extendedScore": null, "score": 0.000103, "legacy": true, "legacyId": "12547", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<address><em>Disclaimer: this started as a comment to <a href=\"/lw/9oe/risk_aversion_vs_concave_utility_function\">Risk aversion vs. concave utility function</a> but it grew way too big so I turned it into a full-blown article. I posted it to main since I believe it to be useful enough, and since it replies to an article of main.</em></address>\n<h2 id=\"Abstract\">Abstract</h2>\n<p>When you have to chose between two options, one with a certain (or almost certain) outcome, and another which involves more risk, even if in term of utilons (paperclips, money, ...) the gamble has a higher expectancy, there is always a cost in a gamble : between the time when you take your decision and know if your gamble fails or succeeded (between the time you bought your lottery ticket,and the time the winning number is called), you've less precise information about the world than if you took the \"safe\" option. That uncertainty may force you to make suboptimal choices during that period of doubt, meaning that \"risk aversion\" is not totally irrational.</p>\n<p>Even shorter : knowledge has value since it allows you to optimize, taking a risk temporary lowers your knoweldge, and this is a cost.</p>\n<h2 id=\"Where_does_risk_aversion_comes_from__\">Where does risk aversion comes from ?<br></h2>\n<p>In his (or her?) article, <span class=\"author\"><a id=\"author_t3_9oe\" href=\"/user/dvasya\">dvasya</a> gave one possible reason for it : risk aversion comes from a concave utility function. Take food for example. When you're really hungry, didn't eat for days, a bit of food has a very high value. But when you just ate, and have some stocks of food at home, food has low value. Many other things follow, more or less strongly, a non-linear utility function.</span></p>\n<p>But if you adjust the bets for the utility, then, if you're a perfect utility maximizer, you should chose the highest expectancy, regardless of the risk involved. Between being sure of getting 10 utilons and having a 0.1 chance of getting 101 utilons (and 0.9 chance to get nothing), you should chose to take the bet. Or you're not rational, says dvasya.</p>\n<p>My first objection to it is that we aren't perfect utility maximizer. We run on limited (and flawed) hardware. We have a limited power for making computation. The first problem of taking a risk is that it'll make all further computations much harder. You buy a lottery ticket, and until you know if you won or not, every time you decide what to do, you'll have to ponder things like \"if I win the lottery, then I'll buy a new house, so is it really worth it to fix that broken door now ?\" Asking yourself all those questions mean you're less <a href=\"/lw/xb/free_to_optimize\">Free to Optimize</a>, and will use your limited hardware to ponder those issues, leading to stress, fatigue and less-efficient decision making.</p>\n<p>For us humans with limited and buggy hardware, those problems are significant, and are the main reason for which I am personally (slightly) risk-averse. I don't like uncertainty, it makes planning harder, it makes me waste precious computing power in pondering what to do. But that doesn't seem apply to a perfect utility maximizer, with infinite computing power. So, it seems to be a consequence of biases, if not a bias in itself. Is it really ?</p>\n<h2 id=\"The_double_bet_of_Clippy\">The double-bet of Clippy</h2>\n<p>So, let's take Clippy. Clippy is a pet paper-clip optimizer, using the utility function proposed by dvasya : <em>u</em> = sqrt(<em>p</em>), where <em>p</em> is the number of paperclips in the room he lives in. In addition to being cute and loving paperclips, our Clippy has lots of computing power, so much he has no issue with tracking probabilities. Now, we'll offer our Clippy to take bets, and see what he should do.</p>\n<h3 id=\"Timeless_double_bet\">Timeless double-bet</h3>\n<p>At the beginning, we put 9 paperclips in the room. Clippy has a utilon of 3. He purrs a bit to show us he's happy of those 9 paperclips, looks at us with his lovely eyes, and hopes we'll give him more.</p>\n<p>But we offer him a bet : either we give him 7 paperclips, or we flip a coin. If the coin comes up heads, we give him 18 paperclips. If it comes up tails, we give him nothing.</p>\n<p>If Clippy doesn't take the bet, he gets 16 paperclips in total, so <em>u=4</em>. If Clippy takes the bet, he has 9 paperclips (<em>u=3</em>) with p=0.5 or 9+18=27 paperclips (<em>u=5.20</em>) with p=0.5. His utility expectancy is <em>u=4.10</em>, so he should take the bet.</p>\n<p>Now, regardless of whatever he took the first bet (called B1 starting from now), we offer him a second bet (B2) : this time, he has to pay us 9 paperclips to enter. Then, we roll a 10-sided die. If it gives 1 or 2, we give him a jackpot of 100 paperclips, else nothing. Clippy can be in three states when offered the second deal :</p>\n<ol>\n<li>He didn't take B1. Then, he has 16 clips. If he doesn't take B2, he'll stay with 16 clips, and <em>u=4</em>. If takes B2, he'll have 7 clips with p=0.8 and 107 clips with p=0.2, for an expected utility of <em>u=4.19</em>.</li>\n<li>He did take B1, and lost it. He has 9 clips. If he doesn't take B2, he'll stay with 9 clips, and <em>u=3</em>. If takes B2, he'll have 0 clips with p=0.8 and 100 clips with p=0.2, for an expected utility of <em>u=2</em>.</li>\n<li>He did take B1, and won it. He has 27 clips. If he doesn't take B2, he'll stay with 27 clips, and <em>u=5.20</em>. If takes B2, he'll have 18 clips with p=0.8 and 118 clips with p=0.2, for an expected utility of <em>u=5.57</em>.</li>\n</ol>\n<p>So, if Clippy didn't take the first bet or if he won it, he should take the second bet. If he did take the first bet and lost it, he can't afford to take the second bet, since he's risking a very bad outcome : no more paperclips, not even a single tiny one !</p>\n<h3 id=\"And_the_devil__time__comes_in___\">And the devil \"time\" comes in...<br></h3>\n<p>Now, let's make things a bit more complicated, and realistic. Before we were running things fully sequentially : first we resolved B1, and then we offered and resolved B2. But let's change a tiny bit B1. We don't flip the coin and give the clips to Clippy now. Clippy tells us if he takes B1 or not, but we'll wait one day before giving him the clips if he didn't take the bet, or before flipping the coin and then giving him the clips if he did take the bet.</p>\n<p>The utility function of Clippy doesn't involve time, and we'll consider it doesn't change if he gets the clips tomorrow instead of today. So for him, the new B1 is exactly like the old B1.</p>\n<p>But now, we offer him B2 <strong>after</strong> Clippy made his choice in B1 (taking the bet or not) but <strong>before</strong> flipping the coin for B1, if he did take the bet.</p>\n<p>Now, for Clippy, we only have two situations : he took B1 or he didn't. If he didn't take B1, we are in the same situation than before, with an expected utility of <em>u=4.19</em>.</p>\n<p>If he did take B1, we have to consider 4 possibilities :</p>\n<ol>\n<li>He loses the two bets. Then he ends up with no paperclip (9+0-9), and is very unhappy. He has <em>u=0</em> utilons. That'll arise with p=0.4.</li>\n<li>He wins B1 and loses B2. Then he ends up with 9+18-9 = 18 paperclips, so <em>u=4.24</em> with p=0.4.</li>\n<li>He loses B1 and wins B2. Then he ends up with 9-9+100 = 100 paperclips, so <em>u=10</em> with p = 0.1.</li>\n<li>He wins both bets. Then he gets 9+18-9+100 = 118 paperclips, so <em>u=10.86</em> with p=0.1.</li>\n</ol>\n<p>At the end, if he takes B2, he ends up with an expectancy of <em>u=3.78</em>.</p>\n<p>So, if Clippy takes B1, he then shouldn't take B2. Since he doesn't know if he won or lost B1, he can't afford the risk to take B2.</p>\n<p>But should he take B1 at first ? If, when offered to take B1, he knows he'll be offered to take B2 later on, then he should refuse B1 and take B2, for an utility of 4.19. If, when offered B1, he doesn't know about B2, then taking B1 seems the more rational choice. But once he took B1, <strong>until he knows</strong> if he won or not, he cannot afford to take B2.</p>\n<h3 id=\"The_Python_code\">The Python code</h3>\n<p>For people interested about those issues, <a href=\"http://kilobug.pilotsystems.net/lw/clippy_bets.py\">here is a simple Python script</a> I used to fine tune that numerical parameters of&nbsp; double-bet issue so my numbers lead to the problem I was pointing to. Feel free to play with it ;)</p>\n<h2 id=\"A_hunter_gatherer_tale\">A hunter-gatherer tale<br></h2>\n<p>If you didn't like my Clippy, despite him being cute, and purring of happiness when he sees paperclips, let's shift to another tale.</p>\n<p>Daneel is a young hunter-gatherer. He's smart, but his father committed a crime when he was still a baby, and was exiled from the tribe. Daneel doesn't know much about the crime - no one speaks about it, and he doesn't dare to bring the topic by himself. He has a low social status in the tribe because of that story. Nonetheless, he's attracted to Dors, the daughter of the chief. And he knows Dors likes him back, for she always smiles at him when she sees him, never makes fun of him, and gave him a nice knife after his coming-of-age ceremony.</p>\n<p>According to the laws of the tribe, Dors can chose her husband freely, and the husband will become the new chief. But Dors also have to chose a husband that is accepted by the rest of the tribe, if the tribe doesn't accept the leadership, they could revolt, or fail to obey. And that could lead to disaster for the whole tribe. Daneel knows he has to raise his status in the tribe if he wants Dors to be able to chose him.</p>\n<p>So Daneel wanders further and further in the forest. He wants to find something new to show the tribe his usefulness. That day, going a bit further than usual, he finds a place which is more humid than the forest the tribe usually wanders in. It has a new kind of trees, he never saw before. Lots of them. And they carry a yellow-red fruit which looks yummy. \"I could tell about that place to the others, and bring them a few fruits. But then, what if the fruit makes them sick ? They'll blame me, I'll lose all chances... they may even banish me. But I can do better. I'll eat one of the fruits myself. If tomorrow I'm not sick, then I'll bring fruits to the tribe, and show them where I found them. They'll praise me for it. And maybe Dors will then be able to take me more seriously... and if I get sick, well, everyone gets sick every now and then, just one fruit shouldn't kill me, it won't be a big deal\". So Daneel makes his utility calculation (I told you he was smart !), finds a positive outcome. So he takes the risk, he picks one fruit, and eats it. Sweet, a bit acid but not too much. Nice !</p>\n<p>Now, Daneel goes back to the tribe. On the way back, he got a rabbit, a few roots and plants for the shaman, an average day. But then, he sees the tribe gathered around the central totem. In the middle of the tribe, Dors with... no... not him... Eto ! Eto is the strongest lad of Daneel's age. He wants Dors too. And he's strong, and very skilled with the bow. The other hunters like him, he's a real man. And Eto's father died proudly, defending the tribe's stock of dried meat against hungry wolves two winters ago. But no ! Not that ! Eto is asking Dors to marry him. In public. Dors can refuse, but if she does with no reason, she'll alienate half of the tribe against her, she can't afford it. Eto is way too popular.</p>\n<p>\"Hey, Daneel ! You want Dors ? Challenge Eto ! He's strong and good with the bow, but in unarmed combat, you can defeat him, I know it.\", whispers Hari, one of the few friends of Daneel.</p>\n<p>Daneel starts thinking faster he never did. \"Ok, I can challenge Eto in unarmed combat. If I lose, I'll be wounded, Eto won't be nice with me. But he won't kill or cripple me, that would make half of the tribe to hate him. If I lose, it'll confirm I'm physical weak, but I'll also win prestige for daring to defy the strong Eto, so it shouldn't change much. And if I win, Dors will be able to refuse Eto, since he lost a fight against someone weaker than him, that's a huge win. So I should take that gamble... but then, there is the fruit. If the fruit gets me sick, in addition of my wounds from Eto, I may die. Even if I win ! And if I lose, get beaten, and then gets sick... they'll probably let me die. They won't take care of a fatherless lad who lose a fight and then gets sick. Too weak to be worth it. So... should I take the gamble ? If Eto waited just one day more... Or <strong>if only I knew if I'll get sick or not...</strong>\"</p>\n<h2 id=\"The_key___information_loss\">The key : information loss</h2>\n<p>Until Clippy knows ? If Daneel knew ? That's the key of risk aversion, and why a perfect utility maximizer, if he has a concave utility function in at least some aspects, should still have some risk aversion. Because risk comes with information loss. That's the difference between the timeless double-bet and the one with one day of delay for Clippy. Or the problem Daneel got stuck into.</p>\n<p>If you take a bet, until you know the outcome of your bet, you'll have less information about the state of the world, and especially about the state that directly concerns you, than if you chose the safe situation (a situation with a lower deviation). Having less information means you're less free to optimize.</p>\n<p>Even a perfect utility maximizer can't know what bets he'll be offered, and what decisions he'll have to take, unless he's omniscient (and then he wouldn't take bets or risks, but he would know the future - probability only reflects lack of information). So he has to consider the loss of information of taking a bet.</p>\n<p>In real life, the most common case of it is the non-linearity of bad effects : you can lose 0.5L of blood without too much side-effects (drink lots a water, sleep well, and next day you're ok, that's what happens when you go give your blood), but if you lose 2L, you'll likely die. Or if you lose some money, you'll be in trouble, but if you lose the same amount again, you may end up being kicked from you house since you can't pay the rent - and that'll be more than twice as bad as the initial lost.</p>\n<p>So when you took a bet, risking to get a bad effect, you can't afford to take another bet (even with, in absolute, a higher gain expectancy), until you know if you won or lose the first bet - because losing them both means death, or being kicked from your house, or ultimate pain of not having any paperclip.</p>\n<p>Taking a bet always as a cost : it costs you part of your ability to predict, and therefore to optimize.</p>\n<h2 id=\"A_possible_solution\">A possible solution<br></h2>\n<p>A possible solution to that problem would be to consider all possible decisions you may to take while in the time period when you don't know if you lost or won your first bet, ponder them with the probability of being offered those decisions, and their possible outcomes if you take the first bet and you don't. But how do you compute \"their possible outcomes\" ? That needs to consider all the possible bets you could be offered during the time required for the resolution of your second bet, and their possible outcomes. So you need to... <em>stack overflow: maximal recursion depth exceeded.</em></p>\n<p>Since taking a bet will affect your ability to evaluate possible outcomes in the future, you've a \"strange loop to the meta-level\", an infinite recursion. Your decision algorithm has to consider the impact the decision will have on the future instances of your decision algorithm.</p>\n<p>I don't know if there is a mathematical solution to that infinite recursion that manages to make it converge (like you can in some cases). But the problem looks really hard, and may not be computable.</p>\n<p>Just factoring an average \"risk aversion\" that penalizes outcome which involve a risk (and the more you've to wait to know if you won or lose, the higher the penalty) sounds more a way to fix that problem than a bias.</p>", "sections": [{"title": "Abstract", "anchor": "Abstract", "level": 1}, {"title": "Where does risk aversion comes from ?", "anchor": "Where_does_risk_aversion_comes_from__", "level": 1}, {"title": "The double-bet of Clippy", "anchor": "The_double_bet_of_Clippy", "level": 1}, {"title": "Timeless double-bet", "anchor": "Timeless_double_bet", "level": 2}, {"title": "And the devil \"time\" comes in...", "anchor": "And_the_devil__time__comes_in___", "level": 2}, {"title": "The Python code", "anchor": "The_Python_code", "level": 2}, {"title": "A hunter-gatherer tale", "anchor": "A_hunter_gatherer_tale", "level": 1}, {"title": "The key : information loss", "anchor": "The_key___information_loss", "level": 1}, {"title": "A possible solution", "anchor": "A_possible_solution", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "65 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aFzLYnoLN65xWw4Xj", "EZ8GniEPSechjDYP9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T21:03:55.180Z", "modifiedAt": null, "url": null, "title": "Existential risk for non-consequentialists", "slug": "existential-risk-for-non-consequentialists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:03.724Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "vallinder", "createdAt": "2011-03-14T11:52:07.333Z", "isAdmin": false, "displayName": "vallinder"}, "userId": "onAYNLtS8wFHFH6k5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mkaovnQEgWW5Yrh9N/existential-risk-for-non-consequentialists", "pageUrlRelative": "/posts/mkaovnQEgWW5Yrh9N/existential-risk-for-non-consequentialists", "linkUrl": "https://www.lesswrong.com/posts/mkaovnQEgWW5Yrh9N/existential-risk-for-non-consequentialists", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Existential%20risk%20for%20non-consequentialists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExistential%20risk%20for%20non-consequentialists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkaovnQEgWW5Yrh9N%2Fexistential-risk-for-non-consequentialists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Existential%20risk%20for%20non-consequentialists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkaovnQEgWW5Yrh9N%2Fexistential-risk-for-non-consequentialists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkaovnQEgWW5Yrh9N%2Fexistential-risk-for-non-consequentialists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>Many people on Less Wrong believe reducing existential risk is one of the most important causes. Most arguments to this effect point out the horrible consequences: everyone now living would die (or face something even worse). The situation becomes even worse if we also consider future generations. Such an argument, as spelt out in Nick Bostrom's <a href=\"http://www.existential-risk.org/concept.pdf\">latest paper on the topic</a>, for instance, should strike many consequentialists as persuading. But of course, not everyone's a consequentialist, and on other approaches&nbsp;it's far from obvious that existential risk should come out on top. Might it be worth to spend some more time investigating arguments for existential risk reduction that don't presuppose consequentialism?&nbsp;Of course, \"non-consequentialism\" is a very diverse category, and I'd be surprised if there were a single argument that covered all its members.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mkaovnQEgWW5Yrh9N", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 8.410986773116738e-07, "legacy": true, "legacyId": "12549", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T21:06:12.040Z", "modifiedAt": null, "url": null, "title": "\"The Conditional Fallacy in Contemporary Philosophy\"", "slug": "the-conditional-fallacy-in-contemporary-philosophy", "viewCount": null, "lastCommentedAt": "2018-02-02T18:06:26.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/35P62KXiqR2DfG8e7/the-conditional-fallacy-in-contemporary-philosophy", "pageUrlRelative": "/posts/35P62KXiqR2DfG8e7/the-conditional-fallacy-in-contemporary-philosophy", "linkUrl": "https://www.lesswrong.com/posts/35P62KXiqR2DfG8e7/the-conditional-fallacy-in-contemporary-philosophy", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22The%20Conditional%20Fallacy%20in%20Contemporary%20Philosophy%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22The%20Conditional%20Fallacy%20in%20Contemporary%20Philosophy%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F35P62KXiqR2DfG8e7%2Fthe-conditional-fallacy-in-contemporary-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22The%20Conditional%20Fallacy%20in%20Contemporary%20Philosophy%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F35P62KXiqR2DfG8e7%2Fthe-conditional-fallacy-in-contemporary-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F35P62KXiqR2DfG8e7%2Fthe-conditional-fallacy-in-contemporary-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1127, "htmlBody": "<blockquote>\n<p>Split from \"<a href=\"/lw/9oa/against_utilitarianism_sobels_attack_on_judging\">Against Utilitarianism: Sobel's attack on judging lives' goodness</a>\" for length.</p>\n</blockquote>\n<p>Robert K. Shope, back in his 1978 paper <a href=\"http://www.gwern.net/docs/1978-shope.pdf\">\"The Conditional Fallacy in Contemporary Philosophy\"</a>, identified a kind of argument that us transhumanists will find painfully familiar: you propose idea X, the other person says bad thing Y is a possible counterexample if X were true, so X can't be true - ignoring that Y <em>may not</em> happen, and X can just be modified to deal with Y if it's really that important.</p>\n<p>(\"If we augment our brains, we may forget how to love!\" \"So don't remove love when you're augmenting, sheesh.\" \"But it might not be possible!\" \"But wouldn't you agree that augmentation without loss of love would be better than the status quo?\")</p>\n<p>Excerpts follow:</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"i.-one-version-of-the-conditional-fallacy\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> I. ONE VERSION OF THE CONDITIONAL FALLACY</a></h1>\n<blockquote>\n<p>A mistake one makes in analyzing or defining a statement <em>p</em> by presenting its truth as dependent, in at least some specified situations, upon the truth (falsity) of a subjunctive conditional <em>O</em><sup>3</sup> of the form: &lsquo;If state of affairs <em>a</em> were to occur, then state of affairs <em>b</em> would occur&rsquo;,<sup>4</sup> when</p>\n<blockquote>\n<p>(Version 1) one has failed to notice that the truth value of <em>p</em> sometimes depends on whether <em>a</em> actually occurs and does not depend merely upon the truth value of the <a href=\"http://johnmacfarlane.net/135/glossary.html\">analysans</a> or <a href=\"http://en.wiktionary.org/wiki/definiens\">definiens</a>; moreover, one has failed to notice this because one has overlooked the fact that in some of the specified situations:</p>\n<ol style=\"list-style-type: lower-roman\">\n<li>conditional <em>O</em> is true (false),</li>\n<li>the analysans or definiens is true,</li>\n<li>state of affairs <em>a</em> does not occur, and</li>\n<li>if <em>a</em> were to occur then the occurrence of <em>a</em> or the occurrence of <em>b</em> or their combination (the occurrence of <em>a</em> or the absence of <em>b</em> or their combination) would be at least part of the cause of something that would make <em>p</em> true, although is actually false.</li>\n</ol></blockquote>\n</blockquote>\n<h1 id=\"ii.-a-second-version-of-the-conditional-fallacy\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> II. A SECOND VERSION OF THE CONDITIONAL FALLACY</a></h1>\n<blockquote>\n<p>&hellip;An illustration of a second version of this mistake appears in the following definition, offered by Keith Lehrer: &ldquo;But what does it mean to say that reasons give a man knowledge? It means that if he were asked, &lsquo;How do you know that?&rsquo; and he were to give those reasons, his answer would be correct. Those reasons explain how he knows.&rdquo;<sup>5</sup> But suppose that Mr.&nbsp;Silent, the only friend of Mr.&nbsp;Faker, knows that they will remain friends in the immediate future. Yet Mr.&nbsp;Faker pretends to all others that he himself is a misanthrope, and the continuation of the friendship depends on Mr.&nbsp;Silent&rsquo;s keeping the secret. Mr.&nbsp;Nosey, who suspects that the former two are friends, asks Mr.&nbsp;Silent in front of Mr.&nbsp;Faker, &ldquo;How do you know that you will remain friends with Mr.&nbsp;Faker in the immediate future?&rdquo; Mr.&nbsp;Silent does know this, but would not if he were to state his actual reasons.</p>\n<p>In this example, giving an answer would cause the end of the friendship and make the answer incorrect.</p>\n<p>&hellip;Thus, on the present reading, Lehrer&rsquo;s definition illustrates a second version of the conditional fallacy:</p>\n<blockquote>\n<p>(Version 2) one has overlooked the fact that, in some of the specified situations, statement <em>p</em> is actually true, but, if <em>a</em> were to occur, then it would be at least a partial cause of something that would make <em>b</em> fail to occur (make <em>b</em> occur).</p>\n</blockquote>\n</blockquote>\n<h1 id=\"iv.-other-fallacies-concerning-conditionals\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> IV. OTHER FALLACIES CONCERNING CONDITIONALS</a></h1>\n<blockquote>\n<p>&hellip;One can commit the second version of the conditional fallacy without committing the <a href=\"http://en.wikipedia.org/wiki/Ceteris_paribus#Philosophy\"><em>ceteris paribus</em> fallacy</a>. A type of example in which this happens is when one commits the conditional fallacy together with what may be called the <em>fallacy of contrary conditionals</em>. In the latter fallacy, one overlooks the fact that the conditional that one&rsquo;s account presents as true (false) is simply false (true) and cannot even be said to be true (false) other things equal. For instance, some unsophisticated phenomenalist might try to analyze the statement that Dr.&nbsp;Crippen murdered his wife when and where he did in terms of conditionals about the multitude of appearances that would have been manifested to hypothetical viewers at many different spots in the room, overlooking the fact that Crippen would not have committed the crime in front of a witness (other than the victim).</p>\n</blockquote>\n<h1 id=\"vi.-the-conditional-fallacy-in-ethics\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> VI. THE CONDITIONAL FALLACY IN ETHICS</a></h1>\n<blockquote>\n<p>Treatments of prima facie obligations sometimes commit the first version of the conditional fallacy. For example, philosophers sometimes explain the statement that person <em>S</em> has a prima facie moral obligation to do action <em>A</em> as follows: Doing <em>A</em> would be what <em>S</em> morally ought to do, all things considered (or would be the morally right thing for <em>S</em> to do), if <em>S</em> were to have no moral obligations to perform any alternative action. However, Socrates takes himself to have, among his various moral obligations, a moral obligation to teach Alcibiades during the symposium, as well as a moral obligation not to harm him physically during the proceedings, and we wish to speak of these as prima facie obligations (even if they are also part of what Socrates morally ought to do, all things considered). But if the obligation not to harm Alcibiades physically were missing, it would have to be absent for a reason, and this might very well be a reason that would remove the other obligation as well, e.g., Alcibiades&rsquo; total absence from the occasion or his insanely attempting to assassinate Socrates.<sup>22</sup></p>\n<p>Another example of the conditional fallacy in ethics appears in the course of John Rawls&rsquo;s attempt to find a constant sense for the term &lsquo;good&rsquo;. Rawls defines a person&rsquo;s real good by reference to what is for that person the most rational plan of life given reasonably favorable circumstances; he lists as a necessary condition for the most rational plan of life that it would be chosen by the person if that person were to have full deliberative rationality.<sup>23</sup> But in defining full deliberative rationality, Rawls requires &ldquo;that there are no errors of calculation or reasoning, and that the facts are correctly assessed . . . also that the agent is under no misconceptions as to what he really wants&rdquo; (417). Satisfaction of the antecedent in Rawls&rsquo;s conditional entails that, in the hypothetical situation, the person would <em>have</em> the competence involved in complete deliberative rationality and know that he has it, and would not, for example, be out of touch with his desires in a way that can be overcome only through psychotherapy. Since it is irrational to plan to obtain something when one knows that one already has it, the conditional incorrectly leads us to say that it is not part of a rational plan of life (and thus not part of anyone&rsquo;s real good) to come closer to deliberative rationality by, for example, seeking psychiatric help.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "35P62KXiqR2DfG8e7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 2, "extendedScore": null, "score": 8.410995595113888e-07, "legacy": true, "legacyId": "12550", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>Split from \"<a href=\"/lw/9oa/against_utilitarianism_sobels_attack_on_judging\">Against Utilitarianism: Sobel's attack on judging lives' goodness</a>\" for length.</p>\n</blockquote>\n<p>Robert K. Shope, back in his 1978 paper <a href=\"http://www.gwern.net/docs/1978-shope.pdf\">\"The Conditional Fallacy in Contemporary Philosophy\"</a>, identified a kind of argument that us transhumanists will find painfully familiar: you propose idea X, the other person says bad thing Y is a possible counterexample if X were true, so X can't be true - ignoring that Y <em>may not</em> happen, and X can just be modified to deal with Y if it's really that important.</p>\n<p>(\"If we augment our brains, we may forget how to love!\" \"So don't remove love when you're augmenting, sheesh.\" \"But it might not be possible!\" \"But wouldn't you agree that augmentation without loss of love would be better than the status quo?\")</p>\n<p>Excerpts follow:</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"1_I__ONE_VERSION_OF_THE_CONDITIONAL_FALLACY\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> I. ONE VERSION OF THE CONDITIONAL FALLACY</a></h1>\n<blockquote>\n<p>A mistake one makes in analyzing or defining a statement <em>p</em> by presenting its truth as dependent, in at least some specified situations, upon the truth (falsity) of a subjunctive conditional <em>O</em><sup>3</sup> of the form: \u2018If state of affairs <em>a</em> were to occur, then state of affairs <em>b</em> would occur\u2019,<sup>4</sup> when</p>\n<blockquote>\n<p>(Version 1) one has failed to notice that the truth value of <em>p</em> sometimes depends on whether <em>a</em> actually occurs and does not depend merely upon the truth value of the <a href=\"http://johnmacfarlane.net/135/glossary.html\">analysans</a> or <a href=\"http://en.wiktionary.org/wiki/definiens\">definiens</a>; moreover, one has failed to notice this because one has overlooked the fact that in some of the specified situations:</p>\n<ol style=\"list-style-type: lower-roman\">\n<li>conditional <em>O</em> is true (false),</li>\n<li>the analysans or definiens is true,</li>\n<li>state of affairs <em>a</em> does not occur, and</li>\n<li>if <em>a</em> were to occur then the occurrence of <em>a</em> or the occurrence of <em>b</em> or their combination (the occurrence of <em>a</em> or the absence of <em>b</em> or their combination) would be at least part of the cause of something that would make <em>p</em> true, although is actually false.</li>\n</ol></blockquote>\n</blockquote>\n<h1 id=\"2_II__A_SECOND_VERSION_OF_THE_CONDITIONAL_FALLACY\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> II. A SECOND VERSION OF THE CONDITIONAL FALLACY</a></h1>\n<blockquote>\n<p>\u2026An illustration of a second version of this mistake appears in the following definition, offered by Keith Lehrer: \u201cBut what does it mean to say that reasons give a man knowledge? It means that if he were asked, \u2018How do you know that?\u2019 and he were to give those reasons, his answer would be correct. Those reasons explain how he knows.\u201d<sup>5</sup> But suppose that Mr.&nbsp;Silent, the only friend of Mr.&nbsp;Faker, knows that they will remain friends in the immediate future. Yet Mr.&nbsp;Faker pretends to all others that he himself is a misanthrope, and the continuation of the friendship depends on Mr.&nbsp;Silent\u2019s keeping the secret. Mr.&nbsp;Nosey, who suspects that the former two are friends, asks Mr.&nbsp;Silent in front of Mr.&nbsp;Faker, \u201cHow do you know that you will remain friends with Mr.&nbsp;Faker in the immediate future?\u201d Mr.&nbsp;Silent does know this, but would not if he were to state his actual reasons.</p>\n<p>In this example, giving an answer would cause the end of the friendship and make the answer incorrect.</p>\n<p>\u2026Thus, on the present reading, Lehrer\u2019s definition illustrates a second version of the conditional fallacy:</p>\n<blockquote>\n<p>(Version 2) one has overlooked the fact that, in some of the specified situations, statement <em>p</em> is actually true, but, if <em>a</em> were to occur, then it would be at least a partial cause of something that would make <em>b</em> fail to occur (make <em>b</em> occur).</p>\n</blockquote>\n</blockquote>\n<h1 id=\"3_IV__OTHER_FALLACIES_CONCERNING_CONDITIONALS\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> IV. OTHER FALLACIES CONCERNING CONDITIONALS</a></h1>\n<blockquote>\n<p>\u2026One can commit the second version of the conditional fallacy without committing the <a href=\"http://en.wikipedia.org/wiki/Ceteris_paribus#Philosophy\"><em>ceteris paribus</em> fallacy</a>. A type of example in which this happens is when one commits the conditional fallacy together with what may be called the <em>fallacy of contrary conditionals</em>. In the latter fallacy, one overlooks the fact that the conditional that one\u2019s account presents as true (false) is simply false (true) and cannot even be said to be true (false) other things equal. For instance, some unsophisticated phenomenalist might try to analyze the statement that Dr.&nbsp;Crippen murdered his wife when and where he did in terms of conditionals about the multitude of appearances that would have been manifested to hypothetical viewers at many different spots in the room, overlooking the fact that Crippen would not have committed the crime in front of a witness (other than the victim).</p>\n</blockquote>\n<h1 id=\"4_VI__THE_CONDITIONAL_FALLACY_IN_ETHICS\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> VI. THE CONDITIONAL FALLACY IN ETHICS</a></h1>\n<blockquote>\n<p>Treatments of prima facie obligations sometimes commit the first version of the conditional fallacy. For example, philosophers sometimes explain the statement that person <em>S</em> has a prima facie moral obligation to do action <em>A</em> as follows: Doing <em>A</em> would be what <em>S</em> morally ought to do, all things considered (or would be the morally right thing for <em>S</em> to do), if <em>S</em> were to have no moral obligations to perform any alternative action. However, Socrates takes himself to have, among his various moral obligations, a moral obligation to teach Alcibiades during the symposium, as well as a moral obligation not to harm him physically during the proceedings, and we wish to speak of these as prima facie obligations (even if they are also part of what Socrates morally ought to do, all things considered). But if the obligation not to harm Alcibiades physically were missing, it would have to be absent for a reason, and this might very well be a reason that would remove the other obligation as well, e.g., Alcibiades\u2019 total absence from the occasion or his insanely attempting to assassinate Socrates.<sup>22</sup></p>\n<p>Another example of the conditional fallacy in ethics appears in the course of John Rawls\u2019s attempt to find a constant sense for the term \u2018good\u2019. Rawls defines a person\u2019s real good by reference to what is for that person the most rational plan of life given reasonably favorable circumstances; he lists as a necessary condition for the most rational plan of life that it would be chosen by the person if that person were to have full deliberative rationality.<sup>23</sup> But in defining full deliberative rationality, Rawls requires \u201cthat there are no errors of calculation or reasoning, and that the facts are correctly assessed . . . also that the agent is under no misconceptions as to what he really wants\u201d (417). Satisfaction of the antecedent in Rawls\u2019s conditional entails that, in the hypothetical situation, the person would <em>have</em> the competence involved in complete deliberative rationality and know that he has it, and would not, for example, be out of touch with his desires in a way that can be overcome only through psychotherapy. Since it is irrational to plan to obtain something when one knows that one already has it, the conditional incorrectly leads us to say that it is not part of a rational plan of life (and thus not part of anyone\u2019s real good) to come closer to deliberative rationality by, for example, seeking psychiatric help.</p>\n</blockquote>", "sections": [{"title": "1 I. ONE VERSION OF THE CONDITIONAL FALLACY", "anchor": "1_I__ONE_VERSION_OF_THE_CONDITIONAL_FALLACY", "level": 1}, {"title": "2 II. A SECOND VERSION OF THE CONDITIONAL FALLACY", "anchor": "2_II__A_SECOND_VERSION_OF_THE_CONDITIONAL_FALLACY", "level": 1}, {"title": "3 IV. OTHER FALLACIES CONCERNING CONDITIONALS", "anchor": "3_IV__OTHER_FALLACIES_CONCERNING_CONDITIONALS", "level": 1}, {"title": "4 VI. THE CONDITIONAL FALLACY IN ETHICS", "anchor": "4_VI__THE_CONDITIONAL_FALLACY_IN_ETHICS", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ovLpQqpRLtpXPdKKP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T21:11:02.625Z", "modifiedAt": null, "url": null, "title": "Hugo Awards - HP:MoR (part 2)", "slug": "hugo-awards-hp-mor-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:52.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/scshGmDzXNswQxqjv/hugo-awards-hp-mor-part-2", "pageUrlRelative": "/posts/scshGmDzXNswQxqjv/hugo-awards-hp-mor-part-2", "linkUrl": "https://www.lesswrong.com/posts/scshGmDzXNswQxqjv/hugo-awards-hp-mor-part-2", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hugo%20Awards%20-%20HP%3AMoR%20(part%202)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHugo%20Awards%20-%20HP%3AMoR%20(part%202)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FscshGmDzXNswQxqjv%2Fhugo-awards-hp-mor-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hugo%20Awards%20-%20HP%3AMoR%20(part%202)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FscshGmDzXNswQxqjv%2Fhugo-awards-hp-mor-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FscshGmDzXNswQxqjv%2Fhugo-awards-hp-mor-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 422, "htmlBody": "<p>Summary: please <a href=\"https://chicon.org/hugo/nominate.php\">nominate</a>&nbsp;<a href=\"http://hpmor.libsyn.com/\">HPMoR:Podcast</a>&nbsp;for consideration to the brand-new <a href=\"http://www.thehugoawards.org/2012/01/2012-hugo-award-nominations-open/\">FanCast</a> category.</p>\n<p>&nbsp;</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Hugo_Award\">Hugo Awards</a>, as many are probably aware, are awards given to outstanding Science Fiction/Fantasy works, as voted upon by the reading public. They work by letting anyone who attends the annual World Science Fiction Convention nominate &amp; vote on their favorite works. The Hugos and the Nebulas are generally considered the two major awards of the SF/F genre.<br /><br />Last year I <a href=\"/lw/3wg/hugo_awards_hpmor/\">suggested nominating</a> HPMoR, which was complicated by the fact that it's not complete, is fanfic, and isn't \"published\" in the traditional sense.<br /><br />However this year there is a Special Category called <strong>Best FanCast</strong>, defined as &ldquo;Any non-professional audio- or video-casting with at least four (4) episodes that had at least one (1) episode released in 2011.&rdquo; (There is an amendment to the WSFS Constitution pending ratification at Chicon 7 that would make Best Fancast a permanent category.)<br /><br />This seems like a perfect fit for <a href=\"http://itunes.apple.com/us/podcast/harry-potter-methods-rationality/id431784580?ign-mpt=uo%3D4\">HP:MoR The Podcast</a>.</p>\n<p>If you are already participating in this year's Hugos or participated in last year's Hugos, you are eligible to <a href=\"https://chicon.org/hugo/nominate.php\">nominate</a> works. Please consider nominating it. A nomination could help bring knowledge of MoR to many more people, regardless of a win. And it's not terribly hard to get nominated, especially in these smaller catagories. Last year&nbsp;<a href=\"http://www.youtube.com/watch?v=e1IxOS4VzKM&amp;oref=http%3A%2F%2Fwww.youtube.com%2Fresults%3Fsearch_query%3Dfuck%2Bme%2Bray%2Bbrad%26oq%3Dfuck%2Bme%2Bray%2Bbrad%26aq%3Df%26aqi%3D%26aql%3D%26gs_sm%3De%26gs_upl%3D301l3109l0l3333l20l17l0l15l0l1l247l468l2-2l2l0\">Fuck Me, Ray Bradbury</a>&nbsp;was nominated to the final ballot in \"Best Dramatic Presentation, Short Form\" with only 50 nominations.<br /><br />If you aren't participating yet, well, there may still be a chance to sign up, depending on when you're reading this. Turns out the deadline for buying into the convention is 11:59pm 1/31/12. <a href=\"https://chicon.org/online-reg.php\">You can get</a> a full ticket (admission to the con plus the voting packet) for $195, or simply buy a \"supporting membership\" for $50. The supporting membership only lets you nominate and vote - not attend the con - but it does get you a \"voting packet\" which is the full text of all the nominated novels, novellas, novelettes, and short stories (five nominees in each catagory) for free in multiple formats (.epub, MOBI, txt, and Word, I believe). If you buy a lot of SF anyway it's a decent deal, and you get to be a small part of SF history in the process. :) But it isn't an&nbsp;efficient&nbsp;use of money if you're simply nominating HPMoR, so this is really more of a request to those who are already nominating/voting anyway.<br /><br />Links Recap:<br /><a href=\"https://chicon.org/hugo/nominate.php\">To Nominate</a><br /><a href=\"https://chicon.org/online-reg.php\">To Register</a><br /><a href=\"http://itunes.apple.com/us/podcast/harry-potter-methods-rationality/id431784580?ign-mpt=uo%3D4\">Podcast in iTunes</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "scshGmDzXNswQxqjv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 36, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "12551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y9nxFGLdrwu572f8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-31T22:31:52.713Z", "modifiedAt": null, "url": null, "title": "Another Real World Example of Cognitive Bias", "slug": "another-real-world-example-of-cognitive-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:52.346Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J9KXSpsmSXCbwTc8k/another-real-world-example-of-cognitive-bias", "pageUrlRelative": "/posts/J9KXSpsmSXCbwTc8k/another-real-world-example-of-cognitive-bias", "linkUrl": "https://www.lesswrong.com/posts/J9KXSpsmSXCbwTc8k/another-real-world-example-of-cognitive-bias", "postedAtFormatted": "Tuesday, January 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20Real%20World%20Example%20of%20Cognitive%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20Real%20World%20Example%20of%20Cognitive%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9KXSpsmSXCbwTc8k%2Fanother-real-world-example-of-cognitive-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20Real%20World%20Example%20of%20Cognitive%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9KXSpsmSXCbwTc8k%2Fanother-real-world-example-of-cognitive-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9KXSpsmSXCbwTc8k%2Fanother-real-world-example-of-cognitive-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p>An example of real world problems from cognitive biases:</p>\n<p><a href=\"http://www.economist.com/node/21543121\">Forensic science - Ignorance is bliss</a></p>\n<blockquote>According to Dr Rudin, the attitude that cognitive bias can somehow be willed away, by education, training or good intentions, is still pervasive.</blockquote>\n<p>Hat-tip to <a href=\"http://www.schneier.com/blog/archives/2012/01/biases_in_foren.html\">Bruce Schneier</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J9KXSpsmSXCbwTc8k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "12552", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}